<!DOCTYPE html>
<html>
<head>
  <title>How to Actually Change Your Mind by Big Yud | Boris Smus</title>

  <meta charset='utf-8' />
  <meta name='viewport' content='width=device-width, user-scalable=no, minimum-scale=1.0, maximum-scale=1.0' />

  <meta name="description" content="Technically the second book in a giant epic entitled "Rationality: From AI to Zombies". The author is a well known rationalist and active member/founder of lesswrong.org, Center for Applied Rationality (CFAR), etc.    Essentially, this is a series of essays written and published as "The sequences", around 2009. They are loosely related and cover a wide array of topics, many of them highlighting irrational modes of thought. Much of the work focuses on biases central to behavioral economics, focusing on Kahneman-style results. But the author goes beyond that, and also introduces a lot of opinion for how a rationalist should behave. At the same time, there is a tendency to be incredibly nerdy, which is alternatingly endearing and borderline autistic. I found myself asking the question: if one becomes a purely rational agent, isn't a computer strictly better? On the path to rationality, what aspects of humanity is worth preserving?    Here's some new stuff I learned. A fair amount of the book covers behavioral economics concepts that I read about already in TF&amp;S.    Litanies    Aumann’s Agreement Theorem suggests that no two rationalists can agree to disagree, given that they have the same information.    Litany of  Gendlin :        What is true is already so.   Owning up to it doesn't make it worse.   Not being open about it doesn't make it go away.   And because it's true, it is what is there to be interacted with.   Anything untrue isn't there to be lived.   People can stand what is true,   for they are already enduring it.      Litany of  Tarski :        If the box contains a diamond,   I desire to believe that the box contains a diamond;   If the box does not contain a diamond,   I desire to believe that the box does not contain a diamond;   Let me not become attached to beliefs I may not want.      Against black and white thinking    Yudkowsky is especially effective in his attacks on binary thinking. For example, on partisanship:        Politics is an extension of war by other means. Arguments are soldiers. Once   you know which side you're on, you must support all arguments of that side,   and attack all arguments that appear to favor the enemy side, otherwise it's   like stabbing your soldiers in the back - providing aid and comfort to the   enemy. People who would be level-headed about evenhandedly weighing all sides   of an issue in their professional life as scientists can suddenly turn into   slogan-chanting zombies when there's a Blue or Green position on an issue.      On the tendency and fallacy in binary thought:        There is a natural tendency to treat discussion as a form of combat, an   extension of war, a sport; and in sports you only need to keep track of how   many points have been scored by each team. There are only two sides, and   every point scored against one side is a point in favor of the other.   Everyone in the audience keeps a mental running count of how many points   each speaker scores against the other. At the end of the debate, the speaker   who has scored more points is, obviously, the winner; so everything that   speaker says must be true, and everything the loser says must be wrong.      The horns effect - all negative qualities correlate:        Stalin also believed that 2 + 2 = 4. Yet if you defend any statement made by   Stalin, even “2 + 2 = 4,” people will see only that you are agreeing with   stalin and you must be on his side.      And a very nice summary of a better way of thinking:        Not all arguments reduce to mere up or down. Lady Rationality carries a     notebook, wherein she writes down all the facts that aren’t on anyone’s side.      Real belief vs. belief in belief:        Roger Zelazny once distinguished between “wanting to be an author” versus     “wanting to write.” Mark Twain said: “A classic is something that everyone     wants to have read and no one wants to read.” Criticizing yourself from a     sense of duty leaves you wanting to have investigated, so that you’ll be     able to say afterward that your faith is not blind. This is not the same as     wanting to investigate.      Assume your interlocutor is good    "To argue against an idea honestly, you should argue against the best arguments of the strongest advocates". This, and the closely related concept of the principle of charity aka Steelmanning (the opposite of strawmanning), which I heard from a Sam Harris interview, sent me on a long reading tangent of arguments  for  and  against . Insightful tidbit from that last link:        First, seek to understand the actual viewpoints people you disagree with are   actually advocating.  Second, seek out intelligent and well-informed   advocates of viewpoints you disagree with. You don’t have to make up what   your opponents believe! As it happens, you have many smart opponents!   Third, whenever possible, try to switch conversations from a debate focus to   a collaborative truth-seeking focus.      Back to Big Yud. Some wisdom on focusing on the argument, not on the person:        Someone once said "Not all conservatives are stupid, but most stupid people     are conservatives". If you cannot place yourself in a state of mind where     this statement, true or false, seems completely irrelevant as a critique of     conservatism, you are not ready to think rationally about politics.      A variation on the  reasonable person principle  (harkens back to my time at CMU).        To understand why people act the way they do, we must first realize that     everyone sees themselves as behaving normally. Don’t ask what strange,     mutant disposition they were born with, which directly corresponds to their     surface behavior. Rather, ask what situations people see themselves as being     in. [...] Realistically, most people don’t construct their life stories with     themselves as the villains.      Deciding which side to argue    Great distinction between rationality and rationalization. Very related to Haidt's position that beliefs are intuitive but their defence is rational. But Haidt makes no distinction like this. Would love to hear his thoughts on it.        Rationality is not for winning debates, it is for deciding which side to     join. If you’ve already decided which side to argue for, the work of     rationality is done within you, whether well or poorly. But how can you,     yourself, decide which side to argue?      Eliezer suggests enumerating the evidence: "Lady Rationality carries a notebook, wherein she writes down all the facts that aren’t on anyone’s side". Here's how to construct an honest ultrarational argument for a particular political candidate:      Gather all evidence about the different candidates   Make a checklist which you will use to decide which candidate is best   Process the checklist   Go to the winning candidate   Offer to become their campaign manager   Use the checklist as the campaign literature      The future is hard to predict    Herd instinct in venture capitalism:        The majority of venture capitalists at any given time are all chasing the     same Revolutionary Innovation, and it’s the Revolutionary Innovation that     IPO’d six months ago. This is an especially crushing observation in venture     capital, because there’s a direct economic motive to not follow the herd.      And what to do about it. DFJ (a VC) has a rule  favoring a passionate minority  to outweigh a negative majority.  This also reminds me of the  Tenth Man Rule .        Only two partners need to agree in order to fund any startup up to $1.5     million. And if all the partners agree that something sounds like a good     idea, they won’t do it.      Movies and books have a huge effect on the human psyche. This will probably compound with more immersive storytelling mediums:        So far as I can tell, few movie viewers act as if they have directly     observed Earth’s future. [...] But those who commit the fallacy seem to act     as if they had seen the movie events occurring on some other planet; not     Earth, but somewhere similar to Earth.      Predicting numbers is especially difficult:        I observe that many futuristic predictions are, likewise, best considered as     attitude expressions. Take the question, “How long will it be until we have     human-level AI?” The responses I’ve seen to this are all over the map.      Avoid having THE Great Idea and get granular and specific    Avoiding partisanship by focusing on the minimum viable argument, reminds me of Sunstein's  Judicial minimalism :        But try to resist getting in those good, solid digs if you can possibly     avoid it. If your topic legitimately relates to attempts to ban evolution in     school curricula, then go ahead and talk about it—but don’t blame it     explicitly on the whole Republican Party; some of your readers may be     Republicans.      Avoid overly large uhh Thingies, and chop them up.        Cut up your Great Thingy into smaller independent ideas and treat them as     independent. For instance, a marxist would cut up Marx's Great Thingy into     theories of 1) value of labor 2) political relations between classes 3)     wages 4) the ultimate political state of mankind.      Other interesting stuff    Taber and Lodge's  "Motivated skepticism in the evaluation of political beliefs"  describes six predictions which are very Haidt-y. It's a list of political thinkos that are driven by behavioral economic biases.    Beliefs don't need to be completely bullet proof. But this contradicts science, where a single counter example can topple a theory.        A probabilistic model can take a hit or two, and still survive, so long as     the hits don’t keep on coming in. Yet it is widely believed, especially in     the court of public opinion, that a true theory can have no failures and a     false theory no successes.      On the uselessness of "Deep Wisdom":        Surely the wisest of all human beings are the New Age gurus who say,     “Everything is connected to everything else.” If you ever say this aloud,     you should pause, so that everyone can absorb the sheer shock of this Deep     Wisdom. There is a trivial mapping between a graph and its complement. A     fully connected graph, with an edge between every two vertices, conveys the     same amount of information as a graph with no edges at all.      There's a distinction between Traditional rationalism and Bayesian rationalism. And I worry that the Bayesian variety, which Eliezer is a subscriber of, is a sort of hedgehogginess: a very focused and blindered approach. But I liked the idea that you can go beyond falsification, the ability to relinquish an initial opinion when confronted by clear evidence against it.        I suspect that a more powerful (and more difficult) method is to hold off on     thinking of an answer. To suspend, draw out, that tiny moment when we can’t     yet guess what our answer will be; thus giving our intelligence a longer     time in which to act. Even half a minute would be an improvement over half a     second.      "Make America Great Again":        A key component of a zeitgeist is whether it locates its ideals in its     future or its past. Nearly all cultures before the Enlightenment believed in     a Fall from Grace – that things had once been perfect in the distant past,     but then catastrophe had struck, and everything had slowly run downhill     since then." />
  <meta name="author" content="Boris Smus" />
  <link rel="canonical" href="https://smus.com/books/how-to-actually-change-your-mind-by-big-yud" />

  <!-- Twitter -->
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="How to Actually Change Your Mind by Big Yud" />
  <meta name="twitter:description" content="Technically the second book in a giant epic entitled "Rationality: From AI to Zombies". The author is a well known rationalist and active member/founder of lesswrong.org, Center for Applied Rationality (CFAR), etc.    Essentially, this is a series of essays written and published as "The sequences", around 2009. They are loosely related and cover a wide array of topics, many of them highlighting irrational modes of thought. Much of the work focuses on biases central to behavioral economics, focusing on Kahneman-style results. But the author goes beyond that, and also introduces a lot of opinion for how a rationalist should behave. At the same time, there is a tendency to be incredibly nerdy, which is alternatingly endearing and borderline autistic. I found myself asking the question: if one becomes a purely rational agent, isn't a computer strictly better? On the path to rationality, what aspects of humanity is worth preserving?    Here's some new stuff I learned. A fair amount of the book covers behavioral economics concepts that I read about already in TF&amp;S.    Litanies    Aumann’s Agreement Theorem suggests that no two rationalists can agree to disagree, given that they have the same information.    Litany of  Gendlin :        What is true is already so.   Owning up to it doesn't make it worse.   Not being open about it doesn't make it go away.   And because it's true, it is what is there to be interacted with.   Anything untrue isn't there to be lived.   People can stand what is true,   for they are already enduring it.      Litany of  Tarski :        If the box contains a diamond,   I desire to believe that the box contains a diamond;   If the box does not contain a diamond,   I desire to believe that the box does not contain a diamond;   Let me not become attached to beliefs I may not want.      Against black and white thinking    Yudkowsky is especially effective in his attacks on binary thinking. For example, on partisanship:        Politics is an extension of war by other means. Arguments are soldiers. Once   you know which side you're on, you must support all arguments of that side,   and attack all arguments that appear to favor the enemy side, otherwise it's   like stabbing your soldiers in the back - providing aid and comfort to the   enemy. People who would be level-headed about evenhandedly weighing all sides   of an issue in their professional life as scientists can suddenly turn into   slogan-chanting zombies when there's a Blue or Green position on an issue.      On the tendency and fallacy in binary thought:        There is a natural tendency to treat discussion as a form of combat, an   extension of war, a sport; and in sports you only need to keep track of how   many points have been scored by each team. There are only two sides, and   every point scored against one side is a point in favor of the other.   Everyone in the audience keeps a mental running count of how many points   each speaker scores against the other. At the end of the debate, the speaker   who has scored more points is, obviously, the winner; so everything that   speaker says must be true, and everything the loser says must be wrong.      The horns effect - all negative qualities correlate:        Stalin also believed that 2 + 2 = 4. Yet if you defend any statement made by   Stalin, even “2 + 2 = 4,” people will see only that you are agreeing with   stalin and you must be on his side.      And a very nice summary of a better way of thinking:        Not all arguments reduce to mere up or down. Lady Rationality carries a     notebook, wherein she writes down all the facts that aren’t on anyone’s side.      Real belief vs. belief in belief:        Roger Zelazny once distinguished between “wanting to be an author” versus     “wanting to write.” Mark Twain said: “A classic is something that everyone     wants to have read and no one wants to read.” Criticizing yourself from a     sense of duty leaves you wanting to have investigated, so that you’ll be     able to say afterward that your faith is not blind. This is not the same as     wanting to investigate.      Assume your interlocutor is good    "To argue against an idea honestly, you should argue against the best arguments of the strongest advocates". This, and the closely related concept of the principle of charity aka Steelmanning (the opposite of strawmanning), which I heard from a Sam Harris interview, sent me on a long reading tangent of arguments  for  and  against . Insightful tidbit from that last link:        First, seek to understand the actual viewpoints people you disagree with are   actually advocating.  Second, seek out intelligent and well-informed   advocates of viewpoints you disagree with. You don’t have to make up what   your opponents believe! As it happens, you have many smart opponents!   Third, whenever possible, try to switch conversations from a debate focus to   a collaborative truth-seeking focus.      Back to Big Yud. Some wisdom on focusing on the argument, not on the person:        Someone once said "Not all conservatives are stupid, but most stupid people     are conservatives". If you cannot place yourself in a state of mind where     this statement, true or false, seems completely irrelevant as a critique of     conservatism, you are not ready to think rationally about politics.      A variation on the  reasonable person principle  (harkens back to my time at CMU).        To understand why people act the way they do, we must first realize that     everyone sees themselves as behaving normally. Don’t ask what strange,     mutant disposition they were born with, which directly corresponds to their     surface behavior. Rather, ask what situations people see themselves as being     in. [...] Realistically, most people don’t construct their life stories with     themselves as the villains.      Deciding which side to argue    Great distinction between rationality and rationalization. Very related to Haidt's position that beliefs are intuitive but their defence is rational. But Haidt makes no distinction like this. Would love to hear his thoughts on it.        Rationality is not for winning debates, it is for deciding which side to     join. If you’ve already decided which side to argue for, the work of     rationality is done within you, whether well or poorly. But how can you,     yourself, decide which side to argue?      Eliezer suggests enumerating the evidence: "Lady Rationality carries a notebook, wherein she writes down all the facts that aren’t on anyone’s side". Here's how to construct an honest ultrarational argument for a particular political candidate:      Gather all evidence about the different candidates   Make a checklist which you will use to decide which candidate is best   Process the checklist   Go to the winning candidate   Offer to become their campaign manager   Use the checklist as the campaign literature      The future is hard to predict    Herd instinct in venture capitalism:        The majority of venture capitalists at any given time are all chasing the     same Revolutionary Innovation, and it’s the Revolutionary Innovation that     IPO’d six months ago. This is an especially crushing observation in venture     capital, because there’s a direct economic motive to not follow the herd.      And what to do about it. DFJ (a VC) has a rule  favoring a passionate minority  to outweigh a negative majority.  This also reminds me of the  Tenth Man Rule .        Only two partners need to agree in order to fund any startup up to $1.5     million. And if all the partners agree that something sounds like a good     idea, they won’t do it.      Movies and books have a huge effect on the human psyche. This will probably compound with more immersive storytelling mediums:        So far as I can tell, few movie viewers act as if they have directly     observed Earth’s future. [...] But those who commit the fallacy seem to act     as if they had seen the movie events occurring on some other planet; not     Earth, but somewhere similar to Earth.      Predicting numbers is especially difficult:        I observe that many futuristic predictions are, likewise, best considered as     attitude expressions. Take the question, “How long will it be until we have     human-level AI?” The responses I’ve seen to this are all over the map.      Avoid having THE Great Idea and get granular and specific    Avoiding partisanship by focusing on the minimum viable argument, reminds me of Sunstein's  Judicial minimalism :        But try to resist getting in those good, solid digs if you can possibly     avoid it. If your topic legitimately relates to attempts to ban evolution in     school curricula, then go ahead and talk about it—but don’t blame it     explicitly on the whole Republican Party; some of your readers may be     Republicans.      Avoid overly large uhh Thingies, and chop them up.        Cut up your Great Thingy into smaller independent ideas and treat them as     independent. For instance, a marxist would cut up Marx's Great Thingy into     theories of 1) value of labor 2) political relations between classes 3)     wages 4) the ultimate political state of mankind.      Other interesting stuff    Taber and Lodge's  "Motivated skepticism in the evaluation of political beliefs"  describes six predictions which are very Haidt-y. It's a list of political thinkos that are driven by behavioral economic biases.    Beliefs don't need to be completely bullet proof. But this contradicts science, where a single counter example can topple a theory.        A probabilistic model can take a hit or two, and still survive, so long as     the hits don’t keep on coming in. Yet it is widely believed, especially in     the court of public opinion, that a true theory can have no failures and a     false theory no successes.      On the uselessness of "Deep Wisdom":        Surely the wisest of all human beings are the New Age gurus who say,     “Everything is connected to everything else.” If you ever say this aloud,     you should pause, so that everyone can absorb the sheer shock of this Deep     Wisdom. There is a trivial mapping between a graph and its complement. A     fully connected graph, with an edge between every two vertices, conveys the     same amount of information as a graph with no edges at all.      There's a distinction between Traditional rationalism and Bayesian rationalism. And I worry that the Bayesian variety, which Eliezer is a subscriber of, is a sort of hedgehogginess: a very focused and blindered approach. But I liked the idea that you can go beyond falsification, the ability to relinquish an initial opinion when confronted by clear evidence against it.        I suspect that a more powerful (and more difficult) method is to hold off on     thinking of an answer. To suspend, draw out, that tiny moment when we can’t     yet guess what our answer will be; thus giving our intelligence a longer     time in which to act. Even half a minute would be an improvement over half a     second.      "Make America Great Again":        A key component of a zeitgeist is whether it locates its ideals in its     future or its past. Nearly all cultures before the Enlightenment believed in     a Fall from Grace – that things had once been perfect in the distant past,     but then catastrophe had struck, and everything had slowly run downhill     since then." />

  <!-- Facebook -->
  <meta property="og:type" content="article" />
  <meta property="og:url" content="https://smus.com/books/how-to-actually-change-your-mind-by-big-yud" />
  <meta property="og:title" content="How to Actually Change Your Mind by Big Yud" />
  <meta property="og:description" content="Technically the second book in a giant epic entitled "Rationality: From AI to Zombies". The author is a well known rationalist and active member/founder of lesswrong.org, Center for Applied Rationality (CFAR), etc.    Essentially, this is a series of essays written and published as "The sequences", around 2009. They are loosely related and cover a wide array of topics, many of them highlighting irrational modes of thought. Much of the work focuses on biases central to behavioral economics, focusing on Kahneman-style results. But the author goes beyond that, and also introduces a lot of opinion for how a rationalist should behave. At the same time, there is a tendency to be incredibly nerdy, which is alternatingly endearing and borderline autistic. I found myself asking the question: if one becomes a purely rational agent, isn't a computer strictly better? On the path to rationality, what aspects of humanity is worth preserving?    Here's some new stuff I learned. A fair amount of the book covers behavioral economics concepts that I read about already in TF&amp;S.    Litanies    Aumann’s Agreement Theorem suggests that no two rationalists can agree to disagree, given that they have the same information.    Litany of  Gendlin :        What is true is already so.   Owning up to it doesn't make it worse.   Not being open about it doesn't make it go away.   And because it's true, it is what is there to be interacted with.   Anything untrue isn't there to be lived.   People can stand what is true,   for they are already enduring it.      Litany of  Tarski :        If the box contains a diamond,   I desire to believe that the box contains a diamond;   If the box does not contain a diamond,   I desire to believe that the box does not contain a diamond;   Let me not become attached to beliefs I may not want.      Against black and white thinking    Yudkowsky is especially effective in his attacks on binary thinking. For example, on partisanship:        Politics is an extension of war by other means. Arguments are soldiers. Once   you know which side you're on, you must support all arguments of that side,   and attack all arguments that appear to favor the enemy side, otherwise it's   like stabbing your soldiers in the back - providing aid and comfort to the   enemy. People who would be level-headed about evenhandedly weighing all sides   of an issue in their professional life as scientists can suddenly turn into   slogan-chanting zombies when there's a Blue or Green position on an issue.      On the tendency and fallacy in binary thought:        There is a natural tendency to treat discussion as a form of combat, an   extension of war, a sport; and in sports you only need to keep track of how   many points have been scored by each team. There are only two sides, and   every point scored against one side is a point in favor of the other.   Everyone in the audience keeps a mental running count of how many points   each speaker scores against the other. At the end of the debate, the speaker   who has scored more points is, obviously, the winner; so everything that   speaker says must be true, and everything the loser says must be wrong.      The horns effect - all negative qualities correlate:        Stalin also believed that 2 + 2 = 4. Yet if you defend any statement made by   Stalin, even “2 + 2 = 4,” people will see only that you are agreeing with   stalin and you must be on his side.      And a very nice summary of a better way of thinking:        Not all arguments reduce to mere up or down. Lady Rationality carries a     notebook, wherein she writes down all the facts that aren’t on anyone’s side.      Real belief vs. belief in belief:        Roger Zelazny once distinguished between “wanting to be an author” versus     “wanting to write.” Mark Twain said: “A classic is something that everyone     wants to have read and no one wants to read.” Criticizing yourself from a     sense of duty leaves you wanting to have investigated, so that you’ll be     able to say afterward that your faith is not blind. This is not the same as     wanting to investigate.      Assume your interlocutor is good    "To argue against an idea honestly, you should argue against the best arguments of the strongest advocates". This, and the closely related concept of the principle of charity aka Steelmanning (the opposite of strawmanning), which I heard from a Sam Harris interview, sent me on a long reading tangent of arguments  for  and  against . Insightful tidbit from that last link:        First, seek to understand the actual viewpoints people you disagree with are   actually advocating.  Second, seek out intelligent and well-informed   advocates of viewpoints you disagree with. You don’t have to make up what   your opponents believe! As it happens, you have many smart opponents!   Third, whenever possible, try to switch conversations from a debate focus to   a collaborative truth-seeking focus.      Back to Big Yud. Some wisdom on focusing on the argument, not on the person:        Someone once said "Not all conservatives are stupid, but most stupid people     are conservatives". If you cannot place yourself in a state of mind where     this statement, true or false, seems completely irrelevant as a critique of     conservatism, you are not ready to think rationally about politics.      A variation on the  reasonable person principle  (harkens back to my time at CMU).        To understand why people act the way they do, we must first realize that     everyone sees themselves as behaving normally. Don’t ask what strange,     mutant disposition they were born with, which directly corresponds to their     surface behavior. Rather, ask what situations people see themselves as being     in. [...] Realistically, most people don’t construct their life stories with     themselves as the villains.      Deciding which side to argue    Great distinction between rationality and rationalization. Very related to Haidt's position that beliefs are intuitive but their defence is rational. But Haidt makes no distinction like this. Would love to hear his thoughts on it.        Rationality is not for winning debates, it is for deciding which side to     join. If you’ve already decided which side to argue for, the work of     rationality is done within you, whether well or poorly. But how can you,     yourself, decide which side to argue?      Eliezer suggests enumerating the evidence: "Lady Rationality carries a notebook, wherein she writes down all the facts that aren’t on anyone’s side". Here's how to construct an honest ultrarational argument for a particular political candidate:      Gather all evidence about the different candidates   Make a checklist which you will use to decide which candidate is best   Process the checklist   Go to the winning candidate   Offer to become their campaign manager   Use the checklist as the campaign literature      The future is hard to predict    Herd instinct in venture capitalism:        The majority of venture capitalists at any given time are all chasing the     same Revolutionary Innovation, and it’s the Revolutionary Innovation that     IPO’d six months ago. This is an especially crushing observation in venture     capital, because there’s a direct economic motive to not follow the herd.      And what to do about it. DFJ (a VC) has a rule  favoring a passionate minority  to outweigh a negative majority.  This also reminds me of the  Tenth Man Rule .        Only two partners need to agree in order to fund any startup up to $1.5     million. And if all the partners agree that something sounds like a good     idea, they won’t do it.      Movies and books have a huge effect on the human psyche. This will probably compound with more immersive storytelling mediums:        So far as I can tell, few movie viewers act as if they have directly     observed Earth’s future. [...] But those who commit the fallacy seem to act     as if they had seen the movie events occurring on some other planet; not     Earth, but somewhere similar to Earth.      Predicting numbers is especially difficult:        I observe that many futuristic predictions are, likewise, best considered as     attitude expressions. Take the question, “How long will it be until we have     human-level AI?” The responses I’ve seen to this are all over the map.      Avoid having THE Great Idea and get granular and specific    Avoiding partisanship by focusing on the minimum viable argument, reminds me of Sunstein's  Judicial minimalism :        But try to resist getting in those good, solid digs if you can possibly     avoid it. If your topic legitimately relates to attempts to ban evolution in     school curricula, then go ahead and talk about it—but don’t blame it     explicitly on the whole Republican Party; some of your readers may be     Republicans.      Avoid overly large uhh Thingies, and chop them up.        Cut up your Great Thingy into smaller independent ideas and treat them as     independent. For instance, a marxist would cut up Marx's Great Thingy into     theories of 1) value of labor 2) political relations between classes 3)     wages 4) the ultimate political state of mankind.      Other interesting stuff    Taber and Lodge's  "Motivated skepticism in the evaluation of political beliefs"  describes six predictions which are very Haidt-y. It's a list of political thinkos that are driven by behavioral economic biases.    Beliefs don't need to be completely bullet proof. But this contradicts science, where a single counter example can topple a theory.        A probabilistic model can take a hit or two, and still survive, so long as     the hits don’t keep on coming in. Yet it is widely believed, especially in     the court of public opinion, that a true theory can have no failures and a     false theory no successes.      On the uselessness of "Deep Wisdom":        Surely the wisest of all human beings are the New Age gurus who say,     “Everything is connected to everything else.” If you ever say this aloud,     you should pause, so that everyone can absorb the sheer shock of this Deep     Wisdom. There is a trivial mapping between a graph and its complement. A     fully connected graph, with an edge between every two vertices, conveys the     same amount of information as a graph with no edges at all.      There's a distinction between Traditional rationalism and Bayesian rationalism. And I worry that the Bayesian variety, which Eliezer is a subscriber of, is a sort of hedgehogginess: a very focused and blindered approach. But I liked the idea that you can go beyond falsification, the ability to relinquish an initial opinion when confronted by clear evidence against it.        I suspect that a more powerful (and more difficult) method is to hold off on     thinking of an answer. To suspend, draw out, that tiny moment when we can’t     yet guess what our answer will be; thus giving our intelligence a longer     time in which to act. Even half a minute would be an improvement over half a     second.      "Make America Great Again":        A key component of a zeitgeist is whether it locates its ideals in its     future or its past. Nearly all cultures before the Enlightenment believed in     a Fall from Grace – that things had once been perfect in the distant past,     but then catastrophe had struck, and everything had slowly run downhill     since then." />

  <!-- Coil monetization experiment: https://coil.com/settings/monetize -->
  <meta name="monetization" content="$ilp.uphold.com/4Fnyw8KLaPZG">

  <!-- Icons -->
  <link rel="icon" type="image/x-icon" href="/static/icons/favicon.ico" />
  <link rel="apple-touch-icon" href="/static/icons/apple-touch-icon.png">

  <!-- Styles -->
  <link
  href='//fonts.googleapis.com/css?family=Roboto+Condensed:300|Open+Sans+Condensed:700|Source+Serif+Pro:400,700|Inconsolata' rel='stylesheet' type='text/css'>
  <link rel='stylesheet' href='/static/css/style.css'>
  <link rel='stylesheet' href='/static/css/syntax-highlight.css'>

  <!-- Feed -->
  <link href="//feeds.feedburner.com/smuscom" rel="alternate" title="Boris Smus" type="application/atom+xml"/>
</head>
<body>
<header>
  <div id='title'>
    <h1><a href='/'>Boris Smus</a></h1>
    <h2>interaction engineering</h2>
  </div>
  <nav role='navigation'>
    <a href='/about/' >About</a>
    <a href='/blog/' >Blog</a>
    <a href='/clippings/' >Clippings</a>
  </nav>
</header>

<section id='main'>
  <article>
    <a href='/books/how-to-actually-change-your-mind-by-big-yud'><h1 class='title'>How to Actually Change Your Mind by Big Yud</h1></a>
    
    <div class='body'>
      <p>Technically the second book in a giant epic entitled "Rationality: From AI to
Zombies". The author is a well known rationalist and active member/founder of
lesswrong.org, Center for Applied Rationality (CFAR), etc.</p>

<p>Essentially, this is a series of essays written and published as "The
sequences", around 2009. They are loosely related and cover a wide array of
topics, many of them highlighting irrational modes of thought. Much of the work
focuses on biases central to behavioral economics, focusing on Kahneman-style
results. But the author goes beyond that, and also introduces a lot of opinion
for how a rationalist should behave. At the same time, there is a tendency to be
incredibly nerdy, which is alternatingly endearing and borderline autistic. I
found myself asking the question: if one becomes a purely rational agent, isn't
a computer strictly better? On the path to rationality, what aspects of humanity
is worth preserving?</p>

<p>Here's some new stuff I learned. A fair amount of the book covers behavioral
economics concepts that I read about already in TF&amp;S.</p>

<h4>Litanies</h4>

<p>Aumann’s Agreement Theorem suggests that no two rationalists can agree to
disagree, given that they have the same information.</p>

<p>Litany of <a href="https://en.wikipedia.org/wiki/Eugene_Gendlin">Gendlin</a>:</p>

<blockquote>
  <p>What is true is already so.
  Owning up to it doesn't make it worse.
  Not being open about it doesn't make it go away.
  And because it's true, it is what is there to be interacted with.
  Anything untrue isn't there to be lived.
  People can stand what is true,
  for they are already enduring it.</p>
</blockquote>

<p>Litany of <a href="https://en.wikipedia.org/wiki/Alfred_Tarski">Tarski</a>:</p>

<blockquote>
  <p>If the box contains a diamond,
  I desire to believe that the box contains a diamond;
  If the box does not contain a diamond,
  I desire to believe that the box does not contain a diamond;
  Let me not become attached to beliefs I may not want.</p>
</blockquote>

<h4>Against black and white thinking</h4>

<p>Yudkowsky is especially effective in his attacks on binary thinking. For
example, on partisanship:</p>

<blockquote>
  <p>Politics is an extension of war by other means. Arguments are soldiers. Once
  you know which side you're on, you must support all arguments of that side,
  and attack all arguments that appear to favor the enemy side, otherwise it's
  like stabbing your soldiers in the back - providing aid and comfort to the
  enemy. People who would be level-headed about evenhandedly weighing all sides
  of an issue in their professional life as scientists can suddenly turn into
  slogan-chanting zombies when there's a Blue or Green position on an issue.</p>
</blockquote>

<p>On the tendency and fallacy in binary thought:</p>

<blockquote>
  <p>There is a natural tendency to treat discussion as a form of combat, an
  extension of war, a sport; and in sports you only need to keep track of how
  many points have been scored by each team. There are only two sides, and
  every point scored against one side is a point in favor of the other.
  Everyone in the audience keeps a mental running count of how many points
  each speaker scores against the other. At the end of the debate, the speaker
  who has scored more points is, obviously, the winner; so everything that
  speaker says must be true, and everything the loser says must be wrong.</p>
</blockquote>

<p>The horns effect - all negative qualities correlate:</p>

<blockquote>
  <p>Stalin also believed that 2 + 2 = 4. Yet if you defend any statement made by
  Stalin, even “2 + 2 = 4,” people will see only that you are agreeing with
  stalin and you must be on his side.</p>
</blockquote>

<p>And a very nice summary of a better way of thinking:</p>

<blockquote>
  <p>Not all arguments reduce to mere up or down. Lady Rationality carries a
    notebook, wherein she writes down all the facts that aren’t on anyone’s side.</p>
</blockquote>

<p>Real belief vs. belief in belief:</p>

<blockquote>
  <p>Roger Zelazny once distinguished between “wanting to be an author” versus
    “wanting to write.” Mark Twain said: “A classic is something that everyone
    wants to have read and no one wants to read.” Criticizing yourself from a
    sense of duty leaves you wanting to have investigated, so that you’ll be
    able to say afterward that your faith is not blind. This is not the same as
    wanting to investigate.</p>
</blockquote>

<h4>Assume your interlocutor is good</h4>

<p>"To argue against an idea honestly, you should argue against the best arguments
of the strongest advocates". This, and the closely related concept of the
principle of charity aka Steelmanning (the opposite of strawmanning), which I
heard from a Sam Harris interview, sent me on a long reading tangent of
arguments <a href="http://www.patheos.com/blogs/camelswithhammers/2016/08/on-steelmanning-arguments-and-personally-customizing-them/">for</a> and <a href="https://thingofthings.wordpress.com/2016/08/09/against-steelmanning/">against</a>. Insightful tidbit from that
last link:</p>

<blockquote>
  <p>First, seek to understand the actual viewpoints people you disagree with are
  actually advocating.  Second, seek out intelligent and well-informed
  advocates of viewpoints you disagree with. You don’t have to make up what
  your opponents believe! As it happens, you have many smart opponents!
  Third, whenever possible, try to switch conversations from a debate focus to
  a collaborative truth-seeking focus.</p>
</blockquote>

<p>Back to Big Yud. Some wisdom on focusing on the argument, not on the person:</p>

<blockquote>
  <p>Someone once said "Not all conservatives are stupid, but most stupid people
    are conservatives". If you cannot place yourself in a state of mind where
    this statement, true or false, seems completely irrelevant as a critique of
    conservatism, you are not ready to think rationally about politics.</p>
</blockquote>

<p>A variation on the <a href="http://www.cs.cmu.edu/~weigand/staff/">reasonable person principle</a> (harkens back to my time
at CMU).</p>

<blockquote>
  <p>To understand why people act the way they do, we must first realize that
    everyone sees themselves as behaving normally. Don’t ask what strange,
    mutant disposition they were born with, which directly corresponds to their
    surface behavior. Rather, ask what situations people see themselves as being
    in. [...] Realistically, most people don’t construct their life stories with
    themselves as the villains.</p>
</blockquote>

<h4>Deciding which side to argue</h4>

<p>Great distinction between rationality and rationalization. Very related to
Haidt's position that beliefs are intuitive but their defence is rational. But
Haidt makes no distinction like this. Would love to hear his thoughts on it.</p>

<blockquote>
  <p>Rationality is not for winning debates, it is for deciding which side to
    join. If you’ve already decided which side to argue for, the work of
    rationality is done within you, whether well or poorly. But how can you,
    yourself, decide which side to argue?</p>
</blockquote>

<p>Eliezer suggests enumerating the evidence: "Lady Rationality carries a notebook,
wherein she writes down all the facts that aren’t on anyone’s side". Here's how
to construct an honest ultrarational argument for a particular political
candidate:</p>

<ul>
<li>Gather all evidence about the different candidates</li>
<li>Make a checklist which you will use to decide which candidate is best</li>
<li>Process the checklist</li>
<li>Go to the winning candidate</li>
<li>Offer to become their campaign manager</li>
<li>Use the checklist as the campaign literature</li>
</ul>

<h4>The future is hard to predict</h4>

<p>Herd instinct in venture capitalism:</p>

<blockquote>
  <p>The majority of venture capitalists at any given time are all chasing the
    same Revolutionary Innovation, and it’s the Revolutionary Innovation that
    IPO’d six months ago. This is an especially crushing observation in venture
    capital, because there’s a direct economic motive to not follow the herd.</p>
</blockquote>

<p>And what to do about it. DFJ (a VC) has a rule <a href="http://www.signallake.com/innovation/CrazyIdeasSuccessfulVC101009.pdf">favoring a passionate
minority</a> to outweigh a negative majority.  This also reminds me of the
<a href="https://www.quora.com/World-War-Z-2013-movie-Do-the-Israelis-really-have-a-10th-man-doctrine">Tenth Man Rule</a>.</p>

<blockquote>
  <p>Only two partners need to agree in order to fund any startup up to $1.5
    million. And if all the partners agree that something sounds like a good
    idea, they won’t do it.</p>
</blockquote>

<p>Movies and books have a huge effect on the human psyche. This will probably
compound with more immersive storytelling mediums:</p>

<blockquote>
  <p>So far as I can tell, few movie viewers act as if they have directly
    observed Earth’s future. [...] But those who commit the fallacy seem to act
    as if they had seen the movie events occurring on some other planet; not
    Earth, but somewhere similar to Earth.</p>
</blockquote>

<p>Predicting numbers is especially difficult:</p>

<blockquote>
  <p>I observe that many futuristic predictions are, likewise, best considered as
    attitude expressions. Take the question, “How long will it be until we have
    human-level AI?” The responses I’ve seen to this are all over the map.</p>
</blockquote>

<h4>Avoid having THE Great Idea and get granular and specific</h4>

<p>Avoiding partisanship by focusing on the minimum viable argument, reminds me of
Sunstein's <a href="https://en.wikipedia.org/wiki/Judicial_minimalism">Judicial minimalism</a>:</p>

<blockquote>
  <p>But try to resist getting in those good, solid digs if you can possibly
    avoid it. If your topic legitimately relates to attempts to ban evolution in
    school curricula, then go ahead and talk about it—but don’t blame it
    explicitly on the whole Republican Party; some of your readers may be
    Republicans.</p>
</blockquote>

<p>Avoid overly large uhh Thingies, and chop them up.</p>

<blockquote>
  <p>Cut up your Great Thingy into smaller independent ideas and treat them as
    independent. For instance, a marxist would cut up Marx's Great Thingy into
    theories of 1) value of labor 2) political relations between classes 3)
    wages 4) the ultimate political state of mankind.</p>
</blockquote>

<h4>Other interesting stuff</h4>

<p>Taber and Lodge's <a href="http://onlinelibrary.wiley.com/doi/10.1111/j.1540-5907.2006.00214.x/abstract">"Motivated skepticism in the evaluation of political
beliefs"</a> describes six predictions which are very Haidt-y. It's a list
of political thinkos that are driven by behavioral economic biases.</p>

<p>Beliefs don't need to be completely bullet proof. But this contradicts
science, where a single counter example can topple a theory.</p>

<blockquote>
  <p>A probabilistic model can take a hit or two, and still survive, so long as
    the hits don’t keep on coming in. Yet it is widely believed, especially in
    the court of public opinion, that a true theory can have no failures and a
    false theory no successes.</p>
</blockquote>

<p>On the uselessness of "Deep Wisdom":</p>

<blockquote>
  <p>Surely the wisest of all human beings are the New Age gurus who say,
    “Everything is connected to everything else.” If you ever say this aloud,
    you should pause, so that everyone can absorb the sheer shock of this Deep
    Wisdom. There is a trivial mapping between a graph and its complement. A
    fully connected graph, with an edge between every two vertices, conveys the
    same amount of information as a graph with no edges at all.</p>
</blockquote>

<p>There's a distinction between Traditional rationalism and Bayesian rationalism.
And I worry that the Bayesian variety, which Eliezer is a subscriber of, is a
sort of hedgehogginess: a very focused and blindered approach. But I liked the
idea that you can go beyond falsification, the ability to relinquish an initial
opinion when confronted by clear evidence against it.</p>

<blockquote>
  <p>I suspect that a more powerful (and more difficult) method is to hold off on
    thinking of an answer. To suspend, draw out, that tiny moment when we can’t
    yet guess what our answer will be; thus giving our intelligence a longer
    time in which to act. Even half a minute would be an improvement over half a
    second.</p>
</blockquote>

<p>"Make America Great Again":</p>

<blockquote>
  <p>A key component of a zeitgeist is whether it locates its ideals in its
    future or its past. Nearly all cultures before the Enlightenment believed in
    a Fall from Grace – that things had once been perfect in the distant past,
    but then catastrophe had struck, and everything had slowly run downhill
    since then.</p>
</blockquote>

    </div>
    <div class='subfooter'>
      <div class='tombstone'>▪</div>
      <time class='published'>August 2016</time>
    </div>
  </article>
</section>


<footer>
  <div>
    © Copyright 2005–2021 Boris Smus.
  </div>
  <nav role="footer">
    <a href='//feeds.feedburner.com/smuscom'>RSS</a>
  </nav>
</footer>

<!-- Misc scripts: syntax highlighting, analytics, stats. -->
<script src="/static/js/highlight.pack.js"></script>
<script>
  // Syntax highlighting for code.
  hljs.tabReplace = '  ';
  hljs.initHighlightingOnLoad();
</script>
<script>
  // Google Analytics.
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-17930798-22', 'smus.com');
  ga('require', 'displayfeatures');
  ga('send', 'pageview');
</script>
<script src="/lightning_error.js"></script>

</body>
</html>