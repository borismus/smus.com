<!DOCTYPE html>
<html>
<head>
  <title>Web-based voice command recognition | Boris Smus</title>

  <meta charset='utf-8' />
  <meta name='viewport' content='width=device-width, user-scalable=no, minimum-scale=1.0, maximum-scale=1.0' />

  <meta name="description" content="[Last time][audio-features] we converted audio buffers into images. This time we'll take these images and train a neural network using [deeplearn.js][deeplearn]. The result is [a browser-based demo][infer-yesno] that  lets you speak a command ("yes" or "no"), and see the output of the classifier in real-time, like this:     Curious to play with it, see whether or not it recognizes *yay* or *nay* in addition to *yes* and *no*? [Try it out live][infer-yesno]. You will quickly see that the performance is far from perfect. But that's ok with me: this example is intended to be a reasonable starting point for doing all sorts of audio recognition on the web. Now, let's dive into how this works." />
  <meta name="author" content="Boris Smus" />
  <link rel="canonical" href="https://smus.com/web-voice-command-recognition/" />

  <!-- Twitter -->
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Web-based voice command recognition" />
  <meta name="twitter:description" content="[Last time][audio-features] we converted audio buffers into images. This time we'll take these images and train a neural network using [deeplearn.js][deeplearn]. The result is [a browser-based demo][infer-yesno] that  lets you speak a command ("yes" or "no"), and see the output of the classifier in real-time, like this:     Curious to play with it, see whether or not it recognizes *yay* or *nay* in addition to *yes* and *no*? [Try it out live][infer-yesno]. You will quickly see that the performance is far from perfect. But that's ok with me: this example is intended to be a reasonable starting point for doing all sorts of audio recognition on the web. Now, let's dive into how this works." />

  <!-- Facebook -->
  <meta property="og:type" content="article" />
  <meta property="og:url" content="https://smus.com/web-voice-command-recognition/" />
  <meta property="og:title" content="Web-based voice command recognition" />
  <meta property="og:description" content="[Last time][audio-features] we converted audio buffers into images. This time we'll take these images and train a neural network using [deeplearn.js][deeplearn]. The result is [a browser-based demo][infer-yesno] that  lets you speak a command ("yes" or "no"), and see the output of the classifier in real-time, like this:     Curious to play with it, see whether or not it recognizes *yay* or *nay* in addition to *yes* and *no*? [Try it out live][infer-yesno]. You will quickly see that the performance is far from perfect. But that's ok with me: this example is intended to be a reasonable starting point for doing all sorts of audio recognition on the web. Now, let's dive into how this works." />

  <!-- Mastodon -->
  <link rel="me" href="https://mastodon.social/@borismus">

  <!-- Coil monetization experiment: https://coil.com/settings/monetize -->
  <meta name="monetization" content="$ilp.uphold.com/4Fnyw8KLaPZG">

  <!-- Icons -->
  <link rel="apple-touch-icon" sizes="180x180" href="/static/icons/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/static/icons/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/static/icons/favicon-16x16.png">
  <link rel="manifest" href="/static/icons/site.webmanifest">

  <!-- Styles -->
  <link
  href='//fonts.googleapis.com/css?family=Roboto+Condensed:300|Open+Sans+Condensed:700|Source+Serif+Pro:400,700|Inconsolata' rel='stylesheet' type='text/css'>
  <link rel='stylesheet' href='/static/css/style.css'>
  <link rel='stylesheet' href='/static/css/syntax-highlight.css'>

  <!-- Feed -->
  <link href="https://smus.com/atom.xml" rel="alternate" title="Boris Smus" type="application/atom+xml"/>
</head>
<body>
<header>
  <div id='title'>
    <h1><a href='/'>Boris Smus</a></h1>
    <h2>interaction engineering</h2>
  </div>
  <nav role='navigation'>
    <a href='/about/' >About</a>
    <a href='/blog/' >Blog</a>
    <a href='/projects/' >Projects</a>
  </nav>
</header>

<section id='main'>
  <article>
    <a href='/web-voice-command-recognition'><h1 class='title'>Web-based voice command recognition</h1></a>
    <div class='body'>
      <p>[Last time][audio-features] we converted audio buffers into images. This time
we'll take these images and train a neural network using
[deeplearn.js][deeplearn]. The result is [a browser-based demo][infer-yesno] that 
lets you speak a command ("yes" or "no"), and see the output of the classifier
in real-time, like this:</p>
<video src="/web-voice-command-recognition/inference-demo.mp4" autoplay muted loop type="video/mp4"
style="width: 100%">

Curious to play with it, see whether or not it recognizes *yay* or *nay* in
addition to *yes* and *no*? [Try it out live][infer-yesno]. You will quickly see
that the performance is far from perfect. But that's ok with me: this example is
intended to be a reasonable starting point for doing all sorts of audio
recognition on the web. Now, let's dive into how this works.

<!--more-->

[audio-features]: /web-audio-ml-features
[deeplearn]: https://deeplearnjs.org
[infer-yesno]: https://google.github.io/web-audio-recognition/inference-demo/?model=yesno
[infer-digits]: https://google.github.io/web-audio-recognition/inference-demo/?model=number


# Quick start: training and testing a command recognizer

Here's how you can train your own yes/no classifier:

1. Go to the [model training page][train]. It will take a bit of time to download
   the [training][yes-example] [data][no-example] from the server.
2. Click the train button, and you'll see a graph showing training progress.
   Once you are ready (this will take a while, perhaps 500 iterations or 2
   minutes, depending on your hardware), stop training, and press the save
   weights (file) button.  This will download a JSON file.

    ![Training graph](2-training-graph.png)

3. Then go to [the inference demo page][infer-yesno], press the load weights (file) button
   and select the downloaded JSON file to load the trained model.
4. Flip the switch, grant access to the microphone and try saying "yes" or "no".
   You'll see microphone and confidence levels indicated at the bottom of the
   page.

    ![Inference screenshot](3-inference.png)

The above is a mechanistic account of how the training example works.  If you
are interested in learning about the gory (and interesting) details, read on.

[train]: https://google.github.io/web-audio-recognition/train-model/?data_url=https://storage.googleapis.com/audio-recognition-data&data_extension=mp3


# Data pre-processing and loading

Training a neural net requires a lot of training data. In practice, millions of
examples may be required, but [the dataset][data] we'll be using is small by
modern standards, with *just* 65,000 labeled examples. Each example is a
separate wav file, with the label in the filename.

Loading each training wav as a separate request turned out to be quite slow. The
overhead from each request is small, but when compounded over a few thousand
times, really starts to be felt. An easy optimization to load data more quickly
is to put all examples with the same label into one long audio file. Decoding
audio files is pretty fast, and so is splitting them into one second long
buffers. A further optimization is to use a compressed audio format, such as
mp3. [`scripts/preprocess.py`][preproc] will do this concatenation for you,
producing this [mesmerising result][yes-example].

After we "rehydrate" our raw audio examples, we process buffers of raw data into
features. We do this using the [Audio Feature extractor][audio-features] I
mentioned in the [last post][audio-features], which takes in raw audio, and
produces a log-mel spectrogram. This is relatively slow, and accounts for most
of the time spent loading the dataset.

[data]: https://storage.cloud.google.com/download.tensorflow.org/data/speech_commands_v0.01.tar.gz
[yes-example]: https://storage.googleapis.com/audio-recognition-data/yes.mp3
[no-example]: https://storage.googleapis.com/audio-recognition-data/no.mp3
[preproc]: https://github.com/google/web-audio-recognition/blob/master/train-model/scripts/preprocess.py

# Model training considerations

For the yes/no recognizer, we have only two commands that we care about: "yes",
and "no". But we also want to detect the lack of any such utterances, as well as
silence. We include a set of [random utterances][other] as the "other" category
(none of which are yes or no). This example is also generated by the
[preprocessing script][preproc].

Since we're dealing with real microphones, we never expect to hear pure silence.
Instead, "silence" is some level of ambient noise compounded by crappy
microphone quality. Luckily, the training data also includes background noise
which we mix with our training examples at various volumes. We also generate a
set of silence examples, which includes only the background audio.  Once we've
prepared our samples, we generate our final spectrograms as our input.

To generate these final spectrograms, we need to decide on buffer and hop
length. A reasonable buffer length is 1024, and a hop length of 512. Since we
are dealing with sample rate of 16000 Hz, it works out to a window duration of
about 60ms, sampled every 30ms.

Once we have labeled spectrograms, we need to convert inputs and labels into
deeplearn arrays. Label strings "yes", "no", "other", and "silence" will be
[one-hot][] encoded as a `Array1D`s of four integers, meaning that "yes"
corresponds to `[1, 0, 0, 0]`, and "no" to `[0, 1, 0, 0]`. Spectrograms from the
feature extractor need to be converted into an `Array3D`, which can be fed as
input for the model.

The model we are training consists of two convolution layers, and one fully
connected layer. I took this architecture directly from the MNIST example
of deeplearn.js, and hasn't been customized for dealing with spectrograms at
all. As a result, performance is a far cry from state of the art speech
recognition. To see even more mis-classifications, try out [MNIST for
audio][infer-digits] which recognizes spoken digits (eg. "zero" through "ten").
I am confident that we could do better by following [this paper][paper]. A
real-world speech recognizer might not use convolution at all, instead opting
for an [LSTM][lstm], which is better suited to process time-series data.

Lastly, we want to tell the machine learning framework how to train the model.
In ML parlance, we need to set the [hyperparameters][hyper], which includes
setting the learning rate (how much to follow the gradient at each step) and
batch size (how many examples to ingest at a time). And we're off to the races:

![Training graph](2-training-graph.png)

During training, the gradient descent algorithm tries to minimize cost, which
you can see in blue. We also plot accuracy in orange, which is occasionally
calculated by running inference on a test set. We use a random subset of the
test set because inference takes time, and we'd like to train as quickly as
possible.

Once we are happy with the test accuracy, we can save the model weights and use
them to infer results.

[paper]: http://www.isca-speech.org/archive/interspeech_2015/papers/i15_1478.pdf
[other]: https://storage.googleapis.com/audio-recognition-data/other.mp3
[one-hot]: https://en.wikipedia.org/wiki/One-hot
[lstm]: https://en.wikipedia.org/wiki/Long_short-term_memory
[hyper]: https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)


# Saving and loading model weights

A model is defined by its architecture and the weights of its weight-bearing
nodes. Weights are the values that are learned during the process of model
training, and not all nodes have weights. ReLUs and flatten nodes don't. But
convolution and fully connected nodes have both weights and biases. These
weights are tensors of arbitrary shapes. To save and load models, we need to be
able to save both graphs **and** their weights.

Saving & loading models is important for a few reasons:

  1. Model training takes time, so you might want to train a bit, save weights,
     take a break, and then resume from where you left off. This is called
     checkpointing.
  2. For inference, it's useful to have a self-contained model that you can just
     load and run.

At the time of writing, deeplearn.js didn't have facilities to serialize models
and model weights. For this example, I've implemented a way to load and save
weights, assuming that the model architecture itself is hard-coded. The
[`GraphSaverLoader`][gsl] class can save & load from a local store (IndexedDB),
or from a file. Ultimately, we will need a non-hacky way of saving and loading
models and their corresponding weights, and I'm excited for the near future of
improved ML developer ergonomics.


[gsl]: https://github.com/google/web-audio-recognition/blob/master/train-model/src/GraphSaverLoader.ts

# Wrapping up

Many thanks to [Nikhil](https://twitter.com/nsthorat) and
[Daniel](https://twitter.com/dsmilkov) for their hard work on deeplearn.js, and
willingness to answer my barrages of stupid little questions. Also, to
[Pete](https://twitter.com/petewarden), who is responsible for creating and
releasing the [dataset][data] I used in this post. And thank you dear reader,
for reading this far.

I'm stoked to see how this kind of browser based audio recognition tech can be
applied to exciting, educational ML projects like [Teachable
Machine][teachable-machine]. How cool would it be if you could make a
self-improving system, which trains on every additional spoken utterance? The
ability to train these kinds of models in the browser allows us to entertain
such possibilities in a privacy preserving way, without sending anything to any
server.

So there you have it! This has been an explanation of voice command recognition
on the web. We covered feature extraction in the [previous
post][audio-features], and this time, dug a little bit into [model
training][train] and [real-time inference][infer-yesno] entirely in the browser.

If you build on this example, please drop me a note on
[twitter](https://twitter.com/borismus).

[teachable-machine]: https://teachablemachine.withgoogle.com/
    </div>
    <div class='subfooter'>
      <div class='tombstone'>▪</div>
      <time class='published'>January 4, 2018</time>
    </div>
  </article>
</section>


<footer>
  <div>
    © Copyright 2005–2023 Boris Smus.
  </div>
  <nav role="footer">
    <a href='https://smus.com/atom.xml'>RSS</a>
  </nav>
</footer>

<!-- Misc scripts: syntax highlighting, analytics, stats. -->
<script src="/static/js/highlight.pack.js"></script>
<!-- Syntax highlighting for code. -->
<script>
  hljs.tabReplace = '  ';
  hljs.initHighlightingOnLoad();
</script>
<!-- Lightning builder error check. -->
<script src="/lightning_error.js"></script>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-38Y45XD5VQ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-38Y45XD5VQ');
</script>


</body>
</html>