(lp0
(dp1
S'info'
p2
(dp3
S'meta_description'
p4
V Welcome  
p5
sS'snip'
p6
ccopy_reg
_reconstructor
p7
(cmarkdown2
UnicodeWithAttrs
p8
c__builtin__
unicode
p9
V<p>Welcome</p>\u000a
p10
tp11
Rp12
sS'permalink'
p13
V/
p14
sS'description'
p15
NsS'is_long'
p16
I00
sS'title'
p17
VWelcome
p18
sS'list'
p19
NsS'summary'
p20
V
p21
sS'filter'
p22
(lp23
S'post'
p24
asS'verb'
p25
S'Published'
p26
sS'limit'
p27
I10
sS'content'
p28
g12
sS'type'
p29
S'index'
p30
sS'slug'
p31
S'main'
p32
sS'computed_link'
p33
g14
ssS'path'
p34
S'content/main.md'
p35
sS'modified'
p36
F1433949532.0
sa(dp37
g2
(dp38
g4
V   
p39
sg6
g7
(g8
g9
V<p></p>\u000a
p40
tp41
Rp42
sg13
V/atom
p43
sg15
Nsg16
I00
sg17
VBoris Smus
p44
sS'list'
p45
Nsg20
g21
sS'filter'
p46
(lp47
S'post'
p48
aS'link'
p49
aS'talk'
p50
asg25
g26
sS'limit'
p51
I50
sg28
g42
sS'type'
p52
S'feed'
p53
sg31
S'atom'
p54
sg33
g43
ssg34
S'content/feeds/atom.md'
p55
sg36
F1433976468.0
sa(dp56
g2
(dp57
g4
V      Hello from the west coast!    My passion is to invent and build enjoyable, useful, and novel user interfaces at the cusp of hardware and software. I am a proficient front-end and mobile engineer with experience leading teams. In my projects, I tend to gravitate to web technologies because of the flexibility of the web platform. My professional interests include education, audio and music, sensors and interaction design.    My job at Google is a mix of software engineering and user experience prototyping. I have had the good fortune to help launch  Google Cardboard ,  Google Tone , and build many other exciting products with a small, fast moving team.    At my previous job as an engineer on the Chrome developer relations team we pushed the limits of the modern web platform and educated web developers. I helped launch  Chrome for Android ,  wrote articles ,  gave talks  and released open source libraries and samples. I also authored a short  book on the Web Audio API .    I hold a masters in HCI from  Carnegie Mellon , where I focused on social and  wearable computing research  and built  CrowdForge , a system for crowdsourcing complex tasks. Prior to that, I worked at Apple for two years, contributing to iWeb, iWork and helping start  iWork.com . For the still curious, my  resume is online .    I am Canadian. I hold a BSc in Mathematics and Computer Science from the  University of British Columbia . I was born in Leningrad, RSFSR, and spent my formative years in Vancouver, Canada.      Email:  boris@smus.com    Twitter:  @borismus    GitHub:  @borismus     
p58
sg28
g7
(g8
g9
V<p><img id='image-me' src='/static/images/stipple.png'/></p>\u000a\u000a<p>Hello from the west coast!</p>\u000a\u000a<p>My passion is to invent and build enjoyable, useful, and novel user\u000ainterfaces at the cusp of hardware and software. I am a proficient\u000afront-end and mobile engineer with experience leading teams. In my\u000aprojects, I tend to gravitate to web technologies because of the\u000aflexibility of the web platform. My professional interests include\u000aeducation, audio and music, sensors and interaction design.</p>\u000a\u000a<p>My job at Google is a mix of software engineering and user experience\u000aprototyping. I have had the good fortune to help launch <a href="http://g.co/cardboard">Google\u000aCardboard</a>, <a href="http://g.co/tone">Google Tone</a>, and build many other exciting\u000aproducts with a small, fast moving team.</p>\u000a\u000a<p>At my previous job as an engineer on the Chrome developer relations team\u000awe pushed the limits of the modern web platform and educated web\u000adevelopers. I helped launch <a href="https://play.google.com/store/apps/details?id=com.android.chrome&amp;hl=en">Chrome for Android</a>, <a href="http://www.html5rocks.com/">wrote\u000aarticles</a>, <a href="/talks">gave talks</a> and released open source libraries\u000aand samples. I also authored a short <a href="http://www.amazon.com/Web-Audio-API-Boris-Smus/dp/1449332684">book on the Web Audio API</a>.</p>\u000a\u000a<p>I hold a masters in HCI from <a href="http://www.hcii.cmu.edu/academics/mhci">Carnegie Mellon</a>, where I focused on social\u000aand <a href="https://scholar.google.com/citations?user=bIgFmUwAAAAJ">wearable computing research</a> and built <a href="http://crowdforge.com/">CrowdForge</a>,\u000aa system for crowdsourcing complex tasks. Prior to that, I worked at Apple for\u000atwo years, contributing to iWeb, iWork and helping start <a href="https://www.apple.com/iwork-for-icloud/">iWork.com</a>. For\u000athe still curious, my <a href="/resume">resume is online</a>.</p>\u000a\u000a<p>I am Canadian. I hold a BSc in Mathematics and Computer Science from the\u000a<a href="https://www.cs.ubc.ca/">University of British Columbia</a>. I was born in Leningrad, RSFSR, and spent\u000amy formative years in Vancouver, Canada.</p>\u000a\u000a<ul>\u000a<li>Email: <a href="mailto:boris@smus.com">boris@smus.com</a></li>\u000a<li>Twitter: <a href="http://twitter.com/borismus">@borismus</a></li>\u000a<li>GitHub: <a href="http://github.com/borismus">@borismus</a></li>\u000a</ul>\u000a
p59
tp60
Rp61
sg13
V/about
p62
sg15
Nsg16
I00
sg17
VAbout the author
p63
sg20
V<img id='image-me' src='/static/images/stipple.
p64
sg6
g61
sg25
g26
sS'type'
p65
S'page'
p66
sg31
S'about'
p67
sg33
g62
ssg34
S'content/pages/about.md'
p68
sg36
F1470931739.0
sa(dp69
g2
(dp70
g4
V Everything on this blog.  
p71
sg6
g7
(g8
g9
V<p>Everything on this blog.</p>\u000a
p72
tp73
Rp74
sg13
V/blog
p75
sg15
Nsg16
I00
sg17
VBlog
p76
sS'list'
p77
Nsg20
VEverything on this blog.
p78
sS'filter'
p79
S'post'
p80
sg25
g26
sS'limit'
p81
I1000
sg28
g74
sS'type'
p82
S'archive'
p83
sg31
S'blog'
p84
sg33
g75
ssg34
S'content/pages/blog.md'
p85
sg36
F1433904595.0
sa(dp86
g2
(dp87
g4
V In early 2015, I began writing a highlight/review/summary for every book, audio book, essay or especially memorable podcast I consume, in an attempt to become a more active reader and listener.    Mindstorms by Seymour Papert (4/5) - February 2017    I enjoyed this short book by the inventor of LOGO, which I fondly remember first hearing about from Yuri as a kid. It was a challenging read, especially in the philosophical sections in the end. Also I have this weird thing where Seymour Papert and Mark Gross somehow blend into one person??    Papert is fascinated by embodied learning, and the LOGO Turtle is one of many examples of an "object to think with". This has great benefits for children, since they can imagine becoming a turtle in their mind. For example, drawing a circle as a Turtle is simple: take a small step forward, turn to the right a bit, rinse and repeat. This is easily translated into LOGO:  FORWARD 1; RIGHT 1; .    Papert rails against the way math is presented in school, making compelling arguments in its direction as well as the direction of "New Math", which I guess was a big deal in 1980 but has not aged well. He draws a comparison from our current educational system to QWERTY, which we have kept for a long time despite its obsolescence. He warns about a dumb use of computers in the classroom, in which computers program the child (e.g. Mavis beacon teaches typing). In Papert's world, the child programs the computer and learns in the process.    Conceptually, LOGO represents a third view of geometry. Three views of geometry: Euclidean is the most abstract, providing conceptual underpinnings. Cartesian, which allows for algebraic descriptions and practical applications. And Turtle which allows for an algorithmic approach. Latter is also more intuitively approachable.    Today Papert's ideas live on in the maker movement, which Papert is generally considered to have fathered (at least conceptually). I care about this movement a lot. Also in my work as a software prototyper, I gravitate to "learning through making", which is the lens through which I view my job.    Throughout the book Papert makes amazing analogies I haven't heard of before:       Analogy of artificial intelligence to artificial flight. We started by imitating birds with flapping machines, but that clearly failed. Instead we had to understand the principles (aerodynamics, bernoulli).     Analogy of samba school to education in general. Samba school is a naturally integrated part of life in Brazil. It's informal, highly intense, sought after. Great model for modern edu?     Analogy of teaching someone to juggle to programming computers. I haven't really thought about this, but maybe this is why so many CS people juggle?     Analogy of the skiing revolution of the 70s to what is happening in computers. One of many changes in skiing style that came about alongside a revolution in ski technology. Did the new skiing technology cause the new style or vice versa? Papert suggests a deep entanglement between the two.       "The most powerful idea is that there are powerful ideas."    Don't Think of an Elephant by George Lakoff (2/5) - February 2017    He's way too simple in his breakdown. The two family views is ridiculous. It's not fucking binary. People have much more granular and specific world views. Cambridge analytics showed that very well with both Brexit and Trump.  Very dated. Lots of 2000s era references that are no longer relevant, like Schwarzenegger. Also a very political message, never giving conservatives a steel man.    I forgot the book on a plane. But I won't be attempting it again despite its short length.    The Futurological Congress by Stanislav Lem (5/5) - January 2017    I'm a huge fan of Lem's work. I'm especially fond of Cyberiad, Star Diaries and Solaris, but it has probably been over a decade since I've read one of his books.    I wanted to get back into Lem, so I asked Marcin for recommendations, and he suggested this one. My interest was piqued since I'm vaguely into rational predictions, but as I read through, I was intrigued and surprised.    In the near future, the world has become politically unstable. A group of futurologists meet at a summit in Costa Rica, and find themselves in the middle of a military coup. To combat unrest, the government has begun experimenting with air- and water-borne mind altering chemicals called Love Thy Neighbor (LTN). As a result, most of the attendees of the congress suffer from hallucinations and increased lovey-doveyness.  Lem's vivid descriptions of the insanity inside the congress hotel are very entertaining.    In an attempt to remain rational and escape the vile chemistry, the main hero, Lem's recurring character Ijon Tichy and a ragtag group of futurologists and hotel workers end up in the basement below the hotel. For a while, Tichy fends off the hallucinogens, but ultimately succumbs to them. The remainder of the book (its majority) takes place in Ijon's disturbed imagination. This is revealed only in retrospect, as Ijon wakes from each dream, finding himself in the basement sewer, back in Costa Rica. Each of Tichy's hallucinations is a different vision of a possible dystopian future, full of hilarity.    In his last and most convincing hallucination, Tichy is cryogenically frozen only to be reanimated three decades later in a society that has completely mastered mind-altering chemistry. Lem (and the translator, who did a fine job) play incessantly with words, inventing at least fourty, all derived from the idea of psychem, drugs that regulate every moment. The world is utopian, but something isn't quite right. Tichy meets up with his (also reanimated) futurologist friend, who gives him a whiff of "up'n'at'm, one of the vigilanimides that can temporarily cancel out the effects of other psychem. Tichy finally sees the world for what it is: a disturbing hellscape populated by cyborg zombies.    Nowhere in the book is it clear whether you are in Tichy's hallucionation or in actual reality. Clearly the book predates many of the many modern Sci Fi staples that toy with the notion of reality, like the Matrix. Overall, it reads super quickly and is incredibly engaging. Despite its humor, this book is quite deep. It explores a lot of the same issues that concern me with VR. Chemistry is just another of virtualizing reality.    On Venus, Have We Got a Rabbi by William Tenn - January 2017    An entertaining short story in the newly formed genre of Jewish Science Fiction. Very memorable characters: the conservative father, his liberal son.    The author (previously unknown to me) does a great job of capturing the essence of Jewishness. The dialog is great, and the content is meaty. The theme of constant repression regardless of circumstances comes up again and again.    The core issue is whether or not an alien race can be considered Jewish. Hopefully this is an issue we can grapple with one day.    Loved the punch line:        I don\u2019t know. Let\u2019s talk about something cheerful. How many people would you   say were killed in that earthquake on Callisto?      Thinking and Deciding (fragments) by Jon Baron (4/5) - January 2017    Jon Baron sent me a fragment of his book on actively open minded thinking. His At the outset, in the first chapter, Jon attempts to formalize what thinking is as a process, and then the ninth, on actively open minded thinking.    I liked how at the outset, author makes a case for the idea that thinking rationally is equivalent to thinking better. Also he does a good job of divorcing rationality from the lack of emotion, which is deeply seated in the popular mind (thanks, Spock!) He then proceeds to drop more interesting ideas. That belief forming involves thinking, for example. But also the idea that doubt and thought are intimately interlinked:        ...but it already shows the main characteristic of thinking. It begins with   doubt.      In the meat of the "What is thinking?" chapter, Jon breaks thinking down into a formal process which he calls "search-inference". In this framework, the thinker searches for goals, possibilities and evidence, and then uses inference to come up with an ultimate judgement. This sounded quite mechanistic to me, and I think easier to illustrate in a drawing than in a mound of text:     TODO: Include my table for decision making.      An attempt to formalize my decision making process is not new to me. Before joining Google, I considered doing a startup around the idea of automatically creating personalized product comparison tables, which we toyed with in our crowdsourcing research CrowdForge.     TODO: Include the CrowdForge table.      As I was drawing and refining this sketch, I was reminded of decision matrices. And I even formulated my current career decision using the help of a spreadsheet, which is intended to be easily human-writeable (by me, mostly), and ultimately reduces to a decision matrix under-the-hood.     TODO: Include my decision spreadsheet.      The structure of this spreadsheet is simple: columns are possibilities, rows are goals. Each goal has a weight (1 through 5). The higher the weight, the more important it is for you to achieve that goal. Evidence goes into the cells of the table. At each cell (goal, possibility), you provide a description of evidence in favor or against, and mark it with a weight (-2 through 2). Negative weights are cons, positive weights are pros. Ultimately, the numbers you provide are reduced to a decision matrix, where each cell corresponds to a value between 0 and 1. The possibilities are then scored and the best course of action is the one with the highest score.    Despite the mechanistic appearance of this approach, Baron emphasizes the nonlinearity of the process. You can always find additional goals, possibilities, evidence, etc. In fact, as you do the exercise (whether in your mind, on paper, or with a spreadsheet), you actively engage in search. Once you're done with inference, you can go back and search some more.    In defense of the tabular approach      Consolidate all of the different considerations in one place. Decisions are stressful.   The model encourages more than two possibilities, which might help with the tendency for binary thinking.      Thinkos: inevitable irrationality    People aren't perfect, and neither is our thinking. Biases are sort of like thinking bugs. Baron suggests that certain things that can improve thinking and reduce bias.  These are broadly described as "active open mindedness":      Look for more alternative possibilities, impartially.   Formulate goals better (eg. "protect walls from child's scribbling" vs "prevent child from scribbling on walls").   Look for counterevidence for everything.   Entertain all possibilities, not just initial hunch.   Allocate time that is proportionate to the importance of the decision.   Avoid explaining away evidence that you don't like.   Avoid belief overkill, which happens when there is a strong correlation between different goals (eg. most people are against capital punishment because it is both ineffective and immoral, whereas those for capital punishment are in favor because it's effective and moral. But why do both go together? They are unrelated)      He is especially concerned with what he calls "myside bias", which is also known as confirmation bias, and the related problem of irrational belief persistence.    Changing your mind is difficult, and two sided thinking is better than one sided thinking. (I guess three sided thinking is even better? "Think like an octopus" On one hand, ... on the other, ..., on the third, ..., on the ninth...!")      It can be seen as a weakness.   Loyalty tends to encourage one-sided thinking.      Social decision making    Difficult decisions would benefit from an outside view. Returning for a moment to the CrowdForge angle, I wonder if the social aspect of this framework has merits. Imagine sending your decision spreadsheet to a friend (or stranger) for honest feedback. They could catch your biases and encourage you to search for more possibilities, see if you have other goals, and to question your evidence.    Some ideas for how a social angle could help enforce some of the thinkos discussed above:      Have peers encourage more possibility search   Have peers suggest better formulations for goals   Question your evidence, provide additional evidence you didn't think of.      Random    I liked Baron's idea that memory can be used in conjunction with external aids as a way of remembering more.        I rely on my memory as well, including my memory of how to use [the library]   and of who is likely to be able to help with what.      Fascinated by ample evidence suggesting that neutral evidence actually increases polarization. Makes sense it retrospect, but I guess that's hindsight bias.    Interesting distinction/continuum between advocate (who is permitted and encouraged to be one-sided), and actively open minded thinker. Notably, expertise here is orthogonal to good thinking. In fact, often contradictory.        Because experts /know/ the answer to most questions, they usually do not have   to consider alternatives or counterevidence.      Also interesting is that wishful thinking is sometimes rational. For example if you are captured by terrorists, the idea that you might survive is probably beneficial, if unlikely.    Interesting view of "controversial":        Many controversial issues are considered controversial because there are good   arguments on both sides. A rational decision would involve balancing the   arguments in a quantitative way, a way that takes into account their relative   strengths.      Superforecasters by Phil Tetlock (5/5) - January 2017    Good vs bad experts is apparently thing. I wondered: was there enough data to really know? What if everyone is a bad forecaster in aggregate? Luckily Tetlock addresses it later.    How to superforecast:      Consider the outside view (eg. base rates)   Consider many perspectives on the same issue (eg. on one hand, on the other hand, on the third hand, on the nth hand).   Fermi problem style breakdown   Might benefit from explicit uncertainty tracking software. It's apparently  a genre .   I'm most interested in  guesstimate    Include opinions of others. They are valuable data.      Also interesting is the personality trait of being open to new experiences. And curiosity in general.    Active open mindedness. Seek out dissenting opinions.    Debaters is an instance of active open mindedness too - AOM    "Beliefs are hypotheses to be tested, not treasures to be guarded"    Interesting idea: adversarial collaboration. For example, Kahneman &amp; Klein, usually at odds with one another, still managed to publish an influential paper: http://www.fiddlemath.net/stuff/conditions-for-intuitive-expertise.pdf    Kahneman sneaking in: suggesting that supreforecasters are better at "internalizing" system 2 thinking via system 1.    Tetlock closes with a summary of the book in the form of ten commandments:      Triage. Focus on questions where extra attention is likely to improve accuracy. Not too hard, not too easy. Weigh the consequences of wasting time on an unpredictable event vs failing to predict a predictable event before deciding whether to invest effort on a problem.   Break seemingly intractable problems into tractable sub-problems ie use Fermi estimates.   Strike the right balance between the inside and outside views.   Strike the right balance between under- and over-reacting to new evidence.   Look for the clashing causal forces at work in each problem. Weigh all the perspectives.   Strive to distinguish as many degrees of doubt as the problem permits, but no more ie get comfortable using fine-grained estimates for fine-grained problems.   Strike the right balance between under- and over-confidence, between prudence and decisiveness.   Look for the errors behind your mistakes but beware of rearview-mirror hindsight biases. Own your failures!   Bring out the best in others and let others bring out the best in you. Team dynamics matter as much as team composition.   Master the error-balancing bicycle. These commandments are not enough - you need deep, deliberative practice.   Don\u2019t treat commandments as commandments.      Overall a great book, but I'm left feeling a bit unsatisfied. Another random thought: everything I've read this year is basically like "foxes &gt;&gt; hedgehogs". But surely there must be some good arguments in defense of hedgehogs.    Why Teach Thinking (Essay) by Jonathan Baron (?/5) - December 2016    I learned about J. Baron from reading Superforecasters, where he was cited in the context of his notion of "active open mindedness". I got so excited that I emailed him, and was very happy ( squee ) to receive a response within a day. My favorite part of his response (I indicated that I was a Haidt fan):        I have small disagreements with almost everyone, even myself.      First half: focus on systematic thinking mistakes people make. Two broad categories: "myside" bias (very similar to confirmation bias) and over-reliance on broken heuristics. Not really new but definitely insightful and quite early (written in 1992).    Interesting studies cited: - Thinking of reasons on the other side reduced inappropriate extreme confidence in answers to objective questions (Koriat, Lichtenstein and Fischoff, 1980)       Being provided with evidence against your point of view can make you more convinced of your position (Batson, 1975).      (eg. Striking example involving a group of girls who were convinced of the divinity of Christ became more convinced as a result of the dead sea scrolls which counter the virgin birth and resurrection.)       Some people are convinced that one-sided thinking is actually better than two sided thinking (Baron, 1989), even when the one-sided thinker favors the position opposite of their own.       Overall interesting combination of prescience and datedness. For example:        The impetus for teaching thinking in the USA comes from many desires: beating   the Japanese in commerce; having demagogue-proof citizens; having more   creativity in the popular arts; etc.      Second half: much more focused on expert opinions, and how to evaluate them. A lot more focus on education. Cites Piaget, etc. He sort of lost me and I didn't really see how the first half related.       Update (Dec 2016): After another exchange with Baron, I indicated that I wasn't sold on the expert part of his essay. His response: I missed the point! Quoting Jon:        That last part is the important part. It argues that UNDERSTANDING AOT is just   as important as being able to do it. We need to depend on others, so we need to   know who we can trust. The basic idea was stated by J.S. Mill in "On Liberty":              The whole strength and value, then, of human judgment, depending on the one     property, that it can be set right when it is wrong, reliance can be placed on     it only when the means of setting it right are kept constantly at hand. In the     case of any person whose judgment is really deserving of confidence, how has it     become so?  Because he has kept his mind open to criticism of his opinions and     conduct. Because it has been his practice to listen to all that could be said     against him; to profit by as much of it as was just, and expound to himself, and     upon occasion to others, the fallacy of what was fallacious.             Update (Jan 2017): I read a fragment of a book on thinking that he sent me. Two chapters are relevant: the first, where he attempts to formalize what thinking is as a process, and then the ninth, on actively open minded thinking.    Brief Wondrous Life of Oscar Wao (4/5) - November 2016    Liked:      Copious references to nerd culture: nerdy books (LOTR, Dune, Lovecraft, Asimov, Heinlein), video games, comic books, the 1980s.   Magical realism aspects: The Mongoose, cartoonish villans (The Captain, Trujillo) and heroes (Beli, The Doctor).   Harkening to 100 Years of Solitude in the hispanic family epic.   Nonlinear time, with the story flashing generations back.   Oscar's relatable innocence, goofy character, purity of spirit.   Interesting autobiographical narrator.   Unexpected turns. After a slow start, you are taken into a deep rabbit hole of an incredibly turbulent family story. Things get weirder and weirder and more and more depressing. Fuku!   Also, a really interesting look into Dominican Republic, and the political turmoil it went through in the 20th century.   Fun to try to decipher the Spanish only having a rudimentary knowledge of the language!   Very distinct and conversational tone.      Disliked:      A little too sappy and charicaturey. Oscar's self-loathing is comical, and it's hard to really empathize with the guy.      Future Babble (4/5, audio) - October 2016    The best predictors are foxes, but even they do poorly. Interesting weakness: foxes tend to be more swayed by scenario planning. Because they start vividly believing all of the possible scenarios, so even unlikely ones become more possible. Also interesting is that popular pundits tend to be hedgehogs because they are prone to more extreme predictions that the audience likes.    Lots of good interesting failed predictions, just like I asked!     Anyone have good recommendations for books/articles/lists on predictions from the past? Preferably failed ones :) &mdash; Boris Smus (@borismus)  October 11, 2016         Prediction is fundamentally hard because of chaos.    Good predictors: - Peter Schiff predicting housing crash and recession. - Ross Anderson predicting y2k mildness. - Alan Barnes, pioneering work around predictions for Canada. - James Fallows,  essay     Bad predictors: - David Brooks predicting nonsense. - Paul Erlich predicting ridiculousness in End of Affluence and Population Bomb - F. E. Smith,  The World in 2030  - Arthur Clark: profiles of the future 1962    Idea: correlation between difficulty of times and likelihood to turn to superstition and religion. I should keep that in mind in the future.  Russian superstition  and magical thinking is not caused by leading a happy life. I should be kinder to those afflicted.    Interesting conclusion: accurate predictions aren't necessary for good choices. The goal is to make choices that are good in any event. If needed, skew according to probability.    How to predict better:      Be fox like: aggregate different sources.   Meta cognition: consider how you arrive at your decisions. Know the biases. But to avoid falling into them is hard! Exercise: enumerate reasons why you might be wrong.   Be humble. Predictions are always uncertain. Predictions too far into the future even more so.      I Am A Strange Loop (2/5, unfinished) - October 2016    I read Godel Escher Bach many years ago and really enjoyed it at the time. When I got my hands on this book, I was excited to read a late Doug Hofstadter work. To my surprise, by the end of Chapter 3, I was done with it. This book had none of the variety of its predecessor, and not much in terms of new ideas (as far as I can remember). I competely agreed with a goodreads review of it, courtesy of  Zach .     Conceptually, I guess you could say, I enjoyed it, but the presentation - the language of the author, the over-long format, and the strange mixture of hard math and elementary philosophy - diminished and diluted the content to the point that it was barely worth reading. The first problem is Hofstadter\u2019s \u201caww shucks\u201d Uncle Fluffy writing style. His language is so steeped in a fireside chat mentality that the meat of his ideas is completely devoured by his good-natured cleverness.      The Hedgehog and the Fox (4/5) - September 2016     The fox knows many things, but the hedgehog knows one big thing. -- Archilochus      According to Isaiah Berlin, Tolstoy is a natural fox who wanted to be a hedgehog. IB suggests that Tolstoy was too broad minded to be able to fully embrace a single doctrine, which could satisfactorily explain how the world works:     Any comforting theory which attempted to collect, relate, 'synthesize' reveal hidden substrata and concealed inner connections, which, though not apparent to the naked eye, nevertheless guaranteed the unity of all things... [Tolstoy] exploded without difficulty.      At the same time, Tolstoy wanted to be able to find some Einsteinian unified theory of everything (TOE), but was too talented as a critic to be able to do so.     What oppressed Tolstoy most was his lack of positive convictions; and that famous passage in Anna Karenina in which Levin's brother tells him that he- Levin- has no positive beliefs.      This division within himself caused him great pain at the end of his life.    As you can see, the writing here is not flowery, but rather complex and requires a high degree of attention. Unfortunately, this is exactly what I lacked given the circumstances of moving to Seattle.    This short book / essay was supposedly intended as a joke (an elaborate one, at 90 pages). Part of the fun is classifying people into animals, for example, here's IB's take:      Foxes: Herodotus, Aristotle, Montaigne, Erasmus, Moliere, Goethe, Pushkin, Shakespeare, Balzac and Joyce   Hedgehogs: Dante, Plato, Lucretius, Pascal, Hegel, Dostoevsky, Neitzsche, Ibsen and Proust      My conclusion is that Foxes &gt; Hedgehogs. The world really is complex, and a simple unified theory seems very unlikely. Also, I'll have to read more IB, in less turbulent circumstances. And more Tolstoy!    Sapiens (5/5) - September 2016    Yuval Noah Harari presents an outside view of humanity, from the perspective of a complete outsider, an alien with no particular interest in the human species. This is fascinating.    The scope of the book is very broad and often high level, starting with physics and chemistry 13.5 billion years ago at the dawn of the universe. Then, he turns to biology, which begins 3.8 billion years ago on earth, and finally to history and the cognitive revolution which starts about 70,000 years ago. Since then, 12,000 years ago, we had agriculture, which sped up the cognitive revolution. More recently, 500 years ago, the scientific revolution really turned it up a notch. The orders of magnitude of difference are appropriately humbling.    I liked the distinction between objective, subjective, and inter-subjective. The latter meaning something that exists within the communication network linking the subjective consciousness of many individuals. If a single individual changes his or her beliefs, or even dies, it is of little importance.    Also striking was Harari's disdain for the agricultural revolution. He claims that it made most people worse off. The general point being that "history's choices are not made for the benefit of humans".    Another major theme is the unification of humankind, which compounds inter-subjective effects since societies grow bigger and bigger. Over time, cultures coalesce and form bigger and more complex civilisations. Even though today's civilizations seem to be clashing, they actually agree on many things, for example money. "For thousands of years, philosophers, thinkers, and prophets have besmirched money and called it the root of all evil. Be that as it may, money is also the apogee of human tolerance."    On religion, Harari has insights as well:     So, monotheism explains order, but is mystified by evil. Dualism explains evil, but is puzzled by order. There is one logical way of solving the riddle: to argue that there is a single omnipotent God who created the entire universe - and He's evil. But nobody in history has had the stomach for such a belief.      Also, interestingly, he considers most "isms" to be religions. And this leads to interesting analyses. Harari delves deeply into humanism which he breaks down into three kinds:      Liberal humanism: that 'humanity' is a quality of individual humans, and that the liberty of individuals is therefore sacrosanct.   Socialist humanism: hold as sacred not the inner voice of each individual, but the species Homo sapiens as a whole.   Evolutionary humanism: humankind is not something universal and eternal, but rather a mutable species that can evolve or degenerate.      The latter has a long connection to Nazism, but Harari suggests (and I agree) that modern biology is really pushing us back into that vein.    On the scientific revolution, Harari echoes the rational sentiment of accepting ignorance. As soon as you truly accept that you do not know, you leave the door open to new insights.     The Scientific Revolution has not been a revolution of knowledge. It has been above all a revolution of ignorance. The great discovery that launched the Scientific Revolution was the discovery that humans do not know the answers to their most important questions.      Overall, very worth a read (or listen, as I did).    How to actually change your mind by Eliezer Yudkowsky (4/5) - August 2016    Technically the second book in a giant epic entitled "Rationality: From AI to Zombies". The author is a well known rationalist and active member/founder of lesswrong.org, Center for Applied Rationality (CFAR), etc.    Essentially, this is a series of essays written and published as "The sequences", around 2009. They are loosely related and cover a wide array of topics, many of them highlighting irrational modes of thought. Much of the work focuses on biases central to behavioral economics, focusing on Kahneman-style results. But the author goes beyond that, and also introduces a lot of opinion for how a rationalist should behave. At the same time, there is a tendency to be incredibly nerdy, which is alternatingly endearing and borderline autistic. I found myself asking the question: if one becomes a purely rational agent, isn't a computer strictly better? On the path to rationality, what aspects of humanity is worth preserving?    Here's some new stuff I learned. A fair amount of the book covers behavioral economics concepts that I read about already in TF&amp;S.    Litanies    Aumann\u2019s Agreement Theorem suggests that no two rationalists can agree to disagree, given that they have the same information.    Litany of  Gendlin :     What is true is already so. Owning up to it doesn't make it worse. Not being open about it doesn't make it go away. And because it's true, it is what is there to be interacted with. Anything untrue isn't there to be lived. People can stand what is true, for they are already enduring it.      Litany of  Tarski :     If the box contains a diamond, I desire to believe that the box contains a diamond; If the box does not contain a diamond, I desire to believe that the box does not contain a diamond; Let me not become attached to beliefs I may not want.      Against black and white thinking    Yudkowsky is especially effective in his attacks on binary thinking. For example, on partisanship:     Politics is an extension of war by other means. Arguments are soldiers. Once you know which side you're on, you must support all arguments of that side, and attack all arguments that appear to favor the enemy side, otherwise it's like stabbing your soldiers in the back - providing aid and comfort to the enemy. People who would be level-headed about evenhandedly weighing all sides of an issue in their professional life as scientists can suddenly turn into slogan-chanting zombies when there's a Blue or Green position on an issue.      On the tendency and fallacy in binary thought:     There is a natural tendency to treat discussion as a form of combat, an extension of war, a sport; and in sports you only need to keep track of how many points have been scored by each team. There are only two sides, and every point scored against one side is a point in favor of the other. Everyone in the audience keeps a mental running count of how many points each speaker scores against the other. At the end of the debate, the speaker who has scored more points is, obviously, the winner; so everything that speaker says must be true, and everything the loser says must be wrong.      The horns effect - all negative qualities correlate:     Stalin also believed that 2 + 2 = 4. Yet if you defend any statement made by Stalin, even \u201c2 + 2 = 4,\u201d people will see only that you are agreeing with stalin and you must be on his side.      And a very nice summary of a better way of thinking:     Not all arguments reduce to mere up or down. Lady Rationality carries a notebook, wherein she writes down all the facts that aren\u2019t on anyone\u2019s side.      Real belief vs. belief in belief:     Roger Zelazny once distinguished between \u201cwanting to be an author\u201d versus \u201cwanting to write.\u201d Mark Twain said: \u201cA classic is something that everyone wants to have read and no one wants to read.\u201d Criticizing yourself from a sense of duty leaves you wanting to have investigated, so that you\u2019ll be able to say afterward that your faith is not blind. This is not the same as wanting to investigate.      Assume your interlocutor is good    "To argue against an idea honestly, you should argue against the best arguments of the strongest advocates". This, and the closely related concept of the principle of charity aka Steelmanning (the opposite of strawmanning), which I heard from a Sam Harris interview, sent me on a long reading tangent of arguments  for  and  against . Insightful tidbit from that last link:     First, seek to understand the actual viewpoints people you disagree with are actually advocating. Second, seek out intelligent and well-informed advocates of viewpoints you disagree with. You don\u2019t have to make up what your opponents believe! As it happens, you have many smart opponents! Third, whenever possible, try to switch conversations from a debate focus to a collaborative truth-seeking focus.      Back to Big Yud. Some wisdom on focusing on the argument, not on the person:     Someone once said "Not all conservatives are stupid, but most stupid people are conservatives". If you cannot place yourself in a state of mind where this statement, true or false, seems completely irrelevant as a critique of conservatism, you are not ready to think rationally about politics.      A variation on the  reasonable person principle  (harkens back to my time at CMU).     To understand why people act the way they do, we must first realize that everyone sees themselves as behaving normally. Don\u2019t ask what strange, mutant disposition they were born with, which directly corresponds to their surface behavior. Rather, ask what situations people see themselves as being in. [...] Realistically, most people don\u2019t construct their life stories with themselves as the villains.      Deciding which side to argue    Great distinction between rationality and rationalization. Very related to Haidt's position that beliefs are intuitive but their defence is rational. But Haidt makes no distinction like this. Would love to hear his thoughts on it.     Rationality is not for winning debates, it is for deciding which side to join. If you\u2019ve already decided which side to argue for, the work of rationality is done within you, whether well or poorly. But how can you, yourself, decide which side to argue?      Eliezer suggests enumerating the evidence: "Lady Rationality carries a notebook, wherein she writes down all the facts that aren\u2019t on anyone\u2019s side". Here's how to construct an honest ultrarational argument for a particular political candidate:      Gather all evidence about the different candidates   Make a checklist which you will use to decide which candidate is best   Process the checklist   Go to the winning candidate   Offer to become their campaign manager   Use the checklist as the campaign literature      The future is hard to predict    Herd instinct in venture capitalism:     The majority of venture capitalists at any given time are all chasing the same Revolutionary Innovation, and it\u2019s the Revolutionary Innovation that IPO\u2019d six months ago. This is an especially crushing observation in venture capital, because there\u2019s a direct economic motive to not follow the herd.      And what to do about it. DFJ (a VC) has a rule  favoring a passionate minority  to outweigh a negative majority.  This also reminds me of the  Tenth Man Rule .     Only two partners need to agree in order to fund any startup up to $1.5 million. And if all the partners agree that something sounds like a good idea, they won\u2019t do it.      Movies and books have a huge effect on the human psyche. This will probably compound with more immersive storytelling mediums:     So far as I can tell, few movie viewers act as if they have directly observed Earth\u2019s future. [...] But those who commit the fallacy seem to act as if they had seen the movie events occurring on some other planet; not Earth, but somewhere similar to Earth.      Predicting numbers is especially difficult:     I observe that many futuristic predictions are, likewise, best considered as attitude expressions. Take the question, \u201cHow long will it be until we have human-level AI?\u201d The responses I\u2019ve seen to this are all over the map.      Avoid having THE Great Idea and get granular and specific    Avoiding partisanship by focusing on the minimum viable argument, reminds me of Sunstein's  Judicial minimalism :     But try to resist getting in those good, solid digs if you can possibly avoid it. If your topic legitimately relates to attempts to ban evolution in school curricula, then go ahead and talk about it\u2014but don\u2019t blame it explicitly on the whole Republican Party; some of your readers may be Republicans.      Avoid overly large uhh Thingies, and chop them up.     Cut up your Great Thingy into smaller independent ideas and treat them as independent. For instance, a marxist would cut up Marx's Great Thingy into theories of 1) value of labor 2) political relations between classes 3) wages 4) the ultimate political state of mankind.      Other interesting stuff    Taber and Lodge's  "Motivated skepticism in the evaluation of political beliefs"  describes six predictions which are very Haidt-y. It's a list of political thinkos that are driven by behavioral economic biases.    Beliefs don't need to be completely bullet proof. But this contradicts science, where a single counter example can topple a theory.     A probabilistic model can take a hit or two, and still survive, so long as the hits don\u2019t keep on coming in. Yet it is widely believed, especially in the court of public opinion, that a true theory can have no failures and a false theory no successes.      On the uselessness of "Deep Wisdom":     Surely the wisest of all human beings are the New Age gurus who say, \u201cEverything is connected to everything else.\u201d If you ever say this aloud, you should pause, so that everyone can absorb the sheer shock of this Deep Wisdom. There is a trivial mapping between a graph and its complement. A fully connected graph, with an edge between every two vertices, conveys the same amount of information as a graph with no edges at all.      There's a distinction between Traditional rationalism and Bayesian rationalism. And I worry that the Bayesian variety, which Eliezer is a subscriber of, is a sort of hedgehogginess: a very focused and blindered approach. But I liked the idea that you can go beyond falsification, the ability to relinquish an initial opinion when confronted by clear evidence against it.     I suspect that a more powerful (and more difficult) method is to hold off on thinking of an answer. To suspend, draw out, that tiny moment when we can\u2019t yet guess what our answer will be; thus giving our intelligence a longer time in which to act. Even half a minute would be an improvement over half a second.      "Make America Great Again":     A key component of a zeitgeist is whether it locates its ideals in its future or its past. Nearly all cultures before the Enlightenment believed in a Fall from Grace \u2013 that things had once been perfect in the distant past, but then catastrophe had struck, and everything had slowly run downhill since then.      Exploring the World of Lucid Dreaming (half way), (3/5) - August 2016    I bought this book 10 years ago, and it just sat on my shelf ever since. Not sure how I got into it in the first place, but my interest in lucid dreaming has increased as a result of my work on VR. As I delved into questions intimately tied to human perception, I've become more and more interested in the inner workings of our mind.    VR is sensory replacement, AR is sensory augmentation, but dreaming is sensory deprivation. Yet the latter can generate completely fascinating experiences never possible in a sense-rich environment.    I read the first half of the book, and realized that the thing I'm actually interested in is not necessarily crafting the perfect dream, solving problems in my dreams, etc. But a more introspective pursuit, focused on trying to understand my own dreams. Which dovetails nicely given that the first step of lucid dreaming is reliable dream recall. So, while I'm unlikely to become an oneironaut, at the very least I've started a handwritten dream journal, which I keep on my bedside table.    The Persuaders (3/5) - July 2016    Rationality used to pervade English scene. Now no appeal to reason is even attempted.    Mostly arguments today are adversarial. In the past it was possible to have people with differing opinions not to resort to personal attacks. Appeal to reason hasn't always been the MO. It comes and goes and the author claims that were in an especially reason free era.     How crazy PR is : Nice case study looking into the political journalistic and other heavy machinery that is used to sway people. Particularly at the first Iraq war (about Kuwait), and swaying public opinion to the side of the US. Especially fascinating is the "Naira" girl who cried wolf, who claimed to be a Kuwaiti refugee suffering in Iraqi camps, but was actually a member of the Kuwaiti royal family. Especially interesting is the chain of money and influence and PR firms that takes that sort of inflammatory material and inject it into peoples' minds.  Remember "Free Kuwait" t-shirts? Courtesy of Hill and Nolton, a PR firm which was fed this information.    Astroturfing - creating fake grassroots movements that are actually sponsored by some corporation    Logical follow up or supplement to The Century of the Self.      Lebeau: the crowd is lacking reason, influenced by emotion, images.   Trotters: herd instinct, and group identity is key.   Lippmann: anti democratic conclusion that experts should run the show, not unwashed masses. regain critical faculties by understanding how PR works. also regulatory moves to make public deception illegal.       Soft vs. hard nudging . fundamentally people don't always know what is best for them, and government can step in. seatbelt laws force you to wear seat belts always (hard paternalism). GPS navigation systems don't force you to take their route, cigarette packs with graphic images: soft paternalism.     Shoving will backfire : if people feel nudged, they nudge back. U.K. had a test which gave job seekers a strength profile that was bogus. OIRA branch of the US and sunstein. what does the most good and least harm? regulations like showing calorie counts in fast food, environmental friendliness of consumer products, hidden costs in air tickets and credit card fees. good defaults matter too.     Reason &gt; nudging : reason based decisions will be more generally effective. rather than appealing to some auto response, if you can reach the underlying understanding, you're better off.    Some weaknesses: some parts, especially the ad related section, lists a lot of studies, but not the strengths of the effect. Also, largely a derivative work, mainly a summarization of other works: "Thinking Fast and Slow", "The Hidden Persuaders", "Orality and Literacy".    Strongest part: chapter 5 about politics, etc.     Efficacy of arguments : if dealing with people that agree already (eg. DNC to Democrats), provide a one-sided argument. Otherwise, go both ways. Discrediting the other POV is effective.    Garvey calls for a kind of thoughtful anger about persuasion. I concur, and this is what I'm channeling into Catma, for better or worse.    The Righteous Mind - July 2016 (5/5)    I discovered The Righteous Mind via  Jon Haidt's conversation with Sam Harris . I read it while on our Honeymoon, while Sarah was enjoying another book about morality. It was very romantic! Anyway, some thoughts on the book:    Haidt presents his argument in three parts. It's well structured, and super well written:      Intuitions come first, strategic reasoning second (ie. people's morality is driven by intuition/gut reactions, and then they are really good at justifying their position through reasoning)   There's more to morality than harm and fairness (ie. conservatives care about many other things: authority, loyalty, sanctity -- this is the foundation of Moral Foundation Theory)   Morality binds and blinds (ie. arguments around group selection, social cohesion and other benefits of religion and partisanship)       Not moral relativism . In addition to borrowing from anthropology, Haidt recounts a personal story of his visit to India, and how initial visceral discomfort eventually transformed into some understanding of the cultural context he found himself in:     Understanding the simple fact that morality differs around the world, and even within societies, is the first step toward understanding your righteous mind.      I don't think that this is moral relativism, but moral descriptivism. Haidt does not claim that morality  ought  to differ around the world, simply that it does differ.     Nice analogies : I found the analogy of a small rider (the rational) riding a large elephant (the intuitive) apt and useful. Sort of like a lizard brain idea.      The elephant is far more powerful than the rider, but it is not an absolute dictator.  Under normal circumstances the rider takes its cue from the elephant, just as a lawyer takes instructions from a client. But if you force the two to sit around and chat for a few minutes, the elephant actually opens up to advice from the rider.       Against rationalism : Haidt presents an interesting argument against moral rationalism. Firstly, he makes a compelling case for religion as something that increases social capital via group selection, arguing against the New Atheist view that religion is evil, period, with no redeeming qualities.    Haidt argues for the Humeian view of morality, and against Bentham's utilitarian and Kant's deontological positions:     As Western societies became more educated, industrialized, rich, and democratic, the minds of its intellectuals changed. They became more analytic and less holistic.26 Utilitarianism and deontology became far more appealing to ethicists than Hume\u2019s messy, pluralist, sentimentalist approach.      Haidt finds support for Hume's position, which is that intuition (or sentiment) comes first, and then reason tries really hard to come up with arguments to support that position. Haidt's subjects were     Morally dumbfounded \u2014 rendered speechless by their inability to explain verbally what they knew intuitively. These subjects were reasoning. They were working quite hard at reasoning. But it was not reasoning in search of truth; it was reasoning in support of their emotional reactions.       On changing minds :     The main way that we change our minds on moral issues is by interacting with other people.  If you can have at least one friendly interaction with a member of the \u201cother\u201d group, you\u2019ll find it far easier to listen to what they\u2019re saying, and maybe even see a controversial issue in a new light. You may not agree, but you\u2019ll probably shift from Manichaean disagreement to a more respectful and constructive yin-yang disagreement      He presents a framework for how this can happen:          Intuitions come first and reasoning is usually produced after a judgment is made, in order to influence other people. But as a discussion progresses, we sometimes change our intuitions and judgements.       Against Rawls :     Beware of anyone who insists that there is one true morality for all people, times, and places\u2014particularly if that morality is founded upon a single moral foundation. Human societies are complex; their needs and challenges are variable.       Salient critiques of the left :     If you are trying to change an organization or a society and you do not consider the effects of your changes on moral capital, you\u2019re asking for trouble. This, I believe, is the fundamental blind spot of the left. It explains why liberal reforms so often backfire, and why communist revolutions usually end up in despotism.       Conservative thinkers , like Frederich Hayek, Thomas Sowell, Jerry Muller, and Edmond Burke.  I ought to read them! Also, distinctions between conservatism and orthodoxy:     Christians who look to the Bible as a guide for legislation, like Muslims who want to live under sharia, are examples of orthodoxy. They want their society to match an externally ordained moral order, so they advocate change, sometimes radical change.  This can put them at odds with true conservatives, who see radical change as dangerous      2016 is a very confusing year. The media appears to have completely lost its ability to reason about or predict future political outcomes. Trump's nomination, Brexit, the rise of populism all over the world, appears mysterious and scary. In my left coast circles, the right is generally portrayed as either rich and evil, or poor and dumb.    The Righteous Mind provides a more convincing explanation for contemporary political divisions around differences in morality. Rather than just identifying low intellect, sheepishness, or fascist tendencies with right leaning populists, Haidt offers a useful framework for understanding the conservative world view. This book is a must read for any political observer with any hope for understanding.    Must Mankind Repeat History's Great Mistakes (Audio) - July 2016 (3/5)    Nice insight and guiding theme: international politics is complicated because there isn't really an organization that stands above all nations. Unlike other human endeavors, if a rogue entity misbehaves, there isn't really anyone to go to by default. Each country has its own skin to worry about, and they need to figure out the best strategy.    I liked the concept of the balance of power, presented in 3 versions by the lecturer:      Simply the distribution of existing power.   The state in which all parties in question have equal power   A tendency for new power entering a conflict to join the weak side in order to avoid a monopoly of power.      After one or two theoretical lectures, the course ventured into territory of WW1, the interwar years, and WW2. I found this part to be somewhat less engaging since most of the historical details were review for me, and little of the theory introduced at the beginning was brought into the fold. The last lecture was more interesting, since it tried to apply the theory introduced at the beginning to predict the future. What made it even more interesting is the fact that the lecture was recorded in the early 90s.    The lecturer wasn't very engaging, and I found myself listening to a lot of the lectures without getting much out of them. Additionally, the volume was low and the recording was poor quality, which contributed to my meta objections.    The man who loved only numbers (4/5) - July 2016    I really loved this one. It captures perfectly the quirkiness of Erdos, and his mathematician circles. Much more than a biography, this book popularizes a lot of incredibly interesting mathematics. It took me back 10 years to my math undergrad days.    The author does an incredible job of capturing the essence of many complex mathematical concepts in appealing and entertaining ways. It's instructive to compare the way this book tackles certain topics to the corresponding Wikipedia page, which is completely undecipherable by a non-mathematician, and often even by a mathematician that doesn't work in that domain. That said, I'm  not sure that this book would be as appreciated by someone without a background in math. But it may serve as inspiration for a motivated, mathematically-inclined high school student. I'll find out!    Despite having spent 4 years taking a bunch of math courses,  I learned a lot of new math from this book, and was reminded by some favorites.    Ramsey Theory: the inevitability of order in large quantities. We barely touched on Ramsey theory in 4th year Graph Theory, and it seemed really obscure. Not true! Draw any 5 points on a plane, and as long as 3 of them don't form a line, you are guaranteed to be able to form a convex quad using them as vertices. Take any sequence of 101 numbers, and you're guaranteed to find a subsequence of at least 10 increasing numbers.    Infinite series: a technical subject, but some really incredible results, like Taylor expansions of e (sum of 1/k!), pi (6 times sum of 1/k^2).    Cantor's analysis of various infinities (alpha numbers): firstly the beautiful argument about the countability of rational numbers, and then the famous diagonalization argument which proves that the set of real numbers is a bigger infinity.    Mother of All Demos - June 2016    http://web.stanford.edu/dept/SUL/library/extra4/sloan/mousesite/1968Demo.html    As a Human Computer Interaction nerd, I feel the need to pay homage to history of the field. One glaring omission in my education is that I haven't fully watched "The Demo" where Douglas Englebart shows off the state of the art of "Man-Computer Interaction" circa 1968. The subject of the demo is NLS (Online system), which is intended as a synthesis of a bunch of ideas coming from SRI. It's quite impressive how far we've come, but also how many elements are still recognizable, and in some cases impressively ahead of their time.    The first ten minutes introduce a series of key concepts that are staples of desktop computing: the mouse and keyboard, mouse pointers, copy/paste, saving and loading files, numbered lists. Less mainstream, VI-like features are also introduced, such as reorderable, heirarchical lists, and folding sections. Much of what goes on under the hood in NLS reminded me of VIM script and other similar DSLs. Big focus on expert-oriented power usage.    The Demo heavily alludes to the web, with a lot of provisions for interlinking, cross referencing, and even an explicit mention of ARPA close to the end of the demo (Clip 33), which reminds the viewer that this is all pre-Intenet. In contrast, their (presumably telephone-based) video calling was quite impressive. The quality of the remote speaker seemed to be about as good (bad?) as Englebart's. Some collaborative features were quite impressive, with a real-time cursor sharing scheme which was decades ahead of its time. Others were less interesting, like leaving a message for other people in a file to "get a response within minutes".    Some time is spent showing off hardware. Most interestingly is the  Keyset , which was a 5-key ancestor of the chorded keyboard, where each key combination (31 in total) led to a character, and was intended as a one handed keyboard. Also interesting as a point of comparison was the cutting edge display tech of the time: black and white CRTs with 15 Hz refresh rates and super high (3 frame) persistence.    The philosophy of the lab behind MOAD was close to my heart, and lives on in R&amp;D groups I've had the pleasure of being part of:      Build and try: big focus on prototyping!   Evolutionary: make incremental improvements to a real system, not grand visions.   Eat your own dogfood: use the thing you build, and test it on yourself.      The One Minute Manager - June 2016 (4/5)    This little book was recommended to me by a ski instructor after a ski lesson at Squaw.  The man was probably in his 70s, but skied incredibly well, and had a lot of wisdom to pass on. He was tough, but nice, like a One Minute Manager.    More of a pamphlet than a book, One Minute Manager unfolds as a story about a young man wanting to learn to be a good people manager. The story makes the points more vivid, but at its core, there is an interesting and somewhat surprising strategy outlined for being a good hands off manager. The approach reminds me a little bit of Christian's management style. There are three parts:      Set goals and expectations with the employee, write them down succinctly and have them ready for review. (This is similar to Google OKRs, and definitely worthwhile).   Especially early on, look for behaviors that are praise-worthy, and give due praise honestly and directly. (This seems like a great idea to try on interns).   Reprimand unwanted behavior as it happens, but emphasize their worth as people. (This seems quite hard to pull off without being a dick).      Gantenbein by Max Frisch - June 2016 (4/5)    Gantenbein was recommended to me by a close friend, who said it made a really big impact on him. It took me a long time to get through this book, and I often struggled to follow the narrative. The author writes in a very experimental style, alternating between three different men, all of which are in love with Lila, a beautiful stage actress. Events that happen don't seem to have chronology or causation. Sometimes they branch out into multiple futures, other times you reset in another time, another place, and as another person.  The book is a set of sketches around these people, more so than a coherent whole. Adding to the complexity, the narrator himself seems to not be a single person, but alternates between people. And this smoke and mirrors is explicit. The author often writes "I imagine...", and the title is more accurately translated "Let's assume my name is Gantenbein".    I found this book to be a piercing view into human nature. I could relate to much of what the characters within it struggle with, which made reading it difficult and slow. Gantenbein, one of the protagonists, decides to pretend to be blind for his whole life, and builds a relationship with Lila. He wears dark glasses, which conceal his lie from others. But he himself has to be careful not to reveal his secret. He can see, but must pretend that he can't, and this leads to painful allegory on good relationships.    This book was hard to read. I never got into a flow, and ended up reading very unevenly, often wanting to set it down for a while. But some parts I really loved, like when one of the characters attends his own funeral. And I was surprised to, after finishing it, have a feeling that some coherence emerged by the end. My understanding of this book would benefit from a re-reading, but I know that this is unlikely to happen.    Audio lectures: Physics Beyond the Edge - June 2016 (4/5)    An interesting follow up to what I remember from high school &amp; first year physics. My guess is that the subject of the class is too basic for those with a degree in physics, but this is something I should ask Alex! I really liked how all of the topics flowed into each other. I found this to be somewhat surprising given Einstein's failed struggle to find some unified theory of everything. This is probably credit to the lecturer, who, in addition to being a great instructor, also coined the term  qubit .    The structure of the class follows Arthur C. Clark's second law:     The only way of discovering the limits of the possible is to venture a little way past them into the impossible.      The lecturer, Benjamin Schumacher, is quite engaging, and starts by breaking impossibility down in terms of logical, physical and statistical terms. He then goes into thermodynamics, framing the first and second laws in terms of  perpetual motion machines .    I enjoyed the various Demons that show up in this lecture.  Maxwell's demon  can control a flap between two chambers to make sure that eventually all of the molecules end up in one of the chambers (thus decreasing entropy).  Laplace's demon  is able to, from knowing the current state of the world, predict the future perfectly. Both of these thought experiments are forays into the impossible that, when disproven, set the stage for the third law of thermodynamics and chaos / quantum mechanics.    The section on space-time was fascinating, and served as a good introduction to space-time diagrams (aka  Minkowski diagrams ), which I really liked. Although, one thing that was missing is the visual element. Although very lightweight in mathematical treatment, there were clearly some materials that supplemented the lectures, which would have made learning easier. The whole discussion on faster-than-light travel, time travel, and quantum cloning was tied together nicely through the lens of the impossible. By the way, space-time diagrams seem to be the reason for this  observable universe thing .    Which brings us to Quantum. I've been interested in being interested in the topic for a while because it's so unintuitive and strange. Schumacher's overview is good, but I'm obviously still very confused. He outlines basics of Quantum Mechanics:      Wave-particle duality: light has both wave and particle like behaviors.   Uncertainty: position and momentum of a quantum particle cannot both be known.   Tunneling: because of this uncertainty, quantum particles can tunnel through what in newtonian mechanics are inpenetrable barriers.      There seem to be some fundamental truths about symmetry as pertaining to conservation laws.  Noether's theorem  seems remarkably deep but way over my head, which states that every symmetry in physics has some associated conservation law.    I was also somewhat lost in the information theory asides (the lecturer's specialty is information theory). That information theory is a branch of physics at all is mysterious to me. Probably worth learning more about, though, given my information-related profession.    Audio lectures: Great Ideas of Philosophy - May 2016 (5/5)    Really great series of 60 lectures \u2013that's 24 hours worth as an overview of philosophers and their main points. I took very few sporadic notes, as the majority of my listening was done while riding a bicycle. Here are some of my favorite philosophers and their quotes:    Thomas Reid's [principle of credulity][]:     If no proposition that is uttered in discourse would be believed, until it was examined and tried by reason \u2026 most men would be unable to find reasons for believing the thousandth part of what is told them. Such distrust and incredulity would deprive us of the greatest benefits of society.       Wittgenstein's beetle :     Imagine, he says, that everyone has a small box in which they keep a beetle. However, no one is allowed to look in anyone else\u2019s box, only in their own. Over time, people talk about what is in their boxes and the word \u201cbeetle\u201d comes to stand for what is in everyone\u2019s box.      Everything about  Socrates . This is a treasure trove to be revisited.     The unexamined life is not worth living.      The stoic mindset, especially this quote attributed to  Epictetus :     Never say of anything, "I have lost it"; but, "I have returned it."      Kant's [Categorical Imperative][kant], in a few formulations:     Act only according to that maxim whereby you can at the same time will that it should become a universal law without contradiction.      ...and     Act in such a way that you treat humanity, whether in your own person or in the person of any other, never merely as a means to an end, but always at the same time as an end.      and...     Therefore, every rational being must so act as if he were through his maxim always a legislating member in the universal kingdom of ends.      And closely related, Rawls'  Original Position , although the lecturer  Daniel N. Robinson  is surprisingly dismissive of his ideas.    Karl Popper's ideas around  falsification :     Falsifiability or refutability of a statement, hypothesis, or theory is the inherent possibility that it can be proven false. A statement is called falsifiable if it is possible to conceive of an observation or an argument which negates the statement in question. In this sense, falsify is synonymous with nullify, meaning to invalidate or "show to be false".      The debate between realism and anti-realism: whether or not scientific theory describes what the world is actually like, or if it just serves as a nice model. A succinct summary of the two positions:     Realists see scientific inquiry as discovery while anti-realists sees it as invention.      Clarence Irving Lewis and the idea of qualia, especially well illustrated in  "Mary's room" , which produces an interesting anti-materialist argument:     The sensation of color cannot be accounted for by the physicist's objective picture of light-waves. Could the physiologist account for it, if he had fuller knowledge than he has of the processes in the retina and the nervous processes set up by them in the optical nerve bundles and in the brain? I do not think so.      After nearly 24 hours of lecturing, Daniel synthesizes a lot of philosophical thought into what constitutes a good life:      Fatalism: being OK with your fate.   Hedonism: the good kind, aiming to increase long term pleasure.   Selflessness: benevolent, philanthropic behavior   Activity: don't become a brain in a vat :)      How to fail at almost everything and still win big by Scott Adams - April 2016 (1/5)    Let's be clear: this is a self help book. I somehow got suckered into reading it because I liked one of his blog posts. There are very few interesting or original things in the book, and the author comes off as brash and trying too hard to be funny. That said, I liked some ideas:       Systems vs goals. Focusing on achievement leads to emptiness after the fact. Instead focus on a system and process.      Simplifiers vs optimizers is an interesting dichotomy. Usually it's about optimizers vs satisficers but in this case I can see myself as a simplifier.      Don't listen to friends and family feedback on business ideas. Listen to the market.        Most of the rest of the book can be reduced to common sense: eat well, exercise daily. Some of Scott's specific advice is really uninspired and downright depressing. He insists that you should learn to play golf, learn a second language, how to use proper English grammar, how to be outgoing, a good conversationalist and a public speaker. He devotes a chapter of the book to learning a special command voice in order to manipulate people better. Move along!    art of tidying by Marie Kondo - April 2016 (3/5)    An endearing manual for tidying up. I'm naturally inclined to minimalism as it stands and did a massive cleanup of my old room in my parents house just last month so consider myself proficient. But Marie takes it to the next level, revealing an autistic inclination towards the personification of inanimate objects.    This manifesto can easily be reduced to an illustrated one pager, but Marie insists on treading over the same ground multiple times.      Discard everything you don't need by determining if it sparks joy. Go by category, starting with clothes, moving on to books and personal items.   Once you've donated garbage bags full of items, organize them so that everything you own has a place. Stand everything including clothes vertically.      By the end if the book I was curious more about Marie's eccentricity than her organizational method.    The Moral Landscape by Sam Harris - March 2016 (4/5)    In many ways a rehashing of many ideas in philosophy, I think Sam did a good job of bringing some longstanding ideas and debates to the fore without requiring a degree in philosophy. The section on religion can be skipped, since he rehashes old theses from End of Faith (oh yeah, hobby horse!)    The overall thesis is that the well being of conscious entities is of supreme importance, and that morality is all about optimizing that. Intuitively, I agree that this is a good, pragmatic, and secular definition. I am less interested in the philosophical debate around it, but I don't think Sam adequately addresses the other side of the argument adequately. Debates between utilitarian ideas and other branches of philosophy have been raging for centuries, and with all due respect, I don't think Sam is the guy to put them to rest.    Despite well being's illusive nature, Sam draws a parallel to health, which is also difficult to pin down precisely. While the analogy is illustrative, health is easier to define, and has been done out of practical necessity with metrics like QALYs. Though flawed, a QALY-like metric that includes all aspects of well being seems out of reach in 2016.    With that said, Sam hedges heavily against this reality, and repeatedly says that we may not know what the best collective course of action is, but that one (or in fact many) must surely exist. These many local minima are the peaks of The Moral Landscape.    A Canticle for Leibowitz by Walter m. Miller Jr - February 2016 (5/5)    Amazing.    Q masterpiece in three parts. The setting is post apocalyptic earth after a mutually assured nuclear destruction scenario occurred in the 20th century.    While most of humanity has undergone the simplification, a small order of Christian monks is dedicated to preserving human civilization. Their patron saint, St. Leibowitz, an engineer from before The Deluge. After a bomb shelter with his blueprint is discovered by a primitive monk, a religious scandal unfolds, reminiscent of what might have happened in the dark ages to Galileo.    Many centuries later the blueprint is finally understood by a scientist frustrated by a lack of truly novel discoveries. And an engineer from the same order builds it. It is a simple electrical generator which powersq a lamp.    Eventually humanity returns to its pre Deluge level of technology and surpasses it, achieving the ability to travel to exoplanets. But political strife is once again at an alarming level. And as earth nears nuclear annihilation again, the order of leibowitz prepares to preserve the blueprints and other critical documents and send them, along with a small order of monks, to another planet for resettlement.    The characters are vividly painted, and the historical projection of the dark ages onto a modern technological background is fascinating and masterfully crafted. The book serves as a stark reminder that there is no natural progression toward better  times, and just how fragile our current paradise is.    The threat of nuclear war may feel like a bygone but I'm quite concerned about it still.    In Praise of Idleness and Other Essays by Bertrand Russell (4/5) - December 2015    Fascinating collection of essays. In "In praise of idleness", Russell in 1930 predicts that automation will reduce the working day drastically. If your quota of 1000 buttons can be fulfilled in 1/10th the time, you should have a lot more time for pursuing your hobbies. Unfortunately this never happened.    In one of the essays, he declares himself as a socialist while simultaneously distancing from marx, and Russian communism. Essentially he lays groundwork for social democracy. His views on socialism are very reasonable and still relevant today.    Gateway by Frederik Pohl (4/5) - December 2015    Humanity discovers an advanced civilization, which has become extinct, but has left a mysterious Gateway in our solar system. Gateway is a small world which houses many ships capable of taking adventurers on missions to undisclosed locations. The controls are cryptic, but slowly residents of gateways gain a better understanding of how they work. Gateway is controlled by a conspicuously evil trans-national corporation, which pays prospectors to take dangerous trips on these ships with the  in hopes of discovery. They pay out lucrative bonuses.    The protagonist, an unsympathetic, and cowardly Robinette (aka Bob), comes to gateway to try to strike it rich. He struggles with his own fears of going on missions, and falls in love with Klara. Their dysfunctional relationship, his troubled childhood, and pathological fear is woven deeply into the structure of the book. Chapters alternate between Bob's memories on Gateway and Bob the rich ex-prospector getting therapy sessions from a sentient Robo-psychoanalyst on Earth. Bob's last mission ends in disaster, as he is separated from his beloved as their ships try to escape from a black hole. Bob returns the sole survivor, gets an inordinate amount of money from the corporation, but remains forever unhappy, missing Klara and suffering from immense guilt.    Overall I liked the book. It was well written and captivating, reminding in some ways of Robert Heinlein, but maybe a bit more dystopian. There were many familiar tropes such as a very inter-ratial cast with exotic combinations of first and last names, like Dane Metchnikov. Bob's adventures with hot space babes are also I think a hallmark of this style of science fiction. I also enjoyed Pohl's excerpts of Gateway public announcements, mission reports and classifieds littered throughout the book.    Sophie's World by Jostein Gaardner (5/5) - November 2015    I really enjoyed this one. Clearly a novel for kids, this book provides a nice overview of philosophy. However it is also quite entertaining. Events center around Sophie and her mysterious philosophy teacher. Initially I was disappointed, since it read like a philosophy class given to the reader through letters sent to Sophie, but as the plot thickened, the format changed to socratic dialog.    After a somewhat dry start, it is revealed that in fact the author is himself a fictional character writing a book to her daughter, which reveals a lot of the strangeness that happens in Sophie's World. Towards the end, her world becomes increasingly exciting and fantastical, with cameos from more and more strange fictional characters including Nils and the wild geese. And then it unravels, when Sophie's fictional philosophy teacher reveals to everyone in their world that they are living inside a fiction book. This self-referential trick reminds of Borges and Hofstadter.    I was quite impressed that the Gaardner was able to fit a full intro to philosophy course into a relatively short book, and still make it entertaining. It passed my bar with flying colors and I'm keen to give it to Ben. It can be his high school philosophy class!    Audio lectures: the United States and the Middle East (3/5) - October 2015    A historical account of modern middle eastern history from a US lens. Covers the period between the late Ottoman Empire and September 11th, 2001 and its aftermath.    I found the course illuminating, with some clarity on various infighting rivalries within the Muslim world. Also a recap of Israeli history through an Arab lens was interesting to hear along with additional details on all of the major wars and their historical impact. I really liked the late 20th century lens on US presidents as well: fascinating to view it through the middle eastern lens.     The lecturer is quite biased toward the Arab states, which is not a stance I am used to. At the same time it is clear he tries to moderate himself to better appeal to a wider liberal audience. The argument that Israel and American imperialism is largely to blame for problems in the Middle East runs deeply through the lecturers narrative. And I suppose that this stance is not so controversial among liberals too!    Often citing nuances like the definition of Jihad, and underplaying ideological opposition in favor of external factor mentality, the lecturer makes his bias very clear in parts. The continuous emphasis on the Israeli occupation gets old fast. But his stance is not so extreme that he denies that there are genuinely endemic problems, such as bitter infighting, a general reluctance of the Arab world to come to the aid of the Palestinian cause, and homegrown (i.e. Not created by the us) terrorists like Osama bin Laden and organizations like Hamas, though he never actually calls it a terrorist organization.     Anyway mixed feeling at the end, since too many of my thoughts were wasted on decomposing the lecturers bias and not enough spent to process the actual content. Also the speaking cadence made it quite hard to listen to. Ended up going to 1.5x by the end, which I rarely do.     Science and Human Values (3/5) - September 2015    Broznowski tries really hard to impress with his breadth of knowledge. He constantly drops references to art, literature and philosophy to make his point. It's as if he is using himself as an example of a scientist who is also well versed in liberal art (there are dozens of us!!). I found most of the writing to be very fluffy, deliberately convoluted, and kind of annoying. I had a hard time finishing despite it being such a short read. I set the book down for a month, and returned to it later (upon returning from vacation).    In the first part, Broznowski makes the argument that science isn't in and of itself good or evil, but just a tool. The bombing of Hiroshima and Nagasaki, he attributes not to the invention of the atomic bomb, but to the politicians that chose to use it. Seems like a pretty basic argument, but also flawed and not new. Not being well versed in the philosophy of Science, not in a good position to judge it.    In the third and final essay, Broznowski considers science as a social institution, and compares it to the rest of society. This part I thought was interesting.  Science has many attractive, democratic properties that I agree could really stand to be ported into other human domains.    Thinking Fast and Slow (5/5) - September 2015    An excellent read. Kahneman basically summarizes his career as a psychologist, in which he systematically found flaws in human ability to think rationally. He's a founder of behavioral economics, which describes the ways in which humans make systematic thinking errors (deviating from "pure" rationality)..    Audio: Skeptic's Guide to American History (4/5) - August 2015    Well done overview of American History with a historiographical twist. Reminded me of Mr. Begin's history classes, but probably a little more sophisticated. I really liked the lecturer's call to skepticism which rang throughout, as he questioned theme after theme of cliche Americanisms. I really liked his spiral model of historical progress, and also the fact that he dropped enough references that I now have plenty of follow up work to do. A worthwhile course for any non-American. Buy it on  TTC .    Digital Signal Processing Coursera (3/5) - Finished in July 2015    https://class.coursera.org/dsp-004/    Professors Martin Vetterli &amp; Paolo Prandoni give an in-depth introduction to digital signals, filtering, etc. A caveat: I only went through the first 10 (out of 18) days of lectures, and did only some of the homework assignments.    Initially spurred on by a broad interest in music, audio, and sensor processing, this course served as a very solid theoretical foundation.  The course starts off with a very dry, theoretical exploration into Hilbert spaces, which served as an excellent reminder that I can still sort of understand and enjoy mathematics. However my somewhat pathetic mathematical abilities were stretched to their absolute limit by Vetterli's systematic barrage of theory, and I would have surely given up on day 2 if not for some of those upper year linear algebra classes (thanks UBC!).    The lecture's first focus is on  how to understand signals . Still shaky on some of the theory, the lecture delves deep into the theory of fourier analysis and the formal definition of the discrete fourier analysis and various related methods like the DTFT. The treatment of the subject is so dry, it's hard to breathe. Until as if in a sudden rainstorm, the theoretical desert can breathe again when finally they bring up examples of the STFT, which I have been using for a while for various audio-related applications at work, without knowing what it was called.    The next big subject is  transforming signals , or signal processing.  Here, the professors introduce LTIs formally, and derive the idea that an impulse response fully characterizes an LTI. Next they dive into simple filters, FIRs and IIRs, covering various types of averages, discussing filter stability and other formal properties. They try to build some intuition about filters from a frequency-domain perspective, which is very useful to me. Lastly, the subject of ideal filters is explored, as well as why such filters cannot be implemented in real life.     Finally, they discuss how to actually design filters using the z-transform, whose derivation remains somewhat mysterious to me, which is unfortunately normal for an engineering course. Prandoni gives a good interpretation for the pole-zero plot, which is a really elegant way of looking at filters. The lecture turns practical for a brief moment, when they reveal that implementations already exist for non-ideal versions of many of these filters and explore a few differences between Butterworth, Chebyshev and Elliptic low pass filters. They also talk about FIRs, and I finally understand the jargon of 'taps', which I just heard about at a lecture held at Noisebridge.    I still struggle to convert block diagrams into equations, but feel like I have a much better sense of second order IIRs that ship with the Web Audio API. Overall a mathematically challenging, but interesting class.  I really wish there was a better way to build intuition for a lot of the theory, though.    Audio: Understanding the Secrets of Human Perception (4/5) -- June 2015    http://www.thegreatcourses.com/courses/understanding-the-secrets-of-human-perception.html    Excellent overview of perception. I learned a ton from these lectures, and really want to enumerate all of the illusions and experiments that the lecturer cites which give insight into the way our senses work. Lots of in-depth material about our visual system, how we percieve motion, the incredible variety of depth cues, as well as after-images and color effects. Some senses, like audition, were largely review for me, but others, like olfaction and taste were completely new to me. Multi-modal perception gets especially interesting, the canonical illusion being the McGurk effect, but  there are many more .    This course is much more than just enumerated sensory illusions, though.  The great thing about it is that Vishton spends a fair amount of time on the general principles of operation of our brains, from fundamentals of neurons and synapses to opponent-process theory, which explains a great deal of perception phenomena across the senses. From a UX and VR perspective, the material in this course is totally invaluable.     John Napier - Hands (4/5) -- May 2015    Really interesting deep-dive into human hands from many different perspectives: anatomical, evolutionary, functional and social. The book is packed with facts. For example, I had no idea the carpal bones were so numerous and complex! Also, it's nice to understand why human opposition differs from monkeys and the Opposability Index. Lots of interesting insights into distribution of handedness, including that certain species of monkeys are predominantly left handed, which is surprising on several levels. Napier also goes a bit into fingerprints, basic patterns, and mentioning some relation to worn-off finger prints in people with Celiac. It was fun to cross-correlate what was written in the book with my own hands.    Overall a thoroughly interesting account and I found good insights from an HCI perspective. I did think that the second part on social and cultural aspects was too brief and incomplete.        With the eye, the hand is our main source of contact with the physical environment. The hand has advantages over the eye because it can observe the environment by means of touch, and having observed it, it can immediately proceed to do something about it. The hand has other great advantages over the eye. It can see around corners and it can see in the dark.      Vernor Vinge (2/5) - Rainbow's End -- April 2015    Real science fiction, in a bad way, focusing more on the science than the fiction, Asimov style. Not that great of a book, underdeveloped characters, contrived plot. Really felt like it was written by a technologist, unlike say Bashert by my former CS professor. The main reason I read it was for is focus on VR, but even for that I got bored about half way through and had to force myself to finish.    The library allegory felt too obvious, as if the author was just shoving the out with the old, in with the new in your face.    Blue Mind (3/5) -- March 2015    The message was good and clear, but the writing was tedious. Like Gladwell books, this one could be compressed to a short pamphlet. I liked the distinction between hedonism and eudaimonia, which was a very ancillary point in the book, but still new to me. I also liked the blue marble finish. But the meat of the book consists of an glorified enumeration of activities related to water.  
p88
sg28
g7
(g8
g9
V<p>In early 2015, I began writing a highlight/review/summary for every book, audio\u000abook, essay or especially memorable podcast I consume, in an attempt to become a\u000amore active reader and listener.</p>\u000a\u000a<h2>Mindstorms by Seymour Papert (4/5) - February 2017</h2>\u000a\u000a<p>I enjoyed this short book by the inventor of LOGO, which I fondly remember first\u000ahearing about from Yuri as a kid. It was a challenging read, especially in the\u000aphilosophical sections in the end. Also I have this weird thing where Seymour\u000aPapert and Mark Gross somehow blend into one person??</p>\u000a\u000a<p>Papert is fascinated by embodied learning, and the LOGO Turtle is one of many\u000aexamples of an "object to think with". This has great benefits for children,\u000asince they can imagine becoming a turtle in their mind. For example, drawing a\u000acircle as a Turtle is simple: take a small step forward, turn to the right a\u000abit, rinse and repeat. This is easily translated into LOGO: <code>FORWARD 1; RIGHT\u000a1;</code>.</p>\u000a\u000a<p>Papert rails against the way math is presented in school, making compelling\u000aarguments in its direction as well as the direction of "New Math", which I guess\u000awas a big deal in 1980 but has not aged well. He draws a comparison from our\u000acurrent educational system to QWERTY, which we have kept for a long time despite\u000aits obsolescence. He warns about a dumb use of computers in the classroom, in\u000awhich computers program the child (e.g. Mavis beacon teaches typing). In\u000aPapert's world, the child programs the computer and learns in the process.</p>\u000a\u000a<p>Conceptually, LOGO represents a third view of geometry. Three views of geometry:\u000aEuclidean is the most abstract, providing conceptual underpinnings. Cartesian,\u000awhich allows for algebraic descriptions and practical applications. And Turtle\u000awhich allows for an algorithmic approach. Latter is also more intuitively\u000aapproachable.</p>\u000a\u000a<p>Today Papert's ideas live on in the maker movement, which Papert is generally\u000aconsidered to have fathered (at least conceptually). I care about this movement\u000aa lot. Also in my work as a software prototyper, I gravitate to "learning\u000athrough making", which is the lens through which I view my job.</p>\u000a\u000a<p>Throughout the book Papert makes amazing analogies I haven't heard of before:</p>\u000a\u000a<ul>\u000a<li><p>Analogy of artificial intelligence to artificial flight. We started by\u000aimitating birds with flapping machines, but that clearly failed. Instead we\u000ahad to understand the principles (aerodynamics, bernoulli).</p></li>\u000a<li><p>Analogy of samba school to education in general. Samba school is a naturally\u000aintegrated part of life in Brazil. It's informal, highly intense, sought\u000aafter. Great model for modern edu?</p></li>\u000a<li><p>Analogy of teaching someone to juggle to programming computers. I haven't\u000areally thought about this, but maybe this is why so many CS people juggle?</p></li>\u000a<li><p>Analogy of the skiing revolution of the 70s to what is happening in computers.\u000aOne of many changes in skiing style that came about alongside a revolution in\u000aski technology. Did the new skiing technology cause the new style or vice\u000aversa? Papert suggests a deep entanglement between the two.</p></li>\u000a</ul>\u000a\u000a<p>"The most powerful idea is that there are powerful ideas."</p>\u000a\u000a<h2>Don't Think of an Elephant by George Lakoff (2/5) - February 2017</h2>\u000a\u000a<p>He's way too simple in his breakdown. The two family views is ridiculous. It's\u000anot fucking binary. People have much more granular and specific world views.\u000aCambridge analytics showed that very well with both Brexit and Trump.  Very\u000adated. Lots of 2000s era references that are no longer relevant, like\u000aSchwarzenegger. Also a very political message, never giving conservatives a\u000asteel man.</p>\u000a\u000a<p>I forgot the book on a plane. But I won't be attempting it again despite its\u000ashort length.</p>\u000a\u000a<h2>The Futurological Congress by Stanislav Lem (5/5) - January 2017</h2>\u000a\u000a<p>I'm a huge fan of Lem's work. I'm especially fond of Cyberiad, Star Diaries and\u000aSolaris, but it has probably been over a decade since I've read one of his\u000abooks.</p>\u000a\u000a<p>I wanted to get back into Lem, so I asked Marcin for recommendations, and he\u000asuggested this one. My interest was piqued since I'm vaguely into rational\u000apredictions, but as I read through, I was intrigued and surprised.</p>\u000a\u000a<p>In the near future, the world has become politically unstable. A group of\u000afuturologists meet at a summit in Costa Rica, and find themselves in the middle\u000aof a military coup. To combat unrest, the government has begun experimenting\u000awith air- and water-borne mind altering chemicals called Love Thy Neighbor\u000a(LTN). As a result, most of the attendees of the congress suffer from\u000ahallucinations and increased lovey-doveyness.  Lem's vivid descriptions of the\u000ainsanity inside the congress hotel are very entertaining.</p>\u000a\u000a<p>In an attempt to remain rational and escape the vile chemistry, the main hero,\u000aLem's recurring character Ijon Tichy and a ragtag group of futurologists and\u000ahotel workers end up in the basement below the hotel. For a while, Tichy fends\u000aoff the hallucinogens, but ultimately succumbs to them. The remainder of\u000athe book (its majority) takes place in Ijon's disturbed imagination. This is\u000arevealed only in retrospect, as Ijon wakes from each dream, finding himself in\u000athe basement sewer, back in Costa Rica. Each of Tichy's hallucinations is a\u000adifferent vision of a possible dystopian future, full of hilarity.</p>\u000a\u000a<p>In his last and most convincing hallucination, Tichy is cryogenically frozen\u000aonly to be reanimated three decades later in a society that has completely\u000amastered mind-altering chemistry. Lem (and the translator, who did a fine job)\u000aplay incessantly with words, inventing at least fourty, all derived from\u000athe idea of psychem, drugs that regulate every moment. The world is utopian,\u000abut something isn't quite right. Tichy meets up with his (also reanimated)\u000afuturologist friend, who gives him a whiff of "up'n'at'm, one of the\u000avigilanimides that can temporarily cancel out the effects of other psychem.\u000aTichy finally sees the world for what it is: a disturbing hellscape populated by\u000acyborg zombies.</p>\u000a\u000a<p>Nowhere in the book is it clear whether you are in Tichy's hallucionation or in\u000aactual reality. Clearly the book predates many of the many modern Sci Fi staples\u000athat toy with the notion of reality, like the Matrix. Overall, it reads super\u000aquickly and is incredibly engaging. Despite its humor, this book is quite deep.\u000aIt explores a lot of the same issues that concern me with VR. Chemistry is just\u000aanother of virtualizing reality.</p>\u000a\u000a<h2>On Venus, Have We Got a Rabbi by William Tenn - January 2017</h2>\u000a\u000a<p>An entertaining short story in the newly formed genre of Jewish Science Fiction.\u000aVery memorable characters: the conservative father, his liberal son.</p>\u000a\u000a<p>The author (previously unknown to me) does a great job of capturing the essence\u000aof Jewishness. The dialog is great, and the content is meaty. The theme of\u000aconstant repression regardless of circumstances comes up again and again.</p>\u000a\u000a<p>The core issue is whether or not an alien race can be considered Jewish.\u000aHopefully this is an issue we can grapple with one day.</p>\u000a\u000a<p>Loved the punch line:</p>\u000a\u000a<blockquote>\u000a  <p>I don\u2019t know. Let\u2019s talk about something cheerful. How many people would you\u000a  say were killed in that earthquake on Callisto?</p>\u000a</blockquote>\u000a\u000a<h2>Thinking and Deciding (fragments) by Jon Baron (4/5) - January 2017</h2>\u000a\u000a<p>Jon Baron sent me a fragment of his book on actively open minded thinking. His\u000aAt the outset, in the first chapter, Jon attempts to formalize what thinking is\u000aas a process, and then the ninth, on actively open minded thinking.</p>\u000a\u000a<p>I liked how at the outset, author makes a case for the idea that thinking\u000arationally is equivalent to thinking better. Also he does a good job of\u000adivorcing rationality from the lack of emotion, which is deeply seated in the\u000apopular mind (thanks, Spock!) He then proceeds to drop more interesting ideas.\u000aThat belief forming involves thinking, for example. But also the idea that doubt\u000aand thought are intimately interlinked:</p>\u000a\u000a<blockquote>\u000a  <p>...but it already shows the main characteristic of thinking. It begins with\u000a  doubt.</p>\u000a</blockquote>\u000a\u000a<p>In the meat of the "What is thinking?" chapter, Jon breaks thinking down into a\u000aformal process which he calls "search-inference". In this framework, the thinker\u000asearches for goals, possibilities and evidence, and then uses inference to come\u000aup with an ultimate judgement. This sounded quite mechanistic to me, and I think\u000aeasier to illustrate in a drawing than in a mound of text:</p>\u000a\u000a<pre><code>TODO: Include my table for decision making.\u000a</code></pre>\u000a\u000a<p>An attempt to formalize my decision making process is not new to me. Before\u000ajoining Google, I considered doing a startup around the idea of automatically\u000acreating personalized product comparison tables, which we toyed with in\u000aour crowdsourcing research CrowdForge.</p>\u000a\u000a<pre><code>TODO: Include the CrowdForge table.\u000a</code></pre>\u000a\u000a<p>As I was drawing and refining this sketch, I was reminded of decision matrices.\u000aAnd I even formulated my current career decision using the help of a\u000aspreadsheet, which is intended to be easily human-writeable (by me, mostly), and\u000aultimately reduces to a decision matrix under-the-hood.</p>\u000a\u000a<pre><code>TODO: Include my decision spreadsheet.\u000a</code></pre>\u000a\u000a<p>The structure of this spreadsheet is simple: columns are possibilities, rows are\u000agoals. Each goal has a weight (1 through 5). The higher the weight, the more important\u000ait is for you to achieve that goal. Evidence goes into the cells of the table.\u000aAt each cell (goal, possibility), you provide a description of evidence in favor\u000aor against, and mark it with a weight (-2 through 2). Negative weights are cons,\u000apositive weights are pros. Ultimately, the numbers you provide are reduced to a\u000adecision matrix, where each cell corresponds to a value between 0 and 1. The\u000apossibilities are then scored and the best course of action is the one\u000awith the highest score.</p>\u000a\u000a<p>Despite the mechanistic appearance of this approach, Baron emphasizes the\u000anonlinearity of the process. You can always find additional goals,\u000apossibilities, evidence, etc. In fact, as you do the exercise (whether in your\u000amind, on paper, or with a spreadsheet), you actively engage in search. Once\u000ayou're done with inference, you can go back and search some more.</p>\u000a\u000a<h3>In defense of the tabular approach</h3>\u000a\u000a<ul>\u000a<li>Consolidate all of the different considerations in one place. Decisions are\u000astressful.</li>\u000a<li>The model encourages more than two possibilities, which might help with the\u000atendency for binary thinking.</li>\u000a</ul>\u000a\u000a<h3>Thinkos: inevitable irrationality</h3>\u000a\u000a<p>People aren't perfect, and neither is our thinking. Biases are sort of like\u000athinking bugs. Baron suggests that certain things that can improve thinking and\u000areduce bias.  These are broadly described as "active open\u000amindedness":</p>\u000a\u000a<ul>\u000a<li>Look for more alternative possibilities, impartially.</li>\u000a<li>Formulate goals better (eg. "protect walls from child's scribbling" vs\u000a"prevent child from scribbling on walls").</li>\u000a<li>Look for counterevidence for everything.</li>\u000a<li>Entertain all possibilities, not just initial hunch.</li>\u000a<li>Allocate time that is proportionate to the importance of the decision.</li>\u000a<li>Avoid explaining away evidence that you don't like.</li>\u000a<li>Avoid belief overkill, which happens when there is a strong correlation\u000abetween different goals (eg. most people are against capital punishment because\u000ait is both ineffective and immoral, whereas those for capital punishment are in\u000afavor because it's effective and moral. But why do both go together? They are\u000aunrelated)</li>\u000a</ul>\u000a\u000a<p>He is especially concerned with what he calls "myside bias", which\u000ais also known as confirmation bias, and the related problem of irrational belief\u000apersistence.</p>\u000a\u000a<p>Changing your mind is difficult, and two sided thinking is better than one sided\u000athinking. (I guess three sided thinking is even better? "Think like an octopus"\u000aOn one hand, ... on the other, ..., on the third, ..., on the ninth...!")</p>\u000a\u000a<ul>\u000a<li>It can be seen as a weakness.</li>\u000a<li>Loyalty tends to encourage one-sided thinking.</li>\u000a</ul>\u000a\u000a<h3>Social decision making</h3>\u000a\u000a<p>Difficult decisions would benefit from an outside view. Returning for a moment\u000ato the CrowdForge angle, I wonder if the social aspect of this framework has\u000amerits. Imagine sending your decision spreadsheet to a friend (or stranger) for\u000ahonest feedback. They could catch your biases and encourage you to search for\u000amore possibilities, see if you have other goals, and to question your evidence.</p>\u000a\u000a<p>Some ideas for how a social angle could help enforce some of the thinkos\u000adiscussed above:</p>\u000a\u000a<ul>\u000a<li>Have peers encourage more possibility search</li>\u000a<li>Have peers suggest better formulations for goals</li>\u000a<li>Question your evidence, provide additional evidence you didn't think of.</li>\u000a</ul>\u000a\u000a<h3>Random</h3>\u000a\u000a<p>I liked Baron's idea that memory can be used in conjunction with external aids\u000aas a way of remembering more.</p>\u000a\u000a<blockquote>\u000a  <p>I rely on my memory as well, including my memory of how to use [the library]\u000a  and of who is likely to be able to help with what.</p>\u000a</blockquote>\u000a\u000a<p>Fascinated by ample evidence suggesting that neutral evidence actually increases\u000apolarization. Makes sense it retrospect, but I guess that's hindsight bias.</p>\u000a\u000a<p>Interesting distinction/continuum between advocate (who is permitted and\u000aencouraged to be one-sided), and actively open minded thinker. Notably,\u000aexpertise here is orthogonal to good thinking. In fact, often contradictory.</p>\u000a\u000a<blockquote>\u000a  <p>Because experts /know/ the answer to most questions, they usually do not have\u000a  to consider alternatives or counterevidence.</p>\u000a</blockquote>\u000a\u000a<p>Also interesting is that wishful thinking is sometimes rational. For example if\u000ayou are captured by terrorists, the idea that you might survive is probably\u000abeneficial, if unlikely.</p>\u000a\u000a<p>Interesting view of "controversial":</p>\u000a\u000a<blockquote>\u000a  <p>Many controversial issues are considered controversial because there are good\u000a  arguments on both sides. A rational decision would involve balancing the\u000a  arguments in a quantitative way, a way that takes into account their relative\u000a  strengths.</p>\u000a</blockquote>\u000a\u000a<h2>Superforecasters by Phil Tetlock (5/5) - January 2017</h2>\u000a\u000a<p>Good vs bad experts is apparently thing. I wondered: was there enough data to\u000areally know? What if everyone is a bad forecaster in aggregate? Luckily Tetlock\u000aaddresses it later.</p>\u000a\u000a<p>How to superforecast:</p>\u000a\u000a<ul>\u000a<li>Consider the outside view (eg. base rates)</li>\u000a<li>Consider many perspectives on the same issue (eg. on one hand, on the other hand, on the third hand, on the nth hand).</li>\u000a<li>Fermi problem style breakdown</li>\u000a<li>Might benefit from explicit uncertainty tracking software. It's apparently <a href="https://en.wikipedia.org/wiki/List_of_uncertainty_propagation_software">a\u000agenre</a>.</li>\u000a<li>I'm most interested in <a href="https://www.getguesstimate.com/scratchpad">guesstimate</a></li>\u000a<li>Include opinions of others. They are valuable data.</li>\u000a</ul>\u000a\u000a<p>Also interesting is the personality trait of being open to new experiences. And\u000acuriosity in general.</p>\u000a\u000a<p>Active open mindedness. Seek out dissenting opinions.</p>\u000a\u000a<p>Debaters is an instance of active open mindedness too - AOM</p>\u000a\u000a<p>"Beliefs are hypotheses to be tested, not treasures to be guarded"</p>\u000a\u000a<p>Interesting idea: adversarial collaboration. For example, Kahneman &amp; Klein, usually at odds with one another, still managed to publish an influential paper: http://www.fiddlemath.net/stuff/conditions-for-intuitive-expertise.pdf</p>\u000a\u000a<p>Kahneman sneaking in: suggesting that supreforecasters are better at "internalizing" system 2 thinking via system 1.</p>\u000a\u000a<p>Tetlock closes with a summary of the book in the form of ten commandments:</p>\u000a\u000a<ol>\u000a<li>Triage. Focus on questions where extra attention is likely to improve accuracy. Not too hard, not too easy. Weigh the consequences of wasting time on an unpredictable event vs failing to predict a predictable event before deciding whether to invest effort on a problem.</li>\u000a<li>Break seemingly intractable problems into tractable sub-problems ie use Fermi estimates.</li>\u000a<li>Strike the right balance between the inside and outside views.</li>\u000a<li>Strike the right balance between under- and over-reacting to new evidence.</li>\u000a<li>Look for the clashing causal forces at work in each problem. Weigh all the perspectives.</li>\u000a<li>Strive to distinguish as many degrees of doubt as the problem permits, but no more ie get comfortable using fine-grained estimates for fine-grained problems.</li>\u000a<li>Strike the right balance between under- and over-confidence, between prudence and decisiveness.</li>\u000a<li>Look for the errors behind your mistakes but beware of rearview-mirror hindsight biases. Own your failures!</li>\u000a<li>Bring out the best in others and let others bring out the best in you. Team dynamics matter as much as team composition.</li>\u000a<li>Master the error-balancing bicycle. These commandments are not enough - you need deep, deliberative practice.</li>\u000a<li>Don\u2019t treat commandments as commandments.</li>\u000a</ol>\u000a\u000a<p>Overall a great book, but I'm left feeling a bit unsatisfied. Another random\u000athought: everything I've read this year is basically like "foxes &gt;&gt; hedgehogs".\u000aBut surely there must be some good arguments in defense of hedgehogs.</p>\u000a\u000a<h2>Why Teach Thinking (Essay) by Jonathan Baron (?/5) - December 2016</h2>\u000a\u000a<p>I learned about J. Baron from reading Superforecasters, where he was cited in\u000athe context of his notion of "active open mindedness". I got so excited that I\u000aemailed him, and was very happy (<em>squee</em>) to receive a response within a day. My\u000afavorite part of his response (I indicated that I was a Haidt fan):</p>\u000a\u000a<blockquote>\u000a  <p>I have small disagreements with almost everyone, even myself.</p>\u000a</blockquote>\u000a\u000a<p>First half: focus on systematic thinking mistakes people make. Two broad\u000acategories: "myside" bias (very similar to confirmation bias) and over-reliance\u000aon broken heuristics. Not really new but definitely insightful and quite early\u000a(written in 1992).</p>\u000a\u000a<p>Interesting studies cited:\u000a- Thinking of reasons on the other side reduced inappropriate extreme confidence\u000ain answers to objective questions (Koriat, Lichtenstein and Fischoff, 1980)</p>\u000a\u000a<ul>\u000a<li><p>Being provided with evidence against your point of view can make you more\u000aconvinced of your position (Batson, 1975).</p>\u000a\u000a<ul>\u000a<li>(eg. Striking example involving a group of girls who were convinced of the\u000adivinity of Christ became more convinced as a result of the dead sea scrolls\u000awhich counter the virgin birth and resurrection.)</li>\u000a</ul></li>\u000a<li><p>Some people are convinced that one-sided thinking is actually better than two\u000asided thinking (Baron, 1989), even when the one-sided thinker favors the\u000aposition opposite of their own.</p></li>\u000a</ul>\u000a\u000a<p>Overall interesting combination of prescience and datedness. For example:</p>\u000a\u000a<blockquote>\u000a  <p>The impetus for teaching thinking in the USA comes from many desires: beating\u000a  the Japanese in commerce; having demagogue-proof citizens; having more\u000a  creativity in the popular arts; etc.</p>\u000a</blockquote>\u000a\u000a<p>Second half: much more focused on expert opinions, and how to evaluate them. A\u000alot more focus on education. Cites Piaget, etc. He sort of lost me and I didn't\u000areally see how the first half related.</p>\u000a\u000a<hr />\u000a\u000a<p>Update (Dec 2016): After another exchange with Baron, I indicated that I wasn't\u000asold on the expert part of his essay. His response: I missed the point! Quoting\u000aJon:</p>\u000a\u000a<blockquote>\u000a  <p>That last part is the important part. It argues that UNDERSTANDING AOT is just\u000a  as important as being able to do it. We need to depend on others, so we need to\u000a  know who we can trust. The basic idea was stated by J.S. Mill in "On Liberty":</p>\u000a  \u000a  <blockquote>\u000a    <p>The whole strength and value, then, of human judgment, depending on the one\u000a    property, that it can be set right when it is wrong, reliance can be placed on\u000a    it only when the means of setting it right are kept constantly at hand. In the\u000a    case of any person whose judgment is really deserving of confidence, how has it\u000a    become so?  Because he has kept his mind open to criticism of his opinions and\u000a    conduct. Because it has been his practice to listen to all that could be said\u000a    against him; to profit by as much of it as was just, and expound to himself, and\u000a    upon occasion to others, the fallacy of what was fallacious.</p>\u000a  </blockquote>\u000a</blockquote>\u000a\u000a<hr />\u000a\u000a<p>Update (Jan 2017): I read a fragment of a book on thinking that he sent me. Two\u000achapters are relevant: the first, where he attempts to formalize what thinking\u000ais as a process, and then the ninth, on actively open minded thinking.</p>\u000a\u000a<h2>Brief Wondrous Life of Oscar Wao (4/5) - November 2016</h2>\u000a\u000a<p>Liked:</p>\u000a\u000a<ul>\u000a<li>Copious references to nerd culture: nerdy books (LOTR, Dune, Lovecraft,\u000aAsimov, Heinlein), video games, comic books, the 1980s.</li>\u000a<li>Magical realism aspects: The Mongoose, cartoonish villans (The Captain,\u000aTrujillo) and heroes (Beli, The Doctor).</li>\u000a<li>Harkening to 100 Years of Solitude in the hispanic family epic.</li>\u000a<li>Nonlinear time, with the story flashing generations back.</li>\u000a<li>Oscar's relatable innocence, goofy character, purity of spirit.</li>\u000a<li>Interesting autobiographical narrator.</li>\u000a<li>Unexpected turns. After a slow start, you are taken into a deep rabbit hole of\u000aan incredibly turbulent family story. Things get weirder and weirder and more\u000aand more depressing. Fuku!</li>\u000a<li>Also, a really interesting look into Dominican Republic, and the political\u000aturmoil it went through in the 20th century.</li>\u000a<li>Fun to try to decipher the Spanish only having a rudimentary knowledge of the\u000alanguage!</li>\u000a<li>Very distinct and conversational tone.</li>\u000a</ul>\u000a\u000a<p>Disliked:</p>\u000a\u000a<ul>\u000a<li>A little too sappy and charicaturey. Oscar's self-loathing is comical, and\u000ait's hard to really empathize with the guy.</li>\u000a</ul>\u000a\u000a<h2>Future Babble (4/5, audio) - October 2016</h2>\u000a\u000a<p>The best predictors are foxes, but even they do poorly. Interesting weakness:\u000afoxes tend to be more swayed by scenario planning. Because they start vividly\u000abelieving all of the possible scenarios, so even unlikely ones become more\u000apossible. Also interesting is that popular pundits tend to be hedgehogs because\u000athey are prone to more extreme predictions that the audience likes.</p>\u000a\u000a<p>Lots of good interesting failed predictions, just like I asked!</p>\u000a\u000a<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Anyone have good recommendations for books/articles/lists on predictions from the past? Preferably failed ones :)</p>&mdash; Boris Smus (@borismus) <a href="https://twitter.com/borismus/status/785968518562852864">October 11, 2016</a></blockquote>\u000a\u000a<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>\u000a\u000a<p>Prediction is fundamentally hard because of chaos.</p>\u000a\u000a<p>Good predictors:\u000a- Peter Schiff predicting housing crash and recession.\u000a- Ross Anderson predicting y2k mildness.\u000a- Alan Barnes, pioneering work around predictions for Canada.\u000a- James Fallows, <a href="http://www.theatlantic.com/magazine/archive/2010/01/how-america-can-rise-again/307839/">essay</a></p>\u000a\u000a<p>Bad predictors:\u000a- David Brooks predicting nonsense.\u000a- Paul Erlich predicting ridiculousness in End of Affluence and Population Bomb\u000a- F. E. Smith, <a href="https://www.goodreads.com/book/show/7711931-the-world-in-2030-a-d">The World in 2030</a>\u000a- Arthur Clark: profiles of the future 1962</p>\u000a\u000a<p>Idea: correlation between difficulty of times and likelihood to turn to\u000asuperstition and religion. I should keep that in mind in the future. <a href="http://smus.com/hot-bread-delicious-deadly/">Russian\u000asuperstition</a> and magical thinking\u000ais not caused by leading a happy life. I should be kinder to those afflicted.</p>\u000a\u000a<p>Interesting conclusion: accurate predictions aren't necessary for good choices.\u000aThe goal is to make choices that are good in any event. If needed, skew\u000aaccording to probability.</p>\u000a\u000a<p>How to predict better:</p>\u000a\u000a<ul>\u000a<li>Be fox like: aggregate different sources.</li>\u000a<li>Meta cognition: consider how you arrive at your decisions. Know the biases.\u000aBut to avoid falling into them is hard! Exercise: enumerate reasons why\u000ayou might be wrong.</li>\u000a<li>Be humble. Predictions are always uncertain. Predictions too far into the\u000afuture even more so.</li>\u000a</ul>\u000a\u000a<h2>I Am A Strange Loop (2/5, unfinished) - October 2016</h2>\u000a\u000a<p>I read Godel Escher Bach many years ago and really enjoyed it at the time. When\u000aI got my hands on this book, I was excited to read a late Doug Hofstadter work.\u000aTo my surprise, by the end of Chapter 3, I was done with it. This book had none\u000aof the variety of its predecessor, and not much in terms of new ideas (as far as\u000aI can remember). I competely agreed with a goodreads review of it, courtesy of\u000a<a href="http://www.goodreads.com/review/show/148256192?book_show_action=true&amp;from_review_page=1">Zach</a>.</p>\u000a\u000a<pre><code>Conceptually, I guess you could say, I enjoyed it, but the presentation -\u000athe language of the author, the over-long format, and the strange mixture of\u000ahard math and elementary philosophy - diminished and diluted the content to\u000athe point that it was barely worth reading. The first problem is\u000aHofstadter\u2019s \u201caww shucks\u201d Uncle Fluffy writing style. His language is so\u000asteeped in a fireside chat mentality that the meat of his ideas is\u000acompletely devoured by his good-natured cleverness.\u000a</code></pre>\u000a\u000a<h2>The Hedgehog and the Fox (4/5) - September 2016</h2>\u000a\u000a<pre><code>The fox knows many things, but the hedgehog knows one big thing.\u000a-- Archilochus\u000a</code></pre>\u000a\u000a<p>According to Isaiah Berlin, Tolstoy is a natural fox who wanted to be a\u000ahedgehog. IB suggests that Tolstoy was too broad minded to be able to fully\u000aembrace a single doctrine, which could satisfactorily explain how the world works:</p>\u000a\u000a<pre><code>Any comforting theory which attempted to collect, relate, 'synthesize'\u000areveal hidden substrata and concealed inner connections, which, though not\u000aapparent to the naked eye, nevertheless guaranteed the unity of all\u000athings... [Tolstoy] exploded without difficulty.\u000a</code></pre>\u000a\u000a<p>At the same time, Tolstoy wanted to be able to find some Einsteinian unified\u000atheory of everything (TOE), but was too talented as a critic to be able to do\u000aso.</p>\u000a\u000a<pre><code>What oppressed Tolstoy most was his lack of positive convictions; and that\u000afamous passage in Anna Karenina in which Levin's brother tells him that he-\u000aLevin- has no positive beliefs.\u000a</code></pre>\u000a\u000a<p>This division within himself caused him great pain at the end of his life.</p>\u000a\u000a<p>As you can see, the writing here is not flowery, but rather complex and requires\u000aa high degree of attention. Unfortunately, this is exactly what I lacked given\u000athe circumstances of moving to Seattle.</p>\u000a\u000a<p>This short book / essay was supposedly intended as a joke (an elaborate one, at\u000a90 pages). Part of the fun is classifying people into animals, for example,\u000ahere's IB's take:</p>\u000a\u000a<ul>\u000a<li>Foxes: Herodotus, Aristotle, Montaigne, Erasmus, Moliere, Goethe, Pushkin,\u000aShakespeare, Balzac and Joyce</li>\u000a<li>Hedgehogs: Dante, Plato, Lucretius, Pascal, Hegel, Dostoevsky, Neitzsche,\u000aIbsen and Proust</li>\u000a</ul>\u000a\u000a<p>My conclusion is that Foxes &gt; Hedgehogs. The world really is complex, and a\u000asimple unified theory seems very unlikely. Also, I'll have to read more IB,\u000ain less turbulent circumstances. And more Tolstoy!</p>\u000a\u000a<h2>Sapiens (5/5) - September 2016</h2>\u000a\u000a<p>Yuval Noah Harari presents an outside view of humanity, from the perspective of\u000aa complete outsider, an alien with no particular interest in the human species.\u000aThis is fascinating.</p>\u000a\u000a<p>The scope of the book is very broad and often high level, starting with physics\u000aand chemistry 13.5 billion years ago at the dawn of the universe. Then, he turns\u000ato biology, which begins 3.8 billion years ago on earth, and finally to history\u000aand the cognitive revolution which starts about 70,000 years ago. Since then,\u000a12,000 years ago, we had agriculture, which sped up the cognitive revolution.\u000aMore recently, 500 years ago, the scientific revolution really turned it up a\u000anotch. The orders of magnitude of difference are appropriately humbling.</p>\u000a\u000a<p>I liked the distinction between objective, subjective, and inter-subjective. The\u000alatter meaning something that exists within the communication network linking\u000athe subjective consciousness of many individuals. If a single individual changes\u000ahis or her beliefs, or even dies, it is of little importance.</p>\u000a\u000a<p>Also striking was Harari's disdain for the agricultural revolution. He claims\u000athat it made most people worse off. The general point being that "history's\u000achoices are not made for the benefit of humans".</p>\u000a\u000a<p>Another major theme is the unification of humankind, which compounds\u000ainter-subjective effects since societies grow bigger and bigger. Over time,\u000acultures coalesce and form bigger and more complex civilisations. Even though\u000atoday's civilizations seem to be clashing, they actually agree on many things,\u000afor example money. "For thousands of years, philosophers, thinkers, and prophets\u000ahave besmirched money and called it the root of all evil. Be that as it may,\u000amoney is also the apogee of human tolerance."</p>\u000a\u000a<p>On religion, Harari has insights as well:</p>\u000a\u000a<pre><code>So, monotheism explains order, but is mystified by evil. Dualism explains\u000aevil, but is puzzled by order. There is one logical way of solving the\u000ariddle: to argue that there is a single omnipotent God who created the\u000aentire universe - and He's evil. But nobody in history has had the stomach\u000afor such a belief.\u000a</code></pre>\u000a\u000a<p>Also, interestingly, he considers most "isms" to be religions. And this leads to\u000ainteresting analyses. Harari delves deeply into humanism which he breaks down\u000ainto three kinds:</p>\u000a\u000a<ol>\u000a<li>Liberal humanism: that 'humanity' is a quality of individual humans, and that\u000athe liberty of individuals is therefore sacrosanct.</li>\u000a<li>Socialist humanism: hold as sacred not the inner voice of each individual,\u000abut the species Homo sapiens as a whole.</li>\u000a<li>Evolutionary humanism: humankind is not something universal and eternal, but\u000arather a mutable species that can evolve or degenerate.</li>\u000a</ol>\u000a\u000a<p>The latter has a long connection to Nazism, but Harari suggests (and I agree)\u000athat modern biology is really pushing us back into that vein.</p>\u000a\u000a<p>On the scientific revolution, Harari echoes the rational sentiment of accepting\u000aignorance. As soon as you truly accept that you do not know, you leave the door\u000aopen to new insights.</p>\u000a\u000a<pre><code>The Scientific Revolution has not been a revolution of knowledge. It has\u000abeen above all a revolution of ignorance. The great discovery that launched\u000athe Scientific Revolution was the discovery that humans do not know the\u000aanswers to their most important questions.\u000a</code></pre>\u000a\u000a<p>Overall, very worth a read (or listen, as I did).</p>\u000a\u000a<h2>How to actually change your mind by Eliezer Yudkowsky (4/5) - August 2016</h2>\u000a\u000a<p>Technically the second book in a giant epic entitled "Rationality: From AI to\u000aZombies". The author is a well known rationalist and active member/founder of\u000alesswrong.org, Center for Applied Rationality (CFAR), etc.</p>\u000a\u000a<p>Essentially, this is a series of essays written and published as "The\u000asequences", around 2009. They are loosely related and cover a wide array of\u000atopics, many of them highlighting irrational modes of thought. Much of the work\u000afocuses on biases central to behavioral economics, focusing on Kahneman-style\u000aresults. But the author goes beyond that, and also introduces a lot of opinion\u000afor how a rationalist should behave. At the same time, there is a tendency to be\u000aincredibly nerdy, which is alternatingly endearing and borderline autistic. I\u000afound myself asking the question: if one becomes a purely rational agent, isn't\u000aa computer strictly better? On the path to rationality, what aspects of humanity\u000ais worth preserving?</p>\u000a\u000a<p>Here's some new stuff I learned. A fair amount of the book covers behavioral\u000aeconomics concepts that I read about already in TF&amp;S.</p>\u000a\u000a<h4>Litanies</h4>\u000a\u000a<p>Aumann\u2019s Agreement Theorem suggests that no two rationalists can agree to\u000adisagree, given that they have the same information.</p>\u000a\u000a<p>Litany of <a href="https://en.wikipedia.org/wiki/Eugene_Gendlin">Gendlin</a>:</p>\u000a\u000a<pre><code>What is true is already so.\u000aOwning up to it doesn't make it worse.\u000aNot being open about it doesn't make it go away.\u000aAnd because it's true, it is what is there to be interacted with.\u000aAnything untrue isn't there to be lived.\u000aPeople can stand what is true,\u000afor they are already enduring it.\u000a</code></pre>\u000a\u000a<p>Litany of <a href="https://en.wikipedia.org/wiki/Alfred_Tarski">Tarski</a>:</p>\u000a\u000a<pre><code>If the box contains a diamond,\u000aI desire to believe that the box contains a diamond;\u000aIf the box does not contain a diamond,\u000aI desire to believe that the box does not contain a diamond;\u000aLet me not become attached to beliefs I may not want.\u000a</code></pre>\u000a\u000a<h4>Against black and white thinking</h4>\u000a\u000a<p>Yudkowsky is especially effective in his attacks on binary thinking. For\u000aexample, on partisanship:</p>\u000a\u000a<pre><code>Politics is an extension of war by other means. Arguments are soldiers. Once\u000ayou know which side you're on, you must support all arguments of that side,\u000aand attack all arguments that appear to favor the enemy side, otherwise it's\u000alike stabbing your soldiers in the back - providing aid and comfort to the\u000aenemy. People who would be level-headed about evenhandedly weighing all sides\u000aof an issue in their professional life as scientists can suddenly turn into\u000aslogan-chanting zombies when there's a Blue or Green position on an issue.\u000a</code></pre>\u000a\u000a<p>On the tendency and fallacy in binary thought:</p>\u000a\u000a<pre><code>There is a natural tendency to treat discussion as a form of combat, an\u000aextension of war, a sport; and in sports you only need to keep track of how\u000amany points have been scored by each team. There are only two sides, and\u000aevery point scored against one side is a point in favor of the other.\u000aEveryone in the audience keeps a mental running count of how many points\u000aeach speaker scores against the other. At the end of the debate, the speaker\u000awho has scored more points is, obviously, the winner; so everything that\u000aspeaker says must be true, and everything the loser says must be wrong.\u000a</code></pre>\u000a\u000a<p>The horns effect - all negative qualities correlate:</p>\u000a\u000a<pre><code>Stalin also believed that 2 + 2 = 4. Yet if you defend any statement made by\u000aStalin, even \u201c2 + 2 = 4,\u201d people will see only that you are agreeing with\u000astalin and you must be on his side.\u000a</code></pre>\u000a\u000a<p>And a very nice summary of a better way of thinking:</p>\u000a\u000a<pre><code>Not all arguments reduce to mere up or down. Lady Rationality carries a\u000anotebook, wherein she writes down all the facts that aren\u2019t on anyone\u2019s side.\u000a</code></pre>\u000a\u000a<p>Real belief vs. belief in belief:</p>\u000a\u000a<pre><code>Roger Zelazny once distinguished between \u201cwanting to be an author\u201d versus\u000a\u201cwanting to write.\u201d Mark Twain said: \u201cA classic is something that everyone\u000awants to have read and no one wants to read.\u201d Criticizing yourself from a\u000asense of duty leaves you wanting to have investigated, so that you\u2019ll be\u000aable to say afterward that your faith is not blind. This is not the same as\u000awanting to investigate.\u000a</code></pre>\u000a\u000a<h4>Assume your interlocutor is good</h4>\u000a\u000a<p>"To argue against an idea honestly, you should argue against the best arguments\u000aof the strongest advocates". This, and the closely related concept of the\u000aprinciple of charity aka Steelmanning (the opposite of strawmanning), which I\u000aheard from a Sam Harris interview, sent me on a long reading tangent of\u000aarguments <a href="http://www.patheos.com/blogs/camelswithhammers/2016/08/on-steelmanning-arguments-and-personally-customizing-them/">for</a> and <a href="https://thingofthings.wordpress.com/2016/08/09/against-steelmanning/">against</a>. Insightful tidbit from that\u000alast link:</p>\u000a\u000a<pre><code>First, seek to understand the actual viewpoints people you disagree with are actually advocating.\u000aSecond, seek out intelligent and well-informed advocates of viewpoints you disagree with. You don\u2019t have to make up what your opponents believe! As it happens, you have many smart opponents!\u000aThird, whenever possible, try to switch conversations from a debate focus to a collaborative truth-seeking focus.\u000a</code></pre>\u000a\u000a<p>Back to Big Yud. Some wisdom on focusing on the argument, not on the person:</p>\u000a\u000a<pre><code>Someone once said "Not all conservatives are stupid, but most stupid people\u000aare conservatives". If you cannot place yourself in a state of mind where\u000athis statement, true or false, seems completely irrelevant as a critique of\u000aconservatism, you are not ready to think rationally about politics.\u000a</code></pre>\u000a\u000a<p>A variation on the <a href="http://www.cs.cmu.edu/~weigand/staff/">reasonable person principle</a> (harkens back to my time\u000aat CMU).</p>\u000a\u000a<pre><code>To understand why people act the way they do, we must first realize that\u000aeveryone sees themselves as behaving normally. Don\u2019t ask what strange,\u000amutant disposition they were born with, which directly corresponds to their\u000asurface behavior. Rather, ask what situations people see themselves as being\u000ain. [...] Realistically, most people don\u2019t construct their life stories with\u000athemselves as the villains.\u000a</code></pre>\u000a\u000a<h4>Deciding which side to argue</h4>\u000a\u000a<p>Great distinction between rationality and rationalization. Very related to\u000aHaidt's position that beliefs are intuitive but their defence is rational. But\u000aHaidt makes no distinction like this. Would love to hear his thoughts on it.</p>\u000a\u000a<pre><code>Rationality is not for winning debates, it is for deciding which side to\u000ajoin. If you\u2019ve already decided which side to argue for, the work of\u000arationality is done within you, whether well or poorly. But how can you,\u000ayourself, decide which side to argue?\u000a</code></pre>\u000a\u000a<p>Eliezer suggests enumerating the evidence: "Lady Rationality carries a notebook,\u000awherein she writes down all the facts that aren\u2019t on anyone\u2019s side". Here's how\u000ato construct an honest ultrarational argument for a particular political\u000acandidate:</p>\u000a\u000a<ul>\u000a<li>Gather all evidence about the different candidates</li>\u000a<li>Make a checklist which you will use to decide which candidate is best</li>\u000a<li>Process the checklist</li>\u000a<li>Go to the winning candidate</li>\u000a<li>Offer to become their campaign manager</li>\u000a<li>Use the checklist as the campaign literature</li>\u000a</ul>\u000a\u000a<h4>The future is hard to predict</h4>\u000a\u000a<p>Herd instinct in venture capitalism:</p>\u000a\u000a<pre><code>The majority of venture capitalists at any given time are all chasing the\u000asame Revolutionary Innovation, and it\u2019s the Revolutionary Innovation that\u000aIPO\u2019d six months ago. This is an especially crushing observation in venture\u000acapital, because there\u2019s a direct economic motive to not follow the herd.\u000a</code></pre>\u000a\u000a<p>And what to do about it. DFJ (a VC) has a rule <a href="http://www.signallake.com/innovation/CrazyIdeasSuccessfulVC101009.pdf">favoring a passionate\u000aminority</a> to outweigh a negative majority.  This also reminds me of the\u000a<a href="https://www.quora.com/World-War-Z-2013-movie-Do-the-Israelis-really-have-a-10th-man-doctrine">Tenth Man Rule</a>.</p>\u000a\u000a<pre><code>Only two partners need to agree in order to fund any startup up to $1.5\u000amillion. And if all the partners agree that something sounds like a good\u000aidea, they won\u2019t do it.\u000a</code></pre>\u000a\u000a<p>Movies and books have a huge effect on the human psyche. This will probably\u000acompound with more immersive storytelling mediums:</p>\u000a\u000a<pre><code>So far as I can tell, few movie viewers act as if they have directly\u000aobserved Earth\u2019s future. [...] But those who commit the fallacy seem to act\u000aas if they had seen the movie events occurring on some other planet; not\u000aEarth, but somewhere similar to Earth.\u000a</code></pre>\u000a\u000a<p>Predicting numbers is especially difficult:</p>\u000a\u000a<pre><code>I observe that many futuristic predictions are, likewise, best considered as\u000aattitude expressions. Take the question, \u201cHow long will it be until we have\u000ahuman-level AI?\u201d The responses I\u2019ve seen to this are all over the map.\u000a</code></pre>\u000a\u000a<h4>Avoid having THE Great Idea and get granular and specific</h4>\u000a\u000a<p>Avoiding partisanship by focusing on the minimum viable argument, reminds me of\u000aSunstein's <a href="https://en.wikipedia.org/wiki/Judicial_minimalism">Judicial minimalism</a>:</p>\u000a\u000a<pre><code>But try to resist getting in those good, solid digs if you can possibly\u000aavoid it. If your topic legitimately relates to attempts to ban evolution in\u000aschool curricula, then go ahead and talk about it\u2014but don\u2019t blame it\u000aexplicitly on the whole Republican Party; some of your readers may be\u000aRepublicans.\u000a</code></pre>\u000a\u000a<p>Avoid overly large uhh Thingies, and chop them up.</p>\u000a\u000a<pre><code>Cut up your Great Thingy into smaller independent ideas and treat them as\u000aindependent. For instance, a marxist would cut up Marx's Great Thingy into\u000atheories of 1) value of labor 2) political relations between classes 3)\u000awages 4) the ultimate political state of mankind.\u000a</code></pre>\u000a\u000a<h4>Other interesting stuff</h4>\u000a\u000a<p>Taber and Lodge's <a href="http://onlinelibrary.wiley.com/doi/10.1111/j.1540-5907.2006.00214.x/abstract">"Motivated skepticism in the evaluation of political\u000abeliefs"</a> describes six predictions which are very Haidt-y. It's a list\u000aof political thinkos that are driven by behavioral economic biases.</p>\u000a\u000a<p>Beliefs don't need to be completely bullet proof. But this contradicts\u000ascience, where a single counter example can topple a theory.</p>\u000a\u000a<pre><code>A probabilistic model can take a hit or two, and still survive, so long as\u000athe hits don\u2019t keep on coming in. Yet it is widely believed, especially in\u000athe court of public opinion, that a true theory can have no failures and a\u000afalse theory no successes.\u000a</code></pre>\u000a\u000a<p>On the uselessness of "Deep Wisdom":</p>\u000a\u000a<pre><code>Surely the wisest of all human beings are the New Age gurus who say,\u000a\u201cEverything is connected to everything else.\u201d If you ever say this aloud,\u000ayou should pause, so that everyone can absorb the sheer shock of this Deep\u000aWisdom. There is a trivial mapping between a graph and its complement. A\u000afully connected graph, with an edge between every two vertices, conveys the\u000asame amount of information as a graph with no edges at all.\u000a</code></pre>\u000a\u000a<p>There's a distinction between Traditional rationalism and Bayesian rationalism.\u000aAnd I worry that the Bayesian variety, which Eliezer is a subscriber of, is a\u000asort of hedgehogginess: a very focused and blindered approach. But I liked the\u000aidea that you can go beyond falsification, the ability to relinquish an initial\u000aopinion when confronted by clear evidence against it.</p>\u000a\u000a<pre><code>I suspect that a more powerful (and more difficult) method is to hold off on\u000athinking of an answer. To suspend, draw out, that tiny moment when we can\u2019t\u000ayet guess what our answer will be; thus giving our intelligence a longer\u000atime in which to act. Even half a minute would be an improvement over half a\u000asecond.\u000a</code></pre>\u000a\u000a<p>"Make America Great Again":</p>\u000a\u000a<pre><code>A key component of a zeitgeist is whether it locates its ideals in its\u000afuture or its past. Nearly all cultures before the Enlightenment believed in\u000aa Fall from Grace \u2013 that things had once been perfect in the distant past,\u000abut then catastrophe had struck, and everything had slowly run downhill\u000asince then.\u000a</code></pre>\u000a\u000a<h2>Exploring the World of Lucid Dreaming (half way), (3/5) - August 2016</h2>\u000a\u000a<p>I bought this book 10 years ago, and it just sat on my shelf ever since. Not\u000asure how I got into it in the first place, but my interest in lucid dreaming has\u000aincreased as a result of my work on VR. As I delved into questions intimately\u000atied to human perception, I've become more and more interested in the inner\u000aworkings of our mind.</p>\u000a\u000a<p>VR is sensory replacement, AR is sensory augmentation, but dreaming is sensory\u000adeprivation. Yet the latter can generate completely fascinating experiences\u000anever possible in a sense-rich environment.</p>\u000a\u000a<p>I read the first half of the book, and realized that the thing I'm actually\u000ainterested in is not necessarily crafting the perfect dream, solving problems in\u000amy dreams, etc. But a more introspective pursuit, focused on trying to\u000aunderstand my own dreams. Which dovetails nicely given that the first step of\u000alucid dreaming is reliable dream recall. So, while I'm unlikely to become an\u000aoneironaut, at the very least I've started a handwritten dream journal, which I\u000akeep on my bedside table.</p>\u000a\u000a<h2>The Persuaders (3/5) - July 2016</h2>\u000a\u000a<p>Rationality used to pervade English scene. Now no appeal to reason is even\u000aattempted.</p>\u000a\u000a<p>Mostly arguments today are adversarial. In the past it was possible to have\u000apeople with differing opinions not to resort to personal attacks. Appeal to\u000areason hasn't always been the MO. It comes and goes and the author claims that\u000awere in an especially reason free era.</p>\u000a\u000a<p><em>How crazy PR is</em>: Nice case study looking into the political journalistic and\u000aother heavy machinery that is used to sway people. Particularly at the first\u000aIraq war (about Kuwait), and swaying public opinion to the side of the US.\u000aEspecially fascinating is the "Naira" girl who cried wolf, who claimed to be a\u000aKuwaiti refugee suffering in Iraqi camps, but was actually a member of the\u000aKuwaiti royal family. Especially interesting is the chain of money and influence\u000aand PR firms that takes that sort of inflammatory material and inject it into\u000apeoples' minds.  Remember "Free Kuwait" t-shirts? Courtesy of Hill and Nolton, a\u000aPR firm which was fed this information.</p>\u000a\u000a<p>Astroturfing - creating fake grassroots movements that are actually sponsored by\u000asome corporation</p>\u000a\u000a<p>Logical follow up or supplement to The Century of the Self.</p>\u000a\u000a<ul>\u000a<li>Lebeau: the crowd is lacking reason, influenced by emotion, images.</li>\u000a<li>Trotters: herd instinct, and group identity is key.</li>\u000a<li>Lippmann: anti democratic conclusion that experts should run the show, not unwashed masses. regain critical faculties by understanding how PR works. also regulatory moves to make public deception illegal.</li>\u000a</ul>\u000a\u000a<p><em>Soft vs. hard nudging</em>. fundamentally people don't always know what is best for\u000athem, and government can step in. seatbelt laws force you to wear seat belts\u000aalways (hard paternalism). GPS navigation systems don't force you to take their\u000aroute, cigarette packs with graphic images: soft paternalism.</p>\u000a\u000a<p><em>Shoving will backfire</em>: if people feel nudged, they nudge back. U.K. had a test\u000awhich gave job seekers a strength profile that was bogus. OIRA branch of the US\u000aand sunstein. what does the most good and least harm? regulations like showing\u000acalorie counts in fast food, environmental friendliness of consumer products,\u000ahidden costs in air tickets and credit card fees. good defaults matter too.</p>\u000a\u000a<p><em>Reason &gt; nudging</em>: reason based decisions will be more generally effective.\u000arather than appealing to some auto response, if you can reach the underlying\u000aunderstanding, you're better off.</p>\u000a\u000a<p>Some weaknesses: some parts, especially the ad related section, lists a lot of\u000astudies, but not the strengths of the effect. Also, largely a derivative work,\u000amainly a summarization of other works: "Thinking Fast and Slow", "The Hidden\u000aPersuaders", "Orality and Literacy".</p>\u000a\u000a<p>Strongest part: chapter 5 about politics, etc.</p>\u000a\u000a<p><em>Efficacy of arguments</em>: if dealing with people that agree already (eg. DNC to\u000aDemocrats), provide a one-sided argument. Otherwise, go both ways. Discrediting\u000athe other POV is effective.</p>\u000a\u000a<p>Garvey calls for a kind of thoughtful anger about persuasion. I concur, and this\u000ais what I'm channeling into Catma, for better or worse.</p>\u000a\u000a<h2>The Righteous Mind - July 2016 (5/5)</h2>\u000a\u000a<p>I discovered The Righteous Mind via <a href="https://www.samharris.org/podcast/item/evolving-minds">Jon Haidt's conversation with Sam\u000aHarris</a>. I read it while on our Honeymoon, while Sarah was\u000aenjoying another book about morality. It was very romantic! Anyway, some\u000athoughts on the book:</p>\u000a\u000a<p>Haidt presents his argument in three parts. It's well structured, and super well\u000awritten:</p>\u000a\u000a<ol>\u000a<li>Intuitions come first, strategic reasoning second (ie. people's morality is\u000adriven by intuition/gut reactions, and then they are really good at\u000ajustifying their position through reasoning)</li>\u000a<li>There's more to morality than harm and fairness (ie. conservatives care about\u000amany other things: authority, loyalty, sanctity -- this is the foundation of\u000aMoral Foundation Theory)</li>\u000a<li>Morality binds and blinds (ie. arguments around group selection, social\u000acohesion and other benefits of religion and partisanship)</li>\u000a</ol>\u000a\u000a<p><em>Not moral relativism</em>. In addition to borrowing from anthropology, Haidt\u000arecounts a personal story of his visit to India, and how initial visceral\u000adiscomfort eventually transformed into some understanding of the cultural\u000acontext he found himself in:</p>\u000a\u000a<pre><code>Understanding the simple fact that morality differs around the world, and even\u000awithin societies, is the first step toward understanding your righteous mind.\u000a</code></pre>\u000a\u000a<p>I don't think that this is moral relativism, but moral descriptivism. Haidt does\u000anot claim that morality <em>ought</em> to differ around the world, simply that it does\u000adiffer.</p>\u000a\u000a<p><em>Nice analogies</em>: I found the analogy of a small rider (the rational) riding a\u000alarge elephant (the intuitive) apt and useful. Sort of like a lizard brain\u000aidea. </p>\u000a\u000a<pre><code>The elephant is far more powerful than the rider, but it is not an absolute\u000adictator.\u000a\u000aUnder normal circumstances the rider takes its cue from the elephant, just\u000aas a lawyer takes instructions from a client. But if you force the two to\u000asit around and chat for a few minutes, the elephant actually opens up to\u000aadvice from the rider.\u000a</code></pre>\u000a\u000a<p><em>Against rationalism</em>: Haidt presents an interesting argument against moral\u000arationalism. Firstly, he makes a compelling case for religion as something that\u000aincreases social capital via group selection, arguing against the New Atheist\u000aview that religion is evil, period, with no redeeming qualities.</p>\u000a\u000a<p>Haidt argues for the Humeian view of morality, and against Bentham's utilitarian\u000aand Kant's deontological positions:</p>\u000a\u000a<pre><code>As Western societies became more educated, industrialized, rich, and\u000ademocratic, the minds of its intellectuals changed. They became more\u000aanalytic and less holistic.26 Utilitarianism and deontology became far more\u000aappealing to ethicists than Hume\u2019s messy, pluralist, sentimentalist\u000aapproach.\u000a</code></pre>\u000a\u000a<p>Haidt finds support for Hume's position, which is that intuition (or sentiment)\u000acomes first, and then reason tries really hard to come up with arguments to\u000asupport that position. Haidt's subjects were</p>\u000a\u000a<pre><code>Morally dumbfounded \u2014 rendered speechless by their inability to explain\u000averbally what they knew intuitively. These subjects were reasoning. They\u000awere working quite hard at reasoning. But it was not reasoning in search of\u000atruth; it was reasoning in support of their emotional reactions.\u000a</code></pre>\u000a\u000a<p><em>On changing minds</em>:</p>\u000a\u000a<pre><code>The main way that we change our minds on moral issues is by interacting with\u000aother people.\u000a\u000aIf you can have at least one friendly interaction with a member of the\u000a\u201cother\u201d group, you\u2019ll find it far easier to listen to what they\u2019re saying,\u000aand maybe even see a controversial issue in a new light. You may not agree,\u000abut you\u2019ll probably shift from Manichaean disagreement to a more respectful\u000aand constructive yin-yang disagreement\u000a</code></pre>\u000a\u000a<p>He presents a framework for how this can happen:</p>\u000a\u000a<p><img src="http://i.imgur.com/0TIx3sN.png" alt="Social intuitionist model" /></p>\u000a\u000a<pre><code>Intuitions come first and reasoning is usually produced after a judgment is\u000amade, in order to influence other people. But as a discussion progresses, we\u000asometimes change our intuitions and judgements.\u000a</code></pre>\u000a\u000a<p><em>Against Rawls</em>:</p>\u000a\u000a<pre><code>Beware of anyone who insists that there is one true morality for all people,\u000atimes, and places\u2014particularly if that morality is founded upon a single\u000amoral foundation. Human societies are complex; their needs and challenges\u000aare variable.\u000a</code></pre>\u000a\u000a<p><em>Salient critiques of the left</em>:</p>\u000a\u000a<pre><code>If you are trying to change an organization or a society and you do not\u000aconsider the effects of your changes on moral capital, you\u2019re asking for\u000atrouble. This, I believe, is the fundamental blind spot of the left. It\u000aexplains why liberal reforms so often backfire, and why communist\u000arevolutions usually end up in despotism.\u000a</code></pre>\u000a\u000a<p><em>Conservative thinkers</em>, like Frederich Hayek, Thomas Sowell, Jerry Muller, and\u000aEdmond Burke.  I ought to read them! Also, distinctions between conservatism and\u000aorthodoxy:</p>\u000a\u000a<pre><code>Christians who look to the Bible as a guide for legislation, like Muslims\u000awho want to live under sharia, are examples of orthodoxy. They want their\u000asociety to match an externally ordained moral order, so they advocate\u000achange, sometimes radical change.  This can put them at odds with true\u000aconservatives, who see radical change as dangerous\u000a</code></pre>\u000a\u000a<p>2016 is a very confusing year. The media appears to have completely lost its\u000aability to reason about or predict future political outcomes. Trump's\u000anomination, Brexit, the rise of populism all over the world, appears mysterious\u000aand scary. In my left coast circles, the right is generally portrayed as\u000aeither rich and evil, or poor and dumb.</p>\u000a\u000a<p>The Righteous Mind provides a more convincing explanation for contemporary\u000apolitical divisions around differences in morality. Rather than just identifying\u000alow intellect, sheepishness, or fascist tendencies with right leaning populists,\u000aHaidt offers a useful framework for understanding the conservative world view.\u000aThis book is a must read for any political observer with any hope for\u000aunderstanding.</p>\u000a\u000a<h2>Must Mankind Repeat History's Great Mistakes (Audio) - July 2016 (3/5)</h2>\u000a\u000a<p>Nice insight and guiding theme: international politics is complicated because there isn't really an organization that stands above all nations. Unlike other human endeavors, if a rogue entity misbehaves, there isn't really anyone to go to by default. Each country has its own skin to worry about, and they need to figure out the best strategy.</p>\u000a\u000a<p>I liked the concept of the balance of power, presented in 3 versions by the lecturer:</p>\u000a\u000a<ol>\u000a<li>Simply the distribution of existing power.</li>\u000a<li>The state in which all parties in question have equal power</li>\u000a<li>A tendency for new power entering a conflict to join the weak side in order to avoid a monopoly of power.</li>\u000a</ol>\u000a\u000a<p>After one or two theoretical lectures, the course ventured into territory of WW1, the interwar years, and WW2. I found this part to be somewhat less engaging since most of the historical details were review for me, and little of the theory introduced at the beginning was brought into the fold. The last lecture was more interesting, since it tried to apply the theory introduced at the beginning to predict the future. What made it even more interesting is the fact that the lecture was recorded in the early 90s.</p>\u000a\u000a<p>The lecturer wasn't very engaging, and I found myself listening to a lot of the lectures without getting much out of them. Additionally, the volume was low and the recording was poor quality, which contributed to my meta objections.</p>\u000a\u000a<h2>The man who loved only numbers (4/5) - July 2016</h2>\u000a\u000a<p>I really loved this one. It captures perfectly the quirkiness of Erdos, and his\u000amathematician circles. Much more than a biography, this book popularizes a lot\u000aof incredibly interesting mathematics. It took me back 10 years to my math\u000aundergrad days.</p>\u000a\u000a<p>The author does an incredible job of capturing the essence of many complex\u000amathematical concepts in appealing and entertaining ways. It's instructive to\u000acompare the way this book tackles certain topics to the corresponding Wikipedia\u000apage, which is completely undecipherable by a non-mathematician, and often even\u000aby a mathematician that doesn't work in that domain. That said, I'm  not sure\u000athat this book would be as appreciated by someone without a background in math.\u000aBut it may serve as inspiration for a motivated, mathematically-inclined high\u000aschool student. I'll find out!</p>\u000a\u000a<p>Despite having spent 4 years taking a bunch of math courses,  I learned a lot of\u000anew math from this book, and was reminded by some favorites.</p>\u000a\u000a<p>Ramsey Theory: the inevitability of order in large quantities. We barely touched\u000aon Ramsey theory in 4th year Graph Theory, and it seemed really obscure. Not\u000atrue! Draw any 5 points on a plane, and as long as 3 of them don't form a line,\u000ayou are guaranteed to be able to form a convex quad using them as vertices. Take\u000aany sequence of 101 numbers, and you're guaranteed to find a subsequence of at\u000aleast 10 increasing numbers.</p>\u000a\u000a<p>Infinite series: a technical subject, but some really incredible results, like\u000aTaylor expansions of e (sum of 1/k!), pi (6 times sum of 1/k^2).</p>\u000a\u000a<p>Cantor's analysis of various infinities (alpha numbers): firstly the beautiful\u000aargument about the countability of rational numbers, and then the famous\u000adiagonalization argument which proves that the set of real numbers is a bigger\u000ainfinity.</p>\u000a\u000a<h2>Mother of All Demos - June 2016</h2>\u000a\u000a<p>http://web.stanford.edu/dept/SUL/library/extra4/sloan/mousesite/1968Demo.html</p>\u000a\u000a<p>As a Human Computer Interaction nerd, I feel the need to pay homage to history\u000aof the field. One glaring omission in my education is that I haven't fully\u000awatched "The Demo" where Douglas Englebart shows off the state of the art of\u000a"Man-Computer Interaction" circa 1968. The subject of the demo is NLS (Online\u000asystem), which is intended as a synthesis of a bunch of ideas coming from SRI.\u000aIt's quite impressive how far we've come, but also how many elements are still\u000arecognizable, and in some cases impressively ahead of their time.</p>\u000a\u000a<p>The first ten minutes introduce a series of key concepts that are staples of\u000adesktop computing: the mouse and keyboard, mouse pointers, copy/paste, saving\u000aand loading files, numbered lists. Less mainstream, VI-like features are also\u000aintroduced, such as reorderable, heirarchical lists, and folding sections.\u000aMuch of what goes on under the hood in NLS reminded me of VIM script and other\u000asimilar DSLs. Big focus on expert-oriented power usage.</p>\u000a\u000a<p>The Demo heavily alludes to the web, with a lot of provisions for interlinking,\u000across referencing, and even an explicit mention of ARPA close to the end of the\u000ademo (Clip 33), which reminds the viewer that this is all pre-Intenet. In\u000acontrast, their (presumably telephone-based) video calling was quite impressive.\u000aThe quality of the remote speaker seemed to be about as good (bad?) as\u000aEnglebart's. Some collaborative features were quite impressive, with a real-time\u000acursor sharing scheme which was decades ahead of its time. Others were less\u000ainteresting, like leaving a message for other people in a file to "get a\u000aresponse within minutes".</p>\u000a\u000a<p>Some time is spent showing off hardware. Most interestingly is the\u000a<a href="http://dougengelbart.org/firsts/keyset.html">Keyset</a>, which was a 5-key\u000aancestor of the chorded keyboard, where each key combination (31 in total) led\u000ato a character, and was intended as a one handed keyboard. Also interesting as a\u000apoint of comparison was the cutting edge display tech of the time: black and\u000awhite CRTs with 15 Hz refresh rates and super high (3 frame) persistence.</p>\u000a\u000a<p>The philosophy of the lab behind MOAD was close to my heart, and lives on in R&amp;D\u000agroups I've had the pleasure of being part of:</p>\u000a\u000a<ul>\u000a<li>Build and try: big focus on prototyping!</li>\u000a<li>Evolutionary: make incremental improvements to a real system, not grand visions.</li>\u000a<li>Eat your own dogfood: use the thing you build, and test it on yourself.</li>\u000a</ul>\u000a\u000a<h2>The One Minute Manager - June 2016 (4/5)</h2>\u000a\u000a<p>This little book was recommended to me by a ski instructor after a ski lesson at\u000aSquaw.  The man was probably in his 70s, but skied incredibly well, and had a\u000alot of wisdom to pass on. He was tough, but nice, like a One Minute Manager.</p>\u000a\u000a<p>More of a pamphlet than a book, One Minute Manager unfolds as a story about a\u000ayoung man wanting to learn to be a good people manager. The story makes the\u000apoints more vivid, but at its core, there is an interesting and somewhat\u000asurprising strategy outlined for being a good hands off manager. The approach\u000areminds me a little bit of Christian's management style. There are three parts:</p>\u000a\u000a<ol>\u000a<li>Set goals and expectations with the employee, write them down succinctly and\u000ahave them ready for review. (This is similar to Google OKRs, and definitely\u000aworthwhile).</li>\u000a<li>Especially early on, look for behaviors that are praise-worthy, and give due\u000apraise honestly and directly. (This seems like a great idea to try on\u000ainterns).</li>\u000a<li>Reprimand unwanted behavior as it happens, but emphasize their worth as\u000apeople. (This seems quite hard to pull off without being a dick).</li>\u000a</ol>\u000a\u000a<h2>Gantenbein by Max Frisch - June 2016 (4/5)</h2>\u000a\u000a<p>Gantenbein was recommended to me by a close friend, who said it made a really\u000abig impact on him. It took me a long time to get through this book, and I often\u000astruggled to follow the narrative. The author writes in a very experimental\u000astyle, alternating between three different men, all of which are in love with\u000aLila, a beautiful stage actress. Events that happen don't seem to have\u000achronology or causation. Sometimes they branch out into multiple futures, other\u000atimes you reset in another time, another place, and as another person.  The book\u000ais a set of sketches around these people, more so than a coherent whole. Adding\u000ato the complexity, the narrator himself seems to not be a single person, but\u000aalternates between people. And this smoke and mirrors is explicit. The author\u000aoften writes "I imagine...", and the title is more accurately translated "Let's\u000aassume my name is Gantenbein".</p>\u000a\u000a<p>I found this book to be a piercing view into human nature. I could relate to\u000amuch of what the characters within it struggle with, which made reading it\u000adifficult and slow. Gantenbein, one of the protagonists, decides to pretend to\u000abe blind for his whole life, and builds a relationship with Lila. He wears dark\u000aglasses, which conceal his lie from others. But he himself has to be careful not\u000ato reveal his secret. He can see, but must pretend that he can't, and this leads\u000ato painful allegory on good relationships.</p>\u000a\u000a<p>This book was hard to read. I never got into a flow, and ended up reading very\u000aunevenly, often wanting to set it down for a while. But some parts I really\u000aloved, like when one of the characters attends his own funeral. And I was\u000asurprised to, after finishing it, have a feeling that some coherence emerged by\u000athe end. My understanding of this book would benefit from a re-reading, but I\u000aknow that this is unlikely to happen.</p>\u000a\u000a<h2>Audio lectures: Physics Beyond the Edge - June 2016 (4/5)</h2>\u000a\u000a<p>An interesting follow up to what I remember from high school &amp; first year\u000aphysics. My guess is that the subject of the class is too basic for those with a\u000adegree in physics, but this is something I should ask Alex! I really liked how\u000aall of the topics flowed into each other. I found this to be somewhat surprising\u000agiven Einstein's failed struggle to find some unified theory of everything. This\u000ais probably credit to the lecturer, who, in addition to being a great\u000ainstructor, also coined the term <a href="https://en.wikipedia.org/wiki/Qubit">qubit</a>.</p>\u000a\u000a<p>The structure of the class follows Arthur C. Clark's second law:</p>\u000a\u000a<pre><code>The only way of discovering the limits of the possible is to venture a\u000alittle way past them into the impossible.\u000a</code></pre>\u000a\u000a<p>The lecturer, Benjamin Schumacher, is quite engaging, and starts by breaking\u000aimpossibility down in terms of logical, physical and statistical terms. He then\u000agoes into thermodynamics, framing the first and second laws in terms of\u000a<a href="https://en.wikipedia.org/wiki/Perpetual_motion#Classification">perpetual motion machines</a>.</p>\u000a\u000a<p>I enjoyed the various Demons that show up in this lecture. <a href="https://en.wikipedia.org/wiki/Maxwell%27s_demon">Maxwell's\u000ademon</a> can control a flap between two chambers to make sure that\u000aeventually all of the molecules end up in one of the chambers (thus decreasing\u000aentropy). <a href="https://en.wikipedia.org/wiki/Laplace%27s_demon">Laplace's demon</a> is able to, from knowing the current state\u000aof the world, predict the future perfectly. Both of these thought experiments\u000aare forays into the impossible that, when disproven, set the stage for the third\u000alaw of thermodynamics and chaos / quantum mechanics.</p>\u000a\u000a<p>The section on space-time was fascinating, and served as a good introduction to\u000aspace-time diagrams (aka <a href="https://en.wikipedia.org/wiki/Minkowski_diagram">Minkowski diagrams</a>), which I really liked.\u000aAlthough, one thing that was missing is the visual element. Although very\u000alightweight in mathematical treatment, there were clearly some materials that\u000asupplemented the lectures, which would have made learning easier. The whole\u000adiscussion on faster-than-light travel, time travel, and quantum cloning was\u000atied together nicely through the lens of the impossible. By the way, space-time\u000adiagrams seem to be the reason for this <a href="http://physics.stackexchange.com/questions/25460/can-we-see-all-of-the-observable-universe">observable universe\u000athing</a>.</p>\u000a\u000a<p>Which brings us to Quantum. I've been interested in being interested in the\u000atopic for a while because it's so unintuitive and strange. Schumacher's\u000aoverview is good, but I'm obviously still very confused. He outlines basics of\u000aQuantum Mechanics:</p>\u000a\u000a<ol>\u000a<li>Wave-particle duality: light has both wave and particle like behaviors.</li>\u000a<li>Uncertainty: position and momentum of a quantum particle cannot both be\u000aknown.</li>\u000a<li>Tunneling: because of this uncertainty, quantum particles can tunnel through\u000awhat in newtonian mechanics are inpenetrable barriers.</li>\u000a</ol>\u000a\u000a<p>There seem to be some fundamental truths about symmetry as pertaining to\u000aconservation laws. <a href="https://en.wikipedia.org/wiki/Noether%27s_theorem">Noether's theorem</a> seems remarkably deep but way\u000aover my head, which states that every symmetry in physics has some associated\u000aconservation law.</p>\u000a\u000a<p>I was also somewhat lost in the information theory asides (the lecturer's\u000aspecialty is information theory). That information theory is a branch of physics\u000aat all is mysterious to me. Probably worth learning more about, though, given my\u000ainformation-related profession.</p>\u000a\u000a<h2>Audio lectures: Great Ideas of Philosophy - May 2016 (5/5)</h2>\u000a\u000a<p>Really great series of 60 lectures \u2013that's 24 hours worth as an overview of\u000aphilosophers and their main points. I took very few sporadic notes, as the\u000amajority of my listening was done while riding a bicycle. Here are some of my\u000afavorite philosophers and their quotes:</p>\u000a\u000a<p>Thomas Reid's [principle of credulity][]:</p>\u000a\u000a<pre><code>If no proposition that is uttered in discourse would be believed, until it\u000awas examined and tried by reason \u2026 most men would be unable to find reasons\u000afor believing the thousandth part of what is told them. Such distrust and\u000aincredulity would deprive us of the greatest benefits of society.\u000a</code></pre>\u000a\u000a<p><a href="https://web.archive.org/web/20120204195817/http://www.philosophyonline.co.uk/pom/pom_behaviourism_wittgenstein.htm">Wittgenstein's beetle</a>:</p>\u000a\u000a<pre><code>Imagine, he says, that everyone has a small box in which they keep a beetle.\u000aHowever, no one is allowed to look in anyone else\u2019s box, only in their own. Over\u000atime, people talk about what is in their boxes and the word \u201cbeetle\u201d comes to\u000astand for what is in everyone\u2019s box.\u000a</code></pre>\u000a\u000a<p>Everything about <a href="https://en.wikipedia.org/wiki/Socrates">Socrates</a>. This is a treasure trove to be revisited.</p>\u000a\u000a<pre><code>The unexamined life is not worth living.\u000a</code></pre>\u000a\u000a<p>The stoic mindset, especially this quote attributed to <a href="https://en.wikipedia.org/wiki/Epictetus">Epictetus</a>:</p>\u000a\u000a<pre><code>Never say of anything, "I have lost it"; but, "I have returned it."\u000a</code></pre>\u000a\u000a<p>Kant's [Categorical Imperative][kant], in a few formulations:</p>\u000a\u000a<pre><code>Act only according to that maxim whereby you can at the same time will that\u000ait should become a universal law without contradiction.\u000a</code></pre>\u000a\u000a<p>...and</p>\u000a\u000a<pre><code>Act in such a way that you treat humanity, whether in your own person or in\u000athe person of any other, never merely as a means to an end, but always at\u000athe same time as an end.\u000a</code></pre>\u000a\u000a<p>and...</p>\u000a\u000a<pre><code>Therefore, every rational being must so act as if he were through his maxim\u000aalways a legislating member in the universal kingdom of ends.\u000a</code></pre>\u000a\u000a<p>And closely related, Rawls' <a href="http://plato.stanford.edu/entries/original-position/">Original Position</a>, although the lecturer\u000a<a href="http://www.thegreatcourses.com/courses/great-ideas-of-philosophy-2nd-edition.html">Daniel N. Robinson</a> is surprisingly dismissive of his ideas.</p>\u000a\u000a<p>Karl Popper's ideas around <a href="https://en.wikipedia.org/wiki/Falsifiability">falsification</a>:</p>\u000a\u000a<pre><code>Falsifiability or refutability of a statement, hypothesis, or theory is the\u000ainherent possibility that it can be proven false. A statement is called\u000afalsifiable if it is possible to conceive of an observation or an argument which\u000anegates the statement in question. In this sense, falsify is synonymous with\u000anullify, meaning to invalidate or "show to be false".\u000a</code></pre>\u000a\u000a<p>The debate between realism and anti-realism: whether or not scientific theory\u000adescribes what the world is actually like, or if it just serves as a nice model.\u000aA succinct summary of the two positions:</p>\u000a\u000a<pre><code>Realists see scientific inquiry as discovery while anti-realists sees it as\u000ainvention.\u000a</code></pre>\u000a\u000a<p>Clarence Irving Lewis and the idea of qualia, especially well illustrated in\u000a<a href="https://en.wikipedia.org/wiki/Knowledge_argument">"Mary's room"</a>, which produces an interesting anti-materialist argument:</p>\u000a\u000a<pre><code>The sensation of color cannot be accounted for by the physicist's objective\u000apicture of light-waves. Could the physiologist account for it, if he had fuller\u000aknowledge than he has of the processes in the retina and the nervous processes\u000aset up by them in the optical nerve bundles and in the brain? I do not think so.\u000a</code></pre>\u000a\u000a<p>After nearly 24 hours of lecturing, Daniel synthesizes a lot of philosophical\u000athought into what constitutes a good life:</p>\u000a\u000a<ul>\u000a<li>Fatalism: being OK with your fate.</li>\u000a<li>Hedonism: the good kind, aiming to increase long term pleasure.</li>\u000a<li>Selflessness: benevolent, philanthropic behavior</li>\u000a<li>Activity: don't become a brain in a vat :)</li>\u000a</ul>\u000a\u000a<h2>How to fail at almost everything and still win big by Scott Adams - April 2016 (1/5)</h2>\u000a\u000a<p>Let's be clear: this is a self help book. I somehow got suckered into reading it\u000abecause I liked one of his blog posts. There are very few interesting or\u000aoriginal things in the book, and the author comes off as brash and trying too hard\u000ato be funny. That said, I liked some ideas:</p>\u000a\u000a<ul>\u000a<li><p>Systems vs goals. Focusing on achievement leads to emptiness after the fact.\u000aInstead focus on a system and process. </p></li>\u000a<li><p>Simplifiers vs optimizers is an interesting dichotomy. Usually it's about\u000aoptimizers vs satisficers but in this case I can see myself as a simplifier. </p></li>\u000a<li><p>Don't listen to friends and family feedback on business ideas. Listen to the market. </p></li>\u000a</ul>\u000a\u000a<p>Most of the rest of the book can be reduced to common sense: eat well, exercise\u000adaily. Some of Scott's specific advice is really uninspired and downright\u000adepressing. He insists that you should learn to play golf, learn a second\u000alanguage, how to use proper English grammar, how to be outgoing, a good\u000aconversationalist and a public speaker. He devotes a chapter of the book to\u000alearning a special command voice in order to manipulate people better. Move\u000aalong!</p>\u000a\u000a<h2>art of tidying by Marie Kondo - April 2016 (3/5)</h2>\u000a\u000a<p>An endearing manual for tidying up. I'm naturally inclined to minimalism as it\u000astands and did a massive cleanup of my old room in my parents house just last\u000amonth so consider myself proficient. But Marie takes it to the next level,\u000arevealing an autistic inclination towards the personification of inanimate\u000aobjects.</p>\u000a\u000a<p>This manifesto can easily be reduced to an illustrated one pager, but Marie\u000ainsists on treading over the same ground multiple times.</p>\u000a\u000a<ol>\u000a<li>Discard everything you don't need by determining if it sparks joy. Go by\u000acategory, starting with clothes, moving on to books and personal items.</li>\u000a<li>Once you've donated garbage bags full of items, organize them so that\u000aeverything you own has a place. Stand everything including clothes\u000avertically.</li>\u000a</ol>\u000a\u000a<p>By the end if the book I was curious more about Marie's eccentricity than her\u000aorganizational method.</p>\u000a\u000a<h2>The Moral Landscape by Sam Harris - March 2016 (4/5)</h2>\u000a\u000a<p>In many ways a rehashing of many ideas in philosophy, I think Sam did a good job\u000aof bringing some longstanding ideas and debates to the fore without requiring a\u000adegree in philosophy. The section on religion can be skipped, since he rehashes\u000aold theses from End of Faith (oh yeah, hobby horse!)</p>\u000a\u000a<p>The overall thesis is that the well being of conscious entities is of supreme\u000aimportance, and that morality is all about optimizing that. Intuitively, I agree\u000athat this is a good, pragmatic, and secular definition. I am less interested in the\u000aphilosophical debate around it, but I don't think Sam adequately addresses the\u000aother side of the argument adequately. Debates between utilitarian ideas and\u000aother branches of philosophy have been raging for centuries, and with all due\u000arespect, I don't think Sam is the guy to put them to rest.</p>\u000a\u000a<p>Despite well being's illusive nature, Sam draws a parallel to health, which is\u000aalso difficult to pin down precisely. While the analogy is illustrative, health\u000ais easier to define, and has been done out of practical necessity with metrics\u000alike QALYs. Though flawed, a QALY-like metric that includes all aspects of well\u000abeing seems out of reach in 2016.</p>\u000a\u000a<p>With that said, Sam hedges heavily against this reality, and repeatedly says\u000athat we may not know what the best collective course of action is, but that one\u000a(or in fact many) must surely exist. These many local minima are the peaks of\u000aThe Moral Landscape.</p>\u000a\u000a<h2>A Canticle for Leibowitz by Walter m. Miller Jr - February 2016 (5/5)</h2>\u000a\u000a<p>Amazing.</p>\u000a\u000a<p>Q masterpiece in three parts. The setting is post apocalyptic earth after a\u000amutually assured nuclear destruction scenario occurred in the 20th century.</p>\u000a\u000a<p>While most of humanity has undergone the simplification, a small order of\u000aChristian monks is dedicated to preserving human civilization. Their patron\u000asaint, St. Leibowitz, an engineer from before The Deluge. After a bomb shelter\u000awith his blueprint is discovered by a primitive monk, a religious scandal\u000aunfolds, reminiscent of what might have happened in the dark ages to Galileo.</p>\u000a\u000a<p>Many centuries later the blueprint is finally understood by a scientist\u000afrustrated by a lack of truly novel discoveries. And an engineer from the same\u000aorder builds it. It is a simple electrical generator which powersq a lamp.</p>\u000a\u000a<p>Eventually humanity returns to its pre Deluge level of technology and surpasses\u000ait, achieving the ability to travel to exoplanets. But political strife is once\u000aagain at an alarming level. And as earth nears nuclear annihilation again, the\u000aorder of leibowitz prepares to preserve the blueprints and other critical\u000adocuments and send them, along with a small order of monks, to another planet\u000afor resettlement.</p>\u000a\u000a<p>The characters are vividly painted, and the historical projection of the dark\u000aages onto a modern technological background is fascinating and masterfully\u000acrafted. The book serves as a stark reminder that there is no natural\u000aprogression toward better  times, and just how fragile our current paradise is.</p>\u000a\u000a<p>The threat of nuclear war may feel like a bygone but I'm quite concerned about\u000ait still.</p>\u000a\u000a<h2>In Praise of Idleness and Other Essays by Bertrand Russell (4/5) - December 2015</h2>\u000a\u000a<p>Fascinating collection of essays. In "In praise of idleness", Russell in 1930\u000apredicts that automation will reduce the working day drastically. If your quota\u000aof 1000 buttons can be fulfilled in 1/10th the time, you should have a lot more\u000atime for pursuing your hobbies. Unfortunately this never happened.</p>\u000a\u000a<p>In one of the essays, he declares himself as a socialist while simultaneously\u000adistancing from marx, and Russian communism. Essentially he lays groundwork for\u000asocial democracy. His views on socialism are very reasonable and still relevant\u000atoday.</p>\u000a\u000a<h2>Gateway by Frederik Pohl (4/5) - December 2015</h2>\u000a\u000a<p>Humanity discovers an advanced civilization, which has become extinct, but has\u000aleft a mysterious Gateway in our solar system. Gateway is a small world which\u000ahouses many ships capable of taking adventurers on missions to undisclosed\u000alocations. The controls are cryptic, but slowly residents of gateways gain a\u000abetter understanding of how they work. Gateway is controlled by a conspicuously\u000aevil trans-national corporation, which pays prospectors to take dangerous trips\u000aon these ships with the  in hopes of discovery. They pay out lucrative bonuses.</p>\u000a\u000a<p>The protagonist, an unsympathetic, and cowardly Robinette (aka Bob), comes to\u000agateway to try to strike it rich. He struggles with his own fears of going on\u000amissions, and falls in love with Klara. Their dysfunctional relationship, his\u000atroubled childhood, and pathological fear is woven deeply into the structure of\u000athe book. Chapters alternate between Bob's memories on Gateway and Bob the rich\u000aex-prospector getting therapy sessions from a sentient Robo-psychoanalyst on\u000aEarth. Bob's last mission ends in disaster, as he is separated from his beloved\u000aas their ships try to escape from a black hole. Bob returns the sole survivor,\u000agets an inordinate amount of money from the corporation, but remains forever\u000aunhappy, missing Klara and suffering from immense guilt.</p>\u000a\u000a<p>Overall I liked the book. It was well written and captivating, reminding in some\u000aways of Robert Heinlein, but maybe a bit more dystopian. There were many\u000afamiliar tropes such as a very inter-ratial cast with exotic combinations of\u000afirst and last names, like Dane Metchnikov. Bob's adventures with hot space\u000ababes are also I think a hallmark of this style of science fiction. I also\u000aenjoyed Pohl's excerpts of Gateway public announcements, mission reports and\u000aclassifieds littered throughout the book.</p>\u000a\u000a<h2>Sophie's World by Jostein Gaardner (5/5) - November 2015</h2>\u000a\u000a<p>I really enjoyed this one. Clearly a novel for kids, this book provides a nice\u000aoverview of philosophy. However it is also quite entertaining. Events center\u000aaround Sophie and her mysterious philosophy teacher. Initially I was\u000adisappointed, since it read like a philosophy class given to the reader through\u000aletters sent to Sophie, but as the plot thickened, the format changed to\u000asocratic dialog.</p>\u000a\u000a<p>After a somewhat dry start, it is revealed that in fact the author is himself a\u000afictional character writing a book to her daughter, which reveals a lot of the\u000astrangeness that happens in Sophie's World. Towards the end, her world becomes\u000aincreasingly exciting and fantastical, with cameos from more and more strange\u000afictional characters including Nils and the wild geese. And then it unravels,\u000awhen Sophie's fictional philosophy teacher reveals to everyone in their world\u000athat they are living inside a fiction book. This self-referential trick reminds of\u000aBorges and Hofstadter.</p>\u000a\u000a<p>I was quite impressed that the Gaardner was able to fit a full intro to\u000aphilosophy course into a relatively short book, and still make it entertaining.\u000aIt passed my bar with flying colors and I'm keen to give it to Ben. It can be\u000ahis high school philosophy class!</p>\u000a\u000a<h2>Audio lectures: the United States and the Middle East (3/5) - October 2015</h2>\u000a\u000a<p>A historical account of modern middle eastern history from a US lens. Covers the\u000aperiod between the late Ottoman Empire and September 11th, 2001 and its\u000aaftermath.</p>\u000a\u000a<p>I found the course illuminating, with some clarity on various infighting\u000arivalries within the Muslim world. Also a recap of Israeli history through an\u000aArab lens was interesting to hear along with additional details on all of the\u000amajor wars and their historical impact. I really liked the late 20th century\u000alens on US presidents as well: fascinating to view it through the middle eastern\u000alens. </p>\u000a\u000a<p>The lecturer is quite biased toward the Arab states, which is not a stance I am\u000aused to. At the same time it is clear he tries to moderate himself to better\u000aappeal to a wider liberal audience. The argument that Israel and American\u000aimperialism is largely to blame for problems in the Middle East runs deeply\u000athrough the lecturers narrative. And I suppose that this stance is not so\u000acontroversial among liberals too!</p>\u000a\u000a<p>Often citing nuances like the definition of Jihad, and underplaying ideological\u000aopposition in favor of external factor mentality, the lecturer makes his bias\u000avery clear in parts. The continuous emphasis on the Israeli occupation gets old\u000afast. But his stance is not so extreme that he denies that there are genuinely\u000aendemic problems, such as bitter infighting, a general reluctance of the Arab\u000aworld to come to the aid of the Palestinian cause, and homegrown (i.e. Not\u000acreated by the us) terrorists like Osama bin Laden and organizations like Hamas,\u000athough he never actually calls it a terrorist organization. </p>\u000a\u000a<p>Anyway mixed feeling at the end, since too many of my thoughts were wasted on\u000adecomposing the lecturers bias and not enough spent to process the actual\u000acontent. Also the speaking cadence made it quite hard to listen to. Ended up\u000agoing to 1.5x by the end, which I rarely do. </p>\u000a\u000a<h2>Science and Human Values (3/5) - September 2015</h2>\u000a\u000a<p>Broznowski tries really hard to impress with his breadth of knowledge. He\u000aconstantly drops references to art, literature and philosophy to make his point.\u000aIt's as if he is using himself as an example of a scientist who is also well\u000aversed in liberal art (there are dozens of us!!). I found most of the writing to\u000abe very fluffy, deliberately convoluted, and kind of annoying. I had a hard time\u000afinishing despite it being such a short read. I set the book down for a month,\u000aand returned to it later (upon returning from vacation).</p>\u000a\u000a<p>In the first part, Broznowski makes the argument that science isn't in and of\u000aitself good or evil, but just a tool. The bombing of Hiroshima and Nagasaki, he\u000aattributes not to the invention of the atomic bomb, but to the politicians that\u000achose to use it. Seems like a pretty basic argument, but also flawed and not\u000anew. Not being well versed in the philosophy of Science, not in a good position\u000ato judge it.</p>\u000a\u000a<p>In the third and final essay, Broznowski considers science as a social\u000ainstitution, and compares it to the rest of society. This part I thought was\u000ainteresting.  Science has many attractive, democratic properties that I agree\u000acould really stand to be ported into other human domains.</p>\u000a\u000a<h2>Thinking Fast and Slow (5/5) - September 2015</h2>\u000a\u000a<p>An excellent read. Kahneman basically summarizes his career as a psychologist,\u000ain which he systematically found flaws in human ability to think rationally.\u000aHe's a founder of behavioral economics, which describes the ways in which humans\u000amake systematic thinking errors (deviating from "pure" rationality)..</p>\u000a\u000a<h2>Audio: Skeptic's Guide to American History (4/5) - August 2015</h2>\u000a\u000a<p>Well done overview of American History with a historiographical twist. Reminded\u000ame of Mr. Begin's history classes, but probably a little more sophisticated. I\u000areally liked the lecturer's call to skepticism which rang throughout, as he\u000aquestioned theme after theme of cliche Americanisms. I really liked his spiral\u000amodel of historical progress, and also the fact that he dropped enough\u000areferences that I now have plenty of follow up work to do. A worthwhile course\u000afor any non-American. Buy it on <a href="http://www.thegreatcourses.com/courses/the-skeptic-s-guide-to-american-history.html">TTC</a>.</p>\u000a\u000a<h2>Digital Signal Processing Coursera (3/5) - Finished in July 2015</h2>\u000a\u000a<p>https://class.coursera.org/dsp-004/</p>\u000a\u000a<p>Professors Martin Vetterli &amp; Paolo Prandoni give an in-depth introduction to\u000adigital signals, filtering, etc. A caveat: I only went through the first 10 (out\u000aof 18) days of lectures, and did only some of the homework assignments.</p>\u000a\u000a<p>Initially spurred on by a broad interest in music, audio, and sensor processing,\u000athis course served as a very solid theoretical foundation.  The course starts\u000aoff with a very dry, theoretical exploration into Hilbert spaces, which served\u000aas an excellent reminder that I can still sort of understand and enjoy\u000amathematics. However my somewhat pathetic mathematical abilities were stretched\u000ato their absolute limit by Vetterli's systematic barrage of theory, and I would\u000ahave surely given up on day 2 if not for some of those upper year linear algebra\u000aclasses (thanks UBC!).</p>\u000a\u000a<p>The lecture's first focus is on <strong>how to understand signals</strong>. Still shaky on\u000asome of the theory, the lecture delves deep into the theory of fourier analysis\u000aand the formal definition of the discrete fourier analysis and various related\u000amethods like the DTFT. The treatment of the subject is so dry, it's hard to\u000abreathe. Until as if in a sudden rainstorm, the theoretical desert can breathe\u000aagain when finally they bring up examples of the STFT, which I have been using\u000afor a while for various audio-related applications at work, without knowing what\u000ait was called.</p>\u000a\u000a<p>The next big subject is <strong>transforming signals</strong>, or signal processing.  Here,\u000athe professors introduce LTIs formally, and derive the idea that an impulse\u000aresponse fully characterizes an LTI. Next they dive into simple filters, FIRs\u000aand IIRs, covering various types of averages, discussing filter stability and\u000aother formal properties. They try to build some intuition about filters from a\u000afrequency-domain perspective, which is very useful to me. Lastly, the subject of\u000aideal filters is explored, as well as why such filters cannot be implemented in\u000areal life. </p>\u000a\u000a<p>Finally, they discuss how to actually design filters using the z-transform,\u000awhose derivation remains somewhat mysterious to me, which is unfortunately\u000anormal for an engineering course. Prandoni gives a good interpretation for the\u000apole-zero plot, which is a really elegant way of looking at filters. The lecture\u000aturns practical for a brief moment, when they reveal that implementations\u000aalready exist for non-ideal versions of many of these filters and explore a few\u000adifferences between Butterworth, Chebyshev and Elliptic low pass filters. They\u000aalso talk about FIRs, and I finally understand the jargon of 'taps', which I\u000ajust heard about at a lecture held at Noisebridge.</p>\u000a\u000a<p>I still struggle to convert block diagrams into equations, but feel like I have\u000aa much better sense of second order IIRs that ship with the Web Audio API.\u000aOverall a mathematically challenging, but interesting class.  I really wish\u000athere was a better way to build intuition for a lot of the theory, though.</p>\u000a\u000a<h2>Audio: Understanding the Secrets of Human Perception (4/5) -- June 2015</h2>\u000a\u000a<p>http://www.thegreatcourses.com/courses/understanding-the-secrets-of-human-perception.html</p>\u000a\u000a<p>Excellent overview of perception. I learned a ton from these lectures, and\u000areally want to enumerate all of the illusions and experiments that the lecturer\u000acites which give insight into the way our senses work. Lots of in-depth material\u000aabout our visual system, how we percieve motion, the incredible variety of depth\u000acues, as well as after-images and color effects. Some senses, like audition,\u000awere largely review for me, but others, like olfaction and taste were completely\u000anew to me. Multi-modal perception gets especially interesting, the canonical\u000aillusion being the McGurk effect, but <a href="https://en.wikipedia.org/wiki/Multisensory_integration#Multisensory_illusions">there are many more</a>.</p>\u000a\u000a<p>This course is much more than just enumerated sensory illusions, though.  The\u000agreat thing about it is that Vishton spends a fair amount of time on the general\u000aprinciples of operation of our brains, from fundamentals of neurons and synapses\u000ato opponent-process theory, which explains a great deal of perception phenomena\u000aacross the senses. From a UX and VR perspective, the material in this course is\u000atotally invaluable. </p>\u000a\u000a<h2>John Napier - Hands (4/5) -- May 2015</h2>\u000a\u000a<p>Really interesting deep-dive into human hands from many different perspectives:\u000aanatomical, evolutionary, functional and social. The book is packed with facts.\u000aFor example, I had no idea the carpal bones were so numerous and complex! Also,\u000ait's nice to understand why human opposition differs from monkeys and the\u000aOpposability Index. Lots of interesting insights into distribution of\u000ahandedness, including that certain species of monkeys are predominantly left\u000ahanded, which is surprising on several levels. Napier also goes a bit into\u000afingerprints, basic patterns, and mentioning some relation to worn-off finger\u000aprints in people with Celiac. It was fun to cross-correlate what was written in\u000athe book with my own hands.</p>\u000a\u000a<p>Overall a thoroughly interesting account and I found good insights from an HCI\u000aperspective. I did think that the second part on social and cultural aspects was\u000atoo brief and incomplete.</p>\u000a\u000a<blockquote>\u000a  <p>With the eye, the hand is our main source of contact with the physical environment. The hand has advantages over the eye because it can observe the environment by means of touch, and having observed it, it can immediately proceed to do something about it. The hand has other great advantages over the eye. It can see around corners and it can see in the dark.</p>\u000a</blockquote>\u000a\u000a<h2>Vernor Vinge (2/5) - Rainbow's End -- April 2015</h2>\u000a\u000a<p>Real science fiction, in a bad way, focusing more on the science than the\u000afiction, Asimov style. Not that great of a book, underdeveloped characters,\u000acontrived plot. Really felt like it was written by a technologist, unlike say\u000aBashert by my former CS professor. The main reason I read it was for is focus on\u000aVR, but even for that I got bored about half way through and had to force myself\u000ato finish.</p>\u000a\u000a<p>The library allegory felt too obvious, as if the author was just shoving the out\u000awith the old, in with the new in your face.</p>\u000a\u000a<h2>Blue Mind (3/5) -- March 2015</h2>\u000a\u000a<p>The message was good and clear, but the writing was tedious. Like Gladwell\u000abooks, this one could be compressed to a short pamphlet. I liked the distinction\u000abetween hedonism and eudaimonia, which was a very ancillary point in the book,\u000abut still new to me. I also liked the blue marble finish. But the meat of the\u000abook consists of an glorified enumeration of activities related to water.</p>\u000a
p89
tp90
Rp91
sg13
V/books
p92
sg15
Nsg16
I00
sg17
VBook list
p93
sg20
VIn early 2015, I began writing a highlight/review/summary for every book, audio\u000abook, essay or especially memorable podcast I consume, in an attempt to become a\u000amore active reader and listener.
p94
sg6
g91
sg25
g26
sg65
g66
sg31
S'books'
p95
sg33
g92
ssg34
S'content/pages/books.md'
p96
sg36
F1487871945.0
sa(dp97
g2
(dp98
g4
V Interesting, inspirational and informative objects that I've collected from the the digital and physical worlds.  
p99
sg28
g7
(g8
g9
V<p>Interesting, inspirational and informative objects that I've collected\u000afrom the the digital and physical worlds.</p>\u000a
p100
tp101
Rp102
sg13
V/inspiration
p103
sg15
Nsg16
I00
sg17
VBlog
p104
sg20
VInteresting, inspirational and informative objects that I've collected\u000afrom the the digital and physical worlds.
p105
sg6
g102
sg25
g26
sS'type'
p106
S'gallery'
p107
sg31
S'inspiration'
p108
sg33
g103
ssg34
S'content/pages/inspiration.md'
p109
sg36
F1434414313.0
sa(dp110
g2
(dp111
g4
V Experience     2012 - Current: Software Engineer,  Google        Prototyping emerging user interfaces at Google Research.       2010 - 2012: Chrome Developer Programs Engineer,  Google        Released and maintained widely used libraries to help mobile web developers   Wrote significant open source sample applications (see  github )   Wrote technical articles on  http://html5rocks.com  and  http://smus.com  totalling over 500K page views   Presented over 20 public talks, reaching over 100K people (including video views)   Software engineer on the Google +1 chrome extension       Summer 2010: Software Engineer Intern,  Google        Wrote product requirements and designed UI concepts for a major Google TV feature   Implemented a working prototype in Java/Android   Designed and executed a lab usability study       2006 \u2013 2009: Software Engineer,  Apple        Developed software architecture for significant portions of  iWork.com    Implemented new features for iWeb and the iWork suite in Objective-C   Created a text and object selection engine in JavaScript   Built a cross-browser graphics library using SVG, Canvas and VML   Prototyped new product ideas using JavaScript and Ruby on Rails   Worked closely with visual designers on new products   Wrote parts of the Microsoft Office document viewer for iPhone Mail       Summer 2005: Software Engineer Intern,  Research in Motion        Developed features for a 2D graphics engine in J2ME, part of BlackBerry OS   Created a python program to automate SVG content generation to test this engine   Proposed a test suite to verify rendering output against a image       Summer 2004: Software Engineer Intern,  Custom House        Created a GUI to facilitate user friendly mapping from excel into a MSSQL database   Contributed improvements to a large currency exchange system written in C#   Followed Agile software development principles: unit testing, scrums       Summer 2003: Software Engineer Intern, SchemaSoft (acquired by Apple in 2005)       Developed bindings for a subset of the SVG DOM in C++   Implemented in C# an editor for easily editing and navigating XML documents   Maintained a network of 100 computers running Linux, Mac, and Windows   Provided technical support to a large international conference: SVG Open 2003      Education     2009 \u2013 2010: Graduate Student,  Carnegie Mellon University        Masters in Human-Computer Interaction.   Emphasis on physical computing.       2002 \u2013 2007: Undergraduate Student,  University of British Columbia        BSc (with honors) in Mathematics and Computer Science (with Co-op)   Emphasis on computer graphics and discrete math.      Other     Publications        Kittur, N., Smus, B., and Kraut, R.E., Khamkar, S. CrowdForge: Crowdsourcing Complex Work.  Proceedings of UIST 2011, Santa Barbara, CA.      Rogstadius, J., Kittur, N., Kostakos, V., Smus, B., Laredo, J., Vukovic, M. An Assessment of Intrinsic and Extrinsic Motivation on Task Performance in Crowdsourcing Markets.  Conference on Weblogs and Social Media 2011, Barcelona, Spain      Kittur, N., Smus, B., and Kraut, R.E. CrowdForge: Crowdsourcing Complex Work.  Proceedings of the extended abstracts of CHI 2011, Vancouver, Canada.      Smus, B., Kostakos, V. Running gestures: hands-free interaction during physical activity.   Adjunct proceedings of Ubicomp 2010, Copehnagen, Denmark.      Smus, B., Gross, M. Ubiquitous drums: a tangible, wearable musical interface.  Proceedings of the extended abstracts of CHI 2010, Atlanta, Georgia.         Patents        Many more pending.     Smus, B. Determining user handedness and orientation using a touchscreen device  US 9239648      Smus, B., Kauffmann, A., Plagemann, C. Situated multi-person user interface  US 9237182      Smus, B. Automatically switching between input modes for a user interface  US 9026939      Smus, B. Automatic device login based on wearable sensor fusion, Google Inc  US 8928587      Smus, B. Keyboard event detection and network event generation, Google Inc  US 8803808      
p112
sg28
g7
(g8
g9
V<h2>Experience</h2>\u000a\u000a<p><strong>2012 - Current: Software Engineer, <a href="http://www.google.com">Google</a></strong></p>\u000a\u000a<ul>\u000a<li>Prototyping emerging user interfaces at Google Research.</li>\u000a</ul>\u000a\u000a<p><strong>2010 - 2012: Chrome Developer Programs Engineer, <a href="http://www.google.com">Google</a></strong></p>\u000a\u000a<ul>\u000a<li>Released and maintained widely used libraries to help mobile web\u000adevelopers</li>\u000a<li>Wrote significant open source sample applications (see <a href="https://github.com/borismus/">github</a>)</li>\u000a<li>Wrote technical articles on <a href="http://html5rocks.com">http://html5rocks.com</a> and\u000a<a href="http://smus.com">http://smus.com</a> totalling over 500K page views</li>\u000a<li>Presented over 20 public talks, reaching over 100K people (including\u000avideo views)</li>\u000a<li>Software engineer on the Google +1 chrome extension</li>\u000a</ul>\u000a\u000a<p><strong>Summer 2010: Software Engineer Intern, <a href="http://www.google.com">Google</a></strong></p>\u000a\u000a<ul>\u000a<li>Wrote product requirements and designed UI concepts for a major\u000aGoogle TV feature</li>\u000a<li>Implemented a working prototype in Java/Android</li>\u000a<li>Designed and executed a lab usability study</li>\u000a</ul>\u000a\u000a<p><strong>2006 \u2013 2009: Software Engineer, <a href="http://www.apple.com">Apple</a></strong></p>\u000a\u000a<ul>\u000a<li>Developed software architecture for significant portions of\u000a<a href="http://www.iwork.com/">iWork.com</a></li>\u000a<li>Implemented new features for iWeb and the iWork suite in Objective-C</li>\u000a<li>Created a text and object selection engine in JavaScript</li>\u000a<li>Built a cross-browser graphics library using SVG, Canvas and VML</li>\u000a<li>Prototyped new product ideas using JavaScript and Ruby on Rails</li>\u000a<li>Worked closely with visual designers on new products</li>\u000a<li>Wrote parts of the Microsoft Office document viewer for iPhone Mail</li>\u000a</ul>\u000a\u000a<p><strong>Summer 2005: Software Engineer Intern, <a href="http://www.rim.com">Research in Motion</a></strong></p>\u000a\u000a<ul>\u000a<li>Developed features for a 2D graphics engine in J2ME, part of\u000aBlackBerry OS</li>\u000a<li>Created a python program to automate SVG content generation to test\u000athis engine</li>\u000a<li>Proposed a test suite to verify rendering output against a image</li>\u000a</ul>\u000a\u000a<p><strong>Summer 2004: Software Engineer Intern, <a href="http://www.customhouse.ca/">Custom House</a></strong></p>\u000a\u000a<ul>\u000a<li>Created a GUI to facilitate user friendly mapping from excel into a\u000aMSSQL database</li>\u000a<li>Contributed improvements to a large currency exchange system written\u000ain C#</li>\u000a<li>Followed Agile software development principles: unit testing, scrums</li>\u000a</ul>\u000a\u000a<p><strong>Summer 2003: Software Engineer Intern, SchemaSoft (acquired by Apple in 2005)</strong></p>\u000a\u000a<ul>\u000a<li>Developed bindings for a subset of the SVG DOM in C++</li>\u000a<li>Implemented in C# an editor for easily editing and navigating XML\u000adocuments</li>\u000a<li>Maintained a network of 100 computers running Linux, Mac, and\u000aWindows</li>\u000a<li>Provided technical support to a large international conference: SVG\u000aOpen 2003</li>\u000a</ul>\u000a\u000a<h2>Education</h2>\u000a\u000a<p><strong>2009 \u2013 2010: Graduate Student, <a href="http://www.cmu.edu">Carnegie Mellon University</a></strong></p>\u000a\u000a<ul>\u000a<li>Masters in Human-Computer Interaction.</li>\u000a<li>Emphasis on physical computing.</li>\u000a</ul>\u000a\u000a<p><strong>2002 \u2013 2007: Undergraduate Student, <a href="http://www.ubc.edu">University of British Columbia</a></strong></p>\u000a\u000a<ul>\u000a<li>BSc (with honors) in Mathematics and Computer Science (with Co-op)</li>\u000a<li>Emphasis on computer graphics and discrete math.</li>\u000a</ul>\u000a\u000a<h2>Other</h2>\u000a\u000a<p><strong>Publications</strong></p>\u000a\u000a<ul>\u000a<li><p>Kittur, N., Smus, B., and Kraut, R.E., Khamkar, S. CrowdForge:\u000aCrowdsourcing Complex Work. <em>Proceedings of UIST 2011, Santa Barbara,\u000aCA.</em></p></li>\u000a<li><p>Rogstadius, J., Kittur, N., Kostakos, V., Smus, B., Laredo, J., Vukovic, M.\u000aAn Assessment of Intrinsic and Extrinsic Motivation on Task Performance in\u000aCrowdsourcing Markets. <em>Conference on Weblogs and Social Media 2011,\u000aBarcelona, Spain</em></p></li>\u000a<li><p>Kittur, N., Smus, B., and Kraut, R.E. CrowdForge: Crowdsourcing Complex\u000aWork. <em>Proceedings of the extended abstracts of CHI 2011, Vancouver,\u000aCanada.</em></p></li>\u000a<li><p>Smus, B., Kostakos, V. Running gestures: hands-free interaction\u000aduring physical activity.  <em>Adjunct proceedings of Ubicomp 2010,\u000aCopehnagen, Denmark.</em></p></li>\u000a<li><p>Smus, B., Gross, M. Ubiquitous drums: a tangible, wearable musical\u000ainterface. <em>Proceedings of the extended abstracts of CHI 2010, Atlanta,\u000aGeorgia.</em></p></li>\u000a</ul>\u000a\u000a<p><strong>Patents</strong></p>\u000a\u000a<ul>\u000a<li><p>Many more pending.</p></li>\u000a<li><p>Smus, B. Determining user handedness and orientation using a touchscreen\u000adevice <a href="https://www.google.com/patents/US9239648">US 9239648</a></p></li>\u000a<li><p>Smus, B., Kauffmann, A., Plagemann, C. Situated multi-person user interface\u000a<a href="https://www.google.com/patents/US9237182">US 9237182</a></p></li>\u000a<li><p>Smus, B. Automatically switching between input modes for a user\u000ainterface <a href="https://www.google.com/patents/US9026939">US 9026939</a></p></li>\u000a<li><p>Smus, B. Automatic device login based on wearable sensor fusion,\u000aGoogle Inc <a href="https://www.google.com/patents/US8928587">US 8928587</a></p></li>\u000a<li><p>Smus, B. Keyboard event detection and network event generation, Google\u000aInc <a href="https://www.google.com/patents/US20100095198">US 8803808</a></p></li>\u000a</ul>\u000a
p113
tp114
Rp115
sg13
V/resume
p116
sg15
Nsg16
I00
sg17
VResume
p117
sg20
V## Experience\u000a\u000a**2012 - Current: Software Engineer, [Google][]**\u000a\u000a- Prototyping emerging user interfaces at Google Research.
p118
sg6
g115
sg25
g26
sg65
g66
sg31
S'resume'
p119
sg33
g116
ssg34
S'content/pages/resume.md'
p120
sg36
F1453224862.0
sa(dp121
g2
(dp122
g4
V This site is statically generated with  Lightning , a blogging engine I wrote. The basic idea is that I write all of my content like this:     About this site ===============  This site is statically generated with [Lightning][lightning]      Then a script converts it into the HTML/CSS that you are reading now.    There are many such projects, (eg.  Hyde ,  Octopress ). This approach provides clear advantages over a traditional database-based blog:      Use any editor you like.   Use Markdown for structured text.   No worries about database management and security.      Lightning    I wrote Lightning to scratch some specific itches (which may or may not have been addressed in other similar projects by the time you read this):      Incremental builds (only rebuild what changed).   Quick deployment.   Logical placement of content-specific assets (in same dir as content).   Decouple content structure from output structure.   Minimalist metadata header.   Output that doesn't require htaccess, rewrite rules and other HTTPD specific setup.      An early version of  Lightning is available on github .  
p123
sg28
g7
(g8
g9
V<p>This site is statically generated with <a href="https://github.com/borismus/lightning">Lightning</a>, a\u000ablogging engine I wrote. The basic idea is that I write all of my\u000acontent like this:</p>\u000a\u000a<pre><code>About this site\u000a===============\u000a\u000aThis site is statically generated with [Lightning][lightning]\u000a</code></pre>\u000a\u000a<p>Then a script converts it into the HTML/CSS that you are reading now.</p>\u000a\u000a<p>There are many such projects, (eg. <a href="http://ringce.com/hyde">Hyde</a>,\u000a<a href="http://octopress.org/">Octopress</a>). This approach provides clear advantages over a\u000atraditional database-based blog:</p>\u000a\u000a<ul>\u000a<li>Use any editor you like.</li>\u000a<li>Use Markdown for structured text.</li>\u000a<li>No worries about database management and security.</li>\u000a</ul>\u000a\u000a<h2>Lightning</h2>\u000a\u000a<p>I wrote Lightning to scratch some specific itches (which may or may not\u000ahave been addressed in other similar projects by the time you read\u000athis):</p>\u000a\u000a<ul>\u000a<li>Incremental builds (only rebuild what changed).</li>\u000a<li>Quick deployment.</li>\u000a<li>Logical placement of content-specific assets (in same dir as\u000acontent).</li>\u000a<li>Decouple content structure from output structure.</li>\u000a<li>Minimalist metadata header.</li>\u000a<li>Output that doesn't require htaccess, rewrite rules and other HTTPD\u000aspecific setup.</li>\u000a</ul>\u000a\u000a<p>An early version of <a href="https://github.com/borismus/lightning">Lightning is available on github</a>.</p>\u000a
p124
tp125
Rp126
sg13
V/site
p127
sg15
Nsg16
I00
sg17
VAbout this site
p128
sg20
VThis site is statically generated with [Lightning][lightning], a\u000ablogging engine I wrote.
p129
sg6
g126
sg25
g26
sg65
g66
sg31
S'site'
p130
sg33
g127
ssg34
S'content/pages/site.md'
p131
sg36
F1433965388.0
sa(dp132
g2
(dp133
g4
V I'm thankful for many opportunities to co-present with and learn from amazing people. Here are some talks I gave recently.       Presented  VR for every website  at  SVVR 2016  and at the  WebVR Ecosystem and API Update HTML5 meetup .     Presented  Magnetic Input for Mobile Virtual Reality  at  ISWC 2015 .     Co-presented  Cardboard  with  David Coz  and  Christian Plagemann  at  Google I/O 2014 . The  talk was recorded .     Co-presented Point, Click, Tap, Touch with  Rick Byers  at  Google I/O 2013 . The talk was  recorded  and the  slides  are posted.     Presented Building fast UIs for the cross-device web at  Google I/O 2012  and at the  SF HTML5 meetup . The  video  and  slides  are available.     Co-presented Audio on the web with  Lucas Gonze  at the  SF Music Tech conference in 2012 . The  slides are available .     Presented  CrowdForge  at  UIST 2011 . The  slides are available .     Traveled to India and presented the keynote at both the Bangalore and Hyderabad DevFests in 2011. The  slides are available .     Co-organized and presented at the HTML5 in-app payments hackathon in Google Kirkland with  Pete LePage  and  Peng Ying .  Slides  are posted.     Presented the Chrome Developer Tools at  Adobe's HTML5 Camp  and later at  VanJS . Slides  are available .     Presented on mobile web touch events at the  HTML5 Summer Dev Fest  in Vancouver, organized by  Boris Mann . The  slides are available .     In 2011, I traveled to Sao Paulo, Brazil and gave a general presentation on HTML5 and Chrome to a variety of Brazilian media companies. The  slides are available .     Co-presented a workshop on the Chrome Developer Tools with  Paul Irish  at Google I/O 2011 Bootcamp in San Francisco, CA. The materials from the workshop are  available on github .     My first public talk was about freakin' HTML5, co-presented with Paul Irish at SXSW 2011. Our  slides are posted .     
p134
sg28
g7
(g8
g9
V<p>I'm thankful for many opportunities to co-present with and learn from amazing\u000apeople. Here are some talks I gave recently.</p>\u000a\u000a<ul>\u000a<li><p>Presented <a href="https://goo.gl/eBSi6L">VR for every website</a> at <a href="http://vrexpo.com/events/vr-every-website/">SVVR\u000a2016</a> and at the <a href="https://www.meetup.com/sfhtml5/events/230072340/">WebVR Ecosystem\u000aand API Update HTML5\u000ameetup</a>.</p></li>\u000a<li><p>Presented <a href="https://goo.gl/zKA6DN">Magnetic Input for Mobile Virtual Reality</a>\u000aat <a href="http://www.iswc.net/iswc15/">ISWC 2015</a>.</p></li>\u000a<li><p>Co-presented <a href="g.co/cardboard">Cardboard</a> with <a href="https://twitter.com/dav_cz">David\u000aCoz</a> and <a href="http://plagemann.net/">Christian\u000aPlagemann</a> at <a href="https://www.google.com/events/io">Google I/O\u000a2014</a>. The <a href="https://www.youtube.com/watch?v=DFog2gMnm44">talk was\u000arecorded</a>.</p></li>\u000a<li><p>Co-presented Point, Click, Tap, Touch with <a href="https://twitter.com/rickbyers">Rick\u000aByers</a> at <a href="https://developers.google.com/events/io/">Google I/O\u000a2013</a>. The talk was\u000a<a href="http://www.youtube.com/watch?v=DujfpXOKUp8">recorded</a> and the <a href="https://docs.google.com/a/google.com/presentation/d/1-n1qyzewpagREbzW2zm0wOalq33UhbtbSkWf9mEdly8/edit">slides</a> are posted.</p></li>\u000a<li><p>Presented Building fast UIs for the cross-device web at <a href="https://developers.google.com/events/io/2012/">Google I/O\u000a2012</a> and at the <a href="http://www.sfhtml5.org/">SF\u000aHTML5 meetup</a>. The\u000a<a href="http://www.youtube.com/watch?v=ie4I7B-umbA">video</a> and\u000a<a href="http://smustalks.appspot.com/io-12">slides</a> are available.</p></li>\u000a<li><p>Co-presented Audio on the web with <a href="http://gonze.com/">Lucas Gonze</a> at\u000athe <a href="http://www.sfmusictech.com/">SF Music Tech conference in 2012</a>.\u000aThe <a href="http://smustalks.appspot.com/sfmt-12/">slides are available</a>.</p></li>\u000a<li><p>Presented <a href="http://crowdforge.com">CrowdForge</a> at <a href="/uist-2011">UIST\u000a2011</a>. The <a href="http://smustalks.appspot.com/crowdforge-11/">slides are available</a>.</p></li>\u000a<li><p>Traveled to India and presented the keynote at both the Bangalore and\u000aHyderabad DevFests in 2011. The <a href="http://smustalks.appspot.com/keynote-india-devfest-11">slides are\u000aavailable</a>.</p></li>\u000a<li><p>Co-organized and presented at the HTML5 in-app payments hackathon in\u000aGoogle Kirkland with <a href="http://petelepage.com/">Pete LePage</a> and <a href="https://twitter.com/pengying">Peng\u000aYing</a>. <a href="http://html5hack.appspot.com/preso/crhack.html">Slides</a> are posted.</p></li>\u000a<li><p>Presented the Chrome Developer Tools at <a href="http://beta.theexpressiveweb.com/">Adobe's HTML5\u000aCamp</a> and later at\u000a<a href="http://www.meetup.com/vancouver-javascript-developers/">VanJS</a>. Slides\u000a<a href="http://smustalks.appspot.com/devtools-adobe-11">are available</a>.</p></li>\u000a<li><p>Presented on mobile web touch events at the <a href="http://html5summerdevfest.eventbrite.com/">HTML5 Summer Dev\u000aFest</a> in Vancouver,\u000aorganized by <a href="https://twitter.com/bmann">Boris Mann</a>. The <a href="http://smustalks.appspot.com/touch-11/">slides are available</a>.</p></li>\u000a<li><p>In 2011, I traveled to Sao Paulo, Brazil and gave a general\u000apresentation on HTML5 and Chrome to a variety of Brazilian media\u000acompanies. The <a href="http://smustalks.appspot.com/brazil-11/">slides are\u000aavailable</a>.</p></li>\u000a<li><p>Co-presented a workshop on the Chrome Developer Tools with <a href="http://paulirish.com/">Paul\u000aIrish</a> at Google I/O 2011 Bootcamp in San\u000aFrancisco, CA. The materials from the workshop are <a href="https://github.com/borismus/DevTools-Lab">available on\u000agithub</a>.</p></li>\u000a<li><p>My first public talk was about freakin' HTML5, co-presented with Paul\u000aIrish at SXSW 2011. Our <a href="http://smustalks.appspot.com/sxsw-11/">slides are\u000aposted</a>.</p></li>\u000a</ul>\u000a
p135
tp136
Rp137
sg13
V/talks
p138
sg15
Nsg16
I00
sg17
VTalks I've given
p139
sg20
VI'm thankful for many opportunities to co-present with and learn from amazing\u000apeople.
p140
sg6
g137
sg25
g26
sg65
g66
sg31
S'talks'
p141
sg33
g138
ssg34
S'content/pages/talks.md'
p142
sg36
F1476762485.0
sa(dp143
g2
(dp144
g4
V  GALLERY_URL = '/projects/all.json'     Recent projects I've worked on.  
p145
sg28
g7
(g8
g9
V<script>\u000aGALLERY_URL = '/projects/all.json'\u000a</script>\u000a\u000a<p>Recent projects I've worked on.</p>\u000a
p146
tp147
Rp148
sg13
V/projects
p149
sg15
Nsg16
I00
sg17
VBlog
p150
sg20
V<script>\u000aGALLERY_URL = '/projects/all.
p151
sg6
g148
sg25
g26
sS'type'
p152
S'gallery'
p153
sg31
S'projects'
p154
sg33
g149
ssg34
S'content/pages/projects/index.md'
p155
sg36
F1434226389.0
sa(dp156
g2
(dp157
g4
V A rudimentary attempt to write a tool that detects unused CSS styles in a stylesheet.  
p158
sg28
g7
(g8
g9
V<p>I wanted a tool that would analyze a complete web site, and report what\u000aCSS selectors and IDs are dead. By dead, I mean one of two things --\u000aeither the ID or selector is referenced from the HTML and undefined in\u000athe CSS or it is defined in the CSS but never referenced in the HTML.</p>\u000a\u000a<p>A search for some CSS finding utilities proved somewhat fruitful. I\u000afound a brief <a href="http://www.aggiorno.com/blogs/aggiornings/post/Detecting-unused-CSS-selectors-.aspx">survey of related utilities</a> for this finding or\u000acleaning dead CSS, the most promising of which was a Firefox plug-in\u000acalled <a href="http://www.sitepoint.com/dustmeselectors/">Dust-Me Selectors</a>. But I wanted something that could be\u000aintegrated into an automatic build process, easily invokable from the\u000acommand line without requiring a browser, so I started thinking about a\u000acustom solution.</p>\u000a\u000a<p>The problem can be solved as follows:</p>\u000a\u000a<ol>\u000a<li>Find all referenced IDs and classes, called R</li>\u000a<li>Find all defined IDs and classes, called D</li>\u000a<li>Take a difference between the sets so that the list of undefined IDs\u000aand classes is (R - D), and the list of unreferenced IDs and classes\u000ais (D - R)</li>\u000a</ol>\u000a\u000a<p>I've already had the pleasure of using <a href="http://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a> python library\u000ato parse all sorts of HTML documents, and I quickly found <a href="http://cthedot.de/cssutils/">cssutils</a>\u000ato be a very handy CSS parser. In a matter of hours I was able to whip\u000aup a basic dead CSS finder in 100 lines of code using these great tools.\u000aI named it 7sense, after <a href="http://www.imdb.com/title/tt0167404/">the movie</a>. To run it, you just need to\u000ainvoke <code>./7sense.py &lt;list of files and directories&gt;</code> and the specified\u000afiles will be parsed as if they were all part of the same webpage.\u000aDirectories will be walked recursively, with all encountered CSS and\u000aHTML files assumed to be part of the web page.</p>\u000a\u000a<p>But things are not as simple as I had hoped, and my program has several\u000anotable limitations.</p>\u000a\u000a<ol>\u000a<li>Due to lack of time, 7sense does not look at what stylesheets are\u000areferenced by an HTML page. Instead, you have to tell it explicitly\u000awhat stylesheets are used by passing them as arguments.</li>\u000a<li>7sense does not properly decipher heirarchical CSS selectors like\u000a'#myContainer .aboutBox'. Instead, it splits the selector into\u000awhitespace separated tokens, ignoring their structure. I skimped on\u000athis feature also due to lack of time.</li>\u000a<li>More fundamentally, 7sense is not aware of any Javascript\u000amodifications to the DOM. This could be remedied on a case-by-case\u000abasis. For example, one could write a parser to seek jQuery.setClass\u000ainvocations, and extract additional classes from there.</li>\u000a</ol>\u000a\u000a<p>At any rate, here's <a href="http://www.borismus.com/wp-content/uploads/2008/12/7sense.py">7sense so far</a>. Though flawed, it's a useful\u000astart. Ideas and code improvements, especially addressing the above\u000alimitations are very much appreciated!</p>\u000a
p159
tp160
Rp161
sg13
V/i-see-dead-css
p162
sg15
Nsg16
I01
sg17
VI see dead CSS
p163
sg20
V\u000a\u000aI wanted a tool that would analyze a complete web site, and report what\u000aCSS selectors and IDs are dead.
p164
sS'snip'
p165
g7
(g8
g9
V<p>A rudimentary attempt to write a tool that detects unused CSS styles in a stylesheet.</p>\u000a
p166
tp167
Rp168
sg25
S'Published'
p169
sg33
g162
sS'posted_info'
p170
(dp171
S'month_name'
p172
S'Dec'
p173
sS'formatted'
p174
S'December 11, 2008'
p175
sS'month'
p176
I12
sS'rfc'
p177
S'2008-12-11T09:00:00-00:00'
p178
sS'unix'
p179
I1229014800
sS'year'
p180
I2008
sS'day'
p181
I11
ssg65
S'post'
p182
sg31
S'i-see-dead-css'
p183
sS'categories'
p184
(lp185
S'web'
p186
asS'posted'
p187
cdatetime
date
p188
(S'\x07\xd8\x0c\x0b'
p189
tp190
Rp191
ssg34
S'content/posts/2008/i-see-dead-css/index.md'
p192
sg36
F1433825937.0
sa(dp193
g2
(dp194
g4
V A musical hack about using JSONP to fetch lyrics cross domain from LyricWiki.org.  
p195
sg28
g7
(g8
g9
V<p>When I want to learn a new song on guitar, I often search for chords\u000aonline. There are many sites that provide chords and tabs, and Google\u000aindexes them nicely. But the quality of chords is often poor, and\u000athere's no way to submit corrections. When I ran a MoinMoin wiki, I kept\u000amy fixed versions of songs there. Even making modifications to existing\u000achords was painful though, since it involved hand-editing a plain text\u000afile and ensuring that the chords were properly aligned with the lyrics.\u000aMy preferred solution to this problem is to write a web application to\u000afacilitate easy collaborative editing of simple folk/rock/pop songs. </p>\u000a\u000a<p>I started working on a proof-of-concept prototype. To begin with, I wanted\u000aan easy way of finding song lyrics, which provide the skeleton for most\u000aguitar pieces that I'm interested in. Later, I planned to work on\u000aannotating those lyrics. As I prepared to whip up a light django\u000aapplication to scrape lyrics sites, I realized that there may be a\u000asimpler way: if I found a friendly lyrics API, there would be no need to\u000awrite any server side code. Could the entire service be written in\u000aJavascript? </p>\u000a\u000a<p>LyricWiki.org had exactly what I need: a simple way to\u000aaccess <a href="http://lyricwiki.org/api.php?artist=Slipknot&amp;song=Before_I_Forget&amp;fmt=json">lyrics in JSON</a> format. Of course, you can't just do an\u000aXmlHttpRequest to lyricswiki.org because of XSS security restrictions.\u000aInstead, you <em>can</em> work around this cross-domain scripting issue by\u000awriting out <script\u005c&gt; tags, and specifying the script src dynamically.\u000aLuckily, the LyricWiki JSON is wrapped (padded) in a variable named\u000a'song'. As I later discovered, this practice of wrapping JSON in a\u000avariable is well established, and called <a href="http://en.wikipedia.org/wiki/JSONP">JSONP</a>. This technique makes\u000ait easy to simply evaluate the script tag, and wait for the song\u000avariable to change. Now If only there was a safe way of doing this sort\u000aof cross-domain scripting without introducing a <a href="http://personalized20.blogspot.com/2006/02/jsonp-service-and-security.html">host</a> of <a href="http://unclehulka.com/ryan/blog/archives/2005/12/12/jsonpyoure-joking-right/">XSS</a>\u000a<a href="http://www.west-wind.com/Weblog/posts/107136.aspx">vulnerabilities</a>, writing mashups would be a walk in the park! </p>\u000a\u000a<p>After only two hours of hacking around, including learning jQuery, I came up\u000awith a <a href="lyricwiki">little demo application</a>, written in pure Javascript. Hooray for\u000ajQuery, humanmsg, and the LyricsWiki API!</p>\u000a
p196
tp197
Rp198
sg13
V/mashups-in-pure-js
p199
sg15
Nsg16
I01
sg17
VMusical mashups in pure JavaScript
p200
sg20
V\u000a\u000aWhen I want to learn a new song on guitar, I often search for chords\u000aonline.
p201
sS'snip'
p202
g7
(g8
g9
V<p>A musical hack about using JSONP to fetch lyrics cross domain from LyricWiki.org.</p>\u000a
p203
tp204
Rp205
sg25
g169
sg33
g199
sg170
(dp206
g172
S'Dec'
p207
sg174
S'December 4, 2008'
p208
sg176
I12
sg177
S'2008-12-04T09:00:00-00:00'
p209
sg179
I1228410000
sg180
I2008
sg181
I4
ssg65
g182
sg31
S'mashups-in-pure-js'
p210
sS'categories'
p211
(lp212
S'web'
p213
aS'music'
p214
asS'posted'
p215
g188
(S'\x07\xd8\x0c\x04'
p216
tp217
Rp218
ssg34
S'content/posts/2008/mashups-in-pure-js/index.md'
p219
sg36
F1433825943.0
sa(dp220
g2
(dp221
g4
V Fear (of  Shaw  downtime) is the mind killer. So as of today, this blog is hosted on  WebFaction .  
p222
sg28
g7
(g8
g9
V<p>Fear (of <a href="http://www.shaw.ca/en-ca">Shaw</a> downtime) is the mind killer. So as of today, this\u000ablog is hosted on <a href="http://www.webfaction.com/">WebFaction</a>.</p>\u000a
p223
tp224
Rp225
sg13
V/moving-day
p226
sg15
Nsg16
I00
sg17
VMoving day
p227
sg20
V\u000a\u000aFear (of [Shaw][] downtime) is the mind killer.
p228
sg6
g225
sg25
g169
sg33
g226
sg170
(dp229
g172
S'Nov'
p230
sg174
S'November 24, 2008'
p231
sg176
I11
sg177
S'2008-11-24T09:00:00-00:00'
p232
sg179
I1227546000
sg180
I2008
sg181
I24
ssg65
g182
sg31
S'moving-day'
p233
sS'categories'
p234
(lp235
S'personal'
p236
asS'posted'
p237
g188
(S'\x07\xd8\x0b\x18'
p238
tp239
Rp240
ssg34
S'content/posts/2008/moving-day/index.md'
p241
sg36
F1433825946.0
sa(dp242
g2
(dp243
g4
V About a LEGO Mindstorms robot that listens to a melody and then plays it back on the piano.  
p244
sg28
g7
(g8
g9
V<p>After several weeks of casual spare-time research and implementation,\u000aI've finally built a fully working piano playback robot. The usage is\u000asimple: someone plays or sings an arbitrary monophonic melody, and the\u000arobot, parked on a piano bench, will play it back.</p>\u000a\u000a<iframe class="youtube-16x9" title="YouTube video player"\u000a  src="http://www.youtube.com/embed/Bo0eCSkjy-0" frameborder="0"\u000a  allowfullscreen></iframe>\u000a\u000a<p>The physical construction of the robot is very simple: it consists of a car\u000awith a crane-like arm mounted on it. The arm is used to push down and release a\u000asingle piano key. On either end of the car, there are sensors which detect if\u000athe robot has come too close to the edges of the piano bench. The simplicity of\u000athe robot comes at the price of significant limitations, such as only being\u000aable to play back melodies on the white keys. </p>\u000a\u000a<p>Software is the challenging part of the project. The Mindstorms sound sensor is\u000atoo primitive to use for pitch analysis. Without hacking it, you can only\u000aextract the amplitude of the sound signal, not any frequency details.  Instead\u000aof the NXT sound sensor, I use a macbook pro and it's built-in microphone. A\u000acomputer separate from the NXT is involved, so an additional set of\u000acommunication problems arose. </p>\u000a\u000a<p>Here's a rough outline of happens to make the playback work, from capturing the\u000amelody line to playing the melody back.</p>\u000a\u000a<ol>\u000a<li>On the mac, using a <a href="http://appscript.sourceforge.net/">Python AppleScript bridge</a>, the QuickTime\u000aPlayer is invoked and starts capturing audio.</li>\u000a<li>Once the audio is captured into an AIFF file, a very useful \u000a<a href="http://aubio.org/">pitch detection library</a> called aubio processes the audio file and\u000aextracts raw frequency-to-time data, sampled at some tick rate.\u000aCompiling this library on OS X was quite a feat!</li>\u000a<li>Next, the raw data is processed by throwing out extraneous values\u000aand extracting a melody</li>\u000a<li>Once we have the melody line, we inject it into <a href="http://bricxcc.sourceforge.net/nbc/">an NXC program</a>,\u000aand compile it with the nbc compiler</li>\u000a<li>This program is then sent via bluetooth to the robot via nxtcom</li>\u000a<li>Using <a href="http://home.comcast.net/~dplau/nxt_python/">NXT_Python</a> and <a href="http://www.cs.wlu.edu/~levy/software/nxt_lightblue_glue/">Lightblue Glue</a>, the robot is told to\u000aexecute the program.</li>\u000a</ol>\u000a\u000a<p>Please let me know if you have any questions or suggestions!</p>\u000a
p245
tp246
Rp247
sg13
V/robotic-piano-playback
p248
sg15
Nsg16
I01
sg17
VRobotic piano playback
p249
sg20
V\u000a\u000aAfter several weeks of casual spare-time research and implementation,\u000aI've finally built a fully working piano playback robot.
p250
sS'snip'
p251
g7
(g8
g9
V<p>About a LEGO Mindstorms robot that listens to a melody and then plays it back on the piano.</p>\u000a
p252
tp253
Rp254
sg25
g169
sg33
g248
sg170
(dp255
g172
S'Nov'
p256
sg174
S'November 15, 2008'
p257
sg176
I11
sg177
S'2008-11-15T09:00:00-00:00'
p258
sg179
I1226768400
sg180
I2008
sg181
I15
ssg65
g182
sg31
S'robotic-piano-playback'
p259
sS'categories'
p260
(lp261
S'physical'
p262
asS'posted'
p263
g188
(S'\x07\xd8\x0b\x0f'
p264
tp265
Rp266
ssg34
S'content/posts/2008/robotic-piano-playback/index.md'
p267
sg36
F1433825950.0
sa(dp268
g2
(dp269
g4
V Complaining about how terrible it is to create HTML email that renders properly in multiple mail clients.  
p270
sg28
g7
(g8
g9
V<p>Web design used to be a black art. Ten years ago, browser differences used to\u000abe so dramatic that the only viable solution for an HTML designer was to fall\u000aback to the least common denominator for page layout, which was HTML tables. In\u000atoday's web design community, using table layouts is considered to be a heinous\u000acrime, since most popular modern rendering engines (IE, Gecko and WebKit) are\u000aconverging to some shared interpretation of web standards. Unlike layout\u000aengines on the web, though, rich mail interpreters have remained stagnant, and\u000ain some cases have regressed. <strike>Without pointing any fingers...</strike></p>\u000a\u000a<p>Much of the blame for this difference lies in Microsoft's decision to use the\u000asame engine for composing and viewing email in Outlook 2007. They wanted to\u000amake the life of email designers easier, or so the story goes. But since\u000aInternet Explorer doesn't have editing functionality, and Front Page is too\u000aheavy to embed, the remaining choice was Word '07. Unfortunately, Word's\u000arendering engine is extremely limited, in the following notable ways:</p>\u000a\u000a<ol>\u000a<li>No positioning or floating elements, so CSS-based layouts are out</li>\u000a<li>No CSS backgrounds, combined with (1) means that there's no way to\u000ahave an image background at all.</li>\u000a</ol>\u000a\u000a<p>So, if you care a significant population of email readers (7% use Outlook '07),\u000aand want to deliver a rich media email, you are condemned to designing with\u000atable layouts, or to skip HTML altogether, and simply use images. </p>\u000a\u000a<p>For a long time, there was no good way of determining a\u000abreakdown of email client usage. Recently, the good people at\u000afingerprint have come up with a solution, and now use a large sample of\u000apeople to determine global <a href="http://fingerprintapp.com/email-client-stats">email client usage statistics</a>. According\u000ato them, Hotmail, Yahoo Mail and Gmail add up to roughly 50% of all\u000ausage. </p>\u000a\u000a<p>Since the UI of a web-based email client is written in HTML, so it is critical\u000afor webmail developers to ensure that the CSS and HTML found in the email does\u000anot interfere with the global look and feel. The canonical solutions are to\u000adecorate all ids and classes in the email with some kind of prefix to ensure\u000athat there are no name collisions, and use a white list approach to CSS styles.\u000aThese white lists are usually quite long, but lack some important and oft-used\u000aproperties. For example, CSS image backgrounds are disallowed, except on Yahoo.\u000aAnd of course, Microsoft had to leave its bizarre mark too: Hotmail strips the\u000amargin-top property, but not the margin-bottom property. </p>\u000a\u000a<p>Historically,\u000awebmail clients used to ignore anything outside of the <code>&lt;body\u005c&gt;</code> tag,\u000awhich meant that all CSS had to be written inline, leading to\u000aunmaintainable layouts. In recent tests, however, popular webmail\u000aclients no longer ignore <code>&lt;style\u005c&gt;</code> elements in the <code>&lt;head\u005c&gt;</code>, and instead,\u000aapply the CSS sparingly inline. Among modern native mail clients, there\u000ais a positive trend as well. Microsoft Live Mail uses the IE7 rendering\u000aengine, Thunderbird uses Gecko and Mail.app uses WebKit. So it looks\u000alike there is light at the end of the tunnel for downtrodden HTML email\u000adesigners. In the meantime though, I send them my deepest condolences.</p>\u000a
p271
tp272
Rp273
sg13
V/the-sorry-state-of-html-mail
p274
sg15
Nsg16
I01
sg17
VThe sorry state of HTML mail
p275
sg20
V\u000a\u000aWeb design used to be a black art.
p276
sS'snip'
p277
g7
(g8
g9
V<p>Complaining about how terrible it is to create HTML email that renders properly in multiple mail clients.</p>\u000a
p278
tp279
Rp280
sg25
g169
sg33
g274
sg170
(dp281
g172
S'Oct'
p282
sg174
S'October 31, 2008'
p283
sg176
I10
sg177
S'2008-10-31T09:00:00-00:00'
p284
sg179
I1225468800
sg180
I2008
sg181
I31
ssg65
g182
sg31
S'the-sorry-state-of-html-mail'
p285
sS'categories'
p286
(lp287
S'web'
p288
asS'posted'
p289
g188
(S'\x07\xd8\n\x1f'
p290
tp291
Rp292
ssg34
S'content/posts/2008/the-sorry-state-of-html-mail/index.md'
p293
sg36
F1433825958.0
sa(dp294
g2
(dp295
g4
V A recap of my favorite sessions at my first web conference.  
p296
sg28
g7
(g8
g9
V<p>Yesterday I got back from the Web 2.0 Expo in New York. There were many\u000ainteresting sessions, and I, for the most part, took notes! Here is a\u000ashort list of my five favorite speakers, in no particular order:</p>\u000a\u000a<ol>\u000a<li>Cal Henderson's whirlwind introduction to video on the web. Cal\u000areally likes speak really fast, to make typos in his slides and to\u000acuss, all of which makes for a very informative and entertaining\u000ahour.</li>\u000a<li>John Resig's talk about Processing.js, his port of\u000a<a href="http://processing.org/">Processing</a> to canvas. I'm constantly dealing with graphics on\u000athe web, and it's awesome to have such a powerful library available.\u000aPretty demos, too!</li>\u000a<li>Jason Fried's short but sweet keynote talk, focusing on minimalism\u000ain product management, and proper (read: very narrow) scoping of\u000afeatures. The philosophical question of "what would your software be\u000alike if it was physical?" struck me as a very useful thing to think\u000aabout.</li>\u000a<li>A browser panel including Chris Wilson from MS, Brendan Eich\u000afrom Mozilla and an unnamed developer from Chrome responded to a\u000anice set of questions about future directions of browsers. Poor\u000aChris got a beating IE7's flaws, lack of canvas/SVG support,\u000abarriers to plugin development for IE.</li>\u000a<li>Geir Magnusson Jr delivered an excellent introduction to scaling\u000adata in the cloud.</li>\u000a</ol>\u000a\u000a<p>I also had the chance to talk to and bounce some ideas off of Pete\u000aKoomen, a Google App Engine product manager. He told me that there are\u000aplans both for process scheduling, and for support of django-1.0 down\u000athe road, but of course gave no time frame for either. Overall, the\u000aconference was interesting - there were other good sessions and keynotes\u000awhich I simply haven't bothered to write up. The crowd wasn't very\u000atechnical though, comprising in large part designers, marketers and\u000amanagers.</p>\u000a
p297
tp298
Rp299
sg13
V/top-5-sessions-of-web-2-expo-ny
p300
sg15
Nsg16
I01
sg17
VTop 5 sessions of Web 2.0 Expo NY
p301
sg20
V\u000a\u000aYesterday I got back from the Web 2.
p302
sS'snip'
p303
g7
(g8
g9
V<p>A recap of my favorite sessions at my first web conference.</p>\u000a
p304
tp305
Rp306
sg25
g169
sg33
g300
sg170
(dp307
g172
S'Sep'
p308
sg174
S'September 23, 2008'
p309
sg176
I9
sg177
S'2008-09-23T09:00:00-00:00'
p310
sg179
I1222185600
sg180
I2008
sg181
I23
ssg65
g182
sg31
S'top-5-sessions-of-web-2-expo-ny'
p311
sS'categories'
p312
(lp313
S'conference'
p314
aS'web'
p315
asS'posted'
p316
g188
(S'\x07\xd8\t\x17'
p317
tp318
Rp319
ssg34
S'content/posts/2008/top-5-sessions-of-web-2-expo-ny/index.md'
p320
sg36
F1433826442.0
sa(dp321
g2
(dp322
g4
V Using an Arduino and a servo motor to retrofit a nerf gun with an auto-fire mechanism.  
p323
sg28
g7
(g8
g9
V<p>My favorite class at CMU is probably <a href="http://mtifall09.wordpress.com/">Making Things Interactive</a>. For me\u000ait is an opportunity to take my thus far casual electronics hacking to to\u000athe next level. In this article, I'll briefly outline my submission for the\u000a"Motion" assignment. I used a servo motor to control a Nerf gun. I built it\u000aand installed it in my MHCI lab, which has a handful of Nerf pistols\u000afloating around. The idea was to have the gun automatically fire at\u000aunsuspecting visitors as they entered the room. </p>\u000a\u000a<p>To arm it, one manually cocks the gun, loads a dart and resets the program\u000aby pushing the button on the Arduino board. The program then allows 10\u000aseconds for the door to be open before it arms the system. When the system\u000ais armed, the servo activates and shoots the gun as soon as the door is\u000aopened.</p>\u000a\u000a<iframe title="YouTube video player" width="600" height="480"\u000a  src="http://www.youtube.com/embed/-XkLRZ2OBRo" frameborder="0"\u000a  allowfullscreen></iframe>\u000a\u000a<p>As you can see, I generously used rubber bands and binder clips in this\u000aproject. I used them to fasten the servo motor to the Nerf gun. I also used\u000athem to harness a telephone cable by wrapping a rubber band around the RJ11\u000aconnector, carefully inserting jumpers, and applying additional pressure (to\u000aensure contact) with binder clips. This hacked telephone cable stretched\u000afrom the gun to the door sensor. </p>\u000a\u000a<p>The circuit was dead simple.  The door acted as a switch for the pull down\u000aresistor circuit. </p>\u000a
p324
tp325
Rp326
sg13
V/arduino-nerf
p327
sg15
Nsg16
I01
sg17
VArduino-nerf mashup
p328
sg20
V\u000a\u000a\u000aMy favorite class at CMU is probably [Making Things Interactive][].
p329
sS'snip'
p330
g7
(g8
g9
V<p>Using an Arduino and a servo motor to retrofit a nerf gun with an auto-fire mechanism.</p>\u000a
p331
tp332
Rp333
sg25
g169
sg33
g327
sg170
(dp334
g172
S'Oct'
p335
sg174
S'October 20, 2009'
p336
sg176
I10
sg177
S'2009-10-20T09:00:00-00:00'
p337
sg179
I1256054400
sg180
I2009
sg181
I20
ssg65
g182
sg31
S'arduino-nerf'
p338
sS'categories'
p339
(lp340
S'physical'
p341
asS'posted'
p342
g188
(S'\x07\xd9\n\x14'
p343
tp344
Rp345
ssg34
S'content/posts/2009/arduino-nerf/index.md'
p346
sg36
F1433825860.0
sa(dp347
g2
(dp348
g4
V A benchmark for comparing Canvas to SVG performance as a function of render element size and number of objects drawn.  
p349
sg28
g7
(g8
g9
V<p>At the core of the traditional HTML/CSS developer's toolkit is a set of nested\u000aboxes describing offset, margin, border and padding, known as the box model.\u000aVariations on the box theme are sufficient to describe most page layouts, but\u000ain some complex applications, it's necessary to render something more\u000ainteresting, like diagonal lines, or polygons. There are currently two\u000arelatively well-supported web graphics technologies: SVG and Canvas. There are\u000asignificant performance differences, however, which I would like to discuss in\u000athis article. </p>\u000a\u000a<p><a href="http://en.wikipedia.org/wiki/Svg">Scalable Vector Graphics</a> (SVG) is by far the oldest of the two. It is a\u000adeclarative, graphical language used to describe geometrical primitives via DOM\u000aelements. SVG was drafted in the late 90s, and the latest version of \u000a<a href="http://www.w3.org/TR/SVG11/">SVG, version 1.1</a> was finalized in 2003. It took 3 more years for it to be\u000aincorporated into shipping versions of Mozilla Firefox and Safari.\u000aUnfortunately, the length of the SVG adoption process caused the web\u000adevelopment community to seek other options. The <a href="http://en.wikipedia.org/wiki/Canvas_(HTML_element)">HTML5 Canvas</a> element was\u000aintroduced as a much simpler alternative to graphics on the web. It provides an\u000aimage-like graphics context which can be accessed via a set of javascript\u000acalls, similar to a 2D subset of OpenGL. It was originally introduced by Apple\u000ain WebKit builds, but is now supported in Mozilla Firefox as well. </p>\u000a\u000a<p>I produced some metrics to compare the two technologies in terms of performance\u000aby writing a Javascript program for <a href="canvas-svg-benchmark/">collecting performance data</a>. This\u000aprogram draws rows of circles onto a fixed-size drawing area in SVG and in\u000aCanvas, and then compares how long various operations take. It also records the\u000aduration to create the initial drawing context, to render the scene and to\u000aclear the scene. A test runner invokes the benchmark with the following\u000avariables: number of circles, drawing area dimensions and circle size. Each of\u000athese variables are varied independantly and automatically resulting in the\u000afollowing observations: </p>\u000a\u000a<p><img src="varying-number-of-objects.png" alt="Varying the number of objects" /> </p>\u000a\u000a<p>Here are the results of the first fruitful experiment, which clearly shows that\u000aSVG performance degrades quickly (exponentially on Safari?) in the number of\u000aobjects, but Canvas performance remains at a near-constant low. This makes\u000asense, since Canvas is just a bitmap buffer, while SVG has to maintain\u000aadditional references to each object that it renders. Also, though not\u000apictured, note that performance in clearing an SVG element also decreases in\u000athe number of drawn objects. </p>\u000a\u000a<p><img src="varying-drawing-area-height.png" alt="Varying drawing area height" /> </p>\u000a\u000a<p>When varying the size of the drawing area, canvas performance degrades\u000asignificantly, while SVG performance is completely unaffected.  Canvas\u000arendering performance seems to degrade linearly in the number of pixels in the\u000acanvas area. Not pictured on the graph is clear performance for large canvases,\u000awhich also suffers linearly in pixel count.</p>\u000a\u000a<p>I did not include the graph resulting from varying circle size as it had no\u000asignificant impact on render time. Another interesting observation is that\u000acreating the canvas element takes a mysterious 10 ms on Firefox, but not on\u000aSafari. This is not significant unless you are dealing with large numbers of\u000acanvas elements.</p>\u000a\u000a<p>Whether or not you use Canvas or SVG mostly depends on your specific\u000aapplication. A graphics-intensive game, where many objects are redrawn all the\u000atime is probably best implemented in Canvas. On the other hand, applications\u000alike map viewers may involve large rendering areas and might lend themselves\u000abetter to SVG.</p>\u000a\u000a<p>As always, please comment if something is unclear, inconsistent, boring or\u000aomitted. Thanks!</p>\u000a
p350
tp351
Rp352
sg13
V/canvas-vs-svg-performance
p353
sg15
Nsg16
I01
sg17
VPerformance of canvas versus SVG
p354
sg20
V\u000a\u000aAt the core of the traditional HTML/CSS developer's toolkit is a set of nested\u000aboxes describing offset, margin, border and padding, known as the box model.
p355
sS'snip'
p356
g7
(g8
g9
V<p>A benchmark for comparing Canvas to SVG performance as a function of render element size and number of objects drawn.</p>\u000a
p357
tp358
Rp359
sg25
g169
sg33
g353
sg170
(dp360
g172
S'Jan'
p361
sg174
S'January 19, 2009'
p362
sg176
I1
sg177
S'2009-01-19T09:00:00-00:00'
p363
sg179
I1232384400
sg180
I2009
sg181
I19
ssg65
g182
sg31
S'canvas-vs-svg-performance'
p364
sS'categories'
p365
(lp366
S'web'
p367
asS'posted'
p368
g188
(S'\x07\xd9\x01\x13'
p369
tp370
Rp371
ssg34
S'content/posts/2009/canvas-vs-svg-performance/index.md'
p372
sg36
F1433825868.0
sa(dp373
g2
(dp374
g4
V In late March, I was completely thrilled to hear that the [Carnegie Mellon University][]  Human Computer Interaction Institute  accepted me into their  Master 's program! In addition to admission, they offered a very juicy scholarship to spend part of the time on the beautiful island of  Madeira . CMU was my most far fetched reach school, so I attribute this wonderful fortune to a clerical error made by the admission committee.     In addition to studying at one of the best universities in the world, traveling to Europe, and relaxing on the beach, here are a couple of reasons why my inner geek can't contain his excitement, try as he might. These incredible courses are offered at CMU in September 2009:    Making Things Interactive:    In this hands-on design-build class you will learn the skills to embed sensors and actuators (light, sound, touch, motion, etc.) into everyday things (and places etc.) and to program their interactive behavior using a microcontroller. Through weekly exercises and a term project the class will introduce basic analog electronics and microcontroller programming, as well as exploration into using kinetics and materials to make the things you design perform. Emphasis will be on creating innovative experiences using simple robotic technologies. The graduate edition of this course will require additional work including a paper that can be submitted to a peer-reviewed interaction design conference such as CHI, UIST, or TEI.Students from all disciplines are welcome: but please note that the class demands that you master technical material. Experience in at least one of: programming, electronics, or physical fabrication is strongly recommended.    Principles of Human-Robot Interaction:    This course focuses on the emerging field of human-robot interaction, bringing together research and application of methodology from robotics, human factors, human-computer interaction, interaction design, cognitive psychology, education and other fields to enable robots to have more natural and more rewarding interactions with humans throughout their spheres of functioning. This course is a combination of state-of-art reading and discussions, focused team exercises and problem-solving sessions in human-robot interaction, and a special team project resulting in the implementation of a human-robot interaction system.  
p375
sg28
g7
(g8
g9
V<p>In late March, I was completely thrilled to hear that the [Carnegie\u000aMellon University][] <a href="http://www.hcii.cmu.edu/">Human Computer Interaction Institute</a> accepted\u000ame into their <a href="http://www.hcii.cmu.edu/masters-program">Master</a>'s program! In addition to admission, they\u000aoffered a very juicy scholarship to spend part of the time on the\u000abeautiful island of <a href="http://en.wikipedia.org/wiki/Madeira">Madeira</a>. CMU was my most far fetched reach\u000aschool, so I attribute this wonderful fortune to a clerical error made\u000aby the admission committee. </p>\u000a\u000a<p>In addition to studying at one of the best\u000auniversities in the world, traveling to Europe, and relaxing on the\u000abeach, here are a couple of reasons why my inner geek can't contain his\u000aexcitement, try as he might. These incredible courses are offered at CMU\u000ain September 2009:</p>\u000a\u000a<h3>Making Things Interactive:</h3>\u000a\u000a<p>In this hands-on design-build class you will learn the skills to embed\u000asensors and actuators (light, sound, touch, motion, etc.) into everyday\u000athings (and places etc.) and to program their interactive behavior using\u000aa microcontroller. Through weekly exercises and a term project the class\u000awill introduce basic analog electronics and microcontroller programming,\u000aas well as exploration into using kinetics and materials to make the\u000athings you design perform. Emphasis will be on creating innovative\u000aexperiences using simple robotic technologies. The graduate edition of\u000athis course will require additional work including a paper that can be\u000asubmitted to a peer-reviewed interaction design conference such as CHI,\u000aUIST, or TEI.Students from all disciplines are welcome: but please note\u000athat the class demands that you master technical material. Experience in\u000aat least one of: programming, electronics, or physical fabrication is\u000astrongly recommended.</p>\u000a\u000a<h3>Principles of Human-Robot Interaction:</h3>\u000a\u000a<p>This course focuses on the emerging field of human-robot interaction,\u000abringing together research and application of methodology from robotics,\u000ahuman factors, human-computer interaction, interaction design, cognitive\u000apsychology, education and other fields to enable robots to have more\u000anatural and more rewarding interactions with humans throughout their\u000aspheres of functioning. This course is a combination of state-of-art\u000areading and discussions, focused team exercises and problem-solving\u000asessions in human-robot interaction, and a special team project\u000aresulting in the implementation of a human-robot interaction system.</p>\u000a
p376
tp377
Rp378
sg13
V/carnegie-mellon-university
p379
sg15
Nsg16
I00
sg17
VCarnegie Mellon University!?
p380
sg20
V\u000a\u000aIn late March, I was completely thrilled to hear that the [Carnegie\u000aMellon University][] [Human Computer Interaction Institute][] accepted\u000ame into their [Master][]'s program! In addition to admission, they\u000aoffered a very juicy scholarship to spend part of the time on the\u000abeautiful island of [Madeira][].
p381
sg6
g378
sg25
g169
sg33
g379
sg170
(dp382
g172
S'Apr'
p383
sg174
S'April 18, 2009'
p384
sg176
I4
sg177
S'2009-04-18T09:00:00-00:00'
p385
sg179
I1240070400
sg180
I2009
sg181
I18
ssg65
g182
sg31
S'carnegie-mellon-university'
p386
sS'categories'
p387
(lp388
S'personal'
p389
asS'posted'
p390
g188
(S'\x07\xd9\x04\x12'
p391
tp392
Rp393
ssg34
S'content/posts/2009/carnegie-mellon-university/index.md'
p394
sg36
F1332684374.0
sa(dp395
g2
(dp396
g4
V A how-to about implementing file drag and drop from the desktop into Safari, and presenting the user with a nice UI.  
p397
sg28
g7
(g8
g9
V<p>Somehow I often find myself arguing in defense of the web browser as a\u000aviable platform for developing rich applications. In many such\u000adiscussions, the issue of interoperability with the desktop arises.\u000aSomeone will astutely observe that they <strong>can't even drag and drop</strong>\u000afrom their OS file manager into their browser, and all hell will break\u000aloose. </p>\u000a\u000a<p>Happily, this is changing! Since version 3, Safari on Mac OS X\u000ahas had support for <a href="http://www.jakeri.net/2008/04/drag-and-drop-into-file-upload-in-safari/">dragging and dropping files</a> from the finder into\u000afile input boxes. In various kludgy ways, <a href="https://addons.mozilla.org/en-US/firefox/addon/2190">Firefox</a> and <a href="http://www.download.com/HTTP-File-Upload-ActiveX-Control/3000-2206_4-10451672.html">IE</a> are now\u000afollowing suite. </p>\u000a\u000a<p>Unfortunately, even in Safari, the default look of the\u000a<code>&lt;input type="file"&gt;</code> box is quite ugly and the element itself is\u000a<a href="http://www.quirksmode.org/dom/inputfile.html">difficult to style</a>. In addition, clicking anywhere in the file input\u000aelement causes the default open file dialog to appear. I wanted to\u000aprovide drag-and-drop uploading without ugly boxes or browser dialogs.\u000aThe solution I came up with involves hiding the file upload box entirely\u000aby setting its opacity to 0, and then preventing the default action on\u000aclick via <code>event.preventDefault()</code>. Here's a sample of what I mean, with\u000athe entire browser window <a href="drag-drop-upload.html">converted into a drag area</a>. </p>\u000a\u000a<p>Note that the drag area must be the first DOM element to receive the drop event\u000afor this approach to work. Unfortunately I ran into a bug where the file dialog\u000arefuses to bubble click events to other elements below it. This is baffling to\u000ame, since <code>event.preventDefault()</code> should not stop event propagation, but only\u000aprevent the default browser handler from being called. You can see what I mean\u000aby trying to click the link in the <a href="drag-drop-upload.html">sample HTML</a>\u000afile. If this is not a bug, and someone has an answer, I would really\u000aappreciate it.</p>\u000a\u000a<p>Note also that there are <a href="http://www.radinks.com/upload/">java applet</a>-based drag and drop solutions, but\u000athey are reserved for developers who have nothing but disdain for their users.</p>\u000a
p398
tp399
Rp400
sg13
V/clean-drag-and-drop-upload-in-safari
p401
sg15
Nsg16
I01
sg17
VClean drag and drop upload in Safari
p402
sg20
V\u000a\u000aSomehow I often find myself arguing in defense of the web browser as a\u000aviable platform for developing rich applications.
p403
sS'snip'
p404
g7
(g8
g9
V<p>A how-to about implementing file drag and drop from the desktop into Safari, and presenting the user with a nice UI.</p>\u000a
p405
tp406
Rp407
sg25
g169
sg33
g401
sg170
(dp408
g172
S'Feb'
p409
sg174
S'February 15, 2009'
p410
sg176
I2
sg177
S'2009-02-15T09:00:00-00:00'
p411
sg179
I1234717200
sg180
I2009
sg181
I15
ssg65
g182
sg31
S'clean-drag-and-drop-upload-in-safari'
p412
sS'categories'
p413
(lp414
S'web'
p415
aS'design'
p416
asS'posted'
p417
g188
(S'\x07\xd9\x02\x0f'
p418
tp419
Rp420
ssg34
S'content/posts/2009/clean-drag-and-drop-upload-in-safari/index.md'
p421
sg36
F1332684374.0
sa(dp422
g2
(dp423
g4
V A chronicle of some emergent lingo from the Google Wave service.  
p424
sg28
g7
(g8
g9
V<p>For the last few days, I've had the chance to get my feet wet in the\u000a<a href="https://services.google.com/fb/forms/wavesignupfordev/">developer sandbox</a> of Google Wave. My first impressions are very\u000apositive. I am as awed now by the scope and potential impact of Wave as\u000aI was after watching the hour long <a href="http://wave.google.com/">video from Google I/O</a>. </p>\u000a\u000a<p>As consolation to those itching to <em>try it already</em>, bear in mind that the\u000asandbox is in a very raw state right now. Performance issues aside, informative\u000atext such as:</p>\u000a\u000a<blockquote>\u000a  <p>This wave is experiencing some slight turbulence, and may explode. If\u000a  you don't wanna explode, please re-open the wave. Some recent changes\u000a  may not be saved.</p>\u000a</blockquote>\u000a\u000a<p>and</p>\u000a\u000a<blockquote>\u000a  <p>"Everything's shiny, Cap'n. Not to fret!" Unfortunately, you'll need\u000a  to <em>refresh</em>. Wanna tell Dr. Wave what happened?</p>\u000a</blockquote>\u000a\u000a<p>appear quite often. Of course, this is totally expected for pre-alpha\u000asoftware -- I'm merely pointing out the humorous messages!</p>\u000a\u000a<p>For a more intimate look into Wave, especially some of the deeper, more social\u000aissues, here is part of an ongoing ha-ha-only-serious joke from the internal\u000awave-discuss group about extending existing <a href="http://mashable.com/2009/05/28/google-wave-guide/">Wave terminology</a>. The list is\u000areally telling, since indeed, I have read many ridiculous Drips and most of my\u000afresh waves Surge only to get Borked by rickrolley.</p>\u000a\u000a<ul>\u000a<li><strong>Wavejack</strong> -- To hijack the contents of a wave to where it no longer resembles the original idea</li>\u000a<li><strong>Drip</strong> -- Stupid question no one answers</li>\u000a<li><strong>Bork</strong> -- To add useless, noisy or destructive bots to a wave. (aka The Swedish Chef or rickrolley or the borkforceone bot)</li>\u000a<li><strong>Surge</strong> -- The effect of a fresh wave that elicits the chaos of mass editing</li>\u000a<li><strong>Drown</strong> -- To have so many waves to follow that one can't keep up with them </li>\u000a<li><strong>Sea Sick</strong> -- A state of dizziness induced by an overly active wave </li>\u000a<li><strong>Tuna/Noise/Herring</strong> -- Wave speak for Spam</li>\u000a<li><strong>Fishnet</strong> -- Spam filter</li>\u000a</ul>\u000a\u000a<p>Despite instabilities and widespread antisocial behavior, the wave sandbox is\u000aalive with a flurry of activity. People are busy creating annoying bots,\u000aforming role playing communities and writing collaborative books. So Google has\u000aa lot of issues to work out, not in the least how to stop us all from drowning\u000a:) </p>\u000a\u000a<p>So far, I haven't had a chance to write any interesting wave extensions, but\u000ait's in my things to do!</p>\u000a
p425
tp426
Rp427
sg13
V/extending-google-wave-terminology
p428
sg15
Nsg16
I01
sg17
VExtending Google Wave terminology
p429
sg20
V\u000a\u000aFor the last few days, I've had the chance to get my feet wet in the\u000a[developer sandbox][] of Google Wave.
p430
sS'snip'
p431
g7
(g8
g9
V<p>A chronicle of some emergent lingo from the Google Wave service.</p>\u000a
p432
tp433
Rp434
sg25
g169
sg33
g428
sg170
(dp435
g172
S'Jul'
p436
sg174
S'July 22, 2009'
p437
sg176
I7
sg177
S'2009-07-22T09:00:00-00:00'
p438
sg179
I1248278400
sg180
I2009
sg181
I22
ssg65
g182
sg31
S'extending-google-wave-terminology'
p439
sS'categories'
p440
(lp441
S'social'
p442
asS'posted'
p443
g188
(S'\x07\xd9\x07\x16'
p444
tp445
Rp446
ssg34
S'content/posts/2009/extending-google-wave-terminology/index.md'
p447
sg36
F1433825879.0
sa(dp448
g2
(dp449
g4
V A python script that generates all possible ways of playing the specified guitar chord. I used this for Guitar Unleashed.  
p450
sg28
g7
(g8
g9
V<p>One day I wanted to add a feature to <a href="http://www.guitarunleashed.com">Guitar Unleashed</a> which exists\u000ain some of the better guitar tab sites. When a user hovers over a chord,\u000athey are shown a diagram representing the guitar fret with overlaid\u000afinger positions required to produce this chord. Many of the most\u000apopular sites do this by showing a crude, plain-text representation of\u000athe chord.</p>\u000a\u000a<p>For example, a C chord is shown as follows:</p>\u000a\u000a<pre><code>e ---|---|---|---|---|\u000aB -x-|---|---|---|---|\u000aG ---|---|---|---|---|\u000aD ---|-x-|---|---|---|\u000aA ---|---|-x-|---|---|\u000aE ---|---|-o-|---|---|\u000a</code></pre>\u000a\u000a<p>Typeset guitar chord representation looks very different. Two most\u000apopular variants of the C chord would appear above the staff with small\u000asymbols that look something like this: </p>\u000a\u000a<p><img src="C.png" alt="C" title="C" />\u000a<img src="C_1.png" alt="C_1" title="C_1" /> </p>\u000a\u000a<p>Such images have many benefits as compared to the plain text version:</p>\u000a\u000a<ul>\u000a<li>Convention: many guitar players are used to learning from sheet\u000amusic, so this chord notation is familiar to them.</li>\u000a<li>Readability: the image representation is more compact and more\u000apleasant to read.</li>\u000a<li>Completeness: the image version has an easy way to display barre\u000achords and specify fingerings (not pictured)</li>\u000a</ul>\u000a\u000a<p>True to Guitar Unleashed's mission of being a truly usable guitar chords\u000asite, I decided to display typeset fret diagrams. For the\u000aimplementation, a program would pre-generate images on the server based\u000aon the encoded chord shape and then serve these images dynamically via\u000aJavaScript. Unfortunately, I found no software to generate such images\u000afor all popular chords, so I ended up developing my own. </p>\u000a\u000a<p>From some brief\u000aresearch, I decided that <a href="http://lilypond.org/doc/v2.9/Documentation/user/lilypond/Fret-diagrams">lilypond fret diagrams</a> are the most elegant\u000away of creating such images. The alternative was to use <a href="http://www.aei.mpg.de/~peekas/gchords/">GCHORDS</a>\u000awhose output I liked less, and which required depending on TeX.\u000aTypically lilypond typesets an entire piece of sheet music on a staff,\u000awith clefs, key signatures, etc. After discovering a really great\u000a<a href="http://netcetera.org/cgi-bin/tmbundles.cgi#Lilypond">lilypond bundle for textmate</a>, I managed to get rid of these\u000aunnecessary features and display just a fret diagram. The following\u000apython-parametrized lilypond template does the trick:</p>\u000a\u000a<pre><code>\u005cinclude "lilypond-book-preamble.ly"\u000a\u005cversion "2.10.0"\u000a\u005cmarkup\u000a\u005cfret-diagram-terse #"%(chord_markup)s"\u000a</code></pre>\u000a\u000a<p>Then, using lilypond's fret diagram encoding, a python script replaces\u000a<code>%(chord_markup)s</code> with the desired markup. Lilypond's markup is very\u000aterse but mostly manageable. Sample markup for a C chord looks like\u000athis: <code>x;3;2;o;1;o;</code>, while the barre version looks a little bit more\u000abizarre, like this: <code>3-(;3;5;5;5;3-);</code>. </p>\u000a\u000a<p>My python program contains a list of chord shapes for the most popular chords,\u000aencoded in the manner above, then outputs images for each chord and variant\u000ainto a directory.  I've made it <a href="chord-image-generator.zip">available for download</a>, in case your next\u000aproject needs generated fret diagrams with a highly customizable look. You can\u000aalso see the diagrams live at <a href="http://www.guitarunleashed.com">guitarunleashed.com</a>.</p>\u000a
p451
tp452
Rp453
sg13
V/generating-guitar-chord-diagrams
p454
sg15
Nsg16
I01
sg17
VGenerating guitar chord diagrams
p455
sg20
V\u000a\u000aOne day I wanted to add a feature to [Guitar Unleashed][] which exists\u000ain some of the better guitar tab sites.
p456
sS'snip'
p457
g7
(g8
g9
V<p>A python script that generates all possible ways of playing the specified guitar chord. I used this for Guitar Unleashed.</p>\u000a
p458
tp459
Rp460
sg25
g169
sg33
g454
sg170
(dp461
g172
S'Jun'
p462
sg174
S'June 14, 2009'
p463
sg176
I6
sg177
S'2009-06-14T09:00:00-00:00'
p464
sg179
I1244995200
sg180
I2009
sg181
I14
ssg65
g182
sg31
S'generating-guitar-chord-diagrams'
p465
sS'categories'
p466
(lp467
S'music'
p468
asS'posted'
p469
g188
(S'\x07\xd9\x06\x0e'
p470
tp471
Rp472
ssg34
S'content/posts/2009/generating-guitar-chord-diagrams/index.md'
p473
sg36
F1433825885.0
sa(dp474
g2
(dp475
g4
V Bitching and moaning about the perils of deleting your GMail account.  
p476
sg28
g7
(g8
g9
V<p>Since moving to the states, I have had nothing but grief from the .ca at the\u000aend of my all-purpose email address. Even in Canada, people would constantly\u000aconfuse z3.ca for z3.com, resulting in email bounces. To resolve this problem\u000aonce and for all, I decided to switch to Gmail like all the cool kids. I\u000aregistered boris.smus long ago out but never used it. The first thing I did was\u000atry to link my z3.ca to the Gmail. When I couldn't figure out how to do that, I\u000adeleted my Gmail account in order to re-create a new pre-linked account with\u000athe same name. Sounds innocent enough, right? </p>\u000a\u000a<p><em>Wrong</em>! Google has an uncharacteristically evil account deletion policy which\u000ais not at all clearly communicated. The deletion page simply says:</p>\u000a\u000a<blockquote>\u000a  <p>You're trying to delete your Google Account that provides access to\u000a  the Google products listed below. Please select each checkbox to\u000a  confirm you fully understand that you'll no longer be able to use any\u000a  of these products and all information associated with them, and that\u000a  your account will be lost.</p>\u000a</blockquote>\u000a\u000a<p>Meanwhile, the <a href="http://www.google.com/support/accounts/bin/answer.py?hl=en&amp;answer=32046">F.A.Q.</a> reads:</p>\u000a\u000a<blockquote>\u000a  <p>If you use Gmail with your account, you'll no longer be able to access\u000a  that email. You'll also be unable to reuse your Gmail username.</p>\u000a</blockquote>\u000a\u000a<p>Long story short, I tried everything in my power to recover the old username. I\u000afound handfuls of frustrated users in the same position as me; some having\u000adeleted their account by accident, others victims of pranks and identity theft.\u000aI asked on official and unofficial Gmail forums, and even consulted with my\u000aGoogler friends, all to no avail.  After mourning the loss of\u000aboris.smus@gmail.com, it was time to take a critical look at Google's email\u000aofferings. </p>\u000a\u000a<p>I enjoy Gmail's webmail client very much. It's a fast, intuitive, search and\u000atag based model with virtually unlimited mailbox storage. In terms of\u000ausability, however, I much prefer Mail on Mac. On the iPhone, the native Mail\u000aclient is far superior to the mobile web Gmail client. Fortunately, Google\u000aprovides SMTP and IMAP services to fill this need. Sadly, both of these\u000aservices are plagued with issues. </p>\u000a\u000a<p>I recently sent an email which had roughly 50 bcc recipients. Google's SMTP\u000aserver thought I was a spammer and banned me, <em>despite my having authenticated\u000avia SSL</em>.  Perhaps sending email to 50 people is slightly unusual. Still, I\u000awould expect my mail gateway to be capable of performing such a 'feat'. </p>\u000a\u000a<p>The way the Google IMAP maps directories is fundamentally incompatible with\u000aMail.app expectations. Further, the IMAP server has a limit of 10 simultaneous\u000aconnections, which often causes the connection threshold to be reached with\u000a<a href="http://mail.google.com/support/bin/answer.py?hl=en&amp;answer=97150">just two connected clients</a>. Additionally, the IMAP server is often down; I\u000awish I had some data to support that, but I don't. </p>\u000a\u000a<p>When I finally settled on boris@borismus.com, I had a decision to make: do I\u000ause Google Apps or Webfaction for email? I quite like Webfaction, and, perhaps\u000airrationally, trust them more than Google with my private data. With the above\u000alimitations of Gmail in mind, I did not hesitate to choose Webfaction. I still\u000amiss having a first.last@gmail.com though. What's with the oddly draconian\u000aaccount deletion rules?</p>\u000a
p477
tp478
Rp479
sg13
V/gmail-rant
p480
sg15
Nsg16
I01
sg17
VNever delete your gmail account
p481
sg20
V\u000a\u000aSince moving to the states, I have had nothing but grief from the .
p482
sS'snip'
p483
g7
(g8
g9
V<p>Bitching and moaning about the perils of deleting your GMail account.</p>\u000a
p484
tp485
Rp486
sg25
g169
sg33
g480
sg170
(dp487
g172
S'Aug'
p488
sg174
S'August 28, 2009'
p489
sg176
I8
sg177
S'2009-08-28T09:00:00-00:00'
p490
sg179
I1251475200
sg180
I2009
sg181
I28
ssg65
g182
sg31
S'gmail-rant'
p491
sS'categories'
p492
(lp493
S'misc'
p494
asS'posted'
p495
g188
(S'\x07\xd9\x08\x1c'
p496
tp497
Rp498
ssg34
S'content/posts/2009/gmail-rant/index.md'
p499
sg36
F1433825891.0
sa(dp500
g2
(dp501
g4
V A web application that lets users collaboratively edit guitar chords. It aims to globally improve the quality of guitar chords.  
p502
sg28
g7
(g8
g9
V<p>I've been collaborating with my dad on an experimental web-based guitar chord\u000aediting service. It's still a work in progress, but we are ready to launch a\u000abeta version. Please visit <a href="http://www.guitarunleashed.com">http://www.guitarunleashed.com/</a> to check it out\u000aand provide feedback. </p>\u000a\u000a<p>We hope that the major features of the service are easily discoverable, but in\u000acase they are not, here is a list:</p>\u000a\u000a<ul>\u000a<li>Add chords to your lyrics by clicking on any character</li>\u000a<li>Move chords via drag and drop</li>\u000a<li>Search for songs created by other users or from external sources</li>\u000a<li>Save your work and share it with others</li>\u000a<li>Add songs that you like to your favorites list</li>\u000a<li>Already have lyrics with chords? Create a new song, paste lyrics\u000awith chords in and see the chords come to life.</li>\u000a</ul>\u000a\u000a<p>Please note that the application relies heavily on JavaScript, so it needs to\u000abe enabled in your browser.</p>\u000a\u000a<h2>Known issues</h2>\u000a\u000a<ul>\u000a<li>Some lyrics may display incorrectly. This is an issue with\u000a<a href="http://lyricwiki.org/Main_Page">LyricWiki.org</a></li>\u000a<li>Long chords placed at the beginning or the end of lyrics line may\u000aextend outside of the editing area</li>\u000a<li>Long lines in lyrics are not handled elegantly</li>\u000a</ul>\u000a\u000a<h2>Future features</h2>\u000a\u000a<ul>\u000a<li>Editing lyric text</li>\u000a<li>Commenting on songs</li>\u000a<li>Searching sources of lyrics and chords other than LyricWiki</li>\u000a<li>Song books</li>\u000a</ul>\u000a\u000a<p>Please give us feedback! Use the feedback button on guitarunleashed.com or\u000acontact us directly at <a href="mailto:feedback@guitarunleashed.com">feedback@guitarunleashed.com</a></p>\u000a
p503
tp504
Rp505
sg13
V/guitar-unleashed
p506
sg15
Nsg16
I01
sg17
VGuitar Unleashed
p507
sg20
V\u000a\u000aI've been collaborating with my dad on an experimental web-based guitar chord\u000aediting service.
p508
sS'snip'
p509
g7
(g8
g9
V<p>A web application that lets users collaboratively edit guitar chords. It aims to globally improve the quality of guitar chords.</p>\u000a
p510
tp511
Rp512
sg25
g169
sg33
g506
sg170
(dp513
g172
S'Mar'
p514
sg174
S'March 23, 2009'
p515
sg176
I3
sg177
S'2009-03-23T09:00:00-00:00'
p516
sg179
I1237824000
sg180
I2009
sg181
I23
ssg65
g182
sg31
S'guitar-unleashed'
p517
sS'categories'
p518
(lp519
S'web'
p520
aS'music'
p521
asS'posted'
p522
g188
(S'\x07\xd9\x03\x17'
p523
tp524
Rp525
ssg34
S'content/posts/2009/guitar-unleashed/index.md'
p526
sg36
F1433825897.0
sa(dp527
g2
(dp528
g4
V After spending a fair bit of time  monitoring twitter feeds , I was pleasantly surprised by the  world 's  response  to the recently announced  iwork.com . There were some  premature   flames , but mostly as a result of a fundamental misunderstanding of iwork.com's purpose. Someone even gave a  technical breakdown  of the application, which is especially endearing to me as an insider.  
p529
sg28
g7
(g8
g9
V<p>After spending a fair bit of time <a href="http://search.twitter.com/search?q=iwork.com">monitoring twitter feeds</a>, I was\u000apleasantly surprised by the <a href="http://smokingapples.com/opinion/iworkcom-understanding-apples-online-office-extension/">world</a>'s <a href="http://www.appleinsider.com/articles/09/01/07/an_extensive_look_at_apples_new_iwork_com_service.html">response</a> to the recently\u000aannounced <a href="http://www.apple.com/iwork/iwork-dot-com/">iwork.com</a>. There were some <a href="http://www.engadget.com/2009/01/06/apple-announces-iwork-com-beta/">premature</a> <a href="http://www.gizmodo.com.au/2009/01/apple_sends_iwork_to_the_clouds_introduces_iworkcom.html">flames</a>, but\u000amostly as a result of a fundamental misunderstanding of iwork.com's\u000apurpose. Someone even gave a <a href="http://ajaxian.com/archives/technical-details-behind-iworkcom/">technical breakdown</a> of the application,\u000awhich is especially endearing to me as an insider.</p>\u000a
p530
tp531
Rp532
sg13
V/iworkcom-feedback
p533
sg15
Nsg16
I00
sg17
ViWork.com feedback
p534
sg20
V\u000a\u000aAfter spending a fair bit of time [monitoring twitter feeds][], I was\u000apleasantly surprised by the [world][]'s [response][] to the recently\u000aannounced [iwork.
p535
sg6
g532
sg25
g169
sg33
g533
sg170
(dp536
g172
S'Jan'
p537
sg174
S'January 7, 2009'
p538
sg176
I1
sg177
S'2009-01-07T09:00:00-00:00'
p539
sg179
I1231347600
sg180
I2009
sg181
I7
ssg65
g182
sg31
S'iworkcom-feedback'
p540
sS'categories'
p541
(lp542
S'personal'
p543
asS'posted'
p544
g188
(S'\x07\xd9\x01\x07'
p545
tp546
Rp547
ssg34
S'content/posts/2009/iworkcom-feedback/index.md'
p548
sg36
F1433825904.0
sa(dp549
g2
(dp550
g4
V A much needed blog redesign.  
p551
sg28
g7
(g8
g9
V<p>Since starting this site, I've been using a lightly tweaked version of the\u000a<a href="http://wp-themes.com/clockworkair/">ClockWorkAir</a> theme. The old design featured a prominent quote in prime\u000ascreen real estate and a tag cloud in the upper sidebar. A blue title bar,\u000areminiscent of the default wordpress theme, graced the blog header. The main\u000atext was small and there was hardly enough room to place images in the left\u000amargin. Here was my old blog design:</p>\u000a\u000a<p><img src="old-design.png" alt="old" /></p>\u000a\u000a<p>As I had hoped, the design class I'm taking at CMU gave me some good ideas.\u000aHere's my new blog design. I'm much happier with it than with the former look,\u000abut still have a backlog of things to tweak. My design was inspired by several\u000a<a href="http://limi.net/">people's</a> <a href="http://www.joehewitt.com/">web logs</a>, which bring together form and function in a way\u000awhich appeals to me. Please give me constructive feedback on the new design if\u000ayou have time. I will release the wordpress theme if anyone wants it \u2014 I just\u000ahaven't had time to bundle it yet. Here is my new blog design:</p>\u000a\u000a<p><img src="new-design.png" alt="new" /></p>\u000a\u000a<p>As of this month it's been a year since my first blog post. Surprisingly, I've\u000abeen writing one to two blog entries per month and hope to keep my musings\u000aflowing at approximately the same rate in the future. It's very rewarding as a\u000awriter to have a steady flow of visitors, some recurring readership, and even\u000athe odd discussion here or there. Thank you all.</p>\u000a
p552
tp553
Rp554
sg13
V/new-design
p555
sg15
Nsg16
I01
sg17
VNew design
p556
sg20
V\u000a\u000aSince starting this site, I've been using a lightly tweaked version of the\u000a[ClockWorkAir][] theme.
p557
sS'snip'
p558
g7
(g8
g9
V<p>A much needed blog redesign.</p>\u000a
p559
tp560
Rp561
sg25
g169
sg33
g555
sg170
(dp562
g172
S'Sep'
p563
sg174
S'September 28, 2009'
p564
sg176
I9
sg177
S'2009-09-28T09:00:00-00:00'
p565
sg179
I1254153600
sg180
I2009
sg181
I28
ssg65
g182
sg31
S'new-design'
p566
sS'categories'
p567
(lp568
S'design'
p569
aS'web'
p570
asS'posted'
p571
g188
(S'\x07\xd9\t\x1c'
p572
tp573
Rp574
ssg34
S'content/posts/2009/new-design/index.md'
p575
sg36
F1433825907.0
sa(dp576
g2
(dp577
g4
V Attempting to build a plotter based on the radial coordinate system using LEGO Mindstorms. Eventually I gave up and built a cartesian plotter.  
p578
sg28
g7
(g8
g9
V<p>In anticipation of 48-739: Making Things Interactive, I've been itching to\u000abuild something interesting. I decided to create a printing plotter out of my\u000abrother's Mindstorms set. There are already many <a href="http://www.norgesgade14.dk/plotter.php">excellent plotter designs</a>\u000afloating around in the Mindstorms community, so I decided to try something new.\u000aPlotters typically draw straight, edge-aligned lines, since they have a caret\u000amotor which travels along the x-axis and a feed motor which aligns along the\u000ay-axis. There are many variations on this theme. </p>\u000a\u000a<p>What if, instead of using the Cartesian coordinate system, a plotter was built\u000aagainst radial coordinates. That is, one motor would control the rotation of an\u000aarm (theta in radian coordinates), and one would drive a caret along the arm,\u000acontrolling the distance from the origin (r in radian coordinates). I began\u000abuilding, and despite the ardent help of my young brother, we failed to create\u000aa reasonable construction. The problem we ran into was that as r increased to\u000athe maximum length of the arm, the engine driving theta would not have enough\u000astrength to rotate due to the increased torque. I'm ashamed to admit that we\u000agave up. </p>\u000a\u000a<p>A month later, I picked up the project again, and due to limited time, decided\u000ato build a Cartesian plotter after all.  I did not consult state of the art of\u000aNXT plotters, and ended up with a plotter of unconventional design. Instead of\u000aa paper feed, a caret travels along the y-axis, and another caret travels along\u000athe first caret on the x-axis. I borrowed wheels from the RCX set, but the rest\u000ais stock NXT. </p>\u000a\u000a<iframe title="YouTube video player" width="600" height="368"\u000a  src="http://www.youtube.com/embed/tZhqjrHSIfE" frameborder="0"\u000a  allowfullscreen></iframe>\u000a\u000a<p>I named the robot "Malevich 2" in honor of <a href="http://en.wikipedia.org/wiki/Kazimir_Malevich">Kazimir Malevich</a>, an avant-guard\u000aRussian painter, famous for pioneering geometric abstract art, especially a\u000aseries of <a href="http://en.wikipedia.org/wiki/File:Malevich.black-square.jpg">paintings of squares</a>. I prefer Malevich 2's rendition of the\u000asquare, but maybe that's just me.</p>\u000a
p579
tp580
Rp581
sg13
V/nxt-plotter
p582
sg15
Nsg16
I01
sg17
VPlotting something radial
p583
sg20
V\u000a\u000aIn anticipation of 48-739: Making Things Interactive, I've been itching to\u000abuild something interesting.
p584
sS'snip'
p585
g7
(g8
g9
V<p>Attempting to build a plotter based on the radial coordinate system using LEGO Mindstorms. Eventually I gave up and built a cartesian plotter.</p>\u000a
p586
tp587
Rp588
sg25
g169
sg33
g582
sg170
(dp589
g172
S'Aug'
p590
sg174
S'August 7, 2009'
p591
sg176
I8
sg177
S'2009-08-07T09:00:00-00:00'
p592
sg179
I1249660800
sg180
I2009
sg181
I7
ssg65
g182
sg31
S'nxt-plotter'
p593
sS'categories'
p594
(lp595
S'physical'
p596
asS'posted'
p597
g188
(S'\x07\xd9\x08\x07'
p598
tp599
Rp600
ssg34
S'content/posts/2009/nxt-plotter/index.md'
p601
sg36
F1433825913.0
sa(dp602
g2
(dp603
g4
V A how-to about building offline-capable mobile web applications using AppCache. Also, a rough version of the Guitar Unleashed mobile app.  
p604
sg28
g7
(g8
g9
V<p>In the midst of my graduate studies, I somehow found the time to write a\u000asimple prototype for a mobile Guitar Unleashed client. It's more of a\u000aproof-of-concept for some cool new technologies that I've been meaning\u000ato play with. Two things led me down this path:</p>\u000a\u000a<ol>\u000a<li>Since I'm no longer bound by corporate affiliation, I feel compelled\u000ato finally develop an interesting application for iPhone.</li>\u000a<li>I'm very bad at remembering guitar chords and lyrics, but never\u000abother making a cheat sheet to take to the campfire. I nearly always\u000ahave my phone in my pocket, though.</li>\u000a</ol>\u000a\u000a<p>The GU mobile prototype is very simple. Users can manage a list of\u000afavorite songs, and view their lyrics with chords. At first, I set out\u000ato create a native iPhone application to do this, but several\u000aconsiderations made me change my mind:</p>\u000a\u000a<ol>\u000a<li>I don't want to deal with a potential App Store rejection, keeping\u000ain mind the dubious legality of lyrics</li>\u000a<li>The simplicity and data-centric nature of the app lends itself well\u000ato a web implementation.</li>\u000a</ol>\u000a\u000a<p><img src="phone.png" class="left"></p>\u000a\u000a<p>There are a few interesting technical challenges to overcome in implementing\u000athis application. Firstly, the app needs to blend visually with the iPhone look\u000aand feel. I chose the <a href="http://code.google.com/p/iui/">iUI</a> web framework to develop my application. In\u000aretrospect, <a href="http://www.jqtouch.com/">jQTouch</a> would have probably been a better choice, since I ended\u000aup using jQuery. In addition, iUI is very much not bug free, and I don't like\u000aiUI's anchor-based navigation. Apple provides several iPhone-specific meta tags\u000ato enhance the web experience. For example, <code>&lt;link rel="apple-touch-icon"\u000ahref="logo-touch-icon.png" /&gt;</code> allows to specify the icon that will appear in\u000athe iPhone springboard. Setting <code>&lt;meta name="apple-touch-fullscreen"\u000acontent="YES" /&gt;</code> will remove the lower hud of MobileSafari. The full list of\u000aoptions is available <a href="http://developer.apple.com/safari/library/documentation/AppleApplications/Reference/SafariHTMLRef/Articles/MetaTags.html">from Apple</a>.</p>\u000a\u000a<p>The second broad challenge for mine and iPhone web applications in general is\u000athat they need to remain functional without an internet connection. For my\u000amobile client, data would be provided by two calls returning JSON:\u000a<code>/song/search?query=myQuery</code> returning a list of songs and <code>/song/get/myID</code>\u000areturning specific song info. My first instinct was to implement local\u000apersistence via cookies, as is customary in web development. There's a 4K limit\u000aon the size of each cookie, so storing all songs in one cookie was out of the\u000aquestion. A simple alternative was to store a cookie with an array of song IDs,\u000aand a cookie for each song. This strategy worked quite well on WebKit and in\u000athe iPhone simulator. Unfortunately, cookie persistence works differently on\u000athe actual iPhone, likely for security reasons. Every time the phone reboots,\u000aMobileSafari's cookie jar is emptied. </p>\u000a\u000a<p>The alternative to this is a much more modern approach: <a href="http://webkit.org/blog/126/webkit-does-html5-client-side-database-storage/">HTML5 databases</a>.\u000aWebKit now allows you to store structured data locally in an SQLite database. I\u000afound this approach to work very well on iPhone. The database backend persists\u000athrough reboots, as expected. I implemented both storage schemes in separate\u000afiles, available for your scrutiny: the class SongJar for cookies in <a href="offline-mobile/jar.js">jar.js</a>\u000aand SongDatabase for HTML5 storage <a href="offline-mobile/database.js">database.js</a>.</p>\u000a\u000a<p>The last piece of the puzzle is how to force the web application's source files\u000ato get cached on the iPhone, so that the app remains accessible even when the\u000aphone is offline. This is done with a <a href="http://www.w3.org/TR/2009/WD-html5-20090212/offline.html">cache manifest</a>, also new in HTML5. I\u000aran into several problems trying to set this up, and would have benefited from\u000athese tips:</p>\u000a\u000a<ol>\u000a<li>Reference the manifest from the HTML with\u000a<code>&lt;html manifest="cache-manifest"&gt;</code></li>\u000a<li>Serve the manifest with the <code>text/cache-manifest</code> mime type</li>\u000a<li>Ensure that all paths in the manifest are accessible</li>\u000a</ol>\u000a\u000a<p>Though my app is a mere prototype, I'm quite happy with the result.\u000aAfter adding the application to the springboard, it <em>almost</em> feels like\u000aa first class iPhone application. You can try it out at\u000a<a href="http://www.guitarunleashed.com/m/">http://www.guitarunleashed.com/m/</a>, and please bear in mind that it's\u000aa proof of concept.</p>\u000a
p605
tp606
Rp607
sg13
V/offline-web-iphone
p608
sg15
Nsg16
I01
sg17
VOffline web apps on the iPhone
p609
sg20
V\u000a\u000aIn the midst of my graduate studies, I somehow found the time to write a\u000asimple prototype for a mobile Guitar Unleashed client.
p610
sS'snip'
p611
g7
(g8
g9
V<p>A how-to about building offline-capable mobile web applications using AppCache. Also, a rough version of the Guitar Unleashed mobile app.</p>\u000a
p612
tp613
Rp614
sg25
g169
sg33
g608
sg170
(dp615
g172
S'Sep'
p616
sg174
S'September 6, 2009'
p617
sg176
I9
sg177
S'2009-09-06T09:00:00-00:00'
p618
sg179
I1252252800
sg180
I2009
sg181
I6
ssg65
g182
sg31
S'offline-web-iphone'
p619
sS'categories'
p620
(lp621
S'web'
p622
asS'posted'
p623
g188
(S'\x07\xd9\t\x06'
p624
tp625
Rp626
ssg34
S'content/posts/2009/offline-web-iphone/index.md'
p627
sg36
F1433825918.0
sa(dp628
g2
(dp629
g4
V Raving about the Smart Youtube wordpress plugin.  
p630
sg28
g7
(g8
g9
V<p>In late January 2009, <a href="http://www.youtube.com/">YouTube</a> decided to change the default look of their\u000aembedded videos. They silently added an informative header which includes the\u000avideo title and rating. Some time before then, a default search bar appeared at\u000athe top of the video. Thanks to these changes, most haphazardly embedded\u000aYouTube videos on the internet sport a repulsive new look. </p>\u000a\u000a<p>What, you might ask, can be done about this excessive ugliness? Well, you could\u000aswitch to <a href="http://www.vimeo.com">Vimeo</a>, which has a much nicer set of defaults, but that would\u000amean bidding the YouTube community farewell -- and what a tragedy that would\u000abe! A better alternative is to learn the <a href="http://code.google.com/apis/youtube/player_parameters.html">YouTube Embedding API</a>, but there\u000aare still problems:</p>\u000a\u000a<ol>\u000a<li>Embedding code for YouTube videos is ugly.</li>\u000a<li>The YouTube embedding scheme often silently changes.</li>\u000a<li>If you embed multiple videos, there is no way to specify a default\u000aset of embedding preferences.</li>\u000a</ol>\u000a\u000a<p>If you are a wordpress user, all of these issues are resolved by the most\u000aexcellent <a href="http://www.prelovac.com/vladimir/wordpress-plugins/smart-youtube">Smart YouTube</a> plugin. With it you can embed videos by simply\u000ainserting the URL to any YouTube video into your wordpress page or post and\u000areplacing http with http<strong>v</strong>. Through a settings page, you can modify the way\u000aall of your embedded videos look with one fell swoop.</p>\u000a
p631
tp632
Rp633
sg13
V/prettifying-embedded-youtube
p634
sg15
Nsg16
I01
sg17
VPrettifying embedded YouTube
p635
sg20
V\u000a\u000aIn late January 2009, [YouTube][] decided to change the default look of their\u000aembedded videos.
p636
sS'snip'
p637
g7
(g8
g9
V<p>Raving about the Smart Youtube wordpress plugin.</p>\u000a
p638
tp639
Rp640
sg25
g169
sg33
g634
sg170
(dp641
g172
S'Feb'
p642
sg174
S'February 27, 2009'
p643
sg176
I2
sg177
S'2009-02-27T09:00:00-00:00'
p644
sg179
I1235754000
sg180
I2009
sg181
I27
ssg65
g182
sg31
S'prettifying-embedded-youtube'
p645
sS'categories'
p646
(lp647
S'web'
p648
asS'posted'
p649
g188
(S'\x07\xd9\x02\x1b'
p650
tp651
Rp652
ssg34
S'content/posts/2009/prettifying-embedded-youtube/index.md'
p653
sg36
F1433825923.0
sa(dp654
g2
(dp655
g4
V A joke-hack involving a CD, duct tape, an iPhone dock and your car.  
p656
sg28
g7
(g8
g9
V<p>Shortly after buying my first iPhone over a year ago, I found the right,\u000alow-budget solution to my in-car music needs. The ingredients involved\u000aare the following commonly found household items:</p>\u000a\u000a<ul>\u000a<li>One Compact Disk</li>\u000a<li>Some Duct Tape</li>\u000a</ul>\u000a\u000a<p>The idea is to take the iPhone dock and stick it onto the CD with duct\u000atape. Next, jam the CD into a tight slot in the dash of your car (easy\u000ato find for a Del Sol owner). Finally, attach power cables and/or AUX\u000aaudio jacks to the dock. The result is a flexible, shock-absorbent mount\u000afor your everyone's favorite phone. I've used this hack for well over a\u000ayear now with no problems. Here's how it looks in the interior of my\u000acar:</p>\u000a\u000a<iframe title="YouTube video player" width="600" height="368"\u000a  src="http://www.youtube.com/embed/Yg3VrsDXIpI" frameborder="0"\u000a  allowfullscreen></iframe>\u000a\u000a<p>In a word: awesome.</p>\u000a
p657
tp658
Rp659
sg13
V/the-best-iphone-car-kit
p660
sg15
Nsg16
I01
sg17
ViPhone car kit: roll your own
p661
sg20
V\u000a\u000aShortly after buying my first iPhone over a year ago, I found the right,\u000alow-budget solution to my in-car music needs.
p662
sS'snip'
p663
g7
(g8
g9
V<p>A joke-hack involving a CD, duct tape, an iPhone dock and your car.</p>\u000a
p664
tp665
Rp666
sg25
g169
sg33
g660
sg170
(dp667
g172
S'Feb'
p668
sg174
S'February 5, 2009'
p669
sg176
I2
sg177
S'2009-02-05T09:00:00-00:00'
p670
sg179
I1233853200
sg180
I2009
sg181
I5
ssg65
g182
sg31
S'the-best-iphone-car-kit'
p671
sS'categories'
p672
(lp673
S'physical'
p674
asS'posted'
p675
g188
(S'\x07\xd9\x02\x05'
p676
tp677
Rp678
ssg34
S'content/posts/2009/the-best-iphone-car-kit/index.md'
p679
sg36
F1332684374.0
sa(dp680
g2
(dp681
g4
V Is that a portable drum kit in your pants or are you just happy to see me?  
p682
sg28
g7
(g8
g9
V<p>Look at the riders of any city bus. Many of them are plugged into their music\u000aplayers, tapping away to the beat. I propose to augment our natural love of\u000arhythm into a ubiquitous wearable drum system. The target user of this system\u000aisn\u2019t only the typical rhythm loving bus rider, but also an amateur drummer.\u000aDrum kits are heavy and unwieldy, making them difficult to transport to a jam\u000asession. The proposed system can also act as a stand-in for a full drum-kit for\u000aquick, impromptu jamming.</p>\u000a\u000a<p>I took a pair of jeans and imbued them with two <a href="http://www.sparkfun.com/commerce/product_info.php?products_id=9376">force-sensitive resistors</a>,\u000aone on each knee. The left pocket houses a sparkfun box containing an arduino\u000aand a breadboard. Wires run through the pant legs to connect the pads to the\u000abox. Wiring the pants was surprisingly easy, since as I discovered, electric\u000atape easily adheres to denim.</p>\u000a\u000a<iframe title="YouTube video player" width="600" height="368"\u000a  src="http://www.youtube.com/embed/HjWx9fp-8oU" frameborder="0"\u000a  allowfullscreen></iframe>\u000a\u000a<p>The two FSRs are hooked into pull-down switches which connect to analog ports\u000aof the Arduino. Every time a pad is hit, Arduino sends the pad\u000aID and the force of the impact through the serial port. A python program running on my machine\u000alistens on the serial port and synthesizes sounds corresponding to the data\u000ausing <a href="http://pyserial.sourceforge.net/">pyserial</a> and <a href="http://www.pygame.org/news.html">pygame</a> respectively.</p>\u000a\u000a<pre><code>import pygame, sys, serial\u000a\u000a# initialize the pygame mixer\u000apygame.mixer.init(frequency=22050, size=-16, channels=2, buffer=256)\u000a\u000a# map positions to sounds\u000asamples = {\u000a    'lknee': pygame.mixer.Sound('media/hihat-open.aif'),    \u000a    #...\u000a}\u000a\u000a# initialize serial port\u000as = serial.Serial('/dev/tty.usbserial-A6008ea9', 9600)\u000a\u000awhile 1:\u000a    # read from the serial port\u000a    line = s.readline().strip()\u000a    # ...\u000a    sample = samples[pad_id]\u000a    sample.play()\u000a\u000aif __name__ == '__main__':\u000a    pass\u000a</code></pre>\u000a\u000a<p>This first prototype of Drum Pants is intentionally crude. Aside from\u000aincreasing this system\u2019s production value, there are a number of limitations\u000athat should be addressed. The current prototype requires a computer to\u000asynthesize sounds, which greatly hinders portability. By retrofitting the\u000aArduino with a <a href="http://asynclabs.com/wiki/index.php?title=WiShield_1.0">wifi shield</a>, the system could communicate with any\u000awifi-capable synthesizer, such as an Android phone.</p>\u000a\u000a<p>Another issue with this system is that it\u2019s built entirely into a pair of\u000apants. This makes putting drum pads into other items of clothing impossible. To\u000aaddress this problem, the pads could wirelessly communicate to the Arduino\u000adevice. In this case, the pads would be self-contained transmitters that could\u000abe placed anywhere. This opens up a wide variety of applications, such as\u000aplacing the pad onto a pair of shoes to simulate a kick or hi-hat pedal. Do you\u000alike this idea? Please give me feedback below!</p>\u000a\u000a<p><strong>Update:</strong> Do you want to build your own drum pants? Check out this\u000a<a href="http://www.instructables.com/id/Drum-Wear-drums-in-your-clothing/">instructable</a>!</p>\u000a\u000a<p><strong>Update 2:</strong> Accepted as a CHI 2010 WIP. Many thanks to <a href="http://code.arc.cmu.edu/~mdg/">Mark Gross</a>!</p>\u000a
p683
tp684
Rp685
sg13
V/ubiquitous-drums
p686
sg15
Nsg16
I01
sg17
VUbiquitous drums
p687
sg20
V\u000a\u000a\u000aLook at the riders of any city bus.
p688
sS'snip'
p689
g7
(g8
g9
V<p>Is that a portable drum kit in your pants or are you just happy to see me?</p>\u000a
p690
tp691
Rp692
sg25
g169
sg33
g686
sg170
(dp693
g172
S'Nov'
p694
sg174
S'November 18, 2009'
p695
sg176
I11
sg177
S'2009-11-18T09:00:00-00:00'
p696
sg179
I1258563600
sg180
I2009
sg181
I18
ssg65
g182
sg31
S'ubiquitous-drums'
p697
sS'categories'
p698
(lp699
S'physical'
p700
asS'posted'
p701
g188
(S'\x07\xd9\x0b\x12'
p702
tp703
Rp704
ssg34
S'content/posts/2009/ubiquitous-drums/index.md'
p705
sg36
F1433825930.0
sa(dp706
g2
(dp707
g4
V An article on how to write your own PhoneGap plugin for Android. I also write about the WebIntent plugin which lets you create Android intents from the web.  
p708
sg28
g7
(g8
g9
V<p><a href="http://phonegap.com/">PhoneGap</a> is a very useful cross-platform web application wrapper\u000athat lets developers package web applications into native apps. It also\u000alets web apps access functionality traditionally only available in\u000anative apps. This how-to is about exposing additional Android\u000afunctionality to the web with PhoneGap plugins. <a href="http://developer.android.com/reference/android/content/Intent.html">Intents</a> are a\u000afundamental part of the Android ecosystem, allowing a sort of\u000amessage-passing mechanism between applications, but they are not exposed\u000ato web applications. The sample Android plugin I wrote is called\u000aWebIntent, which lets you create a first class Android applications in\u000aJavaScript.</p>\u000a\u000a<h2>WebIntent</h2>\u000a\u000a<p>Firstly, the WebIntent plugin is a means of creating Android activities\u000avia Intents. With just four parameters, <code>action, url, type, extras</code>,\u000ait's possible to get a lot of mileage from the Android OS. For example,\u000ayou can send email: </p>\u000a\u000a<pre><code>Android.sendEmail = function(subject, body) { \u000a  var extras = {};\u000a  extras[WebIntent.EXTRA_SUBJECT] = subject;\u000a  extras[WebIntent.EXTRA_TEXT] = body;\u000a  window.plugins.webintent.startActivity({ \u000a      action: WebIntent.ACTION_SEND,\u000a      type: 'text/plain', \u000a      extras: extras \u000a    }, \u000a    function() {}, \u000a    function() {\u000a      alert('Failed to send email via Android Intent');\u000a    }\u000a  ); \u000a};\u000a</code></pre>\u000a\u000a<p>Or, you can load Google Maps: </p>\u000a\u000a<pre><code>Android.showMap = function (address) {\u000a  window.plugins.webintent.startActivity({\u000a    action: WebIntent.ACTION_VIEW,\u000a    url: 'geo:0,0?q=' + address,\u000a  }, function () {}, function () {\u000a    alert('Failed to open URL via Android Intent');\u000a  });\u000a};\u000a</code></pre>\u000a\u000a<p>Secondly, the plugin lets you react when your PhoneGap\u000aapplication gets invoked with certain intents. To do this, you need to\u000asetup correct intent-filters in the AndroidManifest.xml. For example, to\u000arespond to ACTION_SEND, something like the following should appear in\u000athe manifest: </p>\u000a\u000a<pre><code>&lt;intent-filter&gt; \u000a  &lt;action android:name="android.intent.action.SEND" /&gt;\u000a  &lt;category android:name="android.intent.category.DEFAULT" /&gt;\u000a  &lt;data android:mimeType="text/plain" /&gt;\u000a&lt;/intent-filter&gt;\u000a</code></pre>\u000a\u000a<p>Then, in the JavaScript, you can check what extras were specified during\u000ainvocation:</p>\u000a\u000a<pre><code>// deviceready is PhoneGap's init event\u000adocument.addEventListener('deviceready', function () {\u000a  window.plugins.webintent.getExtra(WebIntent.EXTRA\u005c_TEXT, function (url) {\u000a    // url is the value of EXTRA_TEXT \u000a  }, function() {\u000a    // There was no extra supplied.\u000a  });\u000a});\u000a</code></pre>\u000a\u000a<p>This lets you respond to Intents without writing native code. The plugin\u000acode is available for your use and/or perusal on <a href="https://github.com/phonegap/phonegap-plugins/tree/master/Android/WebIntent/">my github</a>. To install\u000athe plugin, move <a href="https://github.com/phonegap/phonegap-plugins/blob/master/Android/WebIntent/webintent.js">webintent.js</a> to your project's www folder and include a\u000areference to it in your html files. Create a folder called "borismus" within\u000ayour project's src/com/ folder and move <a href="https://github.com/phonegap/phonegap-plugins/blob/master/Android/WebIntent/WebIntent.java">WebIntent.java</a> into it.  </p>\u000a\u000a<h2>Writing Android PhoneGap Plugins</h2>\u000a\u000a<p>The PhoneGap project provides a <a href="http://blogs.nitobi.com/joe/2009/12/17/introducing-ponygap-phonegap-plugins-for-android/">plugin architecture</a>, which isn't very\u000awell documented. Perhaps as a result, there are very few Android plugins\u000a(see <a href="https://github.com/phonegap/phonegap-plugins/tree/master/Android/">the github</a>). There are two parts of an Android plugin: a native\u000aJava class that extends <code>com.phonegap.api.Plugin</code>, and a JavaScript wrapper\u000afor that Java class. The JavaScript wrapper registers the plugin with the\u000aPhoneGap plugin manager. A plugin API is defined in JavaScript, and exposed\u000avia <code>window.plugins.myplugin</code>. Calling the API is done as follows: </p>\u000a\u000a<pre><code>window.plugins.webintent.foo({ arg1: 'val1', arg2: 'val2', // etc });\u000a</code></pre>\u000a\u000a<p>The plugin API is defined in the <code>MyPlugin</code> JavaScript class:</p>\u000a\u000a<pre><code>var MyPlugin = function () {};\u000aMyPlugin.prototype.foo = function (params, success, fail) {\u000a  return PhoneGap.exec(success, fail, 'MyPlugin', 'startActivity', [params]);\u000a};\u000a</code></pre>\u000a\u000a<p>The plugin needs to be registered with PhoneGap before it can be\u000aused. </p>\u000a\u000a<pre><code>PhoneGap.addConstructor(function () {\u000a  // Creates window.plugins.myplugin, an instance of MyPlugin\u000a  PhoneGap.addPlugin('myplugin', new MyPlugin()); \u000a  // Binds MyPlugin to the Java class com.example.MyPlugin \u000a  PluginManager.addService('MyPlugin', 'com.example.MyPlugin');\u000a});\u000a</code></pre>\u000a\u000a<p>Finally, there needs to be a Java class that actually implements the desired\u000abehavior. The execute method takes an action parameter, which is the name of\u000athe function (foo in this example), and an array of arguments: </p>\u000a\u000a<pre><code>public class MyPlugin extends Plugin {\u000a  public PluginResult execute(String\u000a  action, JSONArray args, String callbackId) {\u000a    if (action.equals("foo")) {\u000a      // Implementation \u000a    }\u000a  }\u000a}\u000a</code></pre>\u000a\u000a<p>This architecture is the key to unlock any Android functionality to your\u000aPhoneGap-wrapped mobile web application. One of huge benefits of PhoneGap\u000aand mobile web is that it's a cross-platform solution. I admit that writing\u000aplatform-specific plugins seems counter-productive to this end. However\u000auntil the mobile web gets the love it deserves (via extra sweet mobile\u000abrowsers), there will be a place for platform-specific PhoneGap plugins to\u000amake web applications fit better into native mobile platforms.</p>\u000a
p709
tp710
Rp711
sg13
V/android-phonegap-plugins
p712
sg15
Nsg16
I01
sg17
VWebIntent, an Android PhoneGap plugin
p713
sg20
V\u000a\u000a[PhoneGap][] is a very useful cross-platform web application wrapper\u000athat lets developers package web applications into native apps.
p714
sS'snip'
p715
g7
(g8
g9
V<p>An article on how to write your own PhoneGap plugin for Android. I also write about the WebIntent plugin which lets you create Android intents from the web.</p>\u000a
p716
tp717
Rp718
sg25
g169
sg33
g712
sg170
(dp719
g172
S'Nov'
p720
sg174
S'November 25, 2010'
p721
sg176
I11
sg177
S'2010-11-25T09:00:00-00:00'
p722
sg179
I1290704400
sg180
I2010
sg181
I25
ssg65
g182
sg31
S'android-phonegap-plugins'
p723
sS'categories'
p724
(lp725
S'web'
p726
aS'android'
p727
asS'posted'
p728
g188
(S'\x07\xda\x0b\x19'
p729
tp730
Rp731
ssg34
S'content/posts/2010/android-phonegap-plugins/index.md'
p732
sg36
F1433825738.0
sa(dp733
g2
(dp734
g4
V This LEGO Mindstorms robot is controlled using twitter. When the robot is online, it responds to at-replied command to @mindstorms.  
p735
sg28
g7
(g8
g9
V<p>A few projects around the internet use an Android phone to control the\u000aLEGO Mindstorms NXT brick. Most involve an ugly hack in which the phone\u000acommunicates with a computer over WiFi, and the computer (paired to the\u000aNXT through bluetooth) submits the command to the brick. These projects\u000atypically use Android as a remote control for the NXT robot, and not as\u000apart of the robot itself. Here is a missed opportunity: the NXT could be\u000aaugmented by an <a href="http://developer.android.com/reference/android/hardware/Sensor.html">impressive list</a> of sensors, GPS and Internet access\u000aprovided by an Android phone. </p>\u000a\u000a<p>This project does just that, while eliminating the need for a computer in\u000athe loop, so that the Android directly communicates to the NXT. This allows\u000afor more powerful Android-powered NXT robots. As an example, I made a fully\u000aautonomous twitter-controlled robot. The NXT uses two motors to spin in\u000aplace or move forward, and a third motor to control the tilt of a Android\u000aphone cradle. The Android phone keeps track of its orientation (compass\u000aheading and tilt), polls <a href="http://search.twitter.com/">twitter search</a> for new commands and sends\u000acommands to the NXT brick. After each command completes, the Android phone\u000atakes a picture and sends it to twitter. Any twitter user can look at the\u000alast few photos, decide which command makes sense to perform next, and issue\u000ait. This approach can be summarized succinctly as "<a href="http://en.wikipedia.org/wiki/Crowdsourcing">crowdsourced</a>\u000a<a href="http://en.wikipedia.org/wiki/Teleoperation">teleoperation</a>". </p>\u000a\u000a<p>Here's a demonstration video of the robot in action:</p>\u000a\u000a<iframe title="YouTube video player" width="600" height="368"\u000a  src="http://www.youtube.com/embed/ATQ_0tySttM" frameborder="0"\u000a  allowfullscreen></iframe>\u000a\u000a<p>I think that this marriage of Android and NXT can fuel a very interesting\u000aset of robots impossible to build with the NXT alone. <a href="http://twitter.com/mindstorms">@mindstorms</a> is\u000aoffline for now to save some battery life, but code is available at my new\u000a<a href="http://github.com/borismus/android-nxt">github</a>. If you make use of my code or have some feedback, please reply\u000abelow!</p>\u000a
p736
tp737
Rp738
sg13
V/android-powered-mindstorms
p739
sg15
Nsg16
I01
sg17
VAndroid-powered mindstorms
p740
sg20
V\u000a\u000aA few projects around the internet use an Android phone to control the\u000aLEGO Mindstorms NXT brick.
p741
sS'snip'
p742
g7
(g8
g9
V<p>This LEGO Mindstorms robot is controlled using twitter. When the robot is online, it responds to at-replied command to @mindstorms.</p>\u000a
p743
tp744
Rp745
sg25
g169
sg33
g739
sg170
(dp746
g172
S'Jun'
p747
sg174
S'June 27, 2010'
p748
sg176
I6
sg177
S'2010-06-27T09:00:00-00:00'
p749
sg179
I1277654400
sg180
I2010
sg181
I27
ssg65
g182
sg31
S'android-powered-mindstorms'
p750
sS'categories'
p751
(lp752
S'android'
p753
aS'web'
p754
asS'posted'
p755
g188
(S'\x07\xda\x06\x1b'
p756
tp757
Rp758
ssg34
S'content/posts/2010/android-powered-mindstorms/index.md'
p759
sg36
F1433825741.0
sa(dp760
g2
(dp761
g4
V A fun self-referential experiment in which I asked (via question answering sites) what the best question answering site is.  
p762
sg28
g7
(g8
g9
V<p>There are hundreds of question answering sites on the Internet. Most of\u000athem focus on specific topics (<a href="http://stackoverflow.com">Stack Overflow</a> for software\u000aengineering, <a href="http://news.ycombinator.com">Hacker News</a> for startups, etc). However, a number of\u000aQ&amp;A sites exist for answering any kind of question, including the\u000acontentious <a href="http://www.somethingawful.com/flash/shmorky/babby.swf">"How is Babby formed?"</a>. Open domain Q&amp;A sites have a\u000adistinct advantage over closed domain sites, since there is no need to\u000aknow which closed domain site is most appropriate for the question at\u000ahand. Still, the asker needs to decide which open domain Q&amp;A site to ask\u000aon! What follows is a self-referential approach to ranking popular open\u000adomain Q&amp;A sites.</p>\u000a\u000a<h2>Who questions the Q&amp;A sites?</h2>\u000a\u000a<p>I compiled a list of the top 7 most popular Q&amp;A sites. Since\u000a<a href="http://alexa.com">alexa.com</a> doesn't track subdomains (ex. <a href="http://answers.yahoo.com">answers.yahoo.com</a>, and\u000a<a href="http://askville.amazon.com">askville.amazon.com</a>), I was forced to use <a href="http://compete.com">compete.com</a>. I then\u000aasked each popular site a "recursive" question:</p>\u000a\u000a<blockquote>\u000a  <p>What is the best question answering site (besides $SITENAME), and why?</p>\u000a</blockquote>\u000a\u000a<p>Eventually, most Q&amp;A sites responded with one or two answers, which I\u000aharvested and analyzed. Clearly this sample is too small to draw any\u000ascientifically valid conclusions, so my analysis is anecdotal at best.\u000aStill it paints an interesting picture of the open domain Q&amp;A space. In\u000aaddition to the responses themselves, I looked at other data, such as\u000aresponse time and response length. The following diagram summarizes the\u000aresults. </p>\u000a\u000a<p><img src="qa-analysis.png" alt="image" /> </p>\u000a\u000a<p>The arrows in the above diagram represent\u000arecommendations from one community to another. The surveyed Q&amp;A sites\u000aare grouped based on popularity into three buckets. The most popular\u000abucket includes Yahoo! Answers and Answers.com, with 44 million and 31\u000amillion monthly uniques respectively. Despite having been around for the\u000alongest (Answers.com launched in 1996), these are the most useless Q&amp;A\u000awebsites. It took 4 days to get the answer "Another good answering site\u000ais answers.com" from a Y! Answers member, and no response ever came from\u000aAnswers.com in 3 weeks.</p>\u000a\u000a<p>The moderately popular bucket of Q&amp;A sites\u000aincludes Answerbag, Mahalo Answers and AskVille. These sites get about 4\u000amillion uniques a month, and generally gave reasonable answers. Mahalo\u000aanswers yielded the longest response, a 120 word paragraph recommending\u000aAardvark. The least visited sites and relative newcomers to the scene\u000aare the most promising and active communities. Aardvark's response speed\u000ais unparalleled, the first answer taking just 17 minutes, probably due\u000ato GTalk integration. While Quora didn't actually yield a Q&amp;A site\u000arecommendation, the respondent produced an reasonable retort to my\u000arather questionable question.</p>\u000a\u000a<blockquote>\u000a  <p>A Q&amp;A "uber-site" that will do a good job being all things to all\u000a  people is simply not going to happen - it is like "one size fits all"\u000a  underwear, which never really fits anyone.</p>\u000a</blockquote>\u000a\u000a<h2>The Ranking</h2>\u000a\u000a<p>Overall, here's my anecdotal list of top 7 open domain Q&amp;A sites (and\u000alinks to the questions I posted):</p>\u000a\u000a<ol>\u000a<li><a href="http://vark.com/t/Tljh9Q">Aardvark</a> \u2013 Quick answers to your questions. Welcome to Aardvark!</li>\u000a<li><a href="https://www.quora.com/What-is-the-best-question-answering-site-besides-Quora-and-why">Quora</a> - Your question is flawed. Allow me to explain why.</li>\u000a<li><a href="http://www.mahalo.com/answers/what-is-the-best-question-answering-site-besides-mahalo-and-why">Mahalo</a> \u2013 I'll answer your question with quite a bit of detail,\u000abut it'll take a while.</li>\u000a<li><a href="http://www.answerbag.com/a_view/9737054">Answerbag</a> - heres a decent answer but forget punctuation</li>\u000a<li><a href="http://askville.amazon.com/question-answering-site-askville/AnswerViewer.do?requestId=74225856&amp;startIndex=1&amp;filter=ActivityOnQuestionsOnly">AskVille</a> - Takes forever to give you a really bizarre answer.</li>\u000a<li><a href="http://answers.yahoo.com/question/index;_ylt=AlzEGEI5OLG69C1Ev0RgR4Pty6IX;_ylv=3?qid=20101007131829AA8cjLR">Yahoo! Answers</a> \u2013 This community is brain damaged. Don't bother.</li>\u000a<li><a href="http://wiki.answers.com/Q/What_is_the_best_question_answering_site_besides_Answers_dot_com_and_why">Answers.com</a> \u2013 This community is inactive. Don't bother.</li>\u000a</ol>\u000a\u000a<p>With a lot more answers from the Q&amp;A communities, and some notion of\u000aweight, you could imagine doing a PageRank-style calculation to\u000adetermine a more meaningful ordering of the top Q&amp;A sites. We leave the\u000adetails of that approach to the reader :) PS. Thanks Sean, Jenn and Pat!</p>\u000a
p763
tp764
Rp765
sg13
V/best-question-answering-sites
p766
sg15
Nsg16
I01
sg17
VThe best question answering sites
p767
sg20
V\u000a\u000aThere are hundreds of question answering sites on the Internet.
p768
sS'snip'
p769
g7
(g8
g9
V<p>A fun self-referential experiment in which I asked (via question answering sites) what the best question answering site is.</p>\u000a
p770
tp771
Rp772
sg25
g169
sg33
g766
sg170
(dp773
g172
S'Oct'
p774
sg174
S'October 25, 2010'
p775
sg176
I10
sg177
S'2010-10-25T09:00:00-00:00'
p776
sg179
I1288022400
sg180
I2010
sg181
I25
ssg65
g182
sg31
S'best-question-answering-sites'
p777
sS'categories'
p778
(lp779
S'social'
p780
asS'posted'
p781
g188
(S'\x07\xda\n\x19'
p782
tp783
Rp784
ssg34
S'content/posts/2010/best-question-answering-sites/index.md'
p785
sg36
F1433825748.0
sa(dp786
g2
(dp787
g4
V I'm confirmed to go to  CHI2010  in Atlanta, so I spent some time making a poster for the  Ubiquitous Drums  project that was miraculously accepted as a  WIP . It's nice to pretend to be a visual designer sometimes. Thanks to Mark, Jenn and Rebeca for your input.         I would really appreciate additional suggestions on how to improve the  poster , or your thoughts on academic posters in general. Why are they usually so ugly?  
p788
sg28
g7
(g8
g9
V<p>I'm confirmed to go to <a href="http://chi2010.org/">CHI2010</a> in Atlanta, so I spent some time\u000amaking a poster for the <a href="/ubiquitous-drums">Ubiquitous Drums</a> project that was\u000amiraculously accepted as a <a href="ubiquitous-drums-paper.pdf">WIP</a>. It's nice to pretend to be a visual\u000adesigner sometimes. Thanks to Mark, Jenn and Rebeca for your input.</p>\u000a\u000a<p><img src="ubiquitous-drums-poster.jpg" alt="image" /></p>\u000a\u000a<p>I would really appreciate additional suggestions on how to improve the\u000a<a href="ubiquitous-drums-poster.pdf">poster</a>, or your thoughts on academic posters in general. Why are\u000athey usually so ugly?</p>\u000a
p789
tp790
Rp791
sg13
V/chi-2010-poster
p792
sg15
Nsg16
I00
sg17
VCHI 2010 poster
p793
sg20
V\u000a\u000aI'm confirmed to go to [CHI2010][] in Atlanta, so I spent some time\u000amaking a poster for the [Ubiquitous Drums][] project that was\u000amiraculously accepted as a [WIP][].
p794
sg6
g791
sg25
g169
sg33
g792
sg170
(dp795
g172
S'Mar'
p796
sg174
S'March 21, 2010'
p797
sg176
I3
sg177
S'2010-03-21T09:00:00-00:00'
p798
sg179
I1269187200
sg180
I2010
sg181
I21
ssg65
g182
sg31
S'chi-2010-poster'
p799
sS'categories'
p800
(lp801
S'design'
p802
asS'posted'
p803
g188
(S'\x07\xda\x03\x15'
p804
tp805
Rp806
ssg34
S'content/posts/2010/chi-2010-poster/index.md'
p807
sg36
F1433825753.0
sa(dp808
g2
(dp809
g4
V My first Chrome extension that encapsulates thesixtyone music player in a background page.  
p810
sg28
g7
(g8
g9
V<p>It's a little late to join the Chrome Extension writing party, and\u000aunfortunately <a href="http://www.readwriteweb.com/archives/chrome_web_store_delayed_until_december.php">still too early</a> for the Chrome Web Store launch.\u000aTiming issues aside, <a href="https://chrome.google.com/extensions/detail/hghoinoackbjefgfkbgnkjknmneajoof">here's an extension</a> that presents\u000athesixtyone.com in a smaller, simpler interface, effectively saving a\u000atab in Chrome and removing all of the (useless to me) social game\u000amechanics from thesixtyone. This post walks through some technical\u000adetails of the implementation, message passing between pages of an\u000aextension, JavaScript injection and DOM event generation. </p>\u000a\u000a<p>Wrapping a web application in an extension may sound deceptively easy:\u000ajust load the website in an iframe in the background (via \u000a<a href="https://code.google.com/chrome/extensions/background_pages.html">background pages</a>), and then control the website from the background page\u000avia commands from the popup page that appears when you click the extension icon\u000ain the toolbar. Not so fast! The key issue with this approach is the control\u000apart. For good reason, browser security disallows executing JavaScript in an\u000aembedded iframe if the src is another domain. The solution to this problem is\u000atwo fold:</p>\u000a\u000a<ol>\u000a<li>Chrome extensions can <a href="https://code.google.com/chrome/extensions/content_scripts.html">inject custom javascript</a> into web pages.</li>\u000a<li>It's possible to <a href="https://code.google.com/chrome/extensions/messaging.html">pass messages</a> between multiple pages running in\u000aChrome. This is an implementation of <a href="http://www.whatwg.org/specs/web-apps/current-work/multipage/comms.html">HTML5 postMessage</a>.</li>\u000a</ol>\u000a\u000a<p>With these two tools, we can inject javascript that implements a message\u000alistener into a target page. This lets you define a JavaScript API\u000aaround the target page. In my case, the API around thesixtyone.com was\u000athe following set of simple commands: next, previous, play, pause and\u000agetSongInfo. Once we have injected JavaScript running in\u000athesixtyone.com, however, it's impossible to call the site's native\u000aJavaScript for security reasons \u000a(from <a href="http://code.google.com/chrome/extensions/content_scripts.html">Chrome Extension Developer Guide</a>):</p>\u000a\u000a<blockquote>\u000a  <p>Content scripts execute in a special environment called an isolated\u000a  world. They have access to the DOM of the page they are injected into,\u000a  but not to any JavaScript variables or functions created by the page.</p>\u000a</blockquote>\u000a\u000a<p>Thus, I had to resort to some unfortunate hackery: generating fake mouse\u000aclicks. One of the benefits of Chrome Extension writing is that there's\u000ano cross-browser issues to deal with, which makes this sort of trick\u000amuch more reliable:</p>\u000a\u000a<pre><code>function simulateClick(elementId) {\u000a  var clickEvent = document.createEvent('MouseEvents');\u000a  clickEvent.initMouseEvent('click', true, false,  document,\u000a      0, 0, 0, 0, 0, false, false, false, false, 0, null);\u000a  document.getElementById(elementId).dispatchEvent(clickEvent);\u000a}\u000a</code></pre>\u000a\u000a<p>Overall, the interaction between the popup, background and injected code\u000ais rather complex and looks something like this: </p>\u000a\u000a<p><img src="chrome-extension-diagram.png" alt="image" /></p>\u000a\u000a<p>One Chrome issue came up in the course of development: when you change\u000aan iframe's src attribute such that the only difference compared to the\u000aold one is the hash part of the URL, the iframe src page does not\u000arefresh. I haven't had the chance to test test this on other browsers\u000ayet.</p>\u000a\u000a<p>Strictly speaking, what I described isn't really a mashup, since I only\u000ause one source. However, this approach is scalable to multiple sources,\u000aand could easily be used to mash multiple web applications up inside an\u000aextension. I hope this post is useful for people trying to wrap one or\u000amany cross-domain websites in their extension. </p>\u000a\u000a<p><img src="radio-61-screenshot.png" alt="image" /> </p>\u000a\u000a<p>I added a sprinkle of design and out came a Chrome Extension called\u000aRadio 61! Let me know what you think if you try it out. Source code is\u000aavailable on <a href="https://github.com/borismus/Radio-61">my github</a>.</p>\u000a
p811
tp812
Rp813
sg13
V/chrome-extension-mashups
p814
sg15
Nsg16
I01
sg17
VChrome extension for thesixtyone
p815
sg20
V\u000a\u000aIt's a little late to join the Chrome Extension writing party, and\u000aunfortunately [still too early][] for the Chrome Web Store launch.
p816
sS'snip'
p817
g7
(g8
g9
V<p>My first Chrome extension that encapsulates thesixtyone music player in a background page.</p>\u000a
p818
tp819
Rp820
sg25
g169
sg33
g814
sg170
(dp821
g172
S'Oct'
p822
sg174
S'October 31, 2010'
p823
sg176
I10
sg177
S'2010-10-31T09:00:00-00:00'
p824
sg179
I1288540800
sg180
I2010
sg181
I31
ssg65
g182
sg31
S'chrome-extension-mashups'
p825
sS'categories'
p826
(lp827
S'web'
p828
aS'chrome'
p829
asS'posted'
p830
g188
(S'\x07\xda\n\x1f'
p831
tp832
Rp833
ssg34
S'content/posts/2010/chrome-extension-mashups/index.md'
p834
sg36
F1433825757.0
sa(dp835
g2
(dp836
g4
V At CMU we conducted user research with real life Mechanical Turk users. We also tried to get them to collaboratively write articles using etherpad.  
p837
sg28
g7
(g8
g9
V<p>Last semester at CMU, I was involved in a research project involving\u000a<a href="http://www.mturk.com/">Mechanical Turk</a>. The goal was to get Mechanical Turk users (turkers)\u000ato collaborate on creating online wikipedia-style articles. Prior to my\u000ateam's involvement, an undergraduate created a mediawiki-based platform\u000ato allow turkers to collaborate on articles. Despite a high\u000acompensation, few turkers completed the task. My team tackled the\u000aproblem and came up with some interesting videos on the way. </p>\u000a\u000a<p>We began by conducting contextual interviews with turkers living in\u000aPittsburgh, all of whom rather unexpectedly, were female. The general\u000atakeaway was clear: turkers are used to very short and repetitive tasks,\u000abut article creation requires a prolonged period of concentration. Our\u000asolution was to significantly tweak the task, making it seem less\u000aarduous. In addition to simplifying the HIT's flow, we switched from\u000amediawiki to <a href="http://www.etherpad.com/">etherpad</a> as the article editing and collaboration\u000aplatform. As a result of these changes, we were able to churn out\u000aturker-created articles on a given topic for under ten dollars. Here's a\u000avideo of turkers collaborating on an article about Halloween:</p>\u000a\u000a<iframe title="YouTube video player" width="600" height="480"\u000a  src="http://www.youtube.com/embed/AmUq_Uovqek" frameborder="0"\u000a  allowfullscreen></iframe>\u000a\u000a<p>We started out by creating an etherpad instance with a simple paragraph\u000aabout the topic, as well as some article quality guidelines. Next, we\u000acreated a series of Mechanical Turk HITs referencing the etherpad\u000ainstance's URL. We paid our turkers a quarter up front for accepting the\u000atask, and provided a nickel (up to one dollar) every time they returned\u000ato edit the pad. We had no good way to verify the bonus mechanism, so we\u000agenerally gave out the maximum bonus to every active collaborator.\u000aHere's the evolution of an article on Windows 7: </p>\u000a\u000a<iframe title="YouTube video player" width="600" height="480"\u000a  src="http://www.youtube.com/embed/C7pV9fXIo0M" frameborder="0"\u000a  allowfullscreen></iframe>\u000a\u000a<p>Watching the etherpad explode in color as multiple turkers\u000asimultaneously edit the same article is still mesmerizing. Though the\u000aquality of the articles was quite low, it generally increased with each\u000aturker's successive pass. Also it's worth noting that errors that we\u000adeliberately inserted in the starting paragraph as well as in real time\u000awere swiftly edited out. Not much quantitative analysis of this\u000acollaboration data has been done yet, though there are plans to conduct\u000amore scientific experiments in the future. </p>\u000a\u000a<p>Several other researchers have been conducting interesting studies on\u000amturk. Greg Little's work at MIT generated an interesting project called\u000a<a href="http://groups.csail.mit.edu/uid/turkit/">TurKit</a>, intended to simplify setting up experiments such as the one\u000aoutlined above. Panos Ipeirotis at NYU runs a variety of turk\u000aexperiments as well as an <a href="http://hyperion.stern.nyu.edu/mturk/">mturk statistics monitor</a>, which\u000acontinually scrapes Mechanical Turk and generates summary data. Most\u000arecently, Jennifer Boriss surveyed the Turk community about their\u000abrowser preferences <a href="http://jboriss.wordpress.com/2010/01/13/mechanical-turk-studies-show-ie-users-discontent-a-growing-interest-in-chrome/">projecting a growing interest in Chrome</a>. </p>\u000a\u000a<p>These varied Mechanical Turk projects represent only a small fraction of\u000athe potential of crowd-sourced marketplaces. It's especially interesting\u000ato take complex tasks, break them down into turk-sized morsels, and\u000arecombine them again. To improve the article collaboration scenario\u000adiscussed here, one could provide an outline of an article and task\u000aturkers to elaborate on each point. This seems to be exactly what Greg's\u000agroup is doing in an <a href="http://groups.csail.mit.edu/uid/deneme/?p=603">collaborative essay writing experiment</a>.  Such\u000aan approach may also be effectively applicable to crowd-sourced software\u000adevelopment, which I hope to explore in the near future. Do you know\u000aother interesting projects and resources related to Mechanical Turk? If\u000aso, please respond below!</p>\u000a
p838
tp839
Rp840
sg13
V/crowdsourcing-articles-with-mechanical-turk
p841
sg15
Nsg16
I01
sg17
VCrowdsourcing articles with mechanical turk
p842
sg20
V\u000a\u000aLast semester at CMU, I was involved in a research project involving\u000a[Mechanical Turk][].
p843
sS'snip'
p844
g7
(g8
g9
V<p>At CMU we conducted user research with real life Mechanical Turk users. We also tried to get them to collaboratively write articles using etherpad.</p>\u000a
p845
tp846
Rp847
sg25
g169
sg33
g841
sg170
(dp848
g172
S'Jan'
p849
sg174
S'January 14, 2010'
p850
sg176
I1
sg177
S'2010-01-14T09:00:00-00:00'
p851
sg179
I1263488400
sg180
I2010
sg181
I14
ssg65
g182
sg31
S'crowdsourcing-articles-with-mechanical-turk'
p852
sS'categories'
p853
(lp854
S'social'
p855
asS'posted'
p856
g188
(S'\x07\xda\x01\x0e'
p857
tp858
Rp859
ssg34
S'content/posts/2010/crowdsourcing-articles-with-mechanical-turk/index.md'
p860
sg36
F1433825763.0
sa(dp861
g2
(dp862
g4
V A brief Mechanical Turk survey to determine whether or not turkers might be willing to write code.  
p863
sg28
g7
(g8
g9
V<p>As a follow up to my <a href="/crowdsourcing-articles-with-mechanical-turk/">last post</a>, I posted a HIT on Mechanical Turk\u000aasking 20 turkers if they know Java. I paid them 5 cents to answer the\u000aquestion. Surprisingly, 9 of 20 claimed to know. I was amazed at how\u000astrong selection bias was in this case, since surely not 50% of turkers\u000aknow how to program! I then asked those turkers who know Java to\u000acomplete the following trivial Java method. If they wrote it correctly,\u000aI paid them a 45 cent bonus.</p>\u000a\u000a<pre><code>public static String reverse(String source) {\u000a  // your code here \u000a}\u000a</code></pre>\u000a\u000a<p>Here are the results:</p>\u000a\u000a<ul>\u000a<li>4 turkers used <code>StringBuffer.reverse</code></li>\u000a<li>3 turkers created a new string by iterating through the original\u000astring in reverse</li>\u000a<li>1 used recursion</li>\u000a<li>1 used <code>Collections.sort(l)</code>. I'm not sure what was intended</li>\u000a</ul>\u000a\u000a<p>I was hoping that people would fill in the empty reverse method with\u000atheir code, but many of them implemented their own methods and helpers.\u000aOne person implemented their own class with extensive comments. This\u000adata as a nice existence proof, indicating that turkers can be harnessed\u000afor programming-related crowdsourcing. </p>\u000a\u000a<p>I'd like to turn Mechanical Turkers into Mechanical Coders. Given a set\u000aof unit tests and a method to implement, their work could be\u000aautomatically verified based on passing the unit tests. Furthermore,\u000asome turkers could be tasked to write additional unit tests for this\u000amethod. Through this technique, it's conceivable to harness the power of\u000aThe Turk to implement whole classes. Code quality aside, what sort of\u000asoftware quality could be achieved by following this approach?</p>\u000a
p864
tp865
Rp866
sg13
V/crowdsourcing-code
p867
sg15
Nsg16
I01
sg17
VCrowdsourcing code
p868
sg20
V\u000a\u000aAs a follow up to my [last post][], I posted a HIT on Mechanical Turk\u000aasking 20 turkers if they know Java.
p869
sS'snip'
p870
g7
(g8
g9
V<p>A brief Mechanical Turk survey to determine whether or not turkers might be willing to write code.</p>\u000a
p871
tp872
Rp873
sg25
g169
sg33
g867
sg170
(dp874
g172
S'Jan'
p875
sg174
S'January 16, 2010'
p876
sg176
I1
sg177
S'2010-01-16T09:00:00-00:00'
p877
sg179
I1263661200
sg180
I2010
sg181
I16
ssg65
g182
sg31
S'crowdsourcing-code'
p878
sS'categories'
p879
(lp880
S'social'
p881
asS'posted'
p882
g188
(S'\x07\xda\x01\x10'
p883
tp884
Rp885
ssg34
S'content/posts/2010/crowdsourcing-code/index.md'
p886
sg36
F1433825767.0
sa(dp887
g2
(dp888
g4
V A short snippet of jQuery code that approximately implements the instant search experience.  
p889
sg28
g7
(g8
g9
V<p>Google recently unveiled Instant, a search enhancement which show\u000aresults as you type. The real technical challenge here is scaling the\u000abackend, which now needs to handle a lot more load. The frontend\u000aimplementation, however, is quite simple. Yearning for some web\u000adevelopment, I decided to get my hands dirty. Here's a minimal\u000a<a href="instant-search.html">implementation</a> in under 60 lines of jQuery code.</p>\u000a\u000a<p>Instant Search relies on two separate data sources: a suggestions API,\u000aand a web search API. Roughly speaking, it works as follows: as you type\u000ain the search box, AJAX requests are made to the suggestion API. The top\u000asuggestion is then used as the query to search. In this case, I used the\u000afollowing two services, both of which support JSONP:</p>\u000a\u000a<ul>\u000a<li>Suggest: <a href="http://suggestqueries.google.com/complete/search">http://suggestqueries.google.com/complete/search</a></li>\u000a<li>Search: <a href="http://ajax.googleapis.com/ajax/services/search/web">http://ajax.googleapis.com/ajax/services/search/web</a></li>\u000a</ul>\u000a\u000a<p>To make your own search instant, all you need are your own suggest and\u000asearch feeds, and some tweaks to my code. Just make sure your server can\u000ahandle the load!</p>\u000a
p890
tp891
Rp892
sg13
V/instant-search
p893
sg15
Nsg16
I01
sg17
VInstant search in 60 lines
p894
sg20
V\u000a\u000aGoogle recently unveiled Instant, a search enhancement which show\u000aresults as you type.
p895
sS'snip'
p896
g7
(g8
g9
V<p>A short snippet of jQuery code that approximately implements the instant search experience.</p>\u000a
p897
tp898
Rp899
sg25
g169
sg33
g893
sg170
(dp900
g172
S'Oct'
p901
sg174
S'October 9, 2010'
p902
sg176
I10
sg177
S'2010-10-09T09:00:00-00:00'
p903
sg179
I1286640000
sg180
I2010
sg181
I9
ssg65
g182
sg31
S'instant-search'
p904
sS'categories'
p905
(lp906
S'web'
p907
asS'posted'
p908
g188
(S'\x07\xda\n\t'
p909
tp910
Rp911
ssg34
S'content/posts/2010/instant-search/index.md'
p912
sg36
F1433825770.0
sa(dp913
g2
(dp914
g4
V A Facebook application for finding impromptu jam partners.  
p915
sg28
g7
(g8
g9
V<p>Ever wanted to join a band? I bet you have! Why? Because collaborative\u000amusic making is an incredibly enjoyable and rewarding experience. But\u000athe barriers to entry are high: not only do you need to have baseline\u000amusical skills, you also need considerable managerial talent to find and\u000abring together disorganized musicians. To find partners to jam with,\u000apeople use craigslist and band matching sites to try to establish\u000arelationships with randoms. Why not leverage our social networks for\u000athis purpose?</p>\u000a\u000a<p>Okay, now that you're fully convinced that there's a huge opportunity to\u000atap into this friend-jam space, let me introduce <a href="http://www.jamhunt.com/">Jam Hunt</a>. The idea\u000abehind Jam Hunt is to allow you to manage your musical profile by\u000aspecifying a list of instruments you are skilled at and a list of songs\u000ayou know how to play. If your friends also maintain such profiles, Jam\u000aHunt can look across the social graph and discover friends to try\u000ajamming with. Thus, the application enables spontaneous <em>flash bands</em>\u000a(in the spirit of <a href="http://en.wikipedia.org/wiki/Flash_mob">flash mobs</a>) to form brief, friendly jam sessions. </p>\u000a\u000a<p>I developed a Jam Hunt prototype for <a href="http://www.hcii.cmu.edu/courses/software-architecture-user-interfaces-0">SAUI class</a> while pleasantly\u000astranded in Pittsburgh as a result of <a href="http://www.nytimes.com/2010/04/25/weekinreview/25kimmelman.html">Eyjafjallajkull's eruption</a>.\u000aThe assignment stipulated that I implement a Facebook application, which\u000awas initially distressing to me, due to the <a href="http://www.codinghorror.com/blog/2007/06/avoiding-walled-gardens-on-the-internet.html">walled-garden nature</a> of\u000athe platform. I was slightly mollified when I discovered three things:</p>\u000a\u000a<ol>\u000a<li>that there is a <a href="http://wiki.developers.facebook.com/index.php/User:PyFacebook_Tutorial">way to use django</a> to develop Facebook apps.</li>\u000a<li>that the <em>average</em> Facebook user has a whopping 130 friends.</li>\u000a<li>that there is a potentially <a href="http://developers.facebook.com/docs/opengraph">bright future</a> ahead for Facebook</li>\u000a</ol>\u000a\u000a<p>My prospects for having fun while developing something useful, and\u000apotentially viral, and not entirely evil, were on the rise. </p>\u000a\u000a<p>As it turns out, developing a django application for Facebook is no\u000acakewalk.  Firstly, python is not an officially supported language for\u000aFacebook development. As a result, there are a number of\u000a<a href="http://code.google.com/p/simplefacebook/">semi-abandoned</a> <a href="http://github.com/sciyoshi/pyfacebook/tree/master">open source projects</a> to <a href="http://code.google.com/p/minifb/">bridge that gap</a>.\u000aCoupled with Facebook's outright disregard for API stability, calling\u000aNotifications.send and Stream.write were next to impossible from python.\u000aBut surely writing PHP applications must be a breeze, right? Well,\u000aduring the week that I was developing Jam Hunt,\u000a<a href="http://forum.developers.facebook.com/">forum.developers.facebook.com</a>, one of the most indexed resources on\u000aFacebook API questions, was consistently down. The sorry state of their\u000ahybrid <a href="http://wiki.developers.facebook.com/index.php/New_Design_Platform_Changes">documentation-wiki</a> system was just icing on the cake. </p>\u000a\u000a<p>Anyway, enough bitching! If you have some spare cycles and a Facebook\u000aaccount, please try <a href="http://www.jamhunt.com/">Jam Hunt</a>. Whether you find it interesting,\u000aappealing, pointless, ugly, or just outright broken, let me know.</p>\u000a
p916
tp917
Rp918
sg13
V/jam-hunt
p919
sg15
Nsg16
I01
sg17
VJam Hunt: friendly jam sessions
p920
sg20
V\u000a\u000aEver wanted to join a band? I bet you have! Why? Because collaborative\u000amusic making is an incredibly enjoyable and rewarding experience.
p921
sS'snip'
p922
g7
(g8
g9
V<p>A Facebook application for finding impromptu jam partners.</p>\u000a
p923
tp924
Rp925
sg25
g169
sg33
g919
sg170
(dp926
g172
S'Apr'
p927
sg174
S'April 24, 2010'
p928
sg176
I4
sg177
S'2010-04-24T09:00:00-00:00'
p929
sg179
I1272124800
sg180
I2010
sg181
I24
ssg65
g182
sg31
S'jam-hunt'
p930
sS'categories'
p931
(lp932
S'web'
p933
aS'music'
p934
asS'posted'
p935
g188
(S'\x07\xda\x04\x18'
p936
tp937
Rp938
ssg34
S'content/posts/2010/jam-hunt/index.md'
p939
sg36
F1433825775.0
sa(dp940
g2
(dp941
g4
V My first jQuery Mobile application (using Alpha 1). I found a few bugs and contributed a couple of plugins.  
p942
sg28
g7
(g8
g9
V<p>I'm about to embark on a 3-week heads-down coding spree to finish off\u000athe final project of my masters. I convinced my team and our clients\u000athat it's a good idea to use jQuery Mobile instead of native Android to\u000aimplement our data-centric application, so I've been playing with it\u000aquite a bit over the last week. jQM is still in Alpha 2, so to get my\u000afeet wet, I decided to write a demo app before starting the final\u000aproject. After stumbling over a few kinks and rough edges, a weekend of\u000acoding yielded a <a href="/x/hackernews/">mobile Hacker News client</a>.</p>\u000a\u000a<p><img src="hackernews.png" alt="image" /></p>\u000a\u000a<h2>Loading JavaScript</h2>\u000a\u000a<p>jQuery Mobile is firmly rooted in the progressive enhancement philosophy\u000aof web application development. In practice, this means that the simple\u000amarkup that you write in it is processed by the framework's JavaScript\u000aand transformed into much more complex and JS-enhanced markup. In\u000aparticular, jQM rewrites <code>&lt;a href="page.html"&gt;&lt;/a&gt;</code> into an element that\u000aloads the contents of the page via an AJAX request. Unfortunately, CSS\u000aand JavaScript referenced by <code>page.html</code> is not loaded. </p>\u000a\u000a<p>To work around this, I've been using a makeshift JS loader which latches\u000aon to the jQM pageshow event so that whenever an HTML page loads, the\u000acorresponding JavaScript file also loads. To prevent the JS loader from\u000aloading already-loaded JavaScript whenever a page is reloaded, it\u000aemploys a cache. Still, the JS needs to be triggered when the page is\u000aloaded, so there's a way to bind a page load to a function. Here's some\u000asample usage of the ScriptCache:</p>\u000a\u000a<pre><code>// Create a new ScriptCache in the global scope\u000avar scriptCache = new ScriptCache();\u000a// Now when page.html loads, page.js will load\u000a\u000a// Also, each page can register with the scriptCache\u000ascriptCache.onPageLoad('page.html', pageInitFunction);\u000a\u000a// Each page.js looks like this:\u000a(function() {\u000a  var init = function() {...};\u000a\u000a  // Whenever this page is loaded, call init\u000a  scriptCache.onPageLoad('page.html', init);\u000a  // Call init the first time it's loaded too\u000a  init();\u000a})();\u000a</code></pre>\u000a\u000a<p>This approach was inspired by a discussion on the <a href="http://forum.jquery.com/topic/links-don-t-load-scripts">jQuery Forum</a>, and\u000aI really hope that there will be a better answer from readers or the jQM\u000adevelopers. The <a href="https://github.com/borismus/jQuery-Mobile-Hacker-News/blob/master/assets/www/scriptcache.js">ScriptCache loader is on github</a>.</p>\u000a\u000a<h2>Passing Parameters</h2>\u000a\u000a<p>Another issue I came across was parameter passing between pages. The jQM\u000aHacker News client needs to pass item IDs from the main page listing to\u000aitem pages. Unfortunately jQuery Mobile seems to have no provision for\u000adoing this. A couple approaches come to mind. One way is through a\u000aglobal variable parameter passing convention (possible since jQuery\u000apages aren't navigated to, but loaded with AJAX). The way I'm using is\u000athrough GET parameters after the hash. Thus the URL to an item page\u000alooks like this: http://example.com/index.html#item.html?id=82831.\u000aHowever, the pageshow event fires <em>just slightly</em> before the\u000awindow.location is updated with the requested URL. I'm currently working\u000aaround this with a <code>setTimeout</code>, but this is clearly unacceptable. This\u000aissue is being <a href="https://github.com/jquery/jquery-mobile/issues#issue/450/comment/543394">discussed on github</a>. </p>\u000a\u000a<p>There's also an issue passing parameters via the jQM changePage call:\u000a<code>$.mobile.changePage('page.html?key=value');</code>. The parameters are simply\u000aignored. Thus I was forced to use <code>window.location.href +=\u000a'#page.html?key=value';</code>, which only works if your window.location.href\u000ais the main application page.</p>\u000a\u000a<h2>Hacker News</h2>\u000a\u000a<p>Using the workarounds outlined above, I wrote an application that lets\u000ayou read and post to <a href="http://news.ycombinator.com">Hacker News</a>, a startup-oriented online\u000acommunity run by <a href="http://ycombinator.com">Y Combinator</a>. It uses the <a href="http://api.ihackernews.com">third party HN API</a>\u000aprovided by <a href="http://twitter.com/ronnieroller">@ronnieroller</a>, who has been very responsive on twitter\u000a(thanks!). For fetching posts and comments, the application relies on\u000athe JSONP methods of this API. It doesn't currently permit authenticated\u000aposting and voting on HN. </p>\u000a\u000a<p>For commenting and submitting links, I embed actual Hacker News post\u000apages in an iframe, working around the lack of support and avoiding\u000atrust issues. Unfortunately the <a href="http://code.google.com/p/android/issues/detail?id=12558">Android browser has a bug</a> which\u000aforces iframes to resize to the width of their contained textarea if one\u000aexists in the embedded page. This causes the width of the page to grow\u000awhich forces jQuery Mobile to relayout. The reference implementation\u000a(iOS) does not suffer from this issue. </p>\u000a\u000a<p>I'd be glad to get your feedback on this app. If you want to read the\u000acode or fork the project, it's available <a href="https://github.com/borismus/jQuery-Mobile-Hacker-News/tree/master/assets/www/">on github</a>. </p>\u000a\u000a<p>I've packed this application with PhoneGap, and currently use it as the\u000aHN reader on my phone. Existing HN applications on Android don't provide\u000ashare intent handlers (to share from the browser or your RSS reader).\u000aThis app provides share intent handling through WebIntents, an Android\u000aPhoneGap plugin that I've been working on. More on that next post, so\u000astay tuned!</p>\u000a
p943
tp944
Rp945
sg13
V/jquery-mobile-hacker-news
p946
sg15
Nsg16
I01
sg17
VjQuery mobile hacker news
p947
sg20
V\u000a\u000aI'm about to embark on a 3-week heads-down coding spree to finish off\u000athe final project of my masters.
p948
sS'snip'
p949
g7
(g8
g9
V<p>My first jQuery Mobile application (using Alpha 1). I found a few bugs and contributed a couple of plugins.</p>\u000a
p950
tp951
Rp952
sg25
g169
sg33
g946
sg170
(dp953
g172
S'Nov'
p954
sg174
S'November 17, 2010'
p955
sg176
I11
sg177
S'2010-11-17T09:00:00-00:00'
p956
sg179
I1290013200
sg180
I2010
sg181
I17
ssg65
g182
sg31
S'jquery-mobile-hacker-news'
p957
sS'categories'
p958
(lp959
S'web'
p960
asS'posted'
p961
g188
(S'\x07\xda\x0b\x11'
p962
tp963
Rp964
ssg34
S'content/posts/2010/jquery-mobile-hacker-news/index.md'
p965
sg36
F1433825781.0
sa(dp966
g2
(dp967
g4
V My take on visualizing surprisingly large amounts of travel in 2010. I built a web application that uses Google Earth KML Tours for the purpose.  
p968
sg28
g7
(g8
g9
V<p>Over this last year and a half, I spent a lot of time in airports,\u000aflying nearly 150,000 km (queue environmental angst). I lived in Europe\u000aand both coasts of the US, somehow getting a masters from CMU in the\u000aend. For posterity, I wanted to digitally summarize this extensive\u000aglobetrotting experience, but found no adequate applications. It was an\u000aexperience worth celebrating, and how better to celebrate than by\u000awriting some code...? So I wrote <a href="http://thattrip.appspot.com/">an application demo</a> that renders\u000atravel adventures as a tour in Google Earth. </p>\u000a\u000a<p>First you fill in a pretty silly madlib describing your trip: </p>\u000a\u000a<p><img src="madlib.png" alt="image" /> </p>\u000a\u000a<p>Then, you can view your trip in the browser using the [Google Earth\u000aplugin][], or download the tour and open it in the Google Earth\u000aapplication. Each trip gets a unique URL that can be shared with others.\u000aHere is a youtubified summary of my recent travels generated by this\u000aapp:</p>\u000a\u000a<iframe title="YouTube video player" width="600" height="480"\u000a  src="http://www.youtube.com/embed/N1y8C_w-7Uw" frameborder="0"\u000a  allowfullscreen></iframe>\u000a\u000a<h2>How it works</h2>\u000a\u000a<p>The madlib provides a list of places and transportation modes, the\u000aplaces resolving to (lat, long) coordinates, and the transportation\u000amodes affecting the transition effects between places. Airplane\u000atransitions are shown with the camera oriented north, in a bird's eye\u000aview, while ground transport transitions are more complex, facing in the\u000adirection of travel and changing pitch angle. The application builds KML\u000ain JavaScript and eventually loads it all in a Google Earth plugin:</p>\u000a\u000a<pre><code>// Build the KML\u000avar helper = new KMLHelper();\u000ahelper.processData(data.places, function() {\u000a  // Generate KML tour for this\u000a  data.kml = helper.kml();\u000a  data.distance = helper.distance();\u000a\u000a  var json = JSON.stringify(data);\u000a  // Save the new data to the database\u000a  $.ajax({...})\u000a});\u000a\u000a// Eventually render the KML (ge is a Google Earth instance)\u000a$.getJSON('/trips/' + id + '/', function(data) {\u000a  var kmlObject = ge.parseKml(data.kml);\u000a  ge.getFeatures().appendChild(kmlObject);\u000a});\u000a</code></pre>\u000a\u000a<p>Place-to-place transition effects are achieved by using <a href="http://code.google.com/apis/kml/documentation/touring.html">KML Tours</a>,\u000awhich provide powerful camera control requiring some computation, like\u000agetting the midpoint between (lat, long) pairs. Math for midpoint and\u000aother common geodesy operations are <a href="http://www.movable-type.co.uk/scripts/latlong.html">well documented</a> by Chris Veness,\u000awho also provides an implementation in the form of a\u000a<a href="http://www.movable-type.co.uk/scripts/latlon.js">handy JavaScript library</a>. </p>\u000a\u000a<p>In addition to destinations, travel paths and transitions, this demo\u000auses the <a href="http://code.google.com/apis/picasaweb/overview.html">picasaweb API</a> to get public picasaweb images as\u000aGroundOverlays near places you visited. While it's possible to get a\u000auser's photos tagged with a tag, there is, rather bizarrely, no way to\u000aget a specific user's images tagged with a given location. </p>\u000a\u000a<p>As usual, all of the code is available on <a href="https://github.com/borismus/That-Trip">the github</a>. It's running\u000alive on <a href="http://thattrip.appspot.com/">thattrip.appspot.com</a>, so try it out.</p>\u000a\u000a<h2>Observations</h2>\u000a\u000a<p>The Google Earth plugin provides what is probably the richest map view\u000aavailable on the web, but it is not without drawbacks. The mere fact\u000athat it's a plugin immediately shrinks the possible user base of any app\u000athat relies it and adds to load time. There's no way to pre-cache the\u000adata that will be shown on a tour, causing a lot of chunky terrain\u000aduring quick transitions. While KML is rich and makes it possible to\u000aembed images as ground and photo overlays, the web at large has far more\u000aoptions to handle images, through CSS transformations and animations or\u000aby embedding in a canvas or SVG element. </p>\u000a\u000a<p>This demo is very geo-centric, telling a story almost exclusively about\u000awhere you went, with the images added as an afterthought. In retrospect,\u000aa more compelling story is one about what you personally saw, shown in\u000atravel photos, with a geodesy cherry on top. Stay tuned for future\u000awork...</p>\u000a
p969
tp970
Rp971
sg13
V/kml-tours-google-earth
p972
sg15
Nsg16
I01
sg17
VKML tours in Google Earth
p973
sg20
V\u000a\u000aOver this last year and a half, I spent a lot of time in airports,\u000aflying nearly 150,000 km (queue environmental angst).
p974
sS'snip'
p975
g7
(g8
g9
V<p>My take on visualizing surprisingly large amounts of travel in 2010. I built a web application that uses Google Earth KML Tours for the purpose.</p>\u000a
p976
tp977
Rp978
sg25
g169
sg33
g972
sg170
(dp979
g172
S'Dec'
p980
sg174
S'December 25, 2010'
p981
sg176
I12
sg177
S'2010-12-25T09:00:00-00:00'
p982
sg179
I1293296400
sg180
I2010
sg181
I25
ssg65
g182
sg31
S'kml-tours-google-earth'
p983
sS'categories'
p984
(lp985
S'web'
p986
asS'posted'
p987
g188
(S'\x07\xda\x0c\x19'
p988
tp989
Rp990
ssg34
S'content/posts/2010/kml-tours-google-earth/index.md'
p991
sg36
F1433825786.0
sa(dp992
g2
(dp993
g4
V My experiences moving from the WebFaction shared host to the Slicehost VPS.  
p994
sg28
g7
(g8
g9
V<p>I recently switched from shared hosting to a VPS, expecting to get an\u000aimmediate and automatic performance boost. I was overly optimistic and\u000aran into memory trouble right away. After endlessly struggling with\u000aApache and mod_php configuration, I was ready to give up. Then on a\u000awhim, I switched to nginx/fastcgi to see the average response time drop\u000afrom 1500ms to 300ms: </p>\u000a\u000a<p><img src="performance.png" alt="image" /> </p>\u000a\u000a<p><a href="http://www.webfaction.com">WebFaction</a> is a great shared hosting environment, providing tons of\u000afunctionality and impeccable customer support. However, shared hosting\u000ameans that multiple users share resources on a single physical machine,\u000aresulting in wildly fluctuating <a href="http://www.pingdom.com">site performance and uptime</a> at the\u000awhim of other clients. Also, I prefer to do system administration from\u000athe command line, but WebFaction provides a powerful but clunky\u000aweb-based administrative interface. All in all, despite my respect for\u000aWebFaction, I said goodbye and switched to <a href="http://www.slicehost.com">Slicehost</a>. </p>\u000a\u000a<p>I ordered my shiny new slice and plunged into configuration. I migrated\u000awordpress databases and set up Apache with mod<em>php. Everything seemed\u000ato work reasonably well until Apache ran for a few hours and began\u000aconsuming my memory allowance. The slice started thrashing and\u000aperformance fell to a crawl.  I found the culprit to be in my\u000ampm</em>prefork_module MaxClients and MaxRequestsPerChild configuration,\u000abut even after tweaking those, my slice was hitting the wall pretty\u000aquickly. I nearly returned to cushy WebFaction, where I had marginal\u000aperformance without the headache, but decided to experiment more. </p>\u000a\u000a<p>Nginx is a minimalist HTTP server written by <a href="http://sysoev.ru/en/">Igor Sysoev</a> for\u000a<a href="http://www.rambler.ru/">Rambler</a>. It's now being used by <a href="http://www.wordpress.com">Wordpress</a> and other high profile\u000asites. Seeking help from the internet, I eventually came across Thomasz\u000aSterna's <a href="http://tomasz.sterna.tv/2009/04/php-fastcgi-with-nginx-on-ubuntu/">php-fastcgi init script</a> and adapted it for my slice,\u000ausing</p>\u000a\u000a<pre><code>PHP_FCGI_CHILDREN=5\u000aPHP_FCGI_MAX_REQUESTS=100\u000a</code></pre>\u000a\u000a<p>for the low-memory environment. In addition, I installed the \u000a<a href="http://wordpress.org/extend/plugins/wp-super-cache/">WP Super Cache</a> plugin.</p>\u000a\u000a<p>For my <a href="http://www.guitarunleashed.com">django site</a>, I wrote an init script for \u000alaunching the django fcgi server via django's manage.py. Here's the\u000aimportant part:</p>\u000a\u000a<pre><code>MAXCHILDREN=5\u000aMAXSPARE=5\u000aMINSPARE=2 \u000a# ... \u000astart-stop-daemon --quiet --start \u005c\u000a  --pidfile $PIDFILE --chuid "$USER" \u005c\u000a  --exec /usr/bin/env -- python $SITEPATH/$SITENAME/manage.py runfcgi \u005c\u000a  --settings=settings \u005c\u000a  host=$HOST port=$PORT pidfile=$PIDFILE \u005c\u000a  maxchildren=$MAXCHILDREN maxspare=$MAXSPARE minspare=$MINSPARE\u000a</code></pre>\u000a\u000a<p>I'm happy with the performance both for django and php-powered sites.\u000aHowever, I'm just guessing when it comes to values of <code>MAXCHILDREN</code>,\u000a<code>MAXSPARE</code> and <code>MINSPARE</code>, <code>PHP_FCGI_CHILDREN</code> and\u000a<code>PHP_FCGI_MAX_REQUESTS</code>.</p>\u000a\u000a<p>Do you have insight on how to tweak these parameters? Is your VPS\u000ahosting configuration better?</p>\u000a
p995
tp996
Rp997
sg13
V/lightweight-wordpress-on-slicehost
p998
sg15
Nsg16
I01
sg17
VLightweight Wordpress on Slicehost
p999
sg20
V\u000a\u000aI recently switched from shared hosting to a VPS, expecting to get an\u000aimmediate and automatic performance boost.
p1000
sS'snip'
p1001
g7
(g8
g9
V<p>My experiences moving from the WebFaction shared host to the Slicehost VPS.</p>\u000a
p1002
tp1003
Rp1004
sg25
g169
sg33
g998
sg170
(dp1005
g172
S'Feb'
p1006
sg174
S'February 26, 2010'
p1007
sg176
I2
sg177
S'2010-02-26T09:00:00-00:00'
p1008
sg179
I1267203600
sg180
I2010
sg181
I26
ssg65
g182
sg31
S'lightweight-wordpress-on-slicehost'
p1009
sS'categories'
p1010
(lp1011
S'web'
p1012
asS'posted'
p1013
g188
(S'\x07\xda\x02\x1a'
p1014
tp1015
Rp1016
ssg34
S'content/posts/2010/lightweight-wordpress-on-slicehost/index.md'
p1017
sg36
F1433825792.0
sa(dp1018
g2
(dp1019
g4
V My quest to create the most minimalist business cards possible!  
p1020
sg28
g7
(g8
g9
V<p>Business card design is a tricky art. It's a fairly constrained space,\u000abut that's what design is all about, right? I'm ordering a personal set\u000aof <a href="http://us.moo.com/en/products/minicards.php">moo mini cards</a>. These are small, two sided prints. One side\u000acontains an image, and the other contains contact information. On the\u000aimage side, I'm putting snippets of travel photography. The other side\u000ais by default a conventional list of contact information, but moo\u000aconveniently allows it to be replaced by a custom image. </p>\u000a\u000a<p>Rather than having each field separately labeled, I tried to uncover as\u000amuch information as possible within my email address. Hidden inside are\u000amy first name, last name, website, and twitter account! Here's a minimal\u000adesign concept that tries to break it down. </p>\u000a\u000a<p><img src="business-card.png" alt="image" /> </p>\u000a\u000a<p>In my case, my email address contains all relevant info except the phone\u000anumber which I don't want to include anyway. I hope that this simple\u000aidea can inspire you to come up with something more polished. Looking\u000aforward to what you come up with! P.S. Thanks for the help, <a href="http://www.jennlu.com/">Jenn</a>!</p>\u000a
p1021
tp1022
Rp1023
sg13
V/minimal-business-card-design
p1024
sg15
Nsg16
I01
sg17
VMinimal business card design
p1025
sg20
V\u000a\u000aBusiness card design is a tricky art.
p1026
sS'snip'
p1027
g7
(g8
g9
V<p>My quest to create the most minimalist business cards possible!</p>\u000a
p1028
tp1029
Rp1030
sg25
g169
sg33
g1024
sg170
(dp1031
g172
S'Jul'
p1032
sg174
S'July 21, 2010'
p1033
sg176
I7
sg177
S'2010-07-21T09:00:00-00:00'
p1034
sg179
I1279728000
sg180
I2010
sg181
I21
ssg65
g182
sg31
S'minimal-business-card-design'
p1035
sS'categories'
p1036
(lp1037
S'design'
p1038
asS'posted'
p1039
g188
(S'\x07\xda\x07\x15'
p1040
tp1041
Rp1042
ssg34
S'content/posts/2010/minimal-business-card-design/index.md'
p1043
sg36
F1433825797.0
sa(dp1044
g2
(dp1045
g4
V Porting Onslaught! to a mobile web-based game controller. Also featuring frustrations involving multi-touch on Android.  
p1046
sg28
g7
(g8
g9
V<p>HTML5 games are really picking up. Casual Girl Gamer recently produced a\u000anice <a href="http://www.casualgirlgamer.com/articles/entry/28/The-Best-30-HTML-5-games/">list of impressive titles</a>. The modern web platform (namely,\u000afast javascript and canvas) is incredibly promising to a game developer.\u000aThe advantages that it brings are huge: no installation required, and\u000aubiquitous cross platform compatibility. I took a practical look at\u000agames in the HTML5 mobile space, taking <a href="http://lostdecadegamesapp.appspot.com/">Onslaught!</a>, a particularly\u000afun and well written game, and <a href="onslaught/">porting it to Android/iPhone</a>. </p>\u000a\u000a<p>Perhaps porting is too strong a word here. Onslaught! runs and performs\u000areasonably well on my Nexus One running Android 2.2.1. The only problem\u000ais that the game uses keyboard input, making it completely unplayable on\u000amobile devices. Luckily, the controls are quite simple, and only require\u000aa directional pad and two buttons. So I decided to build an on-screen\u000avirtual game controller, not unlike those found in many native iPhone\u000agames.  At first I was inclined to build the controls by extending the\u000agame itself (using the canvas element), but then decided that an\u000aHTML-based approach is better (since it saves the trouble of hit\u000adetection), and might even work as a generic controller for other games.</p>\u000a\u000a<p>Hoping to create a general game controller for Android/iPhone, I sought\u000ato generate keyboard events from JavaScript, based on touch input. While\u000athis is possible if your key event handlers are written in a particular\u000aframework (<a href="http://api.jquery.com/keydown/">say jQuery</a>), it seems to be generally impossible \u000a<a href="http://stackoverflow.com/questions/1601593/fire-tab-keypress-event-in-javascript">for security reasons</a>. I wrote an Onslaught!-specific <a href="onslaught/js/controller.js">controller</a>\u000athat should be easy to port to any other game. The controller simply\u000aembeds the Onslaught! game in it's original 640x480 resolution, which is\u000athen scaled to the device size using the <a href="http://www.quirksmode.org/mobile/viewports.html">meta viewport</a> element. The\u000aresult is a mobile game that's playable on Nexus One, and\u000anot-quite-playable on iPod Touch (2nd gen) due to slower JavaScript\u000aexecution. These are the only two devices I currently have access to, so\u000aI would appreciate it if you could [try it][porting it to\u000aAndroid/iPhone] on your Android or iOS device (in landscape mode) and\u000alet me know how it goes!</p>\u000a\u000a<p><a href="onslaught/"><img src="onslaught.png" alt="image" /></a> </p>\u000a\u000a<p>I ran into a few stumbling blocks as I was developing and testing this\u000aport, most of which involve the Android browser on Nexus One running\u000aAndroid 2.2.1.</p>\u000a\u000a<ol>\u000a<li>Very immature touch event (ontouchstart, ontouchend, etc) support.\u000aIn fact, the <strong>browser doesn't seem to support multi-touch at all</strong>\u000a(ie. only one touch can be registered at a time). In contrast,\u000aSafari for iOS supports multitouch events quite well. For complete\u000adetails, see this <a href="http://quirksmode.org/mobile/tableTouch.html">quirksmode writeup</a> and this <a href="http://code.google.com/p/android/issues/detail?id=11909">bug report</a>.</li>\u000a<li>The Android browser completely ignores many properties in the meta\u000aviewport element's content attribute. Specifically, the <strong>browser\u000adoesn't react to initial-scale, minimum-scale, maximum-scale and\u000awidth</strong>. As a result, I had to hack around this issue by abusing an\u000aAndroid-only property called <a href="http://developer.android.com/reference/android/webkit/WebView.html">target-densityDpi</a>. I suspect that I\u000amay be doing something wrong here, since it's a pretty fundamental\u000aissue. Still I logged a <a href="http://code.google.com/p/android/issues/detail?id=11912">bug</a>.</li>\u000a<li>Less significant but still noteworthy, the CSS <strong>:active selector\u000adoes not activate</strong> on the Android browser (at least for div\u000aelements). The reference implementation is iOS, where a touchstart\u000aevent on a div element causes it to become :active until a touchend\u000aevent.</li>\u000a</ol>\u000a\u000a<p>In the short term, HTML as a gaming platform is emerging as a real Flash\u000akiller. In the long term, I wouldn't be surprised if HTML games will be\u000awidely played on Mac, PC, TV console, and mobile phone platforms.\u000aWhatever happens, browsers will continue to be pushed to conform to\u000amodern specifications and perform ever better, making mobile rich web\u000aapplications more and more feasible. </p>\u000a\u000a<p>In closing, many thanks to <a href="http://blog.lostdecadegames.com/">Lost Decade Games</a> team for writing such a\u000asweet game and not obfuscating the JavaScript! Oh, and a reminder that\u000aif you're working on a HTML game, be sure to submit it to \u000a<a href="http://mozillalabs.com/gaming/2010/09/30/game-on-2010-is-here/">Mozilla's Game On</a> contest, then add some touch controls and submit\u000ait to this <a href="http://www.html5contest.com/">mobile HTML game contest</a>. Let the games begin!</p>\u000a
p1047
tp1048
Rp1049
sg13
V/mobile-html-games
p1050
sg15
Nsg16
I01
sg17
VAn onslaught of mobile HTML games
p1051
sg20
V\u000a\u000aHTML5 games are really picking up.
p1052
sS'snip'
p1053
g7
(g8
g9
V<p>Porting Onslaught! to a mobile web-based game controller. Also featuring frustrations involving multi-touch on Android.</p>\u000a
p1054
tp1055
Rp1056
sg25
g169
sg33
g1050
sg170
(dp1057
g172
S'Oct'
p1058
sg174
S'October 16, 2010'
p1059
sg176
I10
sg177
S'2010-10-16T09:00:00-00:00'
p1060
sg179
I1287244800
sg180
I2010
sg181
I16
ssg65
g182
sg31
S'mobile-html-games'
p1061
sS'categories'
p1062
(lp1063
S'web'
p1064
asS'posted'
p1065
g188
(S'\x07\xda\n\x10'
p1066
tp1067
Rp1068
ssg34
S'content/posts/2010/mobile-html-games/index.md'
p1069
sg36
F1433825805.0
sa(dp1070
g2
(dp1071
g4
V A deep dive into the Nike+ shoe sensor and USB dongle system, and how to get data from it using Python.  
p1072
sg28
g7
(g8
g9
V<p><a href="http://www.apple.com/ipod/nike/">Nike+</a> is a clever little system designed by Apple and Nike to infer\u000athe runner's speed and augment the running experience. The runner places\u000aa small chip in his shoe which transmits data to the iPod using a\u000aproprietary RF-based protocol. The chip contains a piezoelectric cell\u000awhich measures how long the foot exerted pressure on the ground.\u000aAccording to <a href="http://support.apple.com/kb/HT2293?viewlocale=en_US">Apple's FAQ</a>, this contact time is directly related to\u000ayour pace. In this post I provide a snippet of python code for\u000acollecting data through Sparkfun's adapter. </p>\u000a\u000a<p>Sparkfun <a href="http://www.sparkfun.com/commerce/tutorial_info.php?tutorials_id=41">dissected</a> the transmitter and receiver and currently sell a\u000a<a href="http://www.sparkfun.com/commerce/product_info.php?products_id=8245">Nike+ Serial to USB adapter</a>. There's been a number of <a href="http://www.sparkfun.com/commerce/tutorial_info.php?tutorials_id=135">notable</a>\u000a<a href="http://dub.washington.edu/pubs/46%0A">projects</a> since, and quite a lot of interest in the system for\u000ageneral hackery.</p>\u000a\u000a<p>I wanted to\u000ause Nike+ for my own project (a running bib that would automatically\u000adisplay the runner's speed on the back). Unfortunately, the only\u000aavailable implementations were in <a href="http://www.sparkfun.com/datasheets/DevTools/iPod/Nike_iPod_Serial.zip">Visual Basic</a> and <a href="http://rtadlock.blogspot.com/2009/06/some-perl-code-for-nikeipod-serial.html">perl</a>, neither\u000aof which work on Mac. Here's a small script for Python on Mac OS X to\u000acollect Nike+ data using SparkFun's adapter. </p>\u000a\u000a<pre><code>#!/usr/bin/env python\u000aimport serial\u000afrom hexbyte import *\u000a\u000adef readbytes(number):\u000a    buf = ''\u000a    for i in range(number):\u000a        byte = ser.read()\u000a        buf += byte\u000a\u000a    return buf\u000a\u000a# open the appropriate serial port\u000aser = serial.Serial('/dev/tty.usbserial-A6007uDh', 57600, bytesize=serial.EIGHTBITS)\u000a\u000a# send the following init string to the Nike+ device:\u000ainit1 = 'FF 55 04 09 07 00 25 C7'\u000aser.write(HexToByte(init1))\u000a\u000a# listen for the response string: FF 55 04 09 00 00 07 EC\u000aresponse1 = ByteToHex(readbytes(8))\u000aassert response1 == 'FF 55 04 09 00 00 07 EC'\u000a\u000a# send the second init string\u000ainit2 = 'FF 55 02 09 05 F0'\u000aser.write(HexToByte(init2))\u000a\u000a# listen for the response string: FF 55 04 09 06 00 25 C8\u000aresponse2 = ByteToHex(readbytes(8))\u000aassert response2 == 'FF 55 04 09 06 00 25 C8'\u000a\u000a# now we're ready to listen for actual data\u000aprint "nike+ initialized. listening for data"\u000a\u000awhile True:\u000a    byte = ser.read()\u000a    # if a byte is coming down the port,\u000a    if byte:\u000a        # get the rest of the message (34 chars)\u000a        message = byte + readbytes(33)\u000a        # and decipher it\u000a        data = {\u000a            'number': ByteToHex(message[11]),\u000a            'uid': ByteToHex(message[7:11]),\u000a            'data': ByteToHex(message[12:]),\u000a        }\u000a        print data\u000a</code></pre>\u000a\u000a<p>You'll also need <a href="https://github.com/borismus/Running-Gestures/blob/master/hexbyte.py">hexbyte.py</a>, which contains convenience conversion\u000amethods between binary and hex.</p>\u000a\u000a<p>I'm still unable to fully make sense of this data.  Firstly, each step\u000aseems to inexplicably generate 8 packets instead of one. Second, there\u000aare 22 bytes in the Nike+ data <a href="http://ipodlinux.org/wiki/Apple_Accessory_Protocol#Nike.2B_.28Mode_9.29">with an unknown structure</a>, probably\u000acontaining pressure duration data. If someone figures out how to make\u000asense of this please let me know!</p>\u000a\u000a<p><strong>Update (June 2013)</strong>: Dmitry Grinberg has published a <a href="http://dmitry.gr/index.php?r=05.Projects&amp;proj=05.%20Nike%20plus%20iPod">much more\u000athorough reverse engineering</a> blog post.</p>\u000a
p1073
tp1074
Rp1075
sg13
V/nike-hacking-with-python
p1076
sg15
Nsg16
I01
sg17
VNike+ hacking with python
p1077
sg20
V\u000a\u000a[Nike+][] is a clever little system designed by Apple and Nike to infer\u000athe runner's speed and augment the running experience.
p1078
sS'snip'
p1079
g7
(g8
g9
V<p>A deep dive into the Nike+ shoe sensor and USB dongle system, and how to get data from it using Python.</p>\u000a
p1080
tp1081
Rp1082
sg25
g169
sg33
g1076
sg170
(dp1083
g172
S'May'
p1084
sg174
S'May 18, 2010'
p1085
sg176
I5
sg177
S'2010-05-18T09:00:00-00:00'
p1086
sg179
I1274198400
sg180
I2010
sg181
I18
ssg65
g182
sg31
S'nike-hacking-with-python'
p1087
sS'categories'
p1088
(lp1089
S'physical'
p1090
asS'posted'
p1091
g188
(S'\x07\xda\x05\x12'
p1092
tp1093
Rp1094
ssg34
S'content/posts/2010/nike-hacking-with-python/index.md'
p1095
sg36
F1433825810.0
sa(dp1096
g2
(dp1097
g4
V On how I took Onslaught, wrapped it in PhoneGap published, it to the Android Market and got angry emails.  
p1098
sg28
g7
(g8
g9
V<p>A little while ago I took an existing game called <a href="http://lostdecadegamesapp.appspot.com/">Onslaught!</a> by\u000a<a href="http://blog.lostdecadegames.com/">Lost Decade Games</a> and hacked it to use onscreen controls instead of\u000athe keyboard so that it could be enjoyed on mobile phones. This was\u000aintended as a proof of concept and not a polished product as I\u000aencountered many technical show-stoppers, described in the \u000a<a href="/mobile-html-games/">previous blog post</a>. I've been itching to experiment with <a href="http://www.phonegap.com/">PhoneGap</a> apps\u000ain the Android Market, so despite obvious issues, I decided to pack up my\u000aOnslaught! fork and upload it to the market. </p>\u000a\u000a<p>I expected there to be many gameplay issues with varying screen sizes\u000aand form factors. I strongly suspected my use of the target-densityDpi\u000awould break on non-Nexus One handsets. Indeed, my suspicions were\u000aconfirmed when, after a nominal $25 fee and a short setup, I uploaded\u000athe PhoneGap-created .apk to the Market.</p>\u000a\u000a<h2>Results</h2>\u000a\u000a<p>I've never published any application to any application store before. I\u000awas surprised to see Onslaught! available in the market within seconds\u000aof my submission, with a surge of new users and comments piling in\u000anearly immediately. Apparently the experience is particularly horrible\u000aon the Galaxy S and it doesn't work at all on HTC Hero. The day I\u000aunleashed Onslaught! unto the Market, I was contacted by someone from\u000a<a href="http://zeemote.com/">Zeemote</a>, who politely suggested that the game "could possibly\u000abenefit from the use of a joystick for improved control", and promptly\u000a<strong>sent me a Zeemote</strong> to experiment with. I haven't found the time yet,\u000abut thanks Zeemote, that was pretty sweet of you \u2013 nobody's ever sent me\u000afree stuff before! </p>\u000a\u000a<p><img src="stats.png" alt="image" /> </p>\u000a\u000a<p>Obviously this is a pretty embarrassing state of affairs, and I, like\u000aany slightly self respecting developer decided to remove the app from\u000athe market to hide the evidence! I also received a very friendly\u000a<strong>cease-and-desist</strong> style letter from the good folks at LDG (this is\u000aone of those times when the "ask for forgiveness, not permission"\u000aapproach has its drawbacks). Needless to say, I gladly pulled the game\u000aand apologized profusely to the developers.</p>\u000a\u000a<h2>PhoneGap</h2>\u000a\u000a<p>Creating an Android PhoneGap application out of a web application is\u000apretty simple. It's a matter of following a few steps outline on \u000a<a href="http://wiki.phonegap.com/w/page/16494774/Getting-started-with-Android-PhoneGap-in-Eclipse">this documentation page</a>. The process varies slightly for each platform and\u000awould surely become painful and tedious for developers supporting\u000amultiple platforms. Luckily the PhoneGap folks will soon have just the\u000athing to address this issue. <a href="http://build.phonegap.com/start">PhoneGap Build</a> is a service that\u000apromises to package and compile web apps for a variety of platforms.\u000aLooking forward to trying it out!</p>\u000a
p1099
tp1100
Rp1101
sg13
V/phonegap-games-android-market
p1102
sg15
Nsg16
I01
sg17
VPhoneGap games in the Android Market
p1103
sg20
V\u000a\u000aA little while ago I took an existing game called [Onslaught!][] by\u000a[Lost Decade Games][] and hacked it to use onscreen controls instead of\u000athe keyboard so that it could be enjoyed on mobile phones.
p1104
sS'snip'
p1105
g7
(g8
g9
V<p>On how I took Onslaught, wrapped it in PhoneGap published, it to the Android Market and got angry emails.</p>\u000a
p1106
tp1107
Rp1108
sg25
g169
sg33
g1102
sg170
(dp1109
g172
S'Nov'
p1110
sg174
S'November 11, 2010'
p1111
sg176
I11
sg177
S'2010-11-11T09:00:00-00:00'
p1112
sg179
I1289494800
sg180
I2010
sg181
I11
ssg65
g182
sg31
S'phonegap-games-android-market'
p1113
sS'categories'
p1114
(lp1115
S'web'
p1116
asS'posted'
p1117
g188
(S'\x07\xda\x0b\x0b'
p1118
tp1119
Rp1120
ssg34
S'content/posts/2010/phonegap-games-android-market/index.md'
p1121
sg36
F1433825815.0
sa(dp1122
g2
(dp1123
g4
V A how-to about getting access to a wii remote using Python.  
p1124
sg28
g7
(g8
g9
V<p>I've been working on a couple of researchy projects involving gait\u000arecognition and running foot strike analysis. For my proof of concept, I\u000aturned to the wiimote, everyone's favorite physical interaction\u000aprototyping input device. Wiimotes are portable and rugged, and thus\u000awell suited to high-intensity activities like running. They attach\u000aeasily to legs with a physio band, although the elastic tension tends to\u000acut off circulation. No big deal, though... it's For Science! </p>\u000a\u000a<p>This article is not about attaching Wiis to legs (more on that at a\u000alater date!), but about communicating with the Wii remote using python.\u000aI started out by writing a Cocoa application to harvest accelerometer\u000adata using the WiiRemote framework provided by the <a href="http://darwiin-remote.sourceforge.net/">DarwiinRemote</a>\u000aproject.  After some objective-c iterations of my initial gait\u000arecognizer algorithm, I decided to port to python, an environment better\u000asuited for light prototyping. There's a few packages explicitly\u000adeveloped to integrate with the wiimote. <a href="http://stackoverflow.com/questions/481943/python-with-wiimote-using-pywiiuse-module">Pywiiuse</a> provides a\u000alightweight wrapper around the <a href="http://www.wiiuse.net/">wiiuse</a> library and does not work on\u000aOS X. An alternative, <a href="http://code.google.com/p/pywiimote/">pywiimote</a> claims to be multiplatform but\u000apointedly isn't. Here's the start of their code: </p>\u000a\u000a<pre><code>from ctypes import *\u000akernel = windll.kernel32\u000a</code></pre>\u000a\u000a<p>Having found no existing wiimote-specific python libraries that would\u000awork on my platform, I had no choice but to dig a little into the\u000abluetooth-based protocol that the wiimote uses. I found all the details\u000ain all their gory glory on the <a href="http://wiibrew.org/wiki/Wiimote#Accelerometer">wiibrew wiki</a>. The communication\u000aprotocol involves two open L2CAP sockets between the host and wiimote:\u000aone for reading and one for writing. After an initialization string is\u000asent over the write socket, the wiimote springs into life and sends a\u000astream of data on the read socket. In this data are accelerometer values\u000aand button presses. Here's a simple python snippet using the\u000a<a href="http://lightblue.sourceforge.net/">lightblue</a> library:</p>\u000a\u000a<pre><code>import sys, lightblue, hexbyte\u000a\u000aWIIMOTE_DEVICE_NAME = 'Nintendo RVL-CNT-01'\u000a\u000a# auto-discover nearby bluetooth devices\u000adevs = lightblue.finddevices(getnames=True, length=5)\u000a# find the one with the correct name\u000awiimote = [d for d in devs if d[1] == WIIMOTE_DEVICE_NAME] and d[0] or None\u000aif not wiimote:\u000a    print "No wiimotes found!"\u000a    sys.exit(1)\u000a\u000a# create a socket for writing control data\u000awrite_socket = lightblue.socket(lightblue.L2CAP)\u000awrite_socket.connect((wiimote, 0x11))\u000a\u000a# create a socket for reading accelerometer data\u000aread_socket = lightblue.socket(lightblue.L2CAP)\u000aread_socket.connect((wiimote, 0x13))\u000a\u000a# initialize the socket to the right mode\u000awrite_socket.send(hexbyte.HexToByte('52 12 00 33'))\u000a\u000a# start reading data from it\u000awhile 1:\u000a    byte = read_socket.recv(256 * 7)\u000a    data = hexbyte.ByteToHex(byte)\u000a    # do something interesting with the data\u000a    print data\u000a</code></pre>\u000a\u000a<p>You'll need <a href="https://github.com/borismus/Running-Gestures/blob/master/hexbyte.py">hexbyte.py</a> to run the above snippet. I hope\u000ayou (the wii remote wielding python fan) find this snippet useful. As a\u000aside note, if you've figured how to pair a wiimote with an android phone\u000aand released the code into the public domain, please let me know. Since\u000aAndroid 2.2 still doesn't ship with L2CAP APIs, I hit the wall.</p>\u000a
p1125
tp1126
Rp1127
sg13
V/prototyping-wii-remote-python
p1128
sg15
Nsg16
I01
sg17
VPrototyping with Wii remotes in python
p1129
sg20
V\u000a\u000aI've been working on a couple of researchy projects involving gait\u000arecognition and running foot strike analysis.
p1130
sS'snip'
p1131
g7
(g8
g9
V<p>A how-to about getting access to a wii remote using Python.</p>\u000a
p1132
tp1133
Rp1134
sg25
g169
sg33
g1128
sg170
(dp1135
g172
S'May'
p1136
sg174
S'May 28, 2010'
p1137
sg176
I5
sg177
S'2010-05-28T09:00:00-00:00'
p1138
sg179
I1275062400
sg180
I2010
sg181
I28
ssg65
g182
sg31
S'prototyping-wii-remote-python'
p1139
sS'categories'
p1140
(lp1141
S'physical'
p1142
asS'posted'
p1143
g188
(S'\x07\xda\x05\x1c'
p1144
tp1145
Rp1146
ssg34
S'content/posts/2010/prototyping-wii-remote-python/index.md'
p1147
sg36
F1433825821.0
sa(dp1148
g2
(dp1149
g4
V What if you had accelerometers in your shoes? Could mid-stride gestures replace the headphone remote for changing tracks?  
p1150
sg28
g7
(g8
g9
V<p>Like many other runners, I like to listen to my music while training.\u000aEven with a playlist of running music, I often want to change the\u000acurrently playing track. There are currently two popular options of\u000adoing this: using the device itself, or using a headphone remote. My\u000aubiquitous computing project from last Spring explores a third option:\u000aimagine if your shoes had built-in accelerometers that allowed you to\u000askip mid-stride to change tracks. </p>\u000a\u000a<p>The most obvious way to change tracks is directly through the music\u000aplayer, but operating a touch screen while running is pretty annoying. A\u000amuch better way of changing tracks is by using a button attached to a\u000apair of headphones, but finding it, and then double clicking it is often\u000aa frustrating experience. Some clicks don't get registered, so one often\u000aends up triple-clicking, which skips <em>back</em> a track. My project explores\u000aa novel way of changing tracks: mid-stride skip. This gesture is\u000adetected by having an accelerometer in each shoe which tracks your\u000arunning patterns and detects when you perform the skip gesture. In\u000aaddition to being useful, this running gesture makes an otherwise\u000amonotonous activity more varied and enjoyable.</p>\u000a\u000a<p>I prototyped the skip-to-skip system with a wii remote attached to the\u000arunner's lower leg, as pictured above. A computer is paired to the\u000awiimote, collecting accelerometer data (especially in the axis\u000acorresponding to the runner's vertical movement). This communication is\u000aestablished using a wii library from a <a href="/prototyping-wii-remote-python/">previous post</a>. The naive\u000aalgorithm I use for detecting skips works as follows:</p>\u000a\u000a<ol>\u000a<li>Find peaks by looking at the 1st derivative (positive slope,\u000anegative slope pairs)</li>\u000a<li>Discard insignificant peak values (under a threshold)</li>\u000a<li>Compute distances between peaks</li>\u000a<li>Look at the last 5 distances, and compute the mode. That's the pace.</li>\u000a<li>Look for declinations from the pace characteristic of a skip.</li>\u000a</ol>\u000a\u000a<p>Once my gesture recognition code worked reasonably well, I ran a user\u000astudy to see how people liked this method compared to the touch screen\u000aand headphone remote. After a few hitches (including getting kicked out\u000aof the Madeira Tecnopolo), I managed to test the prototype with many of\u000amy Madeiran classmates (thanks guys!). Most people preferred the\u000askip-to-skip method over both the direct smartphone and headphone remote\u000amethods, which is promising.</p>\u000a\u000a<p>For more details on this project, check out the <a href="running-gestures-paper.pdf">paper</a> that\u000a<a href="http://dme.uma.pt/people/faculty/vassilis.kostakos/">Vassilis Kostakos</a> and I will present at <a href="http://www.ubicomp2010.org/">Ubicomp 2010</a> in\u000aCopenhagen. If you're interested in the source code, it's located on\u000a<a href="http://github.com/borismus/Running-Gestures">github</a>.</p>\u000a\u000a<p><strong>Update:</strong> here's the <a href="running-gestures-poster.pdf">poster</a>. The font-size is scary huge\u000aon A0!</p>\u000a
p1151
tp1152
Rp1153
sg13
V/skip-running-gesture
p1154
sg15
Nsg16
I01
sg17
VSkip to skip: a running gesture
p1155
sg20
V\u000a\u000aLike many other runners, I like to listen to my music while training.
p1156
sS'snip'
p1157
g7
(g8
g9
V<p>What if you had accelerometers in your shoes? Could mid-stride gestures replace the headphone remote for changing tracks?</p>\u000a
p1158
tp1159
Rp1160
sg25
g169
sg33
g1154
sg170
(dp1161
g172
S'Aug'
p1162
sg174
S'August 6, 2010'
p1163
sg176
I8
sg177
S'2010-08-06T09:00:00-00:00'
p1164
sg179
I1281110400
sg180
I2010
sg181
I6
ssg65
g182
sg31
S'skip-running-gesture'
p1165
sS'categories'
p1166
(lp1167
S'physical'
p1168
aS'music'
p1169
asS'posted'
p1170
g188
(S'\x07\xda\x08\x06'
p1171
tp1172
Rp1173
ssg34
S'content/posts/2010/skip-running-gesture/index.md'
p1174
sg36
F1433825830.0
sa(dp1175
g2
(dp1176
g4
V An exploration into using Bluetooth Sensor Networks for tracking patterns in behavior.  
p1177
sg28
g7
(g8
g9
V<p>Most people carry mobile phones, and many of those phones have\u000aintegrated bluetooth functionality. Some of these phones are in\u000adiscoverable mode, making them detectable by other bluetooth devices\u000a(Note: many new phones disable discoverable mode on a timer). By\u000abuilding and deploying such scanners in a bluetooth sensor network, we\u000acan collect a lot of interesting information about people's behavior in\u000apublic places. This post is about a minimal compelling application of\u000athis technology.</p>\u000a\u000a<p>Unfortunately we had just 3 makeshift internet-enabled bluetooth sensor\u000anodes at our disposal. Under this constraint, we brainstormed several\u000ainteresting scenarios where a bluetooth sensor network can be used to\u000acollect data, for example:</p>\u000a\u000a<ul>\u000a<li>People's movement patterns in a bar district</li>\u000a<li>Tourists following informational signs to points of interest</li>\u000a<li>Passengers riding a public bus system</li>\u000a<li>Travel between floors in a building</li>\u000a</ul>\u000a\u000a<p>We decided to study patterns of travel (ie. climbing stairs vs. taking\u000athe elevator) between two floors of a university building. We installed\u000athree sensors: two on separate floors, and the third on the stairwell\u000aconnecting them (blue circles represent bluetooth scanner nodes in the\u000abluetooth sensor network).</p>\u000a\u000a<p><img src="problem.png" alt="image" /> </p>\u000a\u000a<p>This is a specific example of a more general point-to-point problem with\u000atwo possible paths. From this simple setup, It's possible to compute\u000awhenever a trip was made from floor A to floor B, and whether or not the\u000atraveler took the stairs or the elevator. Suppose that there was a trip\u000afrom A to B. If the trip was also made through C, it must have been made\u000avia stairs, otherwise, it must have been made via elevator. Based on\u000athis basic data, we can infer a number of features, some of which are:</p>\u000a\u000a<ul>\u000a<li>Popularity of each route (ex. stairs more popular in the morning)</li>\u000a<li>Habit (ex. people that take the elevator always take the elevator)</li>\u000a<li>Temporal patterns (ex. some people consistently take the elevator in\u000athe morning)</li>\u000a<li>Duration of trips (ex. stairs down is faster, elevator up is faster)</li>\u000a<li>Waiting time at each terminal (ex. waiting times higher for\u000aelevator)</li>\u000a<li>Direction preferences (ex. many people prefer to take elevator up,\u000astairs down)</li>\u000a</ul>\u000a\u000a<p>Unfortunately this project finished before any data was actually\u000acollected! Luckily, the implementation is ready for use if you want to\u000atake over! The following describes a simple implementation using EeePCs\u000aor MacMinis:</p>\u000a\u000a<h2>Implementing the Sensor Network</h2>\u000a\u000a<p>The sensor network necessary for the inter-floor experiment consists of\u000aseveral components:</p>\u000a\u000a<p><img src="architecture.png" alt="image" /> </p>\u000a\u000a<p>Each scanner is a computer with a bluetooth modem and a wireless\u000aconnection. We used two Mac Minis running OS X and one ASUS EeePC\u000arunning Linux. Every computer executed a <a href="bluetooth_scanner.py">bluetooth scanner program</a>,\u000awhich scanned for bluetooth devices in the vicinity every 10 seconds,\u000aand updated its list of nearby device IDs. Whenever the list of nearby\u000adevices changed, it reported an ENTER or EXIT notification to the\u000acentral server via an HTTP POST. </p>\u000a\u000a<p>The central server runs a MySQL database to store all notifications from\u000athe scanners. Periodically, this <a href="trip_analyzer.py">analyzer program</a> crawls the\u000adatabase to extract trips from the data (ie. a device EXITs at one\u000asensor and subsequently ENTERs at another one). It also discerns between\u000atrips taken by stairs and by elevator. Once trips are extracted, they\u000aare appended to a Google Spreadsheet. This happens in "delayed\u000arealtime", since a trip can only be inferred once the start and endpoint\u000aare known.</p>\u000a\u000a<p>The final component is a custom visualization written using <a href="http://raphaeljs.com/">Raphael</a>\u000aand the <a href="http://code.google.com/apis/charttools/index.html">Google Visualization</a> framework. This JavaScript program\u000apolls the spreadsheet every 10 seconds to see if any new trips were\u000aadded. If new trips were found, the visualization would update itself,\u000arepresenting each trip as a circle moving from one terminal to another\u000aalong the stairs or elevator path. In addition, this visualization\u000astores a frequency chart over time to show how popular each of the\u000apossible paths are.</p>\u000a\u000a<p>For more information on related projects, check out <a href="http://dme.uma.pt/people/faculty/vassilis.kostakos/">Vassilis' work</a>\u000aat UMa.</p>\u000a
p1178
tp1179
Rp1180
sg13
V/stairs-elevator-bluetooth
p1181
sg15
Nsg16
I01
sg17
VStairs or elevator? Use bluetooth!
p1182
sg20
V\u000a\u000aMost people carry mobile phones, and many of those phones have\u000aintegrated bluetooth functionality.
p1183
sS'snip'
p1184
g7
(g8
g9
V<p>An exploration into using Bluetooth Sensor Networks for tracking patterns in behavior.</p>\u000a
p1185
tp1186
Rp1187
sg25
g169
sg33
g1181
sg170
(dp1188
g172
S'Sep'
p1189
sg174
S'September 30, 2010'
p1190
sg176
I9
sg177
S'2010-09-30T09:00:00-00:00'
p1191
sg179
I1285862400
sg180
I2010
sg181
I30
ssg65
g182
sg31
S'stairs-elevator-bluetooth'
p1192
sS'categories'
p1193
(lp1194
S'physical'
p1195
asS'posted'
p1196
g188
(S'\x07\xda\t\x1e'
p1197
tp1198
Rp1199
ssg34
S'content/posts/2010/stairs-elevator-bluetooth/index.md'
p1200
sg36
F1433825839.0
sa(dp1201
g2
(dp1202
g4
V A wordpress widget that uses topsy to show all of the twitterverse mentions of your blog posts.  
p1203
sg28
g7
(g8
g9
V<p>Every new post on this blog gets an unsolicited <a href="http://en.wikipedia.org/wiki/Pingback">pingback</a> from\u000a<a href="http://topsy.com">topsy</a>, a service that tracks which users mentioned the post on\u000atwitter (known informally as <a href="http://tomsucks.wordpress.com/2008/05/27/free-idea-tweetback/">tweetbacks</a>). On one hand, topsy is a\u000aparasite, using sites like mine to rise in search engine rankings, but\u000aon the other, it satisfies this blogger's curiosity to learn who reads\u000aand enjoys my posts enough to tweet about them. In this experiment, I've\u000aexposed tweetbacks directly on this blog using topsy's convenient \u000a<a href="http://otter.topsy.com">JSONP API</a>.</p>\u000a\u000a<p>The implementation is entirely in JavaScript, consisting of two\u000ascripts. The first is on the main post listing, showing the number of\u000atweets for each post using <code>http://otter.topsy.com/stats.js</code>. The second\u000ais on the post page itself, showing the tweets posted in response to the\u000apost using <code>http://otter.topsy.com/trackbacks.js</code>. Both scripts execute\u000aafter the page loads, so the only impact on page load time is the extra\u000akilobyte of JavaScript code. </p>\u000a\u000a<p>There are good reasons to use topsy over <a href="http://search.twitter.com">twitter search</a>, even though\u000aat first glance, they both provide similar information:</p>\u000a\u000a<ul>\u000a<li>Twitter search doesn't index old tweets</li>\u000a<li>Topsy distinguishes influential tweets (more on this later)</li>\u000a<li>JSONP API support</li>\u000a</ul>\u000a\u000a<p>It's important not to overwhelm readers by showing too many tweets.\u000aPopular blog posts can have hundreds or even thousands of tweets, and\u000ashowing all of them at once is a bad idea. Topsy associates an influence\u000avalue with each twitter user, giving them a certain weight, making it\u000aeasier to decide which tweets to show, and which to hide. My\u000aimplementation shows at most N tweets; if there are over N total tweets,\u000ait shows at most N 'influential' tweets, as per topsy's definition.</p>\u000a\u000a<pre><code>var MAX_TWEETS = 10;\u000avar BASE = 'http://otter.topsy.com/trackbacks.js?callback=?&amp;perpage=' + \u000a  MAX_TWEETS;\u000avar ALL = BASE + '&amp;url=';\u000avar INFL = BASE + '&amp;infonly=1&amp;url=';\u000a\u000afunction getTweets(url) {\u000a  $.getJSON(ALL + url, function(data) {\u000a    var response = data.response;\u000a    if (response.total &gt; MAX_TWEETS) {\u000a      $.getJSON(INFL + url, function(infl) {\u000a        processTweetList(infl.response.list);\u000a        var count = (infl.response.total &gt; MAX_TWEETS ? MAX_TWEETS : infl.response.total);\u000a        updateTweetCount(count, response.total);\u000a      });\u000a    } else if (response.total &gt; 0) {\u000a      processTweetList(response.list);\u000a      updateTweetCount(response.total);\u000a    }\u000a  });\u000a}\u000a</code></pre>\u000a\u000a<p>I also urlify the\u000atweet, converting all of the URLs and @mentions into <a\u005c&gt; elements using\u000athis function, I found the first regular expression somewhere on the\u000ainternet \u2013 I'd never write such a beast. </p>\u000a\u000a<pre><code>var URL_RE = /(\u005cb(https?|ftp|file):\u005c/\u005c/[-A-Z0-9+&amp;@#\u005c/%?=~_|!:,.;]*[-A-Z0-9+&amp;@#\u005c/%=~_|])/ig;\u000avar TWEET_RE = /@([A-Za-z0-9_]+)/g;\u000a\u000afunction urlify(text) {\u000a  return text.replace(URL_RE,"&lt;a href='$1'&gt;$1&lt;/a&gt;").\u000a              replace(TWEET_RE, "&lt;a href='http://twitter.com/$1'&gt;@$1&lt;/a&gt;");\u000a</code></pre>\u000a\u000a<p>The approach I took has some major advantages to over the <a href="http://yoast.com/wordpress/tweetbacks/">tweetback plugin</a> \u000afor wordpress:</p>\u000a\u000a<ul>\u000a<li>It works... the wordpress plugin doesn't</li>\u000a<li>No wordpress comment pollution since no wordpress comments are\u000acreated</li>\u000a<li>No server-side load since all comments are fetched in JS</li>\u000a<li>No wordpress required. Other blog engines or static sites work just\u000aas well</li>\u000a</ul>\u000a\u000a<p>A drawback of this approach is that it takes some time to run the JS,\u000awhich changes the DOM after the initial page load, resulting in a\u000ajarring experience. By using a fade effect to make twitter information\u000aappear gradually, I try to mitigate this problem. </p>\u000a\u000a<p>I conclude with a shameless plug: tweetbacks.js is running on this blog,\u000aso try it out by posting a tweet referring to this post's URL, and it\u000ashould appear in the list below, or if there are more than 10 tweets, at\u000aleast increment the count. If you find this concept interesting and\u000awould like to run it on your site, let me know and I'll pack\u000atweetbacks.js up into a jQuery plugin or something. Thanks for reading! </p>\u000a\u000a<p><strong>Update:</strong> I no longer use this code, in favor of the official twitter\u000abutton.</p>\u000a
p1204
tp1205
Rp1206
sg13
V/tweetbacks-in-javascript
p1207
sg15
Nsg16
I01
sg17
VTweetbacks in JavaScript
p1208
sg20
V\u000a\u000aEvery new post on this blog gets an unsolicited [pingback][] from\u000a[topsy][], a service that tracks which users mentioned the post on\u000atwitter (known informally as [tweetbacks][]).
p1209
sS'snip'
p1210
g7
(g8
g9
V<p>A wordpress widget that uses topsy to show all of the twitterverse mentions of your blog posts.</p>\u000a
p1211
tp1212
Rp1213
sg25
g169
sg33
g1207
sg170
(dp1214
g172
S'Dec'
p1215
sg174
S'December 16, 2010'
p1216
sg176
I12
sg177
S'2010-12-16T09:00:00-00:00'
p1217
sg179
I1292518800
sg180
I2010
sg181
I16
ssg65
g182
sg31
S'tweetbacks-in-javascript'
p1218
sS'categories'
p1219
(lp1220
S'web'
p1221
asS'posted'
p1222
g188
(S'\x07\xda\x0c\x10'
p1223
tp1224
Rp1225
ssg34
S'content/posts/2010/tweetbacks-in-javascript/index.md'
p1226
sg36
F1332684374.0
sa(dp1227
g2
(dp1228
g4
V A quick stab at scraping Mechanical Turk for statistics into a Google Spreadsheet, and then using Google's Visualization APIs to present that data in a meaningful way.  
p1229
sg28
g7
(g8
g9
V<p>I signed up to do one month of paid research at CMU|Portugal before\u000aspring classes start. My task boils down to creating interesting\u000avisualizations. The bad news is that I have no experience visualizing\u000adata and the dataset I'm to visualize hasn't yet been collected.\u000aFortunately, I've always been <em>theoretically</em> interested in data\u000avisualization, so I was happy to have a solid excuse to explore the\u000asubject. All I needed was a sufficiently rich data set, mad skills and a\u000abit of inspiration.</p>\u000a\u000a<p>Lately, I've been pretty excited about squeezing some new potential out\u000aof Mechanical Turk. Part of my research involves finding patterns in\u000aMechanical Turk requester strategies. A few weeks ago, I began gathering\u000adata with a <a href="turk-visualizer/turkviz_scraper.py">python program</a> that extracts all scrape-able information\u000aabout every HIT group and stores it in a sqlite3 database. This is quite\u000aan interesting data set, so I pounced on the opportunity to visualize\u000ait, killing two birds with one stone.</p>\u000a\u000a<p>Due to lack of time, I decided to skip most of the \u000a<a href="http://www.amazon.com/Visual-Display-Quantitative-Information/dp/096139210X">visualization literature</a>. Instead, I found a great \u000a<a href="http://www.visualcomplexity.com/vc/">visualization project database</a> and started writing simple examples\u000ausing some popular\u000avisualization languages and frameworks. I began with <a href="http://processing.org/">Processing</a> and\u000a<a href="http://processingjs.org/">Processing.js</a> but quickly tired of the raster-based drawing model. I\u000aturned to SVG with <a href="http://raphaeljs.com/">Raphal</a> and jQuery and ended up building a simple\u000abubble chart. I had no specific visualization in mind, so my vague goal\u000awas to be able to visualize data in <a href="http://www.gapminder.org/">Hans Rosling</a>'s favorite\u000afive-dimensional (x, y, size, color, time) graph. Making this graph\u000aanimate, however, proved to be quite difficult.</p>\u000a\u000a<p><img src="raphael.jpg" alt="image" /> </p>\u000a\u000a<p>I turned to\u000athe internet for help, and Itai Raz <a href="http://www.youtube.com/watch?v=guhdYoPY3kM">explained to me</a> in his thick\u000aIsraeli accent that the Google Visualization API is pretty sweet.\u000aMoreover, it comes with the exact visualization I wanted to implement,\u000aapparently called the <a href="http://code.google.com/apis/visualization/documentation/gallery/motionchart.html">Motion Chart</a>. Happily, Google Spreadsheets\u000aexport data in the format that the visualization framework consumes.\u000aThus my problem was greatly reduced to one of analyzing and uploading\u000adata to a Google Spreadsheet. Turns out that this too is quite easy\u000ausing the python <a href="http://code.google.com/p/gdata-python-client/">GData framework</a>.</p>\u000a\u000a<p>I decided to visualize differences between requester strategies on\u000aMechanical Turk. Every time I scrape, I generate a Average Reward (x),\u000aAverage Allotted Time (y), Total Number of Hits (size) graph for the top\u000a50 requesters, and then upload it to a google spreadsheet. Here is the\u000a<a href="turk-visualizer/turkviz_scraper.py">python program</a> I wrote for this purpose. <a href="http://www.borismus.com/wp-content/uploads/2010/02/turk_requester_visualizer.html">The results</a> are\u000ainteresting and fun to play with. Google's Motion Chart visualization is\u000aincredibly powerful and flexible. I won't go in detail into findings\u000afrom the data since it's irrelevant to this largely technical discussion\u000aabout visualization technologies. I'll soon get my hands on the data I'm\u000aexpecting and create a custom visualization for it with the Google\u000aVisualization API. Stay chooned!</p>\u000a\u000a<p>[]: turk-visualizer/</p>\u000a
p1230
tp1231
Rp1232
sg13
V/visualizing-mturk-requester
p1233
sg15
Nsg16
I01
sg17
VVisualizing MTurk requesters
p1234
sg20
V\u000a\u000a\u000aI signed up to do one month of paid research at CMU|Portugal before\u000aspring classes start.
p1235
sS'snip'
p1236
g7
(g8
g9
V<p>A quick stab at scraping Mechanical Turk for statistics into a Google Spreadsheet, and then using Google's Visualization APIs to present that data in a meaningful way.</p>\u000a
p1237
tp1238
Rp1239
sg25
g169
sg33
g1233
sg170
(dp1240
g172
S'Feb'
p1241
sg174
S'February 6, 2010'
p1242
sg176
I2
sg177
S'2010-02-06T09:00:00-00:00'
p1243
sg179
I1265475600
sg180
I2010
sg181
I6
ssg65
g182
sg31
S'visualizing-mturk-requester'
p1244
sS'categories'
p1245
(lp1246
S'web'
p1247
aS'social'
p1248
asS'posted'
p1249
g188
(S'\x07\xda\x02\x06'
p1250
tp1251
Rp1252
ssg34
S'content/posts/2010/visualizing-mturk-requester/index.md'
p1253
sg36
F1433825848.0
sa(dp1254
g2
(dp1255
g4
V Wherein I spoke to Brazilian web developers, ate strange fruits and heard some fantastic samba rock.  
p1256
sg28
g7
(g8
g9
V<p>Last month I took a work trip to So Paulo, Brazil. I gave four Chrome/HTML5\u000apresentations and talked to many engineers and designers over the course of the\u000aweek, trying hard not to sound like a broken record. Luckily Brazil has a lot\u000aof people so the audiences were different each time! Here's a link to my\u000a<a href="http://smustalks.appspot.com/brazil-11/">slides</a>, that I tweaked slightly depending on the audience.</p>\u000a\u000a<p>I'll be dumping all of my slide decks and/or talk videos to <a href="http://smustalks.appspot.com/">smustalks</a> on\u000aAppEngine. I've made the switch from building slides in Keynote to using this\u000afantastic HTML5 <a href="http://code.google.com/p/html5slides/">slide deck</a> template and my favorite <a href="http://www.vim.org/">text editor</a>.</p>\u000a\u000a<h2>Food</h2>\u000a\u000a<ul>\u000a<li>Delicious fruits of all varieties: exotic cashew, persimmon, star fruit, guyava, and way tastier bananas.</li>\u000a<li>Ridiculously massive portions of sashimi at japanese restaurants.</li>\u000a<li>The Rodizio we went to was super exquisite. Largest salad bar ever, including caviar (srsly).</li>\u000a</ul>\u000a\u000a<h2>Life</h2>\u000a\u000a<ul>\u000a<li>SP is a concrete jungle like nothing I've ever seen. The whole city is built up within a 60km radius.</li>\u000a<li>Chaotic traffic patterns and insane motorcyclists abound. Everyone loves to drive between lanes and honk gratuitously.</li>\u000a<li>Super bumpy roads date back to the dictatorship days. If you take the wrong exit, prepare to jump around.</li>\u000a<li>Helicopters fly all over town and sometimes land precariously close to unsuspecting window cleaners dangling from highrise roofs.</li>\u000a<li>Beware foul smelling rivers and slums on the way to Friday morning meetings!</li>\u000a</ul>\u000a\u000a<h2>Work</h2>\u000a\u000a<ul>\u000a<li>Super friendly people but my Portuguese skills failed to topple the language barrier.</li>\u000a<li>Long process to get into any office building. Some ask for ID, others ask for passport. Takes half an hour just to get in...</li>\u000a<li>Somehow, Google Brazil still feels like Google!</li>\u000a</ul>\u000a\u000a<p>After my work was done, I took some <a href="https://picasaweb.google.com/boris.smus/Brazil">travel photos</a> and heard\u000a<a href="https://picasaweb.google.com/boris.smus/Brazil#5612942830070183426">Samba da Minha Terra</a> live, and by so doing, joined the ranks of Don and\u000aMagdalena!</p>\u000a\u000a<pre><code>I've never sailed the Amazon,\u000aI've never reached Brazil;\u000aBut the Don and Magdalena,\u000aThey can go there when they will!\u000a\u000a            - R. Kipling\u000a</code></pre>\u000a
p1257
tp1258
Rp1259
sg13
V/brazil-trip
p1260
sg15
Nsg16
I01
sg17
VBrazil trip
p1261
sg20
V\u000a\u000a\u000aLast month I took a work trip to So Paulo, Brazil.
p1262
sS'snip'
p1263
g7
(g8
g9
V<p>Wherein I spoke to Brazilian web developers, ate strange fruits and heard some fantastic samba rock.</p>\u000a
p1264
tp1265
Rp1266
sg25
g169
sg33
g1260
sg170
(dp1267
g172
S'Jun'
p1268
sg174
S'June 21, 2011'
p1269
sg176
I6
sg177
S'2011-06-21T09:00:00-00:00'
p1270
sg179
I1308672000
sg180
I2011
sg181
I21
ssg65
g182
sg31
S'brazil-trip'
p1271
sS'categories'
p1272
(lp1273
S'web'
p1274
aS'chrome'
p1275
aS'travel'
p1276
asS'posted'
p1277
g188
(S'\x07\xdb\x06\x15'
p1278
tp1279
Rp1280
ssg34
S'content/posts/2011/brazil-trip/index.md'
p1281
sg36
F1433825600.0
sa(dp1282
g2
(dp1283
g4
V A Chrome extension that lets you bind keyboard shortcuts to control your favorite music player.  
p1284
sg28
g7
(g8
g9
V<p>Few people want to synchronize their burgeoning music libraries across\u000acomputers (SSDs are small), or even lift it into the clouds with\u000asomething like Amazon's Music Cloud (uploading would take forever). As a\u000aresult many are saying goodbye to iTunes and moving to web-based\u000astreaming music services like <a href="http://listen.grooveshark.com/">grooveshark</a>, <a href="http://www.last.fm">last.fm</a>,\u000a<a href="http://thesixtyone.com">thesixtyone</a>, <a href="http://rdio.com">rd.io</a> etc. This move has a significant UX drawback:\u000amany keyboards come with multimedia keys to control your music player,\u000abut these are useless if you use a web-based music player. This post\u000aaddresses this inconvenience with <a href="https://chrome.google.com/extensions/detail/cpgegiegacijlefhjkfodppcefjhgdeo/">Media Keys</a>, a Chrome extension\u000athat lets you assign keyboard shortcuts in Chrome control web-based\u000amusic players (currently thesixtyone only).</p>\u000a\u000a<h2>How it works</h2>\u000a\u000a<p>The extension relies on two injected <a href="http://code.google.com/chrome/extensions/content_scripts.html">content scripts</a>: <a href="https://github.com/borismus/Chrome-Media-Keys/blob/master/key.js">key.js</a>, a\u000akeyboard event listener injected into all pages and <a href="https://github.com/borismus/Chrome-Media-Keys/blob/master/t61.js">player.js</a>, a\u000amusic player controller, injected into the player page. </p>\u000a\u000a<p>Key.js intercepts keyboard commands (ex. the next song key), and if they\u000amatch bound music player keyboard shortcuts (ex. next song key =>\u000achange to the next song), it sends a message to player.js, which then\u000adoes the right thing (ex. changes to the next song) by simulating mouse\u000aclicks.  Sounds good on paper, but the snag here is that you can't\u000aeasily do direct tab-to-tab communication in Chrome extensions, except\u000apossibly through a long-lived port connection. However using long lived\u000aconnections doesn't make conceptual sense, since the lifespan of a tab\u000ais relatively short. </p>\u000a\u000a<p>So we go the long way with the help of a <a href="http://code.google.com/chrome/extensions/background_pages.html">background page</a>.</p>\u000a\u000a<ol>\u000a<li>On tab load, injected key.js messages the background page to\u000aretrieve media key bindings</li>\u000a<li>On music player load, injected player.js messages the background\u000apage, reporting its tab ID. The background page <a href="http://code.google.com/chrome/extensions/messaging.html#connect">opens a Port</a>\u000athrough which to communicate with the player page</li>\u000a<li>Matching keyboard events on all tabs send messages to the background\u000apage. The background page then relays those messages through the\u000aport to the music player</li>\u000a</ol>\u000a\u000a<p>Here's an abridged code sample from the <a href="https://github.com/borismus/Chrome-Media-Keys/blob/master/dispatch.html">background page</a>: </p>\u000a\u000a<pre><code>chrome.extension.onRequest.addListener(\u000a  function(request, sender, sendResponse) {\u000a    switch(request.type) {\u000a      case 'command':\u000a        try {\u000a          port.postMessage(request.command);\u000a        } catch(error) {\u000a          if (localStorage['autoload']) {\u000a            chrome.tabs.create({url: SITE_URL});\u000a          }\u000a        }\u000a        sendResponse({});\u000a        break;\u000a      case 'register':\u000a        chrome.tabs.getSelected(null, function(tab) {\u000a          port = chrome.tabs.connect(tab.id);\u000a          sendResponse({});\u000a        });\u000a        break;\u000a    }\u000a  }\u000a);\u000a</code></pre>\u000a\u000a<p>To summarize, this approach enables a client-side remote control for a\u000aspecific web application from any other page. This is potent stuff!</p>\u000a\u000a<h2>Try it out</h2>\u000a\u000a<p>Want to try it out? Media Keys works across all Chrome platforms.</p>\u000a\u000a<ol>\u000a<li>Install <a href="http://kevingessner.com/software/functionflip/">Function Flip</a> <em>(Mac only)</em></li>\u000a<li>Check previous, pause/play and next buttons (F6, F7, F8 here) <em>(Mac\u000aonly)</em></li>\u000a<li>Install the <a href="https://chrome.google.com/extensions/detail/cpgegiegacijlefhjkfodppcefjhgdeo">Media Keys</a> extension</li>\u000a<li>Open up the extension options, bind the keys and save</li>\u000a<li>Open up a new page and press the key bound to play</li>\u000a</ol>\u000a\u000a<p>I made a short screencast showing how to install configure and use the\u000aextension.</p>\u000a\u000a<iframe title="YouTube video player" width="600" height="368"\u000a  src="http://www.youtube.com/embed/SrfsnU2gSyI" frameborder="0"\u000a  allowfullscreen></iframe>\u000a\u000a<h2>Wish list</h2>\u000a\u000a<p>Just to recap, the keyboard shortcut binding pattern described above\u000ainjects a script into all tabs, which essentially listens to all key\u000aevents. A malicious developer could write a key logger watches username\u000aand password fields, correlates to the current domain and sends\u000aharvested data to some private server. </p>\u000a\u000a<p>There are also several limitations to this keyboard shortcut binding\u000apattern. It simply won't work in the following cases:</p>\u000a\u000a<ol>\u000a<li>Chrome is in the background</li>\u000a<li>Focus inside chrome is not on the page (ex. location bar)</li>\u000a<li>Chrome is on a special page (ex. new tab page) where content scripts\u000adon't get injected</li>\u000a<li>The current page intercepts keyboard events and stops propagation\u000a(ex. Google Docs)</li>\u000a</ol>\u000a\u000a<p>Keyboard shortcuts are super important to power users, and Chrome (OS)\u000asurely won't leave us in the dust, so I'm looking forward to helping\u000aaddress the security risks and practical limitations this approach as a\u000aChromium project contributor.</p>\u000a\u000a<h2>Share and enjoy</h2>\u000a\u000a<p>One last thing. If you've read my <a href="/chrome-extension-mashups/">previous post</a>, you know that I'm a\u000abig fan of <a href="http://thesixtyone.com">thesixtyone.com</a> so my initial implementation\u000aworks for this service only. Making it work for other music streaming\u000aservices is just a matter of creating a customized player.js file for\u000ayour favorite music app, and tweaking the manifest to inject the new\u000aplayer.js into the correct domain. Feel free to fork the \u000a<a href="https://github.com/borismus/Chrome-Media-Keys">project on github</a>.</p>\u000a
p1285
tp1286
Rp1287
sg13
V/chrome-media-keys
p1288
sg15
Nsg16
I01
sg17
VChrome media keyboard shortcuts
p1289
sg20
V\u000a\u000aFew people want to synchronize their burgeoning music libraries across\u000acomputers (SSDs are small), or even lift it into the clouds with\u000asomething like Amazon's Music Cloud (uploading would take forever).
p1290
sS'snip'
p1291
g7
(g8
g9
V<p>A Chrome extension that lets you bind keyboard shortcuts to control your favorite music player.</p>\u000a
p1292
tp1293
Rp1294
sg25
g169
sg33
g1288
sg170
(dp1295
g172
S'Apr'
p1296
sg174
S'April 1, 2011'
p1297
sg176
I4
sg177
S'2011-04-01T09:00:00-00:00'
p1298
sg179
I1301673600
sg180
I2011
sg181
I1
ssg65
g182
sg31
S'chrome-media-keys'
p1299
sS'categories'
p1300
(lp1301
S'chrome'
p1302
aS'web'
p1303
aS'keyboard'
p1304
aS'shortcuts'
p1305
asS'posted'
p1306
g188
(S'\x07\xdb\x04\x01'
p1307
tp1308
Rp1309
ssg34
S'content/posts/2011/chrome-media-keys/index.md'
p1310
sg36
F1433825614.0
sa(dp1311
g2
(dp1312
g4
V An extension that lets you bind global keyboard shortcuts to control your favorite music player in chrome.  
p1313
sg28
g7
(g8
g9
V<p>I just skipped to the next Google Music track without leaving vim. Wanna play?\u000aHere's the <a href="https://github.com/downloads/borismus/keysocket/KeySocket.zip">app</a> and <a href="https://chrome.google.com/webstore/detail/fphfgdknbpakeedbaenojjdcdoajihik">Chrome extension</a>. To learn how it works, read on!</p>\u000a\u000a<h2>Script injection limitations</h2>\u000a\u000a<p><a href="/chrome-media-keys">Last time around</a> I implemented keyboard bindings by injecting a content\u000ascript into every tab in Chrome, capturing key events and sending them to a\u000abackground page. This approach has some serious performance drawbacks:</p>\u000a\u000a<ul>\u000a<li>Content scripts injected into each page.</li>\u000a<li>Background pages don't perform very well.</li>\u000a</ul>\u000a\u000a<p>And functional limitations:</p>\u000a\u000a<ul>\u000a<li>Won't work on special URLs like <code>chrome://</code> and <code>file://</code>.</li>\u000a<li>Won't work when the omnibox is focused.</li>\u000a<li>Requires chrome to be in the foreground.</li>\u000a</ul>\u000a\u000a<h2>Global key bindings and websockets</h2>\u000a\u000a<p>What we really want is global key bindings. I don't care where my keyboard\u000afocus happens to be right now, I just want to switch to the next freaking song!\u000aThis sort of thing requires OS-level event capture, which is functionality most\u000abrowsers don't come with. To get around this, I run a standalone app to capture\u000aglobal keys and run a websocket server to send these events to the browser.\u000aNote that this approach generalizes well to other use cases where functionality\u000ais not available in a browser, but can be more readily implemented natively.</p>\u000a\u000a<p>The obvious drawback to this approach is that it requires the user to run a\u000aseparate process to capture events.</p>\u000a\u000a<h2>Media key bindings in Cocoa and Python</h2>\u000a\u000a<p>Rogue Amoeba, maker of some popular OS X audio utilities, has a\u000a<a href="http://rogueamoeba.com/utm/2007/09/29/apple-keyboard-media-key-event-handling/">nice post</a> on their blog on capturing media keys from an OS X\u000aapplication. The basic idea is to subclass NSApplication and override the\u000asendEvent: selector:</p>\u000a\u000a<pre><code>- (void)sendEvent: (NSEvent*)event {\u000a  if( [event type] == NSSystemDefined &amp;&amp; [event subtype] == 8 ) {\u000a      // Event processing\u000a  }\u000a  [super sendEvent: event];\u000a}\u000a</code></pre>\u000a\u000a<p>Which in PyObjC results in the following equivalent code:</p>\u000a\u000a<pre><code>def sendEvent_(self, event):\u000a    if event.type() is NSSystemDefined and event.subtype() is 8:\u000a        # Event processing\u000a\u000a    NSApplication.sendEvent_(self, event)\u000a</code></pre>\u000a\u000a<p>It's pretty neat to be able to implement Cocoa apps without having to write a\u000asingle line of objective C. Writing a statusbar app with no dock item was\u000asurprisingly simple (though I have doubts that this works well for Lions and\u000aTigers and Bears). All that's required is to set <code>LSUIElement</code> to <code>true</code> in the\u000aInfo.plist.</p>\u000a\u000a<p>To package the whole PyObjC application, I wrote a setup.py script and used\u000a<a href="http://svn.pythonmac.org/py2app/py2app/trunk/doc/index.html">py2app</a>, which generates a Mac OS X .app bundle which, from a user's\u000aperspective is indistinguishable from an OS X app written in Objective C.</p>\u000a\u000a<h2>A WebSocket server in python</h2>\u000a\u000a<p>In addition to spawning off a Cocoa application and capturing events, of course\u000aI create a WebSocket server. WebSockets use a pretty simple protocol which can\u000aeasily be implemented using python sockets. I based my implementation heavily\u000aon <a href="https://gist.github.com/512987">this one</a>.</p>\u000a\u000a<p>Since a Cocoa application runs its own event loop which captures the\u000amain thread, the websocket listener needs to run in a separate thread:</p>\u000a\u000a<pre><code>class KeySocketServer(Thread):\u000a    def __init__(self):\u000a        self.server = websocket.WebSocketServer('localhost', 1337, KeySocket)\u000a        Thread.__init__(self)\u000a\u000a    def run(self):\u000a        self.server.listen()\u000a</code></pre>\u000a\u000a<p>The WebSocket standards are still evolving and implementers are, as ever,\u000ascrambling to catch up. The good news is that this means\u000a<a href="http://tools.ietf.org/html/draft-ietf-hybi-thewebsocketprotocol-06#section-4.6">binary support</a> is coming, which is a boon for games and other\u000aintensive network consumers. The bad news is that the latest Chrome canary (at\u000athe time of writing), requires the response to contain the\u000a<code>Sec-WebSocket-Accept</code> header, conforming to the\u000a<a href="http://tools.ietf.org/html/draft-ietf-hybi-thewebsocketprotocol-06">draft-ietf-hybi-thewebsocketprotocol-06</a>, which is incompatible with\u000athe python WebSocket code I'm currently using.</p>\u000a\u000a<p>On my wishlist is a robus python WebSockets implementation that supports\u000amultiple versions of the spec while it's still in flux.</p>\u000a\u000a<h2>Injected scripts</h2>\u000a\u000a<p>On the Chrome extension side, a script is injected into the web player\u000aapplication, which creates a WebSocket client that connects to the python\u000aserver on port 1337. When media keys are pressed, the python server sends\u000amessages to the JS clients and the injected JS simulates DOM events in the web\u000aplayer application, controlling music playback.</p>\u000a\u000a<h2>Try it out</h2>\u000a\u000a<p>If you listen to Google Music, thesixtyone or Grooveshark in Chrome on OS X and\u000awant global key bindings, please install the <a href="https://chrome.google.com/webstore/detail/fphfgdknbpakeedbaenojjdcdoajihik">extension</a> and\u000a<a href="https://github.com/downloads/borismus/keysocket/KeySocket.zip">application</a>. If you're feeling generous, contribute your time and love!\u000aI'd gladly take fixes for</p>\u000a\u000a<ul>\u000a<li>Key Socket servers for Linux and Windows</li>\u000a<li>Content scripts to control other web audio players</li>\u000a<li>Web Socket implementations that work with the new spec</li>\u000a</ul>\u000a\u000a<p>And of course, here's the <a href="https://github.com/borismus/keysocket">source</a>.</p>\u000a
p1314
tp1315
Rp1316
sg13
V/chrome-media-keys-revisited
p1317
sg15
Nsg16
I01
sg17
VGlobal chrome media keys with Key Socket
p1318
sg20
V\u000a\u000a\u000aI just skipped to the next Google Music track without leaving vim.
p1319
sS'snip'
p1320
g7
(g8
g9
V<p>An extension that lets you bind global keyboard shortcuts to control your favorite music player in chrome.</p>\u000a
p1321
tp1322
Rp1323
sg25
g169
sg33
g1317
sg170
(dp1324
g172
S'Jul'
p1325
sg174
S'July 28, 2011'
p1326
sg176
I7
sg177
S'2011-07-28T09:00:00-00:00'
p1327
sg179
I1311868800
sg180
I2011
sg181
I28
ssg65
g182
sg31
S'chrome-media-keys-revisited'
p1328
sS'categories'
p1329
(lp1330
S'chrome'
p1331
aS'web'
p1332
aS'keyboard'
p1333
aS'websockets'
p1334
aS'music'
p1335
asS'posted'
p1336
g188
(S'\x07\xdb\x07\x1c'
p1337
tp1338
Rp1339
ssg34
S'content/posts/2011/chrome-media-keys-revisited/index.md'
p1340
sg36
F1433825609.0
sa(dp1341
g2
(dp1342
g4
V Ever wondered how much time you spend in your browser?  
p1343
sg28
g7
(g8
g9
V<p>Like most people, I'm slowly lifting most of my work into the cloud.\u000aThis leads to a lot of time spent in the browser. Just how much, I'm not\u000areally sure. Enter <a href="https://chrome.google.com/extensions/detail/dbgohgmphghmoghphoiaghbopikmmgop/">Chronos</a>, a Chrome extension to track how much\u000atime you spend on each domain you visit. Chronos gives a per-day\u000abreakdown of time spent actively browsing. In addition to showing a\u000agraphical summary of domain frequency, you also get a total time spent\u000ain Chrome, and how much time your Chrome spends idle.</p>\u000a\u000a<p>Chronos, named after the Greek god of time, quietly sits and monitors\u000akeyboard and mouse events you generate. If you've been active recently,\u000athe domain you're visiting gets recorded. The data structure that stores\u000athis timing information persists on the client side using localStorage.\u000aThis data is never sent to any servers, so your browsing privacy is\u000apreserved. Chronos' visualization is built out of HTML divs. Just for\u000afun, if you become inactive in Chrome, the Chronos icon in the\u000aextensions toolbar fades out.</p>\u000a\u000a<p>Some features that I would find useful to\u000aadd to Chronos revolve around productivity and time management:</p>\u000a\u000a<ol>\u000a<li>Chronos makes it really obvious which sites consume most of your\u000atime. It would make sense to be able to enforce time limits spent on\u000asites, either by interfacing with an extension like <a href="https://chrome.google.com/extensions/detail/laankejkbhbdhmipfmgcngdelahlfoji">StayFocusd</a>,\u000aor by replicating that functionality.</li>\u000a<li>Many people find it helpful to be reminded to take breaks from\u000acomputing, either for RSI purposes or for general productivity.\u000aChronos already tracks activity levels in Chrome, so it could be\u000aaugmented to remind people to take breaks in a way similar to\u000a<a href="http://tech.inhelsinki.nl/antirsi/">AntiRSI</a>.</li>\u000a</ol>\u000a\u000a<p>Have a great idea to add to Chronos? Let me know, or add it yourself! As\u000ausual, the <a href="https://github.com/borismus/Chronos">source code</a> is on github.</p>\u000a
p1344
tp1345
Rp1346
sg13
V/chronos
p1347
sg15
Nsg16
I01
sg17
VChronos: Chrome browsing metrics
p1348
sg20
V\u000a\u000aLike most people, I'm slowly lifting most of my work into the cloud.
p1349
sS'snip'
p1350
g7
(g8
g9
V<p>Ever wondered how much time you spend in your browser?</p>\u000a
p1351
tp1352
Rp1353
sg25
g169
sg33
g1347
sg170
(dp1354
g172
S'Mar'
p1355
sg174
S'March 21, 2011'
p1356
sg176
I3
sg177
S'2011-03-21T09:00:00-00:00'
p1357
sg179
I1300723200
sg180
I2011
sg181
I21
ssg65
g182
sg31
S'chronos'
p1358
sS'categories'
p1359
(lp1360
S'web'
p1361
aS'chrome'
p1362
aS'statistics'
p1363
asS'posted'
p1364
g188
(S'\x07\xdb\x03\x15'
p1365
tp1366
Rp1367
ssg34
S'content/posts/2011/chronos/index.md'
p1368
sg36
F1433825621.0
sa(dp1369
g2
(dp1370
g4
V A MapReduce approach to Human Computation. I wrote a Django app for that.  
p1371
sg28
g7
(g8
g9
V<p>Work marketplaces like MTurk are great for accomplishing small, well\u000adefined nuggets of work, such as labeling images and transcribing audio,\u000abut terrible for many more complex and labor intensive real world tasks.\u000aOver the last year, Robert Kraut, Niki Kittur and I formalized a general\u000aprocess of solving such complex problems using MTurk. We proposed three\u000abasic types of tasks and explored them in two neat experimental\u000aapplications. To facilitate these experiments, I implemented CrowdForge,\u000aa Django framework that takes output from MTurk HITs and uses it to\u000acreate new MTurk HITs. </p>\u000a\u000a<p>Solving complex problems on MTurk has always involved partitioning the\u000acomplex task into simpler sub-tasks.  CastingWords, one of the most\u000apopular MTurk requesters, transcribes long audio recordings by splitting\u000athem into overlapping segments, distributing the work among workers,\u000aperforming quality control and then recombining the transcription\u000afragments. CrowdForge formalizes this approach and takes it to the next\u000alevel. CrowdForge proposes the following task breakdown, roughly\u000ainspired by the MapReduce programming paradigm:</p>\u000a\u000a<ul>\u000a<li><strong>partition</strong> tasks split a problem into sub-problems (one to many)</li>\u000a<li><strong>map</strong> tasks solve a small unit of work (one to one)</li>\u000a<li><strong>reduce</strong> tasks combine multiple results into one (many to one)</li>\u000a</ul>\u000a\u000a<p><img src="crowdforge-simple.png" alt="image" /> </p>\u000a\u000a<p>CastingWords uses human intelligence only for their map tasks, which\u000aconsist of transcribing speech samples to text. Their partition task may\u000ainvolve an algorithm which seeks natural breaks in speech audio samples,\u000awhile the reduce task may involve programmatic stitching of audio\u000asamples. For sufficiently complex cases, however, algorithms may be\u000ainadequate, and the partitioning and reduction require human\u000aintelligence. Here are some experiments that illustrate this idea:</p>\u000a\u000a<h2>Writing Articles</h2>\u000a\u000a<p>In the first experiment, turkers generated encyclopedia-style articles\u000aon a given subject. The approach I took was to first generate an article\u000aoutline using an partition task, then for each heading in the outline,\u000ato collect facts on the topic, next to combine these facts into a\u000aparagraph, finally merging all of the paragraphs into a final article.\u000aThe following diagram illustrates this process for an article on New\u000aYork City: </p>\u000a\u000a<p><img src="crowdforge-article.png" alt="image" /> </p>\u000a\u000a<p>In one incarnation of this experiment, we used this approach to create\u000afive articles about New York City. Articles cost an average of $3.26 to\u000aproduce, required an average of 36 subtasks or HITs, included an average\u000aof 5.3 topics per article and consisted of an average of 658 words. As a\u000acomparison baseline, we created eight HITs which each requested one\u000aworker to write the full article, paying $3.26, the same amount required\u000afor the collaboratively written articles. </p>\u000a\u000a<p>We then evaluated the quality of all articles by asking a new set of\u000aworkers to each rate a single article based on use of facts, spelling\u000aand grammar, article structure, and personal preference. On average the\u000aarticles produced by the group were of rated on par with the Simple\u000aWikipedia article on the same topic, and higher than those produced\u000aindividually. See the paper for gritty details on the stats.</p>\u000a\u000a<h2>Product Comparisons</h2>\u000a\u000a<p>We did another experiment to prove the generality of the approach. We\u000aused CrowdForge to create purchase decision matrices to assist consumers\u000alooking to buy a new car. Given a short description of a consumer need,\u000awe created two partition tasks: one to decide which cars might be\u000aappropriate to consider, and one to decide which features the consumer\u000acares most about. This double partition resulted in an empty product\u000acomparison chart. Each cell in the chart then spawned a map task to\u000acollect related facts. Next, these facts are reduced into a sentence,\u000aresulting in a product comparison chart. Here's an excerpt: </p>\u000a\u000a<p><img src="crowdforge-purchase.png" alt="image" /></p>\u000a\u000a<p>The entire task was completed in 54 different HITs for a total cost of\u000a$3.70. When we tried to compare to the individual case, we had no\u000asuccess getting individuals to generate a similar product comparison\u000achart, even when offering more money than we paid the entire group.</p>\u000a\u000a<h2>Sweet Applications</h2>\u000a\u000a<p>MTurk is a one of my favorite tools for doing <a href="http://www.quora.com/What-are-the-most-creative-uses-of-Amazon-s-Mechanical-Turk">creative and novel</a>\u000aprojects. As illustrated, applying human intelligence to reduce and\u000apartition tasks, we can solve a new set of interesting problems. But\u000athere is much more to explore! For example, imagine collaborative\u000adrawing assignments in which a worker sketches out a picture, and\u000asubsequent workers refine the original picture by drawing sub-pictures\u000aor specific objects. Imagine requesting a trip plan and getting a\u000aresearched day-by-day itinerary of what to see and do. Imagine\u000apartitioning a Java class into methods, outsourcing the implementation\u000aand unit test implementations. </p>\u000a\u000a<p>CrowdForge is written as a Django application that communicates with\u000aMTurk servers using the <a href="http://code.google.com/p/boto/">Boto interface</a>, which is a Python framework\u000athat encapsulates the Amazon Web Services API. CrowdForge regularly\u000apolls MTurk and fires notifications whenever interesting things happen\u000a(a new result comes in for a HIT, all HITs of a certain type are\u000afinished, a HIT is expired or complete). CrowdForge can manage many\u000aflows, which encapsulate code that determines how to respond to these\u000aevents. For example, the article writing flow knows to create fact\u000acollection HITs for each outline topic once the outline is submitted. So\u000ato solve a new kind of complex problem, you would extend CrowdForge with\u000aa custom flow, like the following: </p>\u000a\u000a<p><img src="crowdforge-complex.png" alt="image" /></p>\u000a\u000a<p>"But Boris", you say, "how can I get my hands on this CrowdForge\u000aframework of yours?" Indeed, we just <a href="https://github.com/borismus/CrowdForge">released it</a> for non-commercial\u000ause under <a href="http://creativecommons.org/licenses/by-nc/3.0/">the Creative Commons license</a>. Let me know if you have any\u000aquestions, comments or interest in collaborating or maintaining the framework.</p>\u000a\u000a<p><strong>Update (February 2011):</strong> CrowdForge was covered in <a href="http://www.newscientist.com/article/mg20927985.800-silicon-supervisor-gets-the-job-done-online.html">NewScientist</a>,\u000ain a <a href="http://www.cmu.edu/homepage/society/2011/winter/crowdforge.shtml">CMU article</a>, and in a <a href="http://open.blogs.nytimes.com/2011/10/19/uist-2011-crowdsourcing-research/">NY Times blog</a>.</p>\u000a\u000a<p><strong>Update (November 2011)</strong>: The <a href="crowdforge-uist-11.pdf">full paper</a> was <a href="http://smustalks.appspot.com/crowdforge-11/">presented</a> at\u000a<a href="http://www.acm.org/uist/">UIST 2011</a> in Santa Barbara.</p>\u000a
p1372
tp1373
Rp1374
sg13
V/crowdforge
p1375
sg15
Nsg16
I01
sg17
VCrowdForge: crowdsourcing complex tasks
p1376
sg20
V\u000a\u000aWork marketplaces like MTurk are great for accomplishing small, well\u000adefined nuggets of work, such as labeling images and transcribing audio,\u000abut terrible for many more complex and labor intensive real world tasks.
p1377
sS'snip'
p1378
g7
(g8
g9
V<p>A MapReduce approach to Human Computation. I wrote a Django app for that.</p>\u000a
p1379
tp1380
Rp1381
sg25
g169
sg33
g1375
sg170
(dp1382
g172
S'Feb'
p1383
sg174
S'February 2, 2011'
p1384
sg176
I2
sg177
S'2011-02-02T09:00:00-00:00'
p1385
sg179
I1296666000
sg180
I2011
sg181
I2
ssg65
g182
sg31
S'crowdforge'
p1386
sS'categories'
p1387
(lp1388
S'social'
p1389
aS'web'
p1390
aS'crowdsourcing'
p1391
asS'posted'
p1392
g188
(S'\x07\xdb\x02\x02'
p1393
tp1394
Rp1395
ssg34
S'content/posts/2011/crowdforge/index.md'
p1396
sg36
F1433825628.0
sa(dp1397
g2
(dp1398
g4
V An experiment in which Mechanical Turkers generated sound effects on demand.  
p1399
sg28
g7
(g8
g9
V<p>After having dabbled in crowdsourcing research, I wanted to find a\u000acompelling MTurk application around which to build a real service that\u000awould simultaneously be useful, involve creative tasks, and be fun\u000aenough to capture my attention. Generating sound effects seemed to fit\u000athe bill. Creating sounds through imitation or collecting is creative,\u000afun and (ahem, quality aside) easy due to the ubiquity of microphones.\u000aEnter experimental service <a href="http://soundsourcing.com">soundsourcing.com</a>. </p>\u000a\u000a<p>First I had to verify some basic questions: Would turkers be willing to\u000acreate and submit sound effects? Would their output be sufficiently good\u000afor someone to use in practice? Would it be possible to have some\u000aquality control process to weed out the blatantly bad submissions?\u000aJumping into experimentation, I asked workers to imitate or record a cow\u000amooing, paying ten cents for their efforts. After 3 days I had a set of\u000amoos:</p>\u000a\u000a<p><object height="285" width="100%"><param name="movie"\u000a  value="http://player.soundcloud.com/player.swf?url=http%3A%2F%2Fapi.soundcloud.com%2Fplaylists%2F527826&amp;show_comments=false&amp;auto_play=false&amp;show_playcount=true&amp;show_artwork=true&amp;color=ff7700"></param><param\u000a  name="allowscriptaccess" value="always"></param> <embed\u000a  allowscriptaccess="always" height="285"\u000a  src="http://player.soundcloud.com/player.swf?url=http%3A%2F%2Fapi.soundcloud.com%2Fplaylists%2F527826&amp;show_comments=false&amp;auto_play=false&amp;show_playcount=true&amp;show_artwork=true&amp;color=ff7700"\u000a  type="application/x-shockwave-flash" width="100%"></embed></object></p>\u000a\u000a<p>Taking it up a notch, I requested the more obscure sound of a \u000a<a href="http://soundcloud.com/borismus/sets/turkish-trains-leaving-the-station/">train leaving the station</a>. There were many excellent submissions in\u000athe mix, although I suspect that many of them were not created by the\u000aworkers but found online despite my empty threat to verify their\u000aoriginality and reject. My favorite is probably this train, improvised\u000aon harmonica: </p>\u000a\u000a<p><object height="81" width="100%"><param name="movie" value="http://player.soundcloud.com/player.swf?url=http%3A%2F%2Fapi.soundcloud.com%2Ftracks%2F9563151"></param><param name="allowscriptaccess" value="always"></param> <embed allowscriptaccess="always" height="81" src="http://player.soundcloud.com/player.swf?url=http%3A%2F%2Fapi.soundcloud.com%2Ftracks%2F9563151" type="application/x-shockwave-flash" width="100%"></embed></object></p>\u000a\u000a<p>As usual, the hard problem with crowdsourcing is quality control. I\u000aapproach this problem with a rating step after the sound samples have\u000abeen submitted, in which turkers select their top three samples from the\u000asound set (shuffled in random order to avoid <a href="http://www.jstor.org/pss/3151704">order bias</a>). Each\u000ainstance of such a task can be done cheaply (I tried $0.05), so ordering\u000athe whole set in quality becomes easy. In the future I'd like to write\u000asome JS to verify that the turker in fact listened to all of the\u000asamples. Below is the rating form presented to MTurk users:</p>\u000a\u000a<p><img src="turk-rate.png" alt="image" /> </p>\u000a\u000a<p>When all of the pieces came together and some time remained, I built\u000a<a href="http://soundsourcing.com">soundsourcing.com</a> around this concept. I implemented the service\u000ausing <a href="http://www.djangoproject.com/">Django</a> and <a href="http://code.google.com/p/boto/">Boto</a>, reusing many concepts from the\u000a<a href="/crowdforge">CrowdForge</a> framework. The premise of the service is that people\u000aseeking very specific sound effects that they cannot find on existing\u000asound effects sites can use soundsourcing to get their custom sounds\u000abuilt to order. The basic flow of HITs on MTurk looks like this:</p>\u000a\u000a<p><img src="soundsourcing.jpg" alt="image" /> </p>\u000a\u000a<p>Since the cow and train experiments, I have collected <a href="http://soundsourcing.com/set/a-lion-roaring/">lion roars</a> and\u000asword slashes (submissions are in progress at the time of writing).\u000aPlease check out <a href="http://soundsourcing.com">the site</a>, request a sound sample\u000aand submit some feedback below. Mechanical Turkers, let's make some\u000anoise!!</p>\u000a
p1400
tp1401
Rp1402
sg13
V/crowdsourcing-sound
p1403
sg15
Nsg16
I01
sg17
VSoundsourcing: the sound of the crowd
p1404
sg20
V\u000a\u000aAfter having dabbled in crowdsourcing research, I wanted to find a\u000acompelling MTurk application around which to build a real service that\u000awould simultaneously be useful, involve creative tasks, and be fun\u000aenough to capture my attention.
p1405
sS'snip'
p1406
g7
(g8
g9
V<p>An experiment in which Mechanical Turkers generated sound effects on demand.</p>\u000a
p1407
tp1408
Rp1409
sg25
g169
sg33
g1403
sg170
(dp1410
g172
S'Feb'
p1411
sg174
S'February 20, 2011'
p1412
sg176
I2
sg177
S'2011-02-20T09:00:00-00:00'
p1413
sg179
I1298221200
sg180
I2011
sg181
I20
ssg65
g182
sg31
S'crowdsourcing-sound'
p1414
sS'categories'
p1415
(lp1416
S'social'
p1417
aS'music'
p1418
aS'web'
p1419
asS'posted'
p1420
g188
(S'\x07\xdb\x02\x14'
p1421
tp1422
Rp1423
ssg34
S'content/posts/2011/crowdsourcing-sound/index.md'
p1424
sg36
F1433825634.0
sa(dp1425
g2
(dp1426
g4
V I/O 2011 featured excellent Chrome Developer Tools coverage. New to Chrome DevTools? Check out this cheat sheet!  
p1427
sg28
g7
(g8
g9
V<p>Chrome's Developer Tools have been getting much deserved love at this last\u000aGoogle I/O. Paul Irish and I started with an <a href="http://www.io-bootcamp.com/">bootcamp lab</a>, a hands-on \u000awalk through tweaking a web application using the developer tools. We handed\u000aout a cheatsheet to give developers an overview of available features.</p>\u000a\u000a<p><a href="https://github.com/borismus/DevTools-Lab/raw/master/cheatsheet/chromedev-cheatsheet.pdf"><img src="https://github.com/borismus/DevTools-Lab/raw/master/cheatsheet/chromedev-cheatsheet.jpg" alt="cheatsheet jpg" /></a></p>\u000a\u000a<p>The cheatsheet is available for download in <a href="https://github.com/borismus/DevTools-Lab/raw/master/cheatsheet/chromedev-cheatsheet.pdf">PDF</a> and\u000a<a href="https://github.com/borismus/DevTools-Lab/raw/master/cheatsheet/chromedev-cheatsheet.png">PNG</a>. </p>\u000a\u000a<p>Thanks to the awesome I/O organizers, we also gave away free HTML5 cake (as\u000apromised) after the session ended:</p>\u000a\u000a<p><img src="html5-cake.jpg" alt="html5 cake" /></p>\u000a\u000a<p>The developer tools were also featured at the <a href="http://www.youtube.com/watch?v=MiYND_zvIc0&amp;t=8m30s">start of the Chrome keynote</a>\u000awith a webkit-speech demo. Finally, Pavel Feldman (lead engineer for the dev\u000atools) and Paul gave an great <a href="http://www.youtube.com/watch?v=N8SS-rUEZPg">I/O talk</a>.</p>\u000a\u000a<p>One of the most promising aspects of the Chrome Developer Tools is that they\u000aare easy to extend with (experimental) <a href="http://code.google.com/chrome/extensions/trunk/experimental.devtools.html">chrome extension APIs</a>. I'm stoked to\u000asee what people create to make these developer tools even better!</p>\u000a
p1428
tp1429
Rp1430
sg13
V/devtools-cheatsheet
p1431
sg15
Nsg16
I01
sg17
VChrome developer tools cheatsheet
p1432
sg20
V\u000a\u000a\u000aChrome's Developer Tools have been getting much deserved love at this last\u000aGoogle I/O.
p1433
sS'snip'
p1434
g7
(g8
g9
V<p>I/O 2011 featured excellent Chrome Developer Tools coverage. New to Chrome DevTools? Check out this cheat sheet!</p>\u000a
p1435
tp1436
Rp1437
sg25
g169
sg33
g1431
sg170
(dp1438
g172
S'May'
p1439
sg174
S'May 13, 2011'
p1440
sg176
I5
sg177
S'2011-05-13T09:00:00-00:00'
p1441
sg179
I1305302400
sg180
I2011
sg181
I13
ssg65
g182
sg31
S'devtools-cheatsheet'
p1442
sS'categories'
p1443
(lp1444
S'web'
p1445
asS'posted'
p1446
g188
(S'\x07\xdb\x05\r'
p1447
tp1448
Rp1449
ssg34
S'content/posts/2011/devtools-cheatsheet/index.md'
p1450
sg36
F1433825641.0
sa(dp1451
g2
(dp1452
g4
V Adding a bit of pizzazz to your extension UIs with dynamic, maybe even animated browser and page actions.  
p1453
sg28
g7
(g8
g9
V<p>Extension developers aren't given much freedom to modify Chrome's browser\u000achrome. Without resorting to changing the page itself, or using the new devtools\u000aextension APIs, there are two main ways of doing this. <a href="http://code.google.com/chrome/extensions/pageAction.html">Page actions</a>, which\u000areside in the omnibox, and <a href="http://code.google.com/chrome/extensions/browserAction.html">browser actions</a>, which are positioned to the\u000aright of the omnibox both of which are simple buttons with icons, click actions\u000aand hover states. Chrome conveniently <a href="http://code.google.com/chrome/extensions/browserAction.html#method-setIcon">provides an API</a> to dynamically change\u000athe icon of these buttons.</p>\u000a\u000a<p>You can do this by creating image data by hand or using the canvas API's\u000a<code>getImageData</code> function.:</p>\u000a\u000a<pre><code>var canvas = document.getElementById('canvas');\u000avar context = canvas.getContext('2d');\u000a// ...draw to the canvas...\u000avar imageData = context.getImageData(0, 0, 19, 19);\u000achrome.browserAction.setIcon({\u000a  imageData: imageData\u000a});\u000a</code></pre>\u000a\u000a<p>Note that this would be possible even without this chrome-specific API, by\u000ainstead using <a href="http://en.wikipedia.org/wiki/Data_URI_scheme">data URIs</a> to set the image. There's a GMail <a href="http://googlesystem.blogspot.com/2011/01/dynamic-gmail-favicon.html">labs plugin</a>\u000athat does this to badge the favicon with the unread email count.</p>\u000a\u000a<p>What makes this an interesting design domain is the limitation of the medium.\u000aLimited real estate (browser actions at 19x19 px, page actions at 16x16 px) adds\u000asignificant constraints. Still, one can show numbers, colors, small icons and\u000agraphs in this context, or even small amounts of text.</p>\u000a\u000a<h2>Applications of Dynamic Icons</h2>\u000a\u000a<p>There are extensions that use dynamic icons already, such as the\u000a<a href="https://chrome.google.com/webstore/detail/kpekpmmfocifmbnnoahnclccmjkckpcl">PageRank extension</a>, which effectively shows the Google PageRank for the\u000acurrent page right inside the browser action.</p>\u000a\u000a<p>Here are some other possibilities (not at all an exhaustive list):</p>\u000a\u000a<ul>\u000a<li>Create badges for page actions (which don't implement <code>setBadge*</code> calls).</li>\u000a<li>Icon of the weather forecast, click to toggle between days.</li>\u000a<li>Bandwidth meter: how large was the download of this page?</li>\u000a<li>A random profile pic of people that +1'ed the site.</li>\u000a</ul>\u000a\u000a<h2>The Smallest Music Visualizer</h2>\u000a\u000a<p>I wrote a sample extension to demonstrate the dynamic icon potential of Chrome\u000aextensions. This indispensible extension is a music visualizer that renders\u000ainside of a browser action button. I use the <a href="http://chromium.googlecode.com/svn/trunk/samples/audio/specification/specification.html">Web Audio API</a> to playback a\u000asong and analyse the audio stream, render the visualized audio spectrum with a\u000acanvas element and then transfer the resulting image data to the browser action\u000aicon.</p>\u000a\u000a<p><img src="music-vis.png" alt="screenshot" /></p>\u000a\u000a<p>Try out the <a href="https://chrome.google.com/webstore/detail/befnabfghcghgpmkjoalbecphdgdmick?hl=en">chrome extension</a> here, but note that it requires the Web Audio\u000aAPI flag to be enabled under about:flags (and a browser restart afterward).\u000aCheck out and fork the <a href="https://github.com/borismus/Music-Visualizer-Chrome-Extension">source on github</a>.</p>\u000a\u000a<h2>Learnings</h2>\u000a\u000a<p>This music visualizer extension loads an mp3 file when the extension\u000abackground page loads, which takes a certain amount of time. To provide a\u000abetter user experience, I was hoping to change the icon to reflect that the\u000afile was being loaded, and ran into two issues.</p>\u000a\u000a<p>The first issue was that when I tried to render a small string like "wait" in\u000athe icon, I wanted to use a custom <code>@font-face</code> embedded font, which is now\u000awell supported in CSS3. You can load CSS fonts</p>\u000a\u000a<pre><code>@font-face {\u000a  font-family: "Silkscreen"\u000a  src: url(slkscr.ttf);\u000a}\u000a</code></pre>\u000a\u000a<p>and then use them in a canvas:</p>\u000a\u000a<pre><code>context.font = "8px Silkscreen";\u000acontext.fillText('load');\u000a</code></pre>\u000a\u000a<p>When using custom fonts from HTML, the browser waits for the font to load, and\u000athen does a relayout. When using it from canvas, things get a bit tricky since\u000athe browser of course doesn't do font relayout for you and there's\u000aunfortunately no DOM event that fires when all embedded fonts finished loading.\u000aMore precisely, onload behaviors differ from browser to browser. Mozilla waits\u000afor all fonts to load before firing the event, WebKit doesn't. You can work\u000aaround this problem by assigning the custom font to a div, and observing the\u000adiv's width, which will change when the font loads (<a href="https://github.com/paulirish/font-face-detect/blob/master/isFontLoaded.js">codified</a>).</p>\u000a\u000a<p>Although I ultimately didn't use this font to show loading state, I recommend\u000achecking out the <a href="http://kottke.org/plus/type/silkscreen/">silkscreen font</a> for 8-bit style designs that lend\u000athemselves well to small resolution envirnoments. You can fit about 3x3\u000acharacters inside a 16x16 canvas:</p>\u000a\u000a<p><img src="silkscreen.png" alt="silkscreen" /></p>\u000a\u000a<p>To show that the extension is still loading, I went with a progress bar instead\u000aof a message. A second issue arose when I wanted to show the progress\u000abar animating while the mp3 loads. Unfortunately the Web Audio API doesn't\u000acurrently support asynchronous loading of files, so the UI thread gets blocked\u000aduring the <code>audioContext.createBuffer</code> call of this code snippet:</p>\u000a\u000a<pre><code>var request = new XMLHttpRequest();\u000arequest.onload = function() {\u000a  var audioBuffer = audioContext.createBuffer(request.response, false);\u000a}\u000a</code></pre>\u000a\u000a<p>Async loading of audio buffers is now a <a href="https://bugs.webkit.org/show_bug.cgi?id=61947">tracked issue</a> for you upvote in\u000athe webkit bug tracker. I thought of working around this with Web Workers, but\u000agave up early because of difficulties with passing objects between worker\u000athreads, and no shared memory options that would let workers access the\u000acontext of the main UI thread.</p>\u000a\u000a<p>Another interesting observation is that <code>requestAnimationFrame</code> does not work in\u000aa background page. I initially tried to use it to animate the music visualizer,\u000abut it didn't work. This is of course the API is designed to only callback when\u000athe calling page is in the foreground, and since the background page is never\u000aforegrounded, the callback never fires.</p>\u000a\u000a<p>That's it for me, now it's your turn! So, dearest reader, go forth and write\u000asome awesome Chrome extensions which tastefully use dynamic icons for page\u000aactions, browser actions, and favicons to make our browsing experience even\u000abetter.</p>\u000a
p1454
tp1455
Rp1456
sg13
V/dynamic-icons-chrome-extensions
p1457
sg15
Nsg16
I01
sg17
VDynamic chrome extension icons
p1458
sg20
V\u000a\u000a\u000aExtension developers aren't given much freedom to modify Chrome's browser\u000achrome.
p1459
sS'snip'
p1460
g7
(g8
g9
V<p>Adding a bit of pizzazz to your extension UIs with dynamic, maybe even animated browser and page actions.</p>\u000a
p1461
tp1462
Rp1463
sg25
g169
sg33
g1457
sg170
(dp1464
g172
S'Jun'
p1465
sg174
S'June 6, 2011'
p1466
sg176
I6
sg177
S'2011-06-06T09:00:00-00:00'
p1467
sg179
I1307376000
sg180
I2011
sg181
I6
ssg65
g182
sg31
S'dynamic-icons-chrome-extensions'
p1468
sS'categories'
p1469
(lp1470
S'web'
p1471
aS'chrome'
p1472
asS'posted'
p1473
g188
(S'\x07\xdb\x06\x06'
p1474
tp1475
Rp1476
ssg34
S'content/posts/2011/dynamic-icons-chrome-extensions/index.md'
p1477
sg36
F1433825647.0
sa(dp1478
g2
(dp1479
g4
V Make the devtools/webkit inspector even better by writing devtools extensions and submitting Javascript patches.  
p1480
sg28
g7
(g8
g9
V<p>Chrome\u2019s developer tools (also known as the WebKit inspector) are super useful\u000afor web developers. If you aren't ramped up already, take a look at this\u000a<a href="/devtools-cheatsheet">cheat sheet overview</a>. Also check out some of many online resources,\u000asuch as the <a href="http://code.google.com/chrome/devtools/docs/overview.html">official documentation</a>, which is quite readable, and this\u000a<a href="http://www.youtube.com/watch?v=nOEw9iiopwI">12 tricks screencast</a> from Paul Irish.</p>\u000a\u000a<p>But let\u2019s not get sidetracked here. I wasn't going to write about using the\u000adeveloper tools, it\u2019s about <strong>developing</strong> developer tools. There are two ways\u000ato go:</p>\u000a\u000a<ol>\u000a<li><p>You can extend the tools by writing chrome extensions that use the new\u000a<code>chrome.experimental.devtools</code> APIs. These recently created APIs are still\u000asubject to change, and marked experimental, so it\u2019s a bit more difficult to\u000adistribute and install them compared to other chrome extension APIs (more on\u000athis later).</p></li>\u000a<li><p>You can quite easily hack the devtools code yourself, to customize it to\u000ayour needs. Then, if you do a really good job, you can contribute your changes\u000aback to the chromium project and feel better for being such an awesome person.</p></li>\u000a</ol>\u000a\u000a<p>Let\u2019s dive in, starting from the beginning.</p>\u000a\u000a<h2>Developer tools extensions</h2>\u000a\u000a<p>Chrome provides <a href="http://code.google.com/chrome/extensions/trunk/experimental.devtools.html">three separate APIs</a> for extending the developer tools:</p>\u000a\u000a<ol>\u000a<li><p>Audits - add a new audit to the Audits tab in the developer tools:\u000a<img src="audit.png" alt="auditshot" /></p></li>\u000a<li><p>Panels - add a whole new panel to the developer tools: <img src="panel.png" alt="panelshot" /></p></li>\u000a<li><p>Resources - access a <a href="https://toolbox.googleapps.com/apps/har_analyzer/">HAR file</a> containing known resources</p></li>\u000a</ol>\u000a\u000a<p>There are several <a href="http://code.google.com/chrome/extensions/trunk/samples.html#devtools">extension samples</a> available as starting points,\u000awhich cover all of the devtools extension API. The API is geared towards\u000aAudit-oriented extensions (HTML/CSS/Javascript validators, performance\u000aanalyzers) and Panel-oriented extensions (framework specific tooling, etc).\u000aYou can also write extensions that integrate more loosely with the developer\u000atools (perhaps only using the resources API).</p>\u000a\u000a<h3>My JSHint audit extension</h3>\u000a\u000a<p>I wanted to get my hands dirty, and wrote a <a href="http://jshint.com">JSHint</a>-based Javascript\u000avalidator extension. This extension uses the devtools audit API to create an\u000aaudit that checks all scripts (linked and inline) on the current page. Errors\u000aare shown per-script in the audit results view. I wanted to go further, and\u000aallow the user to click on the error and jump to the resource, but this wasn\u2019t\u000apossible with the developer tools extension API. To get around this limitation,\u000aI patched the developer tools with a pretty quick fix. More on modifying the\u000adevtools themselves later.</p>\u000a\u000a<p>Check out the JSHint audit extension <a href="https://github.com/borismus/jshint-extension">on github</a>.</p>\u000a\u000a<h3>Writing an extension</h3>\u000a\u000a<p>The first thing you should know about developer tools extensions, is that (at\u000athe time of writing), they are experimental. This means that you will need to\u000alaunch chrome with a special flag to use them. I use a shell script called\u000a<code>chrome-devtools</code> in my <code>~/bin</code> (which is in my <code>$PATH</code> of course):</p>\u000a\u000a<pre><code>/Applications/Google\u005c Chrome\u005c Canary.app/Contents/MacOS/Google\u005c Chrome\u005c Canary \u005c\u000a  --user-data-dir=/Users/smus/.chrome-devtools \u005c\u000a  --enable-experimental-extension-apis \u005c\u000a  --debug-devtools-frontend=/Users/smus/devtools_frontend\u000a</code></pre>\u000a\u000a<p>Let me explain these switches:</p>\u000a\u000a<ul>\u000a<li><p><code>--user-data-dir</code>: specifies a custom profile for your chrome (so you have a\u000afresh profile to deal with and don\u2019t accidentally clobber something important\u000ain your main chrome profile).</p></li>\u000a<li><p><code>--enable-experimental-extension-apis</code>: turns on experimental extension APIs.</p></li>\u000a<li><p>More on the last switch later.</p></li>\u000a</ul>\u000a\u000a<p>By the way, all of chrome\u2019s switches are explained on <a href="http://peter.sh/experiments/chromium-command-line-switches/">this page</a>,\u000aprovided by Peter Beverloo.</p>\u000a\u000a<p>Developer tools extensions are based on a devtools page, which gets loaded when\u000athe devtools open. You can specify this page in the manifest like other pages\u000a(such as background and options pages):</p>\u000a\u000a<pre><code>{\u000a  // manifest start\u000a  "devtools_page": "devtools.html",\u000a  // manifest end\u000a}\u000a</code></pre>\u000a\u000a<p>The API favors extensions that are built around an Audit or a Panel, and you\u000aare free to use the resources API, as well as the rest of the chrome extension\u000afeatures.</p>\u000a\u000a<p>One important note is that the devtools page has very limited access to chrome\u000aextension APIs, so you need to use <a href="http://code.google.com/chrome/extensions/messaging.html">messaging</a> and a background page to\u000aaccess the full chrome API.</p>\u000a\u000a<p>devtools.html:</p>\u000a\u000a<pre><code>// Send request to the background page\u000achrome.extension.sendRequest({}, function(response) {\u000a  // Handle response\u000a});\u000a</code></pre>\u000a\u000a<p>background.html:</p>\u000a\u000a<pre><code>chrome.extension.onRequest.addListener(function(request, sender, callback) {\u000a  // Call some chrome extension APIs. For example,\u000a  chrome.tabs.getSelected(null, function(tab) {\u000a    // Etc... and then callback to devtools.html\u000a    callback({data: tab.id});\u000a  });\u000a});\u000a</code></pre>\u000a\u000a<h3>Debugging devtools extensions</h3>\u000a\u000a<p>When writing Chrome extensions, you have the power of the Chrome developer\u000atools at your disposal. You can debug content scripts by inspecting the page\u000ainto which the Javascript has been injected. Background pages can be viewed by\u000arunning <code>chrome://extensions</code> in developer mode, and clicking on\u000abackground.html for your extension.</p>\u000a\u000a<p>However, when you write a developer tools extension, you rely on this\u000adevtools_page as well as the rest of the extension ecosystem. Debugging this\u000apage can get a bit meta \u2013 just inspect the devtools with the devtools!</p>\u000a\u000a<p><img src="meta.png" alt="meta" /></p>\u000a\u000a<p>Now you can inspect your devtools.html page, and debug away!</p>\u000a\u000a<h2>Changing the product itself</h2>\u000a\u000a<p>In addition to being highly extensible via Chrome extensions, the devtools are\u000aalso pretty easy to modify and tinker with. As you noticed earlier, the tools\u000aare a web application written in Javascript, CSS and HTML, and thus inspectable\u000aby the devtools themselves. This web application is hidden inside Chrome, but\u000aluckily Chrome makes it easy to run a custom version of it. Basically, it takes\u000athree steps:</p>\u000a\u000a<ol>\u000a<li>Download a <a href="http://commondatastorage.googleapis.com/chromium-browser-continuous/index.html">devtools frontend zip</a> (pre-packed version of\u000adevtools). You will need to drill down into a directory for\u000a<a href="http://commondatastorage.googleapis.com/chromium-browser-continuous/index.html?path=Mac/91508/">your platform</a>.</li>\u000a<li>Extract the zip to some directory, <code>DIR</code>.</li>\u000a<li>Invoke Chrome with the <code>--debug-devtools-frontend=DIR</code> switch, specifying\u000athe same directory as in the previous step.</li>\u000a</ol>\u000a\u000a<p>Since the <code>devtools_frontend.zip</code> has some dependencies on Chrome, your zip file\u000aand chrome version should be pretty close, or you may run into problems. You can\u000arun the exact same version by also downloading the <code>chrome-platform.zip</code>.</p>\u000a\u000a<p>Note: the Chromium project has a more detailed <a href="http://code.google.com/chrome/devtools/docs/contributing.html">instruction set</a> for\u000agetting started.</p>\u000a\u000a<h3>Making it your own</h3>\u000a\u000a<p>Now that you have your custom devtools frontend, you can make tweaks to the\u000aJS/CSS/HTML source, and see those changes in your development version of\u000achrome after you manually restart the browser.</p>\u000a\u000a<p>For example, a bunch of people wanted to get rid of the yellow highlight that\u000aappears overlaid in the browser window when hovering over DOM elements. At the\u000atime of writing, the method responsible for this is in <code>inspector.js</code> called\u000a<code>WebInspector.highlightDOMNode</code>. By applying this small patch, you can disable\u000athe default behavior.</p>\u000a\u000a<pre><code>1150,1151c1150\u000a&lt;     // Do not highlight the DOM node.\u000a&lt;     //this.highlightDOMNodeForTwoSeconds(nodeId);\u000a---\u000a&gt;     this.highlightDOMNodeForTwoSeconds(nodeId);\u000a</code></pre>\u000a\u000a<p>You can also easily make cosmetic tweaks to the devtools by changing the CSS.\u000aA lot of the base styles are implemented in <code>inspector.css</code>. For instance, for\u000apresentation purposes, the devtools are a bit small, so I find it useful to\u000aincrease the size of the devtools, which is a one line CSS change.</p>\u000a\u000a<p>There's already a ton of useful features packed into the devtools, but I bet\u000ayou can come up with some more awesomeness to add as an extension or chromium\u000apatch. Let me know if you do!</p>\u000a
p1481
tp1482
Rp1483
sg13
V/extending-chrome-developer-tools
p1484
sg15
Nsg16
I01
sg17
VExtending chrome developer tools
p1485
sg20
V\u000a\u000aChrome\u2019s developer tools (also known as the WebKit inspector) are super useful\u000afor web developers.
p1486
sS'snip'
p1487
g7
(g8
g9
V<p>Make the devtools/webkit inspector even better by writing devtools extensions and submitting Javascript patches.</p>\u000a
p1488
tp1489
Rp1490
sg25
g169
sg33
g1484
sg170
(dp1491
g172
S'Jul'
p1492
sg174
S'July 11, 2011'
p1493
sg176
I7
sg177
S'2011-07-11T09:00:00-00:00'
p1494
sg179
I1310400000
sg180
I2011
sg181
I11
ssg65
g182
sg31
S'extending-chrome-developer-tools'
p1495
sS'categories'
p1496
(lp1497
S'web'
p1498
aS'google'
p1499
aS'chrome'
p1500
asS'posted'
p1501
g188
(S'\x07\xdb\x07\x0b'
p1502
tp1503
Rp1504
ssg34
S'content/posts/2011/extending-chrome-developer-tools/index.md'
p1505
sg36
F1470927614.0
sa(dp1506
g2
(dp1507
g4
V An HTML5 filesystem-based approach to loading game assets. Still a work in progress.  
p1508
sg28
g7
(g8
g9
V<p>HTML5 games are here today, and rapidly increasing in complexity. Impressive\u000a<a href="http://madebyevan.com/webgl-water/">demos</a> are <a href="http://www.chromeexperiments.com/">everywhere</a>, and prominent titles like <a href="https://chrome.google.com/webstore/detail/ciamkmigckbgfajcieiflmkedohjjohh">Gun\u000aBros</a> and <a href="http://chrome.angrybirds.com">Angry Birds</a> prove that it's possible to\u000acreate competitive gaming experiences in the browser. Games like these are\u000apossible thanks largely to the modern web stack which includes WebGL, the Web\u000aAudio API, Web Sockets and others.</p>\u000a\u000a<p>Often forgotten, however, is the less sexy story of loading game assets. As the\u000aweb platform progresses and allows for increasingly complex games, game assets\u000a(ex. textures, movies, music and images) grow in size and number, and asset\u000amanagement becomes a sticking point for game developers.</p>\u000a\u000a<p>Let me share with you some truths:</p>\u000a\u000a<ol>\u000a<li>Modern games require gigabytes of assets (textures, movies, etc)</li>\u000a<li>Gamers don't like waiting for their game to load</li>\u000a<li>Browser gamers want to be able to play regardless of internet connectivity</li>\u000a</ol>\u000a\u000a<p>"But wait," you say, "I know! Just use the <a href="http://diveintohtml5.org/offline.html">Application Cache</a>\u000aand yer done!". Not so fast, dear reader... As described below, there are\u000aproblems with this approach, and I propose some solutions.</p>\u000a\u000a<h1>Problems with Application Cache</h1>\u000a\u000a<p>So you've started implementing your awesome asset loading solution using\u000aAppCache. The good news is that there are some useful tools to help you debug\u000aif you have taken this difficult route:</p>\u000a\u000a<ol>\u000a<li>You can get basic information about the site's app cache through the\u000aDeveloper Tools' <a href="http://code.google.com/chrome/devtools/docs/resources.html">resource panel</a>.</li>\u000a<li>You can view (and remove!) caches stored in Chrome by navigating to\u000a<code>chrome://appcache-internals/</code>.</li>\u000a</ol>\u000a\u000a<p>But let me be blunt: <strong>AppCache is annoying to deal with</strong>. If you've made a\u000asmall error in your cache manifest file, you'll quickly hit a brick\u000awall. I ran into an issue where I forgot to include a <code>NETWORK:</code>\u000afallback clause, and wasted hours trying to figure out why all of my\u000aXHRs were responding with status 0.</p>\u000a\u000a<p>Part of what makes AppCache difficult to debug is its very <strong>limited\u000aJavaScript API</strong>. Aside from letting you inspect the status of the entire cache\u000awith <code>window.applicationCache</code> and the <code>updateready</code> event, AppCache doesn't\u000agive us much to work with. There's no way to tell if a particular resource\u000awe're dealing with is cached or not and no programmatic way of clearing the\u000acache.</p>\u000a\u000a<p>AppCache takes a fully transactional approach to asset loading.  Either\u000athe cache is fully loaded, or fully unloaded. Compounding this issue,\u000ait's impossible to resume the download of an AppCache. Thus, if you have\u000aa large amount of assets, your user will have to <strong>wait a long time for\u000aeverything to be loaded</strong>, and if they reload, they will need to restart\u000atheir cache download.</p>\u000a\u000a<p>Lastly, you can only include one cache manifest per page, making it\u000a<strong>impossible to group assets</strong> into multiple bundles. There are hacks that\u000ause multiple iframes with different cache manifests to work around this\u000alimitation (used in <a href="http://chrome.angrybirds.com">Angry Birds</a>), but these are ugly!</p>\u000a\u000a<p>Ultimately, what we need is a well-thought-out Application Cache\u000aenhancement or replacement. Given how quickly web standards bodies move,\u000aI've started thinking a bit about a transitional solution.</p>\u000a\u000a<h1>Designing a game asset loader</h1>\u000a\u000a<p>An ideal asset loading solution requires some of these features:</p>\u000a\u000a<ol>\u000a<li>Granular asset loading. Load all, in groups, or individually.</li>\u000a<li>No asset size limits.</li>\u000a<li>Offline capability.</li>\u000a<li>Programatic control over assets.</li>\u000a</ol>\u000a\u000a<p>It makes sense to group assets in bundles and let the loader take care\u000aof the details. We can even create a custom manifest format, for\u000aexample, in JSON format:</p>\u000a\u000a<pre><code>{\u000a  "assetRoot": "./media/",   // The root of the assets.\u000a  "bundles": [{\u000a    "name": "core",          // A bundle definition.\u000a    "contents": [            // The contents within.\u000a      "theme.mp3",\u000a      "loading.jpg"\u000a    ]\u000a  }, {\u000a    "name": "level1",        // Multiple bundles defined.\u000a    "contents": [            // Note: order implicit since bundles\u000a      "L1/background.jpg",   // objects are stored in an array.\u000a      "L1/blip.wav"\u000a    ]\u000a  }, {\u000a    "name": "level2",\u000a    "contents": [\u000a      "L2/intro.mov"\u000a    ]\u000a  }],\u000a  "autoDownload": false      // If true, download all in order.\u000a}\u000a</code></pre>\u000a\u000a<p>With this manifest format in mind, sample API usage might look like\u000athis:</p>\u000a\u000a<pre><code>// Load the asset library.\u000avar gal = new GameAssetLoader('/path/to/gal.manifest');\u000a\u000a// Read the manifest and other good stuff.\u000agal.init(function() {\u000a  // When ready, download the bundle named 'core'.\u000a  gal.download('core');\u000a});\u000a\u000a// When the core assets are loaded.\u000agal.onLoaded('core', function(result) {\u000a  if (result.success) {\u000a    // Show a loading indicator.\u000a    document.querySelector('img').src = gal.get('loading.jpg');\u000a  }\u000a});\u000a\u000a// Check the progress of the download.\u000agal.onProgress('core', function(status) {\u000a  console.log('status:', status.current/status.total, '%');\u000a});\u000a</code></pre>\u000a\u000a<p>Note that although I've been using the name Game Asset Loader, this\u000aapproach can be used for loading any large non-game assets, such as for\u000aexample, a video or photo gallery.</p>\u000a\u000a<h1>Implementation details</h1>\u000a\u000a<p>Luckily, the modern web stack enables us to create a custom solution to\u000aaddress all of these requirements. By leveraging technologies such as\u000athe HTML5 Filesystem API or Indexed DB, we have programmatic access to\u000aa storage mechanism that we can use to build an asset loader described\u000ahere.</p>\u000a\u000a<p>I used the <a href="http://www.html5rocks.com/en/tutorials/file/filesystem/">Filesystem API</a> to implement a version of the asset\u000aloader. The code requests a large amount of persistent storage using the \u000a<a href="https://groups.google.com/a/chromium.org/group/chromium-html5/msg/5261d24266ba4366?dmode=source">Quota API</a>, which is undocumented, but works anyway:</p>\u000a\u000a<pre><code>// Get quota.\u000astorageInfo.requestQuota(window.PERSISTENT, quota,\u000a  onQuotaGranted, onError);\u000a\u000a// Callback when the quota API has granted quota\u000afunction onQuotaGranted = function(grantedBytes) {\u000a  // Save grantedBytes in the adapter\u000a  that.grantedBytes = grantedBytes;\u000a  // Once quota is grantedBytes, initialize a filesystem\u000a  requestFileSystem(window.PERSISTENT, grantedBytes, onInitFS, onError);\u000a};\u000a\u000a// Callback when the filesystem API has created a filesystem.\u000afunction onInitFS = function(fs) {\u000a  // Create a directory for the root of the assets.\u000a  fs.root.getDirectory(ROOT_DIR, {create: true}, function(dirEntry) {\u000a    that.root = dirEntry;\u000a  }, onError);\u000a};\u000a</code></pre>\u000a\u000a<p>The approach fetches assets with <code>XMLHttpRequest</code>, and stores them in the\u000afilesystem. All files in the filesystem are accessible via the <code>filesystem://</code>\u000aschema, and can be used as any other resource. This filesystem URL is returned\u000aby the library in the <code>get(path)</code> call.</p>\u000a\u000a<p>Note that the writable HTML5 filesystem API is currently available in Chrome\u000aonly, but that it's quite possible to use IndexedDB (supported in Firefox and\u000aIE10) as the data store.</p>\u000a\u000a<h1>Usage scenarios</h1>\u000a\u000a<p>The following section briefly describes what the game asset loader (GAL) does\u000ain several scenarios.</p>\u000a\u000a<p>Player goes to game.com which uses the game asset loader. The game calls\u000a<code>gal.download('core')</code> to download core assets and\u000a<code>gal.download('level1')</code> to load the first level into the player\u2019s\u000afilesystem. While the core bundle loads, the game displays a loading\u000aindicator. Once core is loaded, the game displays the main menu. As soon\u000aas the first level is loaded, the "Play now" button is enabled. As the\u000aplayer plays, the GAL downloads more of the levels in the background.</p>\u000a\u000a<p>Next time, the player tries playing offline. He goes to game.com, whose\u000acode is cached via AppCache, and loads GAL again. This time GAL knows it\u2019s\u000aoffline, looks up its manifest stored on the filesystem and doesn\u2019t try to\u000adownload new assets. The old assets still work though.</p>\u000a\u000a<p>Player is still offline, making good progress, and beats level 5, but\u000athere are no assets downloaded for level 6. Luckily, before starting\u000aeach level, the game calls <code>gal.download('levelBundle')</code> to make sure\u000athat the contents of that bundle are downloaded. The callback returns an\u000aerror and the game displays an error telling the player that he needs to\u000abe online to download the next level.</p>\u000a\u000a<p>So the player goes online and tries again. GAL re-downloads a manifest.\u000aNext, GAL tries re-downloading every asset that the JS requests. Luckily most\u000aof these assets are still in the browser cache, and won't be re-downloaded. The\u000aloader then saves all of the assets in the filesystem, clobbering old files\u000aindiscriminately. (This is bad, and needs to be fixed. Read on!)</p>\u000a\u000a<h1>Future work</h1>\u000a\u000a<p>In particular, re-downloading every asset while online is not desirable\u000abehavior, and we can't always rely on the browser cache for this. For\u000asmaller files, we can probably rely on ETag and Last-Modified headers\u000aand hopefully the browser won't re-download the files. However, the\u000a<strong>asset loader will still overwrite the asset in the filesystem, even if\u000ait's unmodified</strong>. This needs to be fixed. Large files are not likely to be\u000acached by the browser, so we will need more intelligent <strong>caching built into\u000athe asset loader itself</strong>.</p>\u000a\u000a<p>There are other edge cases that need to be considered, such as what happens\u000awhen an <strong>asset is removed from a manifest</strong>. Ideally if this occurs, it\u000a<strong>should also be removed from the filesystem</strong>, but this is not currently\u000aimplemented.</p>\u000a\u000a<p>I'm happy to release the <a href="https://github.com/borismus/game-asset-loader">source</a> under the permissive Apache 2\u000alicense and provide <a href="https://github.com/borismus/game-asset-loader/blob/master/tests/tests.js">unit tests</a> and a <a href="https://github.com/borismus/game-asset-loader/tree/master/tests/game">sample</a> project\u000afor your perusal. It's well documented and should be reasonably easy to\u000aunderstand. I've also made provisions to separate the core library\u000ainterface from the Filesystem-based implementation, making it even\u000aeasier to implement an Indexed DB adapter.</p>\u000a\u000a<p>Before I go, let me reiterate that this library isn't quite production ready,\u000abut a step in the right direction for facilitating real games on the web.\u000aPlease comment below if you have feedback on the idea, or are using the\u000alibrary to write a game of your own!</p>\u000a
p1509
tp1510
Rp1511
sg13
V/game-asset-loader
p1512
sg15
Nsg16
I01
sg17
VLoading large assets in modern HTML5 games
p1513
sg20
V\u000a\u000a\u000aHTML5 games are here today, and rapidly increasing in complexity.
p1514
sS'snip'
p1515
g7
(g8
g9
V<p>An HTML5 filesystem-based approach to loading game assets. Still a work in progress.</p>\u000a
p1516
tp1517
Rp1518
sg25
g169
sg33
g1512
sg170
(dp1519
g172
S'Sep'
p1520
sg174
S'September 22, 2011'
p1521
sg176
I9
sg177
S'2011-09-22T09:00:00-00:00'
p1522
sg179
I1316707200
sg180
I2011
sg181
I22
ssg65
g182
sg31
S'game-asset-loader'
p1523
sS'categories'
p1524
(lp1525
S'web'
p1526
aS'offline'
p1527
aS'games'
p1528
asS'posted'
p1529
g188
(S'\x07\xdb\t\x16'
p1530
tp1531
Rp1532
ssg34
S'content/posts/2011/game-asset-loader/index.md'
p1533
sg36
F1433825663.0
sa(dp1534
g2
(dp1535
g4
V I've been thinking a bit about native and web applications, and how they can (or can't) coexist in the future. This topic has been steeping in my head for months now, so here is a brain dump of some of my thoughts.
p1536
sg28
g7
(g8
g9
V<p>I've been thinking a bit about native and web applications, and how they can\u000a(or can't) coexist in the future. This topic has been steeping in my head for\u000amonths now, so here is a brain dump of some of my thoughts.<!--more--></p>\u000a\u000a<h2>On hybrid apps</h2>\u000a\u000a<p>Hybrid apps embed a browser, using a web view for part of their UI.\u000aThis is a very flexible definition for a very flexible beast. At one\u000aextreme are native apps that embed a Web View to render a small widget\u000a(written in HTML/CSS/JS) in the UI. On the other hand, you could have a\u000aweb application that provides just a native frame around the content\u000awhich is entirely implemented as a web app. The hybrid app spectrum\u000alooks something like this:</p>\u000a\u000a<pre><code>native &lt;--------------- hybrid apps ---------------&gt; web\u000aMail.app                GMail for iOS              GMail\u000a</code></pre>\u000a\u000a<p>Pieces of the web stack have long been useful as building blocks for\u000aapplications, but lately there has been a bloom in features under the\u000aHTML5 moniker. As a result, many developers are gravitating towards the\u000aright end of the hybrid app spectrum.</p>\u000a\u000a<p>There are several frameworks which target this niche by providing thin native\u000aframes, such as <a href="http://phonegap.com/">PhoneGap</a> and <a href="http://fluidapp.com/">Fluid</a>. There are three benefits to these\u000aframeworks, helping developers to:</p>\u000a\u000a<ol>\u000a<li>Make money: capitalize on AppStore and Market earnings.</li>\u000a<li>Use more features: get access to features not available from the web.</li>\u000a<li>Provide good platform-specific native integration.</li>\u000a</ol>\u000a\u000a<p>The first two are relatively well understood, so for the purposes of this post,\u000aI'm more interested in the third. Android PhoneGap apps are launched from the\u000ahome screen, Fluid Mac apps from spotlight or your dock, etc. Then you can\u000aswitch between these apps as if they were regular OS X applications.</p>\u000a\u000a<p>Even though the browser is a single app, it runs tons of applications\u000alike GMail, twitter, etc. The browser allows people to do two\u000afundamentally different things: view content on the web (sites), and <em>do\u000athings</em>, like listen to music, play games and create documents (apps).\u000aFor a deeper discussion on the distinctions between web apps and web\u000asites, check out James Pearce's article <a href="http://tripleodeon.com/2011/09/of-sites-and-apps/">Of Sites and Apps</a>.</p>\u000a\u000a<p>A pure web application does not rely on platform-specific native code\u000aand runs inside the browser, but still has to rely on a native frame\u000asolution to reap the native integration benefit frameworks like PhoneGap\u000aand Fluid provide.</p>\u000a\u000a<h2>Hybrid operating systems</h2>\u000a\u000a<p>Some browsers today have an explicit notion of apps. Without the proper OS\u000aintegration, this is confusing. Now your device (laptop, tablet, phone, tv) has\u000aapps, one of which is a browser, and the browser has apps too. So to launch an\u000aapp, users have to open the browser, and then launch what they want. This\u000areinforces the distinction between web and native apps in people's heads and\u000amakes for a very inelegant solution.</p>\u000a\u000a<p>One approach is for the OS to make the browser special in the operating system,\u000aallowing it also to manage installed web applications. Another approach is for\u000aoperating systems to implement their SDKs using HTML, CSS and JavaScript, but\u000awith non-standard APIs specific to the platform, such as WebOS. The extreme of\u000athis approach is Chrome OS, where browser and OS are indistinguishable.</p>\u000a\u000a<p>Like apps, operating systems vary in how much they embrace web applications.\u000aHere is an interesting spectrum to think about:</p>\u000a\u000a<pre><code>web agnostic &lt;------------ hybrid OS ------------&gt; apps are webapps\u000aiOS           Windows 8              Web OS               Chrome OS\u000a</code></pre>\u000a\u000a<p>Let me define what I mean by "hybrid OS". A hybrid OS is aware of the\u000apresence of not just native, but also web applications, and provides\u000aways of managing web apps, possibly alongside native ones. Windows 8 is an\u000aexample of a hybrid OS, employing the new Metro UI, while also supporting\u000aWindows 7-style UI. Palm's Web OS, has no native mode at all (except via\u000a<a href="https://developer.palm.com/content/api/dev-guide/pdk/overview.html">PDK plugins</a>), and all apps are web apps (though they require the use of a\u000aspecial set of JavaScript libraries).</p>\u000a\u000a<p>Web apps today are actively used, in some cases, even <a href="http://www.readwriteweb.com/archives/financial_times_proves_html5_can_beat_native_mobil.php">surpassing</a> native\u000aapps in popularity. Unfortunately, users of web apps are stuck in their browser\u000awhich is confusing and limiting, given the current landscape of native\u000aapplications. Hybrid operating systems make explicit the idea that in the end,\u000aweb apps are just apps.</p>\u000a\u000a<h2>Future</h2>\u000a\u000a<blockquote>\u000a  <p>The second goal of PhoneGap is for the project to cease to exist. This is not\u000a  a nihilistic sentiment... -- <a href="http://wiki.phonegap.com/w/page/46311152/apache-callback-proposal">Apache Callback Proposal</a></p>\u000a</blockquote>\u000a\u000a<p>My goal is for web apps to become compelling enough to force OS creators to\u000ahybridize their platforms. In other words, I'd like to see rightward movement\u000ain both the app and OS spectrums. As more hybrid operating systems emerge, we\u000aget closer to PhoneGap's second goal.</p>\u000a
p1537
tp1538
Rp1539
sg13
V/hybrid-operating-systems
p1540
sg15
Nsg16
I01
sg17
VHybrid operating systems
p1541
sg20
V\u000aI've been thinking a bit about native and web applications, and how they can\u000a(or can't) coexist in the future.
p1542
sg6
V<p>I've been thinking a bit about native and web applications, and how they can\u000a(or can't) coexist in the future. This topic has been steeping in my head for\u000amonths now, so here is a brain dump of some of my thoughts.
p1543
sg25
g169
sg33
g1540
sg170
(dp1544
g172
S'Nov'
p1545
sg174
S'November 28, 2011'
p1546
sg176
I11
sg177
S'2011-11-28T09:00:00-00:00'
p1547
sg179
I1322499600
sg180
I2011
sg181
I28
ssg65
g182
sg31
S'hybrid-operating-systems'
p1548
sS'categories'
p1549
(lp1550
S'web'
p1551
asS'posted'
p1552
g188
(S'\x07\xdb\x0b\x1c'
p1553
tp1554
Rp1555
ssg34
S'content/posts/2011/hybrid-operating-systems/index.md'
p1556
sg36
F1433825668.0
sa(dp1557
g2
(dp1558
g4
V I went to the Bay Area jQuery Conference and learned some interesting things.  
p1559
sg28
g7
(g8
g9
V<p>A few weekends ago I went to the jQuery Conference held at the MS campus in\u000aMountain View. And I took notes!</p>\u000a\u000a<p>Overall trends about the jQuery community:</p>\u000a\u000a<ul>\u000a<li><p>People are writing more complex apps on top of jQuery and there\u000ais a widely understood need for MVC frameworks, such as <a href="http://documentcloud.github.com/backbone/">Backbone.js</a>, \u000a<a href="http://knockoutjs.com/">Knockout.js</a> and <a href="http://javascriptmvc.com/">JavaScript MVC</a>.</p></li>\u000a<li><p>Feature detection is important!</p>\u000a\u000a<ul>\u000a<li>Polyfill - replicates standard feature with a compatible API</li>\u000a<li>Shim - provides its own API for a future feature</li>\u000a</ul></li>\u000a<li><p>Serious need for templating systems. Boris Moore showed a very performant\u000ademo of jQuery Templates. Many other templating systems exist as well,\u000alike one built into <a href="http://documentcloud.github.com/underscore/">underscore.js</a> and <a href="http://mustache.github.com/">mustache.js</a>.</p></li>\u000a<li><p>Many new mobile performance tools: <a href="http://www.blaze.io/">blaze.io</a> -- a tool that gives a\u000ageneral overview of a site's performance, <a href="http://pcapperf.appspot.com/">pcapperf</a> -- a web performance\u000aanalyzer that uses tcpdump output from mobile device activity, and <a href="http://jdrop.org/">jDrop</a>\u000a-- a service that lets you capture large amounts of data on your mobile\u000adevice and then analyze it on the desktop web browser.</p></li>\u000a<li><p>People are rallying around <a href="http://jshint.com/">JSHint</a>, a fork of Crockford's <a href="http://www.jslint.com/">JSLint</a>\u000aproject, but with more configurable JavaScript sanitation rules.</p></li>\u000a<li><p>Haters gotta hate. Everybody seems to get a kick out of hating Douglas\u000aCrockford. Give the nice opinionated man a break and go write some\u000aJavaScript.</p></li>\u000a</ul>\u000a\u000a<p>I went to a bunch of talks, and I took the most notes for during this talk:</p>\u000a\u000a<h2>State of jQuery</h2>\u000a\u000a<p>John Resig talked about a bunch of changes to the project structure, largely\u000airrelevant to jQuery library consumers. He also covered some of many jQuery 1.6\u000aimprovements:</p>\u000a\u000a<ul>\u000a<li>Rewrite of <code>attr()</code> and <code>val()</code>. For example, <code>attr('val', false)</code> removes\u000athe attribute</li>\u000a<li>Separate <code>prop()</code> from <code>attr()</code>. Indeed!</li>\u000a<li><code>$('input:focus')</code> gets focused input box across platforms</li>\u000a<li>Significant performance boosts:\u000a<ul>\u000a<li><code>attr()</code> performance ~85% faster, <code>val()</code> ~150% faster, <code>data()</code> ~115%\u000afaster</li>\u000a</ul></li>\u000a<li>Integration with requestAnimationFrame for animations</li>\u000a<li><code>$.map(Object, function)</code> now works (as it does for Arrays)</li>\u000a</ul>\u000a\u000a<p>Pro tip: jQuery automatically parses serialized JSON if it's included as the value \u000aof a HTML5 data attribute. Example: <code>&lt;header data-array="[0,1,2]"&gt;</code> then \u000a<code>$('header').data('array')[1] == 1</code></p>\u000a\u000a<h2>State of jQuery Mobile</h2>\u000a\u000a<p>Mobile matters. 5.3 billion mobile subscriptions (cf. global population of 6.8\u000abillion), 10 billion web-enabled mobile devices.</p>\u000a\u000a<p>John Resig also touched on jQuery Mobile, and then Scott Jehl and Todd Parker went \u000ainto a lot more detail.</p>\u000a\u000a<ul>\u000a<li>Navigation model now uses the <a href="https://developer.mozilla.org/en/DOM/Manipulating_the_browser_history">history API</a> for hash-less URLs.</li>\u000a<li>jQM minified and packed is ~18kb!</li>\u000a<li>Nice gallery of goodness at <a href="http://www.jqmgallery.com/">jQuery Mobile Gallery</a>\u000a<ul>\u000a<li>Including <a href="http://www.barackobama.com/m/">Obama's mobile site</a>!</li>\u000a</ul></li>\u000a<li>Media queries\u000a<ul>\u000a<li>Useful as a browser support cutoff heuristic.</li>\u000a<li>CSS classes added based on media queries, facilitating simpler styles</li>\u000a<li>Uses <a href="https://github.com/scottjehl/Respond">Respond.js</a>, a polyfill for browsers that don't support media queries</li>\u000a</ul></li>\u000a<li>Philosophy: easily brandable cross-device experience</li>\u000a<li>All builtin views are ARIA-enabled</li>\u000a</ul>\u000a\u000a<p>Pro tip: mouse events in some mobile browsers are on a <a href="http://cubiq.org/remove-onclick-delay-on-webkit-for-iphone">300ms delay</a> to allow\u000athe browser to interpret user's gestures. jQuery Mobile includes a fix for\u000athis!</p>\u000a\u000a<h2>Prototyping Tools in jQuery</h2>\u000a\u000a<p>Super useful and informative set of tools!</p>\u000a\u000a<p>MockJAX is a library that simulates a server.</p>\u000a\u000a<ul>\u000a<li>Intercepts and simulates AJAX calls\u000a<ul>\u000a<li>Define a URL structure and a response structure</li>\u000a</ul></li>\u000a<li>Can define responses as a function.</li>\u000a<li>Can simulate error responses.</li>\u000a<li>Useful for unit testing as well!</li>\u000a</ul>\u000a\u000a<p>MockJSON: create fake JSON on demand</p>\u000a\u000a<ul>\u000a<li>A way to generate random-ish JSON</li>\u000a<li>For example, <code>{'age|0-99'}</code> outputs <code>{'age': randint_between_0_and_99}</code></li>\u000a</ul>\u000a\u000a<p>Amplify: abstraction layer for all data</p>\u000a\u000a<ul>\u000a<li>Abstracts away shifting server-side APIs</li>\u000a<li>amplify.request.define can define a data store. </li>\u000a</ul>\u000a\u000a<p>For example:</p>\u000a\u000a<pre><code>amplify.request.define("list", "ajax", {\u000a  url: "/todo/",\u000a  dataType: "json",\u000a  type: "GET"\u000a});\u000a</code></pre>\u000a
p1560
tp1561
Rp1562
sg13
V/jquery-conference
p1563
sg15
Nsg16
I01
sg17
VjQuery conference 2011
p1564
sg20
V\u000a\u000a\u000aA few weekends ago I went to the jQuery Conference held at the MS campus in\u000aMountain View.
p1565
sS'snip'
p1566
g7
(g8
g9
V<p>I went to the Bay Area jQuery Conference and learned some interesting things.</p>\u000a
p1567
tp1568
Rp1569
sg25
g169
sg33
g1563
sg170
(dp1570
g172
S'Apr'
p1571
sg174
S'April 26, 2011'
p1572
sg176
I4
sg177
S'2011-04-26T09:00:00-00:00'
p1573
sg179
I1303833600
sg180
I2011
sg181
I26
ssg65
g182
sg31
S'jquery-conference'
p1574
sS'categories'
p1575
(lp1576
S'web'
p1577
aS'conference'
p1578
asS'posted'
p1579
g188
(S'\x07\xdb\x04\x1a'
p1580
tp1581
Rp1582
ssg34
S'content/posts/2011/jquery-conference/index.md'
p1583
sg36
F1433825672.0
sa(dp1584
g2
(dp1585
g4
V Prototyping multi-touch applications? Simulate spec-compatible touch events without a mobile device.  
p1586
sg28
g7
(g8
g9
V<p>In mobile development, it's often easier to start prototyping on the desktop\u000aand then tackle the mobile-specific parts on the devices you intend to support.\u000aMulti-touch is one of those features that's difficult to test on the desktop, since\u000amost desktops didn't have multi-touch hardware, and thus desktop browsers don't\u000ahave touch event support. Things are different today (you hear every mother say). \u000aMost new Macs, for example, ship with multi-touch capable input of some sort.\u000aUnfortunately the browsers haven't really caught up yet.</p>\u000a\u000a<p>Enter Fajran Iman Rusadi, who released a <a href="https://github.com/fajran/npTuioClient">npTuioClient</a> NPAPI plugin with a\u000aJavaScript wrapper. Unfortunately this library provides a non-standard API to\u000amulti-touch, which is not ideal for developers that want to write their\u000amulti-touch application on desktop and then run the same code on their mobile\u000adevices without modifications.</p>\u000a\u000a<h2>Browser Patches</h2>\u000a\u000a<p>As HTML5 grows up, browser vendors struggle to stay current up with the growing\u000avariety of specifications. The result is <a href="http://caniuse.com/">uneven feature support</a> across\u000abrowsers and a complex problem for web developers.</p>\u000a\u000a<p>The web development community has rallied around <strong>shims</strong> and <strong>polyfills</strong>\u000afor the solution. These are bizarre terms that I find confusing and so will\u000adefer to <a href="http://remysharp.com/2010/10/08/what-is-a-polyfill/">Remy Sharp to define</a>. The basic idea of both is to fill in\u000afunctionality that's missing in the browser implementation.</p>\u000a\u000a<p>Since we now have a well established <a href="https://dvcs.w3.org/hg/webevents/raw-file/tip/touchevents.html">touch events specification</a> working\u000agroup at the W3C, I wrote <a href="https://github.com/borismus/MagicTouch">MagicTouch.js</a>, a multi-touch polyfill thatlets\u000ayou, the developer, write the same code, test it on your desktop browser and\u000athen, run it on your real device. Totally tubular!</p>\u000a\u000a<p>MagicTouch.js still relies on the npTuioClient plugin, it just creates\u000aspec-compatible touch events. Incidentally, here's how you can trigger custom\u000aDOM events:</p>\u000a\u000a<pre><code>var event = document.createEvent('CustomEvent');\u000a// Initialize the event, make it bubble up and possible to cancel\u000aevent.initEvent('touchstart', true, true);\u000a// Assign properties to the event\u000aevent.touches = touchArray;\u000a...\u000a// Get the element associated with the event\u000avar element = document.elementFromPoint(...);\u000a// Assign the element\u000aevent.target = element;\u000a// Finally, dispatch the event\u000aelement.dispatchEvent(event);\u000a</code></pre>\u000a\u000a<p>Note that this approach to create custom DOM events is not cross-browser\u000acompatible. I only tested in Chrome.</p>\u000a\u000a<h2>Installation</h2>\u000a\u000a<p>Here how to get multi-touch web events working in Chrome for Mac:</p>\u000a\u000a<ol>\u000a<li>Download and install the <a href="https://github.com/fajran/npTuioClient#readme">npTuioClient NPAPI plugin</a>\u000ainto <code>~/Library/Internet Plug-Ins/</code>.</li>\u000a<li>Download the <a href="https://github.com/fajran/tongseng/downloads">TongSeng TUIO tracker</a> for Mac\u2019s MagicPad and start the\u000aserver</li>\u000a<li>Download <a href="https://github.com/borismus/MagicTouch">MagicTouch.js</a> and include both the script and the plugin in your\u000aapp.</li>\u000a</ol>\u000a\u000a<p>The code for this is as follows:</p>\u000a\u000a<pre><code>&lt;head&gt;\u000a  ...\u000a  &lt;script src="/path/to/magictouch.js"&gt;&lt;/script&gt;\u000a&lt;/head&gt;\u000a&lt;body&gt;\u000a  ...\u000a  &lt;object id="tuio" type="application/x-tuio" style="width:0; height:0;"&gt;\u000a    TUIO Plugin failed to load\u000a  &lt;/object&gt;\u000a&lt;/body&gt;\u000a</code></pre>\u000a\u000a<p>...and you're off to the races! Your multi-touch code will now work. Try out\u000athis <a href="https://github.com/borismus/MagicTouch/blob/master/samples/tracker.html">finger tracking demo</a> on either your multi-touch mobile device or your\u000anewly patched desktop browser.</p>\u000a\u000a<h2>Future Steps</h2>\u000a\u000a<p>As you saw, MagicTouch.js takes some effort to set up initially, requires\u000ayou to use an <code>&lt;object&gt;</code> in the HTML, and also needs you to run a separate\u000aprocess for intercepting touch events. While we can't quite get away without\u000ahaving to run another process, we can eliminate the NPAPI plugin by using the\u000a<a href="http://dev.w3.org/html5/websockets/">WebSocket API</a> to communicate to that process.</p>\u000a\u000a<p>If you're interested in multi-touch mobile web development, check out this\u000a<a href="http://www.html5rocks.com/mobile/touch.html">article on html5rocks.com</a>.</p>\u000a
p1587
tp1588
Rp1589
sg13
V/multi-touch-browser-patch
p1590
sg15
Nsg16
I01
sg17
VMulti-touch for your desktop browser
p1591
sg20
V\u000a\u000a\u000aIn mobile development, it's often easier to start prototyping on the desktop\u000aand then tackle the mobile-specific parts on the devices you intend to support.
p1592
sS'snip'
p1593
g7
(g8
g9
V<p>Prototyping multi-touch applications? Simulate spec-compatible touch events without a mobile device.</p>\u000a
p1594
tp1595
Rp1596
sg25
g169
sg33
g1590
sg170
(dp1597
g172
S'May'
p1598
sg174
S'May 2, 2011'
p1599
sg176
I5
sg177
S'2011-05-02T09:00:00-00:00'
p1600
sg179
I1304352000
sg180
I2011
sg181
I2
ssg65
g182
sg31
S'multi-touch-browser-patch'
p1601
sS'categories'
p1602
(lp1603
S'web'
p1604
asS'posted'
p1605
g188
(S'\x07\xdb\x05\x02'
p1606
tp1607
Rp1608
ssg34
S'content/posts/2011/multi-touch-browser-patch/index.md'
p1609
sg36
F1433825680.0
sa(dp1610
g2
(dp1611
g4
V Introducing osmus, a multiplayer HTML5 game written with web sockets, canvas and a game engine that runs shared code on the client and server.  
p1612
sg28
g7
(g8
g9
V<p>One day I had some friends over at my house introducing me some cool iPad\u000agames. One of the games was Osmos, developed by an Canadian indie studio called\u000a<a href="http://www.hemispheregames.com">Hemisphere Games</a>. You control a little blob that floats in 2D space, and\u000athe only thing your blob can do is shoot pieces of itself in a given\u000adirection, which propels it in the opposite direction. The rules of the game\u000aare simple, the main rule being that when two blobs collide, the larger one\u000awill consume the smaller one. The rest of the rules pretty much follow directly\u000afrom conservation of mass and energy. <a href="http://www.youtube.com/watch?v=pso6UBicLWU">See for yourself</a> - it's way\u000abetter than it sounds!</p>\u000a\u000a<p>Osmos really caught my attention because of its simple but engaging gameplay,\u000ameditative pace and distinct <strong>lack of multiplayer support</strong>, which struck me\u000aas a potentially very interesting problem to tackle. And so, Osmus (mu for\u000amultiplayer) was born as a browser-based multiplayer Osmos clone.</p>\u000a\u000a<h2>How it works</h2>\u000a\u000a<p>When a browser navigates to the osmus landing page, the server sends the new\u000aclient the current state of its universe, which is composed of blobs with\u000arandomized velocities. At this point, the client can passively watch the game\u000aprogress, but of course, can also join the game as a player controlled blob.\u000aOnce a player joins, he can click or tap (on mobile devices) the canvas to\u000ashoot off a new blob.</p>\u000a\u000a<p>As the game progresses, the server decides when someone (possibly one of the\u000aautonomous blobs) is victorious, at which point, players are notified and the\u000agame is restarted.</p>\u000a\u000a<p>The rest of this post is about some development-related details. I've had to take the game offline because it was too expensive for me to keep running on my VPS.\u000aHowever I did record a video:</p>\u000a\u000a<iframe width="640" height="360" src="http://www.youtube.com/embed/NiPZK3g_i1M" frameborder="0" allowfullscreen></iframe>\u000a\u000a<h2>Game architecture</h2>\u000a\u000a<p>I wrote osmus to be split into distinct, loosely coupled components both to\u000amake the codebase more approachable for other contributors, and to make it easy\u000ato experiment with interchangeable technologies.</p>\u000a\u000a<p><img src="osmus-architecture.png" alt="architecture" /></p>\u000a\u000a<p>Osmus uses a shared game engine that runs in both the browser and on\u000athe server. The engine is a simple state machine whose primary function\u000ais to compute the next game state as a function of time using the rules of\u000aphysics defined within.</p>\u000a\u000a<pre><code>Game.prototype.computeState = function(delta) {\u000a  var newState = {};\u000a  // Compute a bunch of stuff based on this.state\u000a  return newState;\u000a}\u000a</code></pre>\u000a\u000a<p>This is a pretty narrow definition of a <em>Game Engine</em>. In the game developer\u000aworld, what's typically meant by a game engine may include anything from a\u000arenderer, sound player, networking layer, <a href="http://en.wikipedia.org/wiki/Game_engine">etc</a>. In this case, I've\u000amade very clear divisions between these components, and the osmus game core\u000aonly includes just the physical state machine, so that both client and server\u000acan compute the next states and be reasonably synchronized in time.</p>\u000a\u000a<p>The client is composed of three main components: a renderer, input\u000amanager and sound manager. I built a very simple canvas-based renderer\u000athat draws blobs as red circles, and player blobs are green ones. My\u000acolleague <a href="http://twitter.com/kurrik">Arne Roomann-Kurrik</a> wrote an alternative\u000a<a href="https://github.com/mrdoob/three.js/">three.js</a> based renderer with some epic shaders and shadows.</p>\u000a\u000a<p>The sound manager handles playback of both sound effects and background music\u000a(taken from <a href="http://feryl.bandcamp.com/album/8-bit-magic-a-module-chiptune-collection">8-bit Magic</a>). The current implementation uses audio tags,\u000awith two <code>&lt;audio&gt;</code> elements, one for the background music channel, and one for\u000athe sound effects. There are known limitations of this approach, but given the\u000amodularity of my implementation, the sound implementation can be swapped out\u000afor one that uses Chrome's <a href="https://dvcs.w3.org/hg/audio/raw-file/tip/webaudio/specification.html">Web Audio API</a>, for example.</p>\u000a\u000a<p>Finally, the input manager handles mouse events, but can be replaced with one\u000athat uses touch instead, for a mobile version. In the mobile context, it will\u000alikely make sense to use CSS3 transformations instead of canvas, since CSS3 is\u000ahardware accelerated on iOS, while HTML5 canvas still isn't, and WebGL is not\u000aimplemented.</p>\u000a\u000a<p>Speaking of mobile, I was happily surprised that osmus works pretty well on\u000aiPad, especially on an iPad 2 running the latest iOS version. This is really\u000agreat, and one of the tangible benefits of writing games for the open web.</p>\u000a\u000a<h2>Networking is hard</h2>\u000a\u000a<p>From a networking perspective, a game is a rather ambitious project that\u000arequires seamless real-time synchronization between clients. Because of this,\u000abidirectional client-server communication is essential. In the modern web\u000astack, this is provided by <a href="http://dev.w3.org/html5/websockets/">Web Sockets</a> which supply a thin layer\u000aabove TCP and hide a lot of gory details from the implementer. To further hide\u000anetwork stack details, I use the <a href="http://socket.io/">socket.io</a> library, which provides a dead\u000asimple event-driven abstraction for the whole thing. Unfortunately there's\u000acurrently no support for binary data, which would greatly compress message\u000asize, perhaps by an order of two magnitudes in the case of osmos.</p>\u000a\u000a<p>From a bit of research which included this <a href="http://www.youtube.com/watch?v=zj1qTrpuXJ8">nice talk from Rob Hawkes</a>, it\u000abecame clear that to have any sort of shared experience, the simplest model is\u000ato have the true game state on the server, and have clients periodically sync\u000awith it. The main trade off here is synchronization quality vs. network\u000atraffic required.</p>\u000a\u000a<p>On one extreme, a game can be written by having the game logic entirely on the\u000aserver and sending updates (or perhaps even <a href="http://www.onlive.com/">simply screenshots</a>) to\u000athe client at 60 FPS, but this is generally not feasible due to the sheer\u000aamount of bandwidth required for this model. On the opposite extreme, you can\u000aimagine a network architecture in which clients connect, get initial state, and\u000aare then largely autonomous.</p>\u000a\u000a<p>In practice, there is a happy medium in which many multiplayer games fall,\u000awhich means replicating non-trivial code in both the client and the server.\u000aLuckily, now that we're in the ubiquitous JavaScript era, there is no longer a\u000aneed to duplicate functionality, but can instead share code by writing the game\u000aengine in JavaScript, and then running it in both a browser on the client, and\u000ain <a href="http://nodejs.org/">node.js</a> on the server.</p>\u000a\u000a<p>There's a lot more to be written about writing the multiplayer bits of osmus,\u000awhich will hopefully turn into a more detailed article at some point in the\u000afuture.</p>\u000a\u000a<h2>Shared JS modules</h2>\u000a\u000a<p>As mentioned earlier, osmus uses a physics engine that's shared between clients\u000aand the server. One might imagine that sharing JavaScript code between the two\u000awould be a breeze, but it's not that simple.</p>\u000a\u000a<p>Module loaders are a mess. There's the <a href="http://www.commonjs.org/">CommonJS spec</a>,\u000a<a href="http://requirejs.org/">RequireJS library</a> and node.js require system, none of which play\u000anicely together. If you want to share code between client and server (one of\u000athe big wins of JS on the server) without a module loader, you can use this\u000asomewhat hacky pattern:</p>\u000a\u000a<pre><code>(function(exports) {\u000a\u000avar MyClass = function() { /* ... */ };\u000avar myObject = {};\u000a\u000aexports.MyClass = MyClass;\u000aexports.myObject = MyObject;\u000a\u000a})(typeof global === "undefined" ? window : exports);\u000a</code></pre>\u000a\u000a<p>This hack relies on the fact that node.js defines a <code>global</code> object while the\u000abrowser does not. With the hack, node.js <code>require()</code> will be happy, and you can\u000aalso include the file in a <code>&lt;script&gt;</code> tag without polluting your namespace,\u000aassuming of course, that no other JS pollutes your namespace with a\u000a<code>window.global</code> object!</p>\u000a\u000a<p>Unfortunately this approach only works well for one shared module. As soon as\u000ayou have multiple modules depending on each other (via <code>require</code>s in node-land,\u000aand globals in browser-land), the difference between node's namespacing and\u000abrowser's inclusion becomes painfully apparent and requires more hacky\u000aworkarounds.</p>\u000a\u000a<p>Another approach is to use <a href="http://substack.net/posts/24ab8c/browserify-browser-side-require-for-your-node-js">browserify</a> to bundle all JS and emulate\u000arequires in the browser. This approach relies on node.js to serve the\u000agenerated JS, which is not ideal, since static files should be served by a\u000a<a href="http://nginx.net/">webserver</a> optimized for the purpose. However node.js + browserify can\u000abe configured to compile JS that can be served statically without relying on\u000anode to serve it. This approach introduces some overhead:</p>\u000a\u000a<ol>\u000a<li>Extra build step for deploying.</li>\u000a<li>Performance overhead of whatever mechanism browserify uses to support\u000a<code>require()</code> calls.</li>\u000a</ol>\u000a\u000a<p>Overall this approach sounds better to me, and I hope to try it out in a future\u000aversion of osmus.</p>\u000a\u000a<h2>Your turn</h2>\u000a\u000a<p>Today I'm releasing <a href="http://o.smus.com/">osmus</a> as a completely open source HTML5 game. Feel\u000afree to <a href="https://github.com/borismus/osmus">fork it</a> to your heart's content. Oh, and for other game related\u000agoodness, check out this article on <a href="http://www.html5rocks.com/en/tutorials/canvas/performance/">HTML5 canvas performance</a> recently\u000aposted on html5rocks.</p>\u000a
p1613
tp1614
Rp1615
sg13
V/multiplayer-html5-games-with-node
p1616
sg15
Nsg16
I01
sg17
VDeveloping multiplayer HTML5 games with node.js
p1617
sg20
V\u000a\u000a\u000aOne day I had some friends over at my house introducing me some cool iPad\u000agames.
p1618
sS'snip'
p1619
g7
(g8
g9
V<p>Introducing osmus, a multiplayer HTML5 game written with web sockets, canvas and a game engine that runs shared code on the client and server.</p>\u000a
p1620
tp1621
Rp1622
sg25
g169
sg33
g1616
sg170
(dp1623
g172
S'Aug'
p1624
sg174
S'August 30, 2011'
p1625
sg176
I8
sg177
S'2011-08-30T09:00:00-00:00'
p1626
sg179
I1314720000
sg180
I2011
sg181
I30
ssg65
g182
sg31
S'multiplayer-html5-games-with-node'
p1627
sS'categories'
p1628
(lp1629
S'web'
p1630
aS'games'
p1631
aS'multiplayer'
p1632
asS'posted'
p1633
g188
(S'\x07\xdb\x08\x1e'
p1634
tp1635
Rp1636
ssg34
S'content/posts/2011/multiplayer-html5-games-with-node/index.md'
p1637
sg36
F1433825687.0
sa(dp1638
g2
(dp1639
g4
V Doing some math to decide when to visit Vancouver in order to make the best of the Canucks final  
p1640
sg28
g7
(g8
g9
V<p>Now that the Vancouver Canucks are in the NHL Finals, a special visit to\u000aVancouver is in order. Unfortunately, the recently posted schedule really\u000asucks. There\u2019s just one weekend game, and the rest are spaced in such a way\u000athat I can only hope to see Game 5 (if it happens) and either Game 4 or Game 6\u000a(if it happens) if I manage to work from home half of the week. The question\u000ais: which game to choose? I want to be in Vancouver when the Canucks take the\u000acup!</p>\u000a\u000a<h2>Yay math!</h2>\u000a\u000a<p>Being super nerdy, I decided to throw math at the problem. Brushing up on\u000aprobability, I calculated the odds of a series ending at each game (4-7). For a\u000aseries to end in game 4, one team has to win all four games. The odds of this,\u000agiven a 50% chance for each team to win, is (1/2)^4. Since there are two teams,\u000amultiply by two, resulting in a 1/8 chance. You can do similar calcuations to\u000aget the following odds:</p>\u000a\u000a<pre><code>p(4)   p(5)   p(6)   p(7)\u000a0.125  0.25   0.3125 0.3125\u000a</code></pre>\u000a\u000a<p>Based on this distribution, it seems that game 6 is more likely to see the end\u000aof the series than game 4, but then I would risk missing a game 4 victory.\u000a<a href="http://www.ted.com/talks/arthur_benjamin_s_formula_for_changing_math_education.html">Arthur Benjamin</a> would be proud of me (unless my numbers are wrong).</p>\u000a\u000a<h2>A bit of history</h2>\u000a\u000a<p>Historical data tells a different story. As it turns out, the NHL is really\u000aold, dating back to 1927, but the league switched to best-of-7 scoring in 1939.\u000aWikipedia has a great chronology of NHL playoffs, complete with scores and\u000abrackets.</p>\u000a\u000a<p>I analyzed series scores from all playoff games in the last 25 years (since\u000a1985) by fetching raw wikipedia articles via wget and running them through a\u000a[python script]. Older records can also be found on wikipedia, but they are\u000aburied inside other pages, in harder to parse formats.</p>\u000a\u000a<pre><code>for year in {1985..2011} do\u000a  wget -O ${year} "http://en.wikipedia.org/w/index.php?title=${year}_Stanley_Cup_playoffs&amp;action=raw"\u000adone\u000a</code></pre>\u000a\u000a<p>This yielded the following distribution for playoff games, based on 383\u000amatches:</p>\u000a\u000a<pre><code>p(4)   p(5)   p(6)   p(7)\u000a0.16   0.24   0.33   0.27\u000a</code></pre>\u000a\u000a<p>For finals games only, the distribution is more skewed, probably in part due to\u000aa shortage of data (just NN finals games from the wikipedia page), but finals\u000agame history is probably a better predictor for finals games than all playoffs\u000agames, so this is worthwhile:</p>\u000a\u000a<pre><code>p(4)   p(5)   p(6)   p(7)\u000a0.28   0.24   0.27   0.21\u000a</code></pre>\u000a\u000a<p>Here are all three distributions plotted in a graph:</p>\u000a\u000a<p><img src="nhl-series-odds.png" alt="graph" /></p>\u000a\u000a<h2>Analyzing...</h2>\u000a\u000a<p>The most striking thing about this graph is the huge difference between finals\u000ahistory and stats, especially in the 4 game series scenario. Perhaps some teams\u000ajust buckle under the pressure, while their opponents remain steadfast. Or this\u000ais just a statistically insignificant fluke that can be attributed to having an\u000ainsufficiently large sample of finals matches.</p>\u000a\u000a<p>As expected, the historical distributions are skewed toward the series\u000afinishing in Game 4 due to unequal strength between teams. In other words, if\u000aone team is stronger than another, then the odds of that team winning each game\u000awould be greater, making the series shorter on average.</p>\u000a\u000a<h2>Decision!</h2>\u000a\u000a<p>Armed with numbers, I can decide whether to pick Game 4 or Game 6. Here are the\u000aodds of seeing the Canucks (yeah yeah, maybe the Bruins) win in games 4 or 5,\u000acompared to 5 or 6, as calculated from each of the three methods:</p>\u000a\u000a<pre><code>         Game 4 or 5            Game 5 or 6\u000aStats        38%                    58%\u000aPlayoffs     40%                    57%\u000aFinals       52%                    51%\u000a</code></pre>\u000a\u000a<p>Interesting. Given how annoying it would be to miss a Game 4 victory and how\u000afavorable the finals history-based odds look, I\u2019ll go for 4 and 5. Now to try\u000ato work from home, book flights and bask in Canuck glory!</p>\u000a
p1641
tp1642
Rp1643
sg13
V/nhl-finals-numbers
p1644
sg15
Nsg16
I01
sg17
VCrunching numbers for the NHL finals
p1645
sg20
V\u000a\u000a\u000aNow that the Vancouver Canucks are in the NHL Finals, a special visit to\u000aVancouver is in order.
p1646
sS'snip'
p1647
g7
(g8
g9
V<p>Doing some math to decide when to visit Vancouver in order to make the best of the Canucks final</p>\u000a
p1648
tp1649
Rp1650
sg25
g169
sg33
g1644
sg170
(dp1651
g172
S'May'
p1652
sg174
S'May 31, 2011'
p1653
sg176
I5
sg177
S'2011-05-31T09:00:00-00:00'
p1654
sg179
I1306857600
sg180
I2011
sg181
I31
ssg65
g182
sg31
S'nhl-finals-numbers'
p1655
sS'categories'
p1656
(lp1657
S'math'
p1658
aS'travel'
p1659
asS'posted'
p1660
g188
(S'\x07\xdb\x05\x1f'
p1661
tp1662
Rp1663
ssg34
S'content/posts/2011/nhl-finals-numbers/index.md'
p1664
sg36
F1433825694.0
sa(dp1665
g2
(dp1666
g4
V A JavaScript library that handles OAuth 2.0 for you, with a dead simple API. Comes with adapters for Google, Facebook and Github.  
p1667
sg28
g7
(g8
g9
V<p>Applications that access online services often need to access a user's private\u000adata. Chrome Extensions are no different. OAuth has emerged as the standard way\u000aof letting users share their private resources across sites without having to\u000ahand out their usernames and passwords. There is already a very nice \u000a<a href="http://code.google.com/chrome/extensions/tut_oauth.html">OAuth library for Chrome Extensions</a> that aims to simplify some of \u000athe pains that developers face when authorizing against OAuth endpoints.</p>\u000a\u000a<p>Since this library was written, the OAuth standard enjoyed a version bump\u000a(<a href="http://oauth.net/2/">OAuth 2.0</a>) which greatly simplifies the flow by no longer requiring\u000acryptography in the client. Also, some adventurous companies (notably\u000aGoogle, Facebook and others) have actually implemented OAuth 2.0\u000aendpoints. At the time of writing, OAuth 2.0 is still a draft spec, but is\u000anearing completion, and Chrome Extensions need some love.</p>\u000a\u000a<p>You may be wondering why you even need an OAuth 2.0 library in the first place.\u000aAs Aaron Parecki pointed out in his <a href="http://www.slideshare.net/aaronpk/the-current-state-of-oauth-2">Current State of OAuth 2</a>\u000apresentation at <a href="http://opensourcebridge.org/">Open Source Bridge</a> in Portland, OAuth 2 is very\u000amuch a moving target. The spec is not yet finalized, and there are 16 versions\u000aof it (although the most popular seems to be v10). Also, today's OAuth 2\u000aimplementations diverge from the spec in varying degrees, adding to the\u000adeveloper pain. The reason this library needs to be chrome extension-specific\u000ais that unfortunately Chrome extensions can't directly use the OAuth 2.0\u000aserver-side or client-side flows because they live at <code>chrome-extension://</code>\u000aURLs.</p>\u000a\u000a<p>When writing the OAuth 2.0 library for Chrome extensions, I had some goals in\u000amind:</p>\u000a\u000a<ol>\u000a<li>Support a variety of OAuth 2.0 providers that implement the spec</li>\u000a<li>Allow one app/extension to use multiple different OAuth 2.0 endpoints</li>\u000a<li>Avoid background pages for performance reasons</li>\u000a</ol>\u000a\u000a<h2>The OAuth 2.0 Library</h2>\u000a\u000a<p>There's a bit of setup involved if you'd like to create a Chrome extension that\u000aconnects to an OAuth 2 endpoint. This brief tutorial will guide you through\u000aconnecting to Google's APIs.</p>\u000a\u000a<p>Register your application with an OAuth 2.0 endpoint that you'd like to\u000ause. If it's a Google API you're calling, go to the <a href="https://code.google.com/apis/console/">Google APIs</a> page,\u000acreate your application and note your client ID and client secret. For more\u000ainfo on this, check out the <a href="http://code.google.com/apis/accounts/docs/OAuth2.html">Google OAuth 2.0</a> docs. When you setup your\u000aapplication, you will be asked to provide redirect URI(s). Please provide the\u000aURI that corresponds to the service you're using.</p>\u000a\u000a<p>Here's a table that will come in handy:</p>\u000a\u000a<p><style>\u000a  #impls { margin-left: -100px; }\u000a  #impls td, #impls th { border: 1px solid #999 }\u000a  #impls td { padding: 5px }\u000a</style></p>\u000a\u000a<table id="impls">\u000a  <tr>\u000a    <th>Adapter</th>\u000a    <th>Redirect URI</th>\u000a    <th>Access Token URI</th>\u000a  </tr>\u000a  <tr>\u000a    <td>google</td>\u000a    <td>http://www.google.com/robots.txt</td>\u000a    <td>https://accounts.google.com/o/oauth2/token</td>\u000a  </tr>\u000a  <tr>\u000a    <td>facebook</td>\u000a    <td>http://www.facebook.com/robots.txt</td>\u000a    <td>https://graph.facebook.com/oauth/access_token</td>\u000a  </tr>\u000a  <tr>\u000a    <td>github</td>\u000a    <td>https://github.com/robots.txt</td>\u000a    <td>https://github.com/login/oauth/access_token</td>\u000a  </tr>\u000a</table>\u000a\u000a<h4>Step 1: Copy library</h4>\u000a\u000a<p>You will need to copy the <a href="https://github.com/borismus/oauth2-extensions/tree/master/lib">oauth2 library</a> into your chrome extension\u000aroot into a directory called <code>oauth2</code>.</p>\u000a\u000a<h4>Step 2: Inject content script</h4>\u000a\u000a<p>Then you need to modify your manifest.json file to include a content script\u000aat the redirect URL used by the Google adapter. The "matches" redirect URI can\u000abe looked up in the table above:</p>\u000a\u000a<pre><code>"content_scripts": [\u000a  {\u000a    "matches": ["http://www.google.com/robots.txt*"],\u000a    "js": ["oauth2/oauth2_inject.js"],\u000a    "run_at": "document_start"\u000a  }\u000a],\u000a</code></pre>\u000a\u000a<h4>Step 3: Allow access token URL</h4>\u000a\u000a<p>Also, you will need to add a permission to Google's access token granting URL,\u000asince the library will do an XHR against it. The access token URI can be looked\u000aup in the table above as well.</p>\u000a\u000a<pre><code>"permissions": [\u000a  "https://accounts.google.com/o/oauth2/token"\u000a]\u000a</code></pre>\u000a\u000a<h4>Step 4: Include the OAuth 2.0 library</h4>\u000a\u000a<p>Next, in your extension's code, you should include the OAuth 2.0 library:</p>\u000a\u000a<pre><code>&lt;script src="/oauth2/oauth2.js"&gt;&lt;/script&gt;\u000a</code></pre>\u000a\u000a<h4>Step 5: Configure the OAuth 2.0 endpoint</h4>\u000a\u000a<p>And configure your OAuth 2 connection by providing clientId, clientSecret and\u000aapiScopes from the registration page. The authorize() method may create a new\u000apopup window for the user to grant your extension access to the OAuth2\u000aendpoint.</p>\u000a\u000a<pre><code>var googleAuth = new OAuth2('google', {\u000a  client_id: '17755888930840',\u000a  client_secret: 'b4a5741bd3d6de6ac591c7b0e279c9f',\u000a  api_scope: 'https://www.googleapis.com/auth/tasks'\u000a});\u000a\u000agoogleAuth.authorize(function() {\u000a  // Ready for action\u000a});\u000a</code></pre>\u000a\u000a<h4>Step 6: Use the access token</h4>\u000a\u000a<p>Now that your user has an access token via <code>auth.getAccessToken()</code>, you can\u000arequest protected data by adding the accessToken as a request header</p>\u000a\u000a<pre><code>xhr.setRequestHeader('Authorization', 'OAuth ' + myAuth.getAccessToken())\u000a</code></pre>\u000a\u000a<p>or by passing it as part of the URL (depending on the server implementation):</p>\u000a\u000a<pre><code>myUrl + '?oauth_token=' + myAuth.getAccessToken();\u000a</code></pre>\u000a\u000a<p><strong>Note</strong>: if you have multiple OAuth 2.0 endpoints that you would like to\u000aauthorize with, you can do that too! Just inject content scripts and add\u000apermissions for all of the providers you would like to authorize with.</p>\u000a\u000a<p>I've provided <a href="https://github.com/borismus/oauth2-extensions/tree/master/samples">some sample extensions</a> that use this library to help\u000ayou get started.</p>\u000a\u000a<h2>Varying OAuth implementations</h2>\u000a\u000a<p>Writing this library for one OAuth 2.0 endpoint was pretty straightforward.\u000aThe issues came when branching out to support multiple OAuth 2.0 server\u000aimplementations which comply to various degrees with differing versions of the\u000aspec.</p>\u000a\u000a<p>Facebook was the worst offender here. They claim to be an OAuth 2.0\u000aimplementation in line with v10, but are actually quite far from it. Here are\u000asome of the issues:</p>\u000a\u000a<ul>\u000a<li><p>Token request method is GET instead of POST.</p></li>\u000a<li><p>Token response is some strange form encoded format instead of JSON.</p></li>\u000a<li><p><a href="http://developers.facebook.com/docs/authentication/permissions/">List of scopes</a> (aka "extended permissions") was really hard to\u000afind.</p></li>\u000a<li><p>Apparently to get a user's favorite music, you need the <code>user_likes</code>\u000apermission. Facebook, please <a href="http://forum.developers.facebook.net/viewtopic.php?pid=283691">fix your docs</a>.</p></li>\u000a<li><p>No refresh tokens but they have an offline_access permission which makes your\u000aaccess token expire later. This is ridiculous!</p></li>\u000a</ul>\u000a\u000a<p>Twitter doesn't even have an OAuth 2.0 API. <a href="http://dev.twitter.com/anywhere">@Anywhere</a> does not\u000acount. Some good <a href="http://www.quora.com/Why-isnt-Twitter-implementing-OAuth-2-0-just-like-Facebooks">questions</a> on <a href="http://www.quora.com/When-is-Twitter-going-to-implement-OAuth-2-0">quora</a> about this.</p>\u000a\u000a<p>Still there are a lot of services that <em>DO</em> implement OAuth 2.0, such as\u000aFoursquare, Gowalla, Windows Live, Salesforce, Soundcloud and many others.</p>\u000a\u000a<h2>Extending the Library</h2>\u000a\u000a<p>To mitigate differences between OAuth 2.0 implementations, I implemented the\u000a<a href="http://en.wikipedia.org/wiki/Adapter_pattern">Adapter pattern</a>. Doing this encapsulates protocol differences\u000ain a separate adapter module for each server implementation.</p>\u000a\u000a<p>The library comes with adapters for Google, Facebook and Github. These adapters\u000aare located in the <a href="https://github.com/borismus/oauth2-extensions/tree/master/lib/adapters">adapters directory</a> here. If you would like to\u000acontribute your own adapter, please take a look at the sample adapter and then\u000a<a href="https://github.com/borismus/oauth2-extensions">fork the project</a>, submit a pull request, and I'll try to add\u000ait to the project.</p>\u000a\u000a<p>Also, please let me know if you experience problems using this library and\u000awe'll sort them out! The best way to do this is via <a href="http://github.com/borismus">github</a> or\u000a<a href="http://twitter.com/borismus">twitter</a>.</p>\u000a
p1668
tp1669
Rp1670
sg13
V/oauth2-chrome-extensions
p1671
sg15
Nsg16
I01
sg17
VOAuth 2.0 from chrome extensions
p1672
sg20
V\u000a\u000a\u000aApplications that access online services often need to access a user's private\u000adata.
p1673
sS'snip'
p1674
g7
(g8
g9
V<p>A JavaScript library that handles OAuth 2.0 for you, with a dead simple API. Comes with adapters for Google, Facebook and Github.</p>\u000a
p1675
tp1676
Rp1677
sg25
g169
sg33
g1671
sg170
(dp1678
g172
S'Jul'
p1679
sg174
S'July 7, 2011'
p1680
sg176
I7
sg177
S'2011-07-07T09:00:00-00:00'
p1681
sg179
I1310054400
sg180
I2011
sg181
I7
ssg65
g182
sg31
S'oauth2-chrome-extensions'
p1682
sS'categories'
p1683
(lp1684
S'web'
p1685
aS'google'
p1686
aS'chrome'
p1687
asS'posted'
p1688
g188
(S'\x07\xdb\x07\x07'
p1689
tp1690
Rp1691
ssg34
S'content/posts/2011/oauth2-chrome-extensions/index.md'
p1692
sg36
F1433825699.0
sa(dp1693
g2
(dp1694
g4
V An effective way of creating video screen captures in Chrome without relying on a plugin.  
p1695
sg28
g7
(g8
g9
V<p>This post is about video capture in Chrome that doesn't rely on any\u000aexternal dependencies like Flash (no fun), NPAPI (not supported on\u000aChrome OS) and Native Client (not <em>yet</em> supported on Chrome OS).</p>\u000a\u000a<p>I take screenshots all the time for bug reporting, image editing, etc.\u000aOn OS X, this functionality is conveniently built in, and available\u000athrough <code>Command</code> - <code>Shift</code> - <code>4</code>. As a web denizen, I find it very\u000auseful to auto-upload these captures to a remote server, so I wrote this\u000a<a href="https://github.com/borismus/screencapture-www">minimal image uploader</a> which replaces the default behavior\u000aon OS X to capture the screenshot, and also uploads it to a picture\u000ahosting service.</p>\u000a\u000a<p>Taking video capture of various UIs is also immensely useful for showing\u000ademonstrations, complex interactions, and subtle bugs. I recently\u000are-discovered that QuickTime on OS X comes with this functionality built\u000ain. Prior to that I used (paid) ScreenFlow, which also has very nice\u000adimension cropping and time dilation features.</p>\u000a\u000a<p>What if we're on a web-only device, such as a Chromebook running Chrome\u000aOS? There is a still screenshotting API, but capturing video is less\u000atrivial. I've released an extension that captures and play backs video\u000acaptures inside Chrome, and also lets you share stills to Picasa (using\u000athe OAuth 2 <a href="http://smus.com/oauth2-chrome-extensions">extension library</a>). It's available on the\u000a<a href="https://chrome.google.com/webstore/detail/omahgjnmfgeeeoekegajhndkncocoofd">webstore</a>, and the source is on <a href="https://github.com/borismus/chrome-screencast">github</a>. Read on to\u000alearn how it works, and see how you can help.</p>\u000a\u000a<h1>Screenshots in Chrome</h1>\u000a\u000a<p>Chrome provides the <a href="http://code.google.com/chrome/extensions/tabs.html#method-captureVisibleTab"><code>captureVisibleTab</code></a> extension API for taking\u000aa screenshot of a tab. It requires host permissions on the page, but as\u000ausual the <all_urls> permission will enable the API across all pages\u000a(with some exceptions). A few successful extensions, such as\u000a<a href="http://awesomescreenshot.com/">Awesome Screenshot</a>, use this API and allow\u000acropping, annotation and sharing of screen grabs.</p>\u000a\u000a<p>What if you want to capture video of a tab? Chrome provides no\u000apre-existing API for this purpose, however, we can piggyback on the\u000astill screenshot API, executing it repeatedly from the background page\u000afor every frame we want to capture:</p>\u000a\u000a<pre><code>var images = [];\u000avar FPS = 30;\u000avar QUALITY = 50;\u000atimer = setInterval(function() {\u000a  chrome.tabs.captureVisibleTab(null, {quality: QUALITY},\u000a    function(img) {\u000a      images.push(img);\u000a    });\u000a}, 1000 / FPS);\u000a</code></pre>\u000a\u000a<p>As we capture, we store the base64-encoded strings representing video\u000aframes in an array. Once we're done capturing, we can simulate video\u000aplayback by rapidly swapping the images in and out:</p>\u000a\u000a<pre><code>var background = chrome.extension.getBackgroundPage();\u000atimer = setInterval(function() {\u000a  if (currentIndex &gt;= images.length - 1) {\u000a    pause();\u000a    return;\u000a  }\u000a  setIndex(currentIndex + 1);\u000a  updateSliderPosition();\u000a}, 1000 / background.FPS);\u000a</code></pre>\u000a\u000a<p>This approach turns out to be surprisingly efficient, with the extension\u000abeing able to capture at 30 FPS on a MacBook Air, and 10 FPS on a\u000aChromebook without too much noticeable slowdown.</p>\u000a\u000a<p>Note that we rely on a fixed FPS for ease of implementation, however one\u000acould imagine using <code>requestAnimationFrame</code> and tracking the variable\u000aframe rate so that the playback speed is reasonable. However, there are\u000adefinitely precision issues with JavaScript's timers, so this is a much\u000amore challenging approach.</p>\u000a\u000a<p>So we can capture and playback videos inside the browser, but getting it\u000aout of the browser is another matter entirely. As a temporary measure,\u000amy colleague <a href="http://greenido.wordpress.com">Ido Green</a> built a screen stitching service which\u000aencodes multiple images into a movie using ffmpeg. Ideally, of course,\u000awe would encode in the browser. Perhaps a JavaScript video encoder could\u000abe implemented, though the performance may be too poor for practical\u000ause. Alternatively, a ffmpeg Native Client-based approach might be\u000asuitable, especially given that ffmpeg has <a href="http://code.google.com/p/naclports/source/browse/trunk/src/libraries/ffmpeg-0.5/">already been ported</a>.</p>\u000a\u000a<h1>Free ideas</h1>\u000a\u000a<p>There are a few logical next steps for this sample. As already\u000amentioned, encoding video in the browser is a top priority, but there\u000aare a slew of other interesting directions, some of which can be seen as\u000afeatures, and others as separate products.</p>\u000a\u000a<ul>\u000a<li><p>The <code>captureVisibleTab</code> API doesn't track the mouse cursor. This could\u000abe done by injecting an overlay onto the current page and tracking\u000amousemove and click events. This data could then either be drawn onto\u000aa canvas context, or encoded separately as <code>mouseData</code>, and then drawn\u000awith JavaScript at playback time.</p></li>\u000a<li><p>Cropping the video dimensions, modifying the video time schedule\u000a(speedup, slowdown, truncation) and annotation are all desired\u000avideo-editing class features that could be implemented by treating\u000aimages as canvases.</p></li>\u000a<li><p>A compelling use case for this technology is creating screen sharing\u000asessions for demos and presentations. Thus, it would be very useful to\u000astream the video to a server, and broadcast it to multiple clients in\u000areal time. <a href="http://updates.html5rocks.com/2011/08/What-s-different-in-the-new-WebSocket-protocol">Binary websockets</a> are now available in Chrome,\u000aand this could be a great application.</p></li>\u000a<li><p>Audio annotations on screen captures make perfect sense, and are\u000awidely supported by desktop applications. APIs for sound capture have\u000abeen a <a href="http://www.w3.org/2009/dap/">long time coming</a>, but finally we may have an answer\u000avia the <a href="http://www.webrtc.org/">WebRTC</a> ecosystem, and the <code>getUserMedia</code> call.</p></li>\u000a</ul>\u000a\u000a<p>By the way, I've switched to exclusively using Markdown for all of my\u000apublished writing, and wrote an <a href="https://github.com/borismus/markdown-preview">markdown preview</a> for Chrome\u000ato make my life a bit easier.</p>\u000a
p1696
tp1697
Rp1698
sg13
V/screen-capture-for-chrome-os
p1699
sg15
Nsg16
I01
sg17
VScreen video capture for Chrome OS
p1700
sg20
V\u000a\u000a\u000aThis post is about video capture in Chrome that doesn't rely on any\u000aexternal dependencies like Flash (no fun), NPAPI (not supported on\u000aChrome OS) and Native Client (not *yet* supported on Chrome OS).
p1701
sS'snip'
p1702
g7
(g8
g9
V<p>An effective way of creating video screen captures in Chrome without relying on a plugin.</p>\u000a
p1703
tp1704
Rp1705
sg25
g169
sg33
g1699
sg170
(dp1706
g172
S'Oct'
p1707
sg174
S'October 4, 2011'
p1708
sg176
I10
sg177
S'2011-10-04T09:00:00-00:00'
p1709
sg179
I1317744000
sg180
I2011
sg181
I4
ssg65
g182
sg31
S'screen-capture-for-chrome-os'
p1710
sS'categories'
p1711
(lp1712
S'web'
p1713
aS'google'
p1714
aS'chrome'
p1715
asS'posted'
p1716
g188
(S'\x07\xdb\n\x04'
p1717
tp1718
Rp1719
ssg34
S'content/posts/2011/screen-capture-for-chrome-os/index.md'
p1720
sg36
F1332684374.0
sa(dp1721
g2
(dp1722
g4
V I've been a member of the Chrome developer relations team for the last 9 months, where one aspect of my job is developer support. I actively monitor the chromium-extensions@chromium.org list, answering questions about developing Chrome extensions. Thankfully, there are many others that help me in this endeavor, both from the Chrome team and from within the developer community itself, notably a certain  PhistucK , the current record holder for the number of messages posted to the list.    Over the last years, another channel of questions has emerged:  Stack Overflow (SO) . SO is a great Q&amp;A community for software developers, providing many advantages over ordinary discussion groups. Google has recognized this and sponsored SO tags, such as  google-chrome , and  google-chrome-extension . As I outlined in an mail to the group, we are now including SO as an official Chrome extension developer support channel.   
p1723
sg28
g7
(g8
g9
V<p>I've been a member of the Chrome developer relations team for the last 9\u000amonths, where one aspect of my job is developer support. I actively\u000amonitor the chromium-extensions@chromium.org list, answering questions\u000aabout developing Chrome extensions. Thankfully, there are many others that\u000ahelp me in this endeavor, both from the Chrome team and from within the\u000adeveloper community itself, notably a certain <a href="http://goo.gl/di3kR">PhistucK</a>, the current\u000arecord holder for the number of messages posted to the list.</p>\u000a\u000a<p>Over the last years, another channel of questions has emerged:\u000a<a href="http://stackoverflow.com/">Stack Overflow (SO)</a>. SO is a great Q&amp;A community for\u000asoftware developers, providing many advantages over ordinary discussion\u000agroups. Google has recognized this and sponsored SO tags,\u000asuch as <a href="http://stackoverflow.com/questions/tagged/google-chrome">google-chrome</a>, and <a href="http://stackoverflow.com/questions/tagged/google-chrome-extension">google-chrome-extension</a>. As I\u000aoutlined in an mail to the group, we are now including SO as\u000aan official Chrome extension developer support channel.</p>\u000a\u000a<!--more-->\u000a\u000a<p>As part of optimizing the support workflow, I wrote a Chrome extension\u000awhich monitors questions with certain tags on SO and other Stack\u000aExchange sites. This extension implements a browser action which gets\u000abadged with the number of unreviewed questions.</p>\u000a\u000a<p>The extension UI uses the same visual style as other Google apps, and\u000ablends well into Chrome. I think it works reasonably well from a design\u000aperspective, both in the options page and in a browser action:</p>\u000a\u000a<p><img src="stack-screenshot.png" alt="screenshot" /></p>\u000a\u000a<p>If you're curious to see how the extension is implemented, the source\u000a<a href="https://github.com/borismus/Question-Monitor-for-Stack-Exchange">is available</a> free of charge! This extension was implemented in\u000a<a href="http://code.google.com/closure/">Closure-style</a> JavaScript.</p>\u000a\u000a<p>If you support developers on Stack Overflow, please <a href="https://chrome.google.com/webstore/detail/bnnkhapbhkejookmhgpgaikfdoegkmdp">install it</a>,\u000agive me constructive feedback and consider monitoring the\u000a<a href="http://stackoverflow.com/questions/tagged/google-chrome-extension">google-chrome-extension</a> tag :)</p>\u000a
p1724
tp1725
Rp1726
sg13
V/stack-exchange-question-notifier
p1727
sg15
Nsg16
I01
sg17
VStack Exchange question notifier
p1728
sg20
V\u000aI've been a member of the Chrome developer relations team for the last 9\u000amonths, where one aspect of my job is developer support.
p1729
sg6
V<p>I've been a member of the Chrome developer relations team for the last 9\u000amonths, where one aspect of my job is developer support. I actively\u000amonitor the chromium-extensions@chromium.org list, answering questions\u000aabout developing Chrome extensions. Thankfully, there are many others that\u000ahelp me in this endeavor, both from the Chrome team and from within the\u000adeveloper community itself, notably a certain <a href="http://goo.gl/di3kR">PhistucK</a>, the current\u000arecord holder for the number of messages posted to the list.</p>\u000a\u000a<p>Over the last years, another channel of questions has emerged:\u000a<a href="http://stackoverflow.com/">Stack Overflow (SO)</a>. SO is a great Q&amp;A community for\u000asoftware developers, providing many advantages over ordinary discussion\u000agroups. Google has recognized this and sponsored SO tags,\u000asuch as <a href="http://stackoverflow.com/questions/tagged/google-chrome">google-chrome</a>, and <a href="http://stackoverflow.com/questions/tagged/google-chrome-extension">google-chrome-extension</a>. As I\u000aoutlined in an mail to the group, we are now including SO as\u000aan official Chrome extension developer support channel.</p>\u000a\u000a
p1730
sg25
g169
sg33
g1727
sg170
(dp1731
g172
S'Nov'
p1732
sg174
S'November 15, 2011'
p1733
sg176
I11
sg177
S'2011-11-15T09:00:00-00:00'
p1734
sg179
I1321376400
sg180
I2011
sg181
I15
ssg65
g182
sg31
S'stack-exchange-question-notifier'
p1735
sS'categories'
p1736
(lp1737
S'web'
p1738
aS'chrome'
p1739
asS'posted'
p1740
g188
(S'\x07\xdb\x0b\x0f'
p1741
tp1742
Rp1743
ssg34
S'content/posts/2011/stack-exchange-question-notifier/index.md'
p1744
sg36
F1433825711.0
sa(dp1745
g2
(dp1746
g4
V My excellent adventures organizing a hackathon-party at South by Southwest.  
p1747
sg28
g7
(g8
g9
V<p>I just returned to San Francisco after having a blast at my second SXSW.\u000aLast time I attended, two years ago, I spent most of my time attending\u000asessions and networking. This year I was mostly on the other side:\u000apreparing talks, staffing booths and organizing events. This year's\u000ahighlight was definitely the LEGO MINDSTORMS hackathon, held as part of\u000a<a href="http://extraordinary-hackers.appspot.com/hackathon.html">The League of Extraordinary Hackers</a> event put on by Google and\u000aothers, that I helped Chris Messina organize. We modeled the event after\u000a<a href="http://breadpig.com/2011/01/18/hack-club-1-total-success/">Breadpig's Hack Club</a>, but scaled it up. The event was so awesome\u000athat here I am blogging about it! </p>\u000a\u000a<p><a href="hackathon.jpg"><img src="hackathon-small.jpg" alt="hackathon image" /></a></p>\u000a\u000a<p>At 7pm, 12 teams of 3-6 people began building MINDSTORMS sumo robots\u000awhich would then compete in a double elimination tournament. We had\u000amixed skill levels, from complete novices to people working at National\u000aInstruments, the company that builds the LEGO MINDSTORMS software. Teams\u000awere given 4 hours to build their robots out of two complete NXT sets\u000aand additional LEGO parts. During battle, their autonomous robots were\u000aplaced back to back, starting with a 180 degree turn before trying to\u000apush each other out of the ring. The battles started at 11:30pm, and\u000awent on for about an hour.</p>\u000a\u000a<iframe title="YouTube video player" width="600" height="480"\u000a  src="http://www.youtube.com/embed/-oELrCuDrrE" frameborder="0"\u000a  allowfullscreen></iframe>\u000a\u000a<p>We had a great turnout of spectators and builders on the third floor of\u000athe <a href="http://www.speakeasyaustin.com/">Speakeasy</a>. The crowd was engaged the whole time, thanks to the\u000alively narration of MC <a href="http://twitter.com/tielure">@tielure</a>, which is quite surprising, given\u000athe amount of party hopping that usually ensues during SXSW. Here's the\u000afinal round (Tiger Blood vs. Beam Me Up):</p>\u000a\u000a<iframe title="YouTube video player" width="600" height="480"\u000a  src="http://www.youtube.com/embed/3EJD6M0zDbw" frameborder="0"\u000a  allowfullscreen></iframe>\u000a\u000a<p>At the end of the day, we made hundreds of geeks happy, and donated\u000asomething on the order of $15,000 of LEGOs to charity. I got to work\u000awith tons of awesome people, including <a href="http://twitter.com/chrismessina">@chrismessina</a>, <a href="http://twitter.com/kcpike">@kcpike</a>,\u000a<a href="http://twitter.com/StevenCanvin">@StevenCanvin</a>, <a href="http://twitter.com/Chrysaora">@Chrysaora</a> and many others. The event even got\u000asome good press on <a href="http://mashable.com/2011/03/13/league-of-extraordinary-hackers/">mashable</a>. </p>\u000a\u000a<p>My only regret is not having my real camera with me, so if you have high\u000aquality footage of the event that I haven't included in this post,\u000aplease let me know. Looking forward to next time!</p>\u000a
p1748
tp1749
Rp1750
sg13
V/sxsw-mindstorms-hackathon
p1751
sg15
Nsg16
I01
sg17
VSXSW LEGO MINDSTORMS hackathon
p1752
sg20
V\u000a\u000aI just returned to San Francisco after having a blast at my second SXSW.
p1753
sS'snip'
p1754
g7
(g8
g9
V<p>My excellent adventures organizing a hackathon-party at South by Southwest.</p>\u000a
p1755
tp1756
Rp1757
sg25
g169
sg33
g1751
sg170
(dp1758
g172
S'Mar'
p1759
sg174
S'March 17, 2011'
p1760
sg176
I3
sg177
S'2011-03-17T09:00:00-00:00'
p1761
sg179
I1300377600
sg180
I2011
sg181
I17
ssg65
g182
sg31
S'sxsw-mindstorms-hackathon'
p1762
sS'categories'
p1763
(lp1764
S'conference'
p1765
aS'physical'
p1766
asS'posted'
p1767
g188
(S'\x07\xdb\x03\x11'
p1768
tp1769
Rp1770
ssg34
S'content/posts/2011/sxsw-mindstorms-hackathon/index.md'
p1771
sg36
F1433825721.0
sa(dp1772
g2
(dp1773
g4
V I went to UIST 2011 in Santa Barbara and presented our research on  CrowdForge .    Here's a sample of some of the great work that was presented this year, in the 3 research areas that interest me most: crowdsourcing/human computation, mobile physical computing, and music.
p1774
sg28
g7
(g8
g9
V<p>I went to UIST 2011 in Santa Barbara and presented our research on\u000a<a href="http://crowdforge.com">CrowdForge</a>.</p>\u000a\u000a<p>Here's a sample of some of the great work that was presented this year, in the\u000a3 research areas that interest me most: crowdsourcing/human computation,\u000amobile physical computing, and music.<!--more--></p>\u000a\u000a<h1>Crowdsourcing</h1>\u000a\u000a<p>Notable work in both workflow-oriented approaches (Jabberwocky, CrowdForge,\u000aPlateMate) and synchronous collaborative approaches (Crowds in seconds,\u000aCollabode, Real-time).</p>\u000a\u000a<h3>PlateMate</h3>\u000a\u000a<ul>\u000a<li>Presents a workflow-based crowdsourcing nutritional analysis from food\u000aphotographs.</li>\u000a<li>Implemented in Django/Python, same as CrowdForge</li>\u000a<li>Found out about CrowdForge 90% of the way into the research.</li>\u000a</ul>\u000a\u000a<p><img src="http://crowdresearch.org/blog/wp-content/uploads/2011/09/dinner.jpg" alt="platemate" /></p>\u000a\u000a<h3>Real-time crowd control</h3>\u000a\u000a<ul>\u000a<li>Nice research from Jeff Bigham's group.</li>\u000a<li>Compares different strategies of merging input from different users.</li>\u000a<li>Reminded me of my <a href="/android-powered-mindstorms">twitter mindstorms project</a>.</li>\u000a</ul>\u000a\u000a<iframe width="560" height="315" src="http://www.youtube.com/embed/P_Tqn-3BF_I" frameborder="0" allowfullscreen></iframe>\u000a\u000a<h3>Crowds in two seconds</h3>\u000a\u000a<ul>\u000a<li>Great research from MIT crowdsourcing folks (Michael Bernstein, Rob Miller)</li>\u000a<li>Retainer: keep workers "on tap" - ready to work by paying $0.30/hour.</li>\u000a<li>Rapid refinement: crowd algorithm to narrow search space to accelerate.</li>\u000a<li>Cool application: take movies, crowdsource the best moment.</li>\u000a</ul>\u000a\u000a<iframe width="560" height="315" src="http://www.youtube.com/embed/9IICXFUP6MM" frameborder="0" allowfullscreen></iframe>\u000a\u000a<h3>The Jabberwocky programming environment</h3>\u000a\u000a<ul>\u000a<li>Really ambitious MTurk like project.</li>\u000a<li>Combine different worker types from different spheres (social groups, paid workers, machines)</li>\u000a<li>Full runtime stack (Dog high level language, ManReduce low level language, Dormouse runtime)</li>\u000a<li>Great potential to mix different spheres of workers, but a rather intimidating project... scope creep!</li>\u000a</ul>\u000a\u000a<p>More in the <a href="http://kamvar.org/assets/papers/jabberwocky.pdf">Jabberwocky</a> paper.</p>\u000a\u000a<h3>Real-time collaborative coding in a web IDE</h3>\u000a\u000a<ul>\u000a<li>More awesome work from MIT (Max Goldman, Greg Little, Rob Miller)</li>\u000a<li>Web-based Java EtherPad editor for collaboration</li>\u000a<li>Main problem: others leave code in semi-working state. When to sync?</li>\u000a<li>Idea: automatic error-aware integration. Auto-sync code when it compiles (or - tests pass)</li>\u000a</ul>\u000a\u000a<p>More on the <a href="http://groups.csail.mit.edu/uid/collabode/">Collabode</a> site.</p>\u000a\u000a<h1>Mobile physical computing</h1>\u000a\u000a<p>Interesting work in mobile virtual and projected UI spaces.</p>\u000a\u000a<h3>PocketTouch</h3>\u000a\u000a<ul>\u000a<li>Great work from Microsoft Research to let you use your phone through fabric (while in the pocket)</li>\u000a<li>Since orientation in-pocket is unclear, there's an orientation setting gesture (not sold on this)</li>\u000a<li>Unfortunately typical front pocket jeans work rather poorly.</li>\u000a</ul>\u000a\u000a<iframe width="560" height="315" src="http://www.youtube.com/embed/fHSDpE0kTag" frameborder="0" allowfullscreen></iframe>\u000a\u000a<h3>Multi-user Interaction with Handheld Projectors</h3>\u000a\u000a<ul>\u000a<li>Most projector-based systems require a fixed place. This one is fully mobile.</li>\u000a<li>System produces visible projection (for image) and invisible projection (for data)</li>\u000a<li>Camera tracks location of nearby projections</li>\u000a<li>Cool applications: virtual boxing, content transfer, 3D model viewing.</li>\u000a</ul>\u000a\u000a<p><img src="http://www.disneyresearch.com/_images/459-00B516CF.jpg" alt="concept" /></p>\u000a\u000a<p>To get a better sense of the project, take a look at <a href="http://www.disneyresearch.com/research/projects/hci_sidebyside_drp.htm">this video</a>.</p>\u000a\u000a<h3>Imaginary phone</h3>\u000a\u000a<ul>\u000a<li>Lets you control a mobile phone device without taking it out of your pocket.</li>\u000a<li>Maps iPhone controls to the palm of your hand similar to <a href="http://en.wikipedia.org/wiki/Guidonian_hand">Guidonian hand</a></li>\u000a<li>Touchscreen progression: fingers replaced styli. Next step: palm replaces phone (bit of a stretch)</li>\u000a</ul>\u000a\u000a<iframe width="560" height="315" src="http://www.youtube.com/embed/aCARtauIS50" frameborder="0" allowfullscreen></iframe>\u000a\u000a<h3>OmniTouch</h3>\u000a\u000a<ul>\u000a<li>Shoulder mounted depth camera/projector.</li>\u000a<li>Works for all sorts of surfaces. Heavy math for tracking and projecting.</li>\u000a<li>Tracks "hover" (haven't seen this notion before) and "touch" states.</li>\u000a</ul>\u000a\u000a<iframe width="560" height="315" src="http://www.youtube.com/embed/Pz17lbjOFn8" frameborder="0" allowfullscreen></iframe>\u000a\u000a<h1>Music</h1>\u000a\u000a<p>I had the pleasure of hearing <a href="https://ccrma.stanford.edu/~ge/">Ge Wang</a> speak\u000aabout some of his older projects, including <a href="http://www.youtube.com/watch?v=RhCJq7EAJJA">Ocarina</a>,\u000a<a href="http://www.youtube.com/watch?v=KetWJi0zou0">Leaf Trombone</a>, the Stanford Mobile orchestra, and others. Notably I\u000ahadn't really seen the <a href="http://chuck.cs.princeton.edu/doc/language/">ChucK language in action</a>, and would be\u000ainterested in seeing if the <a href="http://www.html5rocks.com/en/tutorials/webaudio/intro/">Web Audio API</a> could support this sort of thing.\u000aBrowser-based ChucK, anyone?</p>\u000a\u000a<p>The last piece of research that I really enjoyed was called "onNote: Playing\u000aprinted music scores as a musical instrument". The idea here was to use OMR\u000atechniques for markerless tracking of sheet music using positioning and using a\u000afinger for pointing. The other neat application here was to support\u000acompositional remixing by literally cutting up sheet music and splicing it back\u000atogether.</p>\u000a\u000a<iframe width="420" height="315" src="http://www.youtube.com/embed/fGOk16Cnq7c" frameborder="0" allowfullscreen></iframe>\u000a\u000a<p>This was my first UIST, and though I'm unlikely to have new research to submit\u000afor UIST 2012, I'm seriously considering going anyway, just to stay on top of\u000athe great work that this vibrant community generates.</p>\u000a
p1775
tp1776
Rp1777
sg13
V/uist-2011
p1778
sg15
Nsg16
I01
sg17
VUIST 2011 greatest hits
p1779
sg20
V\u000a\u000aI went to UIST 2011 in Santa Barbara and presented our research on\u000a[CrowdForge][crowdforge].
p1780
sg6
V<p>I went to UIST 2011 in Santa Barbara and presented our research on\u000a<a href="http://crowdforge.com">CrowdForge</a>.</p>\u000a\u000a<p>Here's a sample of some of the great work that was presented this year, in the\u000a3 research areas that interest me most: crowdsourcing/human computation,\u000amobile physical computing, and music.
p1781
sg25
g169
sg33
g1778
sg170
(dp1782
g172
S'Oct'
p1783
sg174
S'October 20, 2011'
p1784
sg176
I10
sg177
S'2011-10-20T09:00:00-00:00'
p1785
sg179
I1319126400
sg180
I2011
sg181
I20
ssg65
g182
sg31
S'uist-2011'
p1786
sS'categories'
p1787
(lp1788
S'research'
p1789
aS'conference'
p1790
asS'posted'
p1791
g188
(S'\x07\xdb\n\x14'
p1792
tp1793
Rp1794
ssg34
S'content/posts/2011/uist-2011/index.md'
p1795
sg36
F1433825725.0
sa(dp1796
g2
(dp1797
g4
V or, how I stopped worrying and switched this blog to a Django static site generator.  
p1798
sg28
g7
(g8
g9
V<p>I've used <a href="http://wordpress.com/">Wordpress</a> for a couple of years now as my web publishing platform of\u000achoice. I customized it a little bit, and made a <a href="/new-design">custom theme</a> for it. But \u000anothing is perfect, and neither is wordpress. As a result of relatively heavy\u000ause, I've collected a list of things I don't much like about it:</p>\u000a\u000a<ul>\u000a<li>Can't upload source code since it restricts allowed file types.</li>\u000a<li>Can't easily edit files with a regular text editor.</li>\u000a<li>Painful theme customization.</li>\u000a<li>PHP makes it unpleasant to extend.</li>\u000a<li>Constant security upgrades required.</li>\u000a</ul>\u000a\u000a<p>All of these things separately are perhaps small and insignificant, but\u000atogether add up to create a certain barrier to blogging.</p>\u000a\u000a<h2>Static files</h2>\u000a\u000a<p>A blog is mostly static. Why should my article be stored in a database? It's hardly \u000aever queried or searched. Comments are perhaps the exception here, but services\u000alike <a href="http://disqus.com/">Disqus</a> implement commenting with embeddable JavaScript. And in general,\u000adynamic parts can be handled in client-side code.</p>\u000a\u000a<p>So I looked to static site generators. There's about <a href="http://stackoverflow.com/questions/186290/best-static-website-generator">a million</a> of these,\u000athe most prominent of which is <a href="https://github.com/mojombo/jekyll">Jekyll</a>, written by the github co-founder. \u000aUltimately I chose <a href="https://github.com/lakshmivyas/hyde/">Hyde</a> for it's use of <a href="http://docs.djangoproject.com/en/dev/ref/templates/builtins/">Django templates</a> and Python.</p>\u000a\u000a<h2>Migrating the old blog</h2>\u000a\u000a<p>The old blog was written in HTML, but I wanted to switch to a format that can\u000abe written faster, such as Markdown, Textile, or many others. To do this I used\u000aa tool called exitwp, which parses the Wordpress export and generates Markdown\u000afiles appropriate for Jekyll. I <a href="https://github.com/borismus/exitwp">forked exitwp</a> and hacked it to generate\u000afiles better suited to Hyde. Thanks <a href="http://thomas.jossystem.se/">Thomas Frssman</a> for exitwp.</p>\u000a\u000a<p>I did spent a fair amount of time tweaking the export, replacing Smart YouTube URLs\u000awith real YouTube embeds, and eliminating Syntax Highlighter markup.</p>\u000a\u000a<h2>Customizing hyde</h2>\u000a\u000a<p>Hyde is basically built on top of Django templates, so is customizable with\u000atemplate tags and filters, written in Python. It comes with a bunch of them,\u000aincluding ones that parse markdown and other structured text formats into HTML.</p>\u000a\u000a<p>Surprisingly I didn't need to customize Hyde much, although I did fix a couple of \u000abugs, which the author, <a href="http://ringce.com/">Lakshmi Vyas</a> accepted. Anyway, I feel much more\u000afuture proof now than with Wordpress, since I can easily write extensions in\u000aPython, or in the worst case take my Markdown-formatted blog posts and easily\u000aexport them to many different formats.</p>\u000a\u000a<h2>Dynamic stuff</h2>\u000a\u000a<p>The bit of custom JS for the site layout is written using jQuery.</p>\u000a\u000a<p>I use the excellent <a href="http://softwaremaniacs.org/soft/highlight/en/">highlight.js</a> plugin to highlight code snippets. The\u000ajQuery <a href="http://timeago.yarp.com/">timeago</a> plugin does a great job of converting absolute dates (April\u000a1, 2010) to relative dates (about a year ago). Finally, Disqus powers comments,\u000awhich I migrated from the wordpress database.</p>\u000a\u000a<h2>New design</h2>\u000a\u000a<p>In my great wisdom, I decided that since there's so much flux in the blog, why not\u000aadd some more chaos by switching to a new layout too? Here's the old layout:</p>\u000a\u000a<p><img src="old-design.png" alt="old" /></p>\u000a\u000a<p>I recently heard from sage advice from some experienced writers. Among things\u000athat stuck were:</p>\u000a\u000a<ul>\u000a<li>Optimal posting time is 9am PST</li>\u000a<li>Writing personal posts is OK sometimes</li>\u000a<li>Show personal information in the sidebar</li>\u000a</ul>\u000a\u000a<p>The last part is expressed in the new design. Overall the redesign was an\u000aexercise in typography, CSS3 features and <a href="http://www.alistapart.com/articles/responsive-web-design/">responsive layout</a>. </p>\u000a\u000a<p><a href="http://www.google.com/webfonts">Google fonts</a> is a nice collection of web fonts. I decided to switch from\u000aMyriad (used on borismus.com) to a serif for the main body for a\u000achange. I ultimately went with <a href="http://www.google.com/webfonts/list?family=PT+Serif">PT Serif</a>, which seemed like a nice\u000aimprovement to Georgia, which is probably my favorite serif web font.</p>\u000a\u000a<p>I used a bunch (too many?) CSS3 features on the new site. Most of these are\u000aCSS box-shadows, gradients, transformations and transitions. I used <a href="http://sass-lang.com/">SCSS</a>.</p>\u000a\u000a<p>Also, following the responsive layout philosophy, I wanted the site to scale well\u000ato a number of different resolutions. For example, the sidebar moves to the end of the \u000aarticles if the window is too narrow. Also, in the <a href="/projects">projects page</a>, the number of \u000acolumns of projects is flexible. This is achieved through <a href="http://www.w3.org/TR/css3-mediaqueries/">CSS media queries</a>.</p>\u000a\u000a<p>For posterity, here is the new layout at the time of writing:</p>\u000a\u000a<p><img src="new-design.png" alt="new" /></p>\u000a\u000a<h2>New domain</h2>\u000a\u000a<p>I also recently bought <a href="http://smus.com">smus.com</a> from a fellow in Germany, and will be migrating \u000amy site there. I'll keep the <a href="http://borismus.com">borismus.com</a> blog intact for a while, but will \u000aplace an annoying banner there, and disable commenting. I guess I should also \u000aswitch the feedburner feed to point to smus.com as well.</p>\u000a\u000a<p>Not long ago, I wrote about how switching from webfaction and apache to slicehost\u000aand NGINX greatly <a href="/lightweight-wordpress-on-slicehost/">enhanced page load time</a>. Well, this seems to have happened again.\u000aOver the same time period, the average response time of my wordpress instance was \u000a<strong>934ms</strong>:</p>\u000a\u000a<p><img src="old-perf.png" alt="old perf" /></p>\u000a\u000a<p>Over the same time frame, my static blog, hosted on the same machine, responded on \u000aaverage in <strong>371ms</strong>:</p>\u000a\u000a<p><img src="new-perf.png" alt="new perf" /></p>\u000a\u000a<p>Oh, finally, the site is fully open source <a href="http://github.com/borismus/smus.com">on github</a>. </p>\u000a\u000a<h2>Thanks</h2>\u000a\u000a<p>Thanks to a bunch of people:</p>\u000a\u000a<ul>\u000a<li>Sol, Kat and Jon for giving useful design feedback!</li>\u000a<li><a href="http://nlevin.com/">Noah Levin</a> for awesome CSS3 tweaks</li>\u000a<li>Steve Losh for his <a href="http://stevelosh.com/blog/2010/01/moving-from-django-to-hyde/">Hyde migration</a> write-up</li>\u000a</ul>\u000a\u000a<p>You, for reading this post and continuing to read this blog. Until next time!</p>\u000a
p1799
tp1800
Rp1801
sg13
V/wordpress-to-hyde
p1802
sg15
Nsg16
I01
sg17
VFrom Wordpress to Hyde
p1803
sg20
V\u000a\u000a\u000aI've used [Wordpress][] for a couple of years now as my web publishing platform of\u000achoice.
p1804
sS'snip'
p1805
g7
(g8
g9
V<p>or, how I stopped worrying and switched this blog to a Django static site generator.</p>\u000a
p1806
tp1807
Rp1808
sg25
g169
sg33
g1802
sg170
(dp1809
g172
S'Apr'
p1810
sg174
S'April 20, 2011'
p1811
sg176
I4
sg177
S'2011-04-20T09:00:00-00:00'
p1812
sg179
I1303315200
sg180
I2011
sg181
I20
ssg65
g182
sg31
S'wordpress-to-hyde'
p1813
sS'categories'
p1814
(lp1815
S'web'
p1816
asS'posted'
p1817
g188
(S'\x07\xdb\x04\x14'
p1818
tp1819
Rp1820
ssg34
S'content/posts/2011/wordpress-to-hyde/index.md'
p1821
sg36
F1332684374.0
sa(dp1822
g2
(dp1823
g4
V When developing in a particular environment, say Android or Cocoa, we are subconsciously aware that the APIs are essentially fixed and beyond our control. There is no effective mechanism to go and tell Apple that some method is poorly named, or tell the Android team how much you wish the Audio APIs were nicer to use.    The web, however, is built by many different companies and individuals, giving consumers of the platform (web developers) a unique chance to also become contributors to its evolution. Rather than griping about how broken something on the web is, remember that you can play a part in fixing it!   
p1824
sg28
g7
(g8
g9
V<p>When developing in a particular environment, say Android or Cocoa, we\u000aare subconsciously aware that the APIs are essentially fixed and beyond\u000aour control. There is no effective mechanism to go and tell Apple that\u000asome method is poorly named, or tell the Android team how much you wish\u000athe Audio APIs were nicer to use.</p>\u000a\u000a<p>The web, however, is built by many different companies and individuals,\u000agiving consumers of the platform (web developers) a unique chance to\u000aalso become contributors to its evolution. Rather than griping about how\u000abroken something on the web is, remember that you can play a part in\u000afixing it!</p>\u000a\u000a<!--more-->\u000a\u000a<h2>Current flow: "top down"</h2>\u000a\u000a<p>Today, features get added to the web platform as a result of competition\u000abetween browsers and some standardization work. The web developer has\u000alittle say in the process until perhaps the very end. Worse, many spec\u000aauthors and browser engineers are not web developers, so they have no\u000aintuition or experience as to what the web platform actually needs.\u000aHere's how it works today:</p>\u000a\u000a<ol>\u000a<li>Browser vendor proposes way for solving a problem and implements it\u000ain their browser. (browser vendor)</li>\u000a<li>In some cases it's concurrently introduced as a W3C specification.\u000a(spec author)</li>\u000a<li>Other browser vendors implement the feature. (browser vendor)</li>\u000a<li>Much later, someone implements a JS wrapper on top of feature to make\u000athe feature practical for web developers to use. (web developer)</li>\u000a</ol>\u000a\u000a<p>Some of the negative features of this flow are:</p>\u000a\u000a<ul>\u000a<li>Takes a long time before any developer feedback is received.</li>\u000a<li>New features have partial support for a very long time.</li>\u000a<li>Innovation is very browser vendor/spec writer centric.</li>\u000a<li>Very slow cycle overall.</li>\u000a<li>Little room for incremental improvements.</li>\u000a</ul>\u000a\u000a<h2>Ideal flow "forward polyfills"</h2>\u000a\u000a<p>Rather than sticking to the top down flow, web developers can be\u000aincluded in the loop. While forming organizations like\u000a<a href="http://www.w3.org/community/coremob/">CoreMob</a> might be a good first step, we can have a more direct\u000aimpact as well. The following alternative flow can work pretty well in\u000aconjunction with the status quo, described above.</p>\u000a\u000a<p>The basic idea is to hit the ground running with a prototype developed\u000ain JavaScript, get early feedback and then propose to W3C when you are\u000aready.</p>\u000a\u000a<ol>\u000a<li>Propose a sane API that developers can use &amp; implement it on top of\u000aexisting API(s). (web developer)</li>\u000a<li>Solicit adoption and iterate on feedback. (web developer)</li>\u000a<li>Push through standardization. (spec author)</li>\u000a<li>Implement natively in browsers. (browser vendor)</li>\u000a</ol>\u000a\u000a<p>This flow has many benefits over the first one.</p>\u000a\u000a<h3>Prototypes have a tighter feedback loop</h3>\u000a\u000a<p>By releasing a JavaScript library quickly, you immediately get feedback\u000afrom developers. As you tweak your API, you can incorporate real world\u000afeedback. This can be a very short cycle, especially with the help of\u000atools like github, where consumers of your library can give you\u000asuggestions and fix your bugs!</p>\u000a\u000a<h3>Useful out of the gate</h3>\u000a\u000a<p>By the time you are ready to propose your library to be standardized as\u000aa core web specification, you already have a reference implementation\u000awith unit tests. These can then be used by browser vendors as they\u000aimplement the specification. This is much more useful to have than\u000athe abstract pseudocode that the W3C specs currently provide, and should\u000alead to a more consistent set of browser implementations.</p>\u000a\u000a<p>Even more importantly, you know that what you are proposing is something\u000auseful because that real web developers have already used it! This is\u000aalready better than many APIs currently offered by the web platform,\u000asome of which has seen little developer uptake.</p>\u000a\u000a<h3>A polyfill at launch</h3>\u000a\u000a<p>By the time the first browsers start implementing the spec, you already\u000ahave a polyfill for developers to use &mdash; the library you wrote in\u000astep 1. This polyfill should feature detect for the presence of the API,\u000aand if not present, load the functionality via the JavaScript library.</p>\u000a\u000a<h2>Limitation: some things need more than JavaScript</h2>\u000a\u000a<p>Not all features can be implemented in JavaScript. In fact, some of the\u000amost exciting ones require browser-level innovation because JavaScript\u000ais heavily sandboxed (eg. contacts API, access to new sensors). Still,\u000amany new features <strong>can</strong> be implemented, such as new layout models (eg.\u000aflexbox, new storage APIs, responsive image solutions).</p>\u000a\u000a<p>Even if you can't provide a JavaScript implementation, there are ways to\u000aprototype these features to see if they are useful or not, giving\u000asome of the benefits describe above. PhoneGap and other WebView wrappers\u000acan be easily instrumented with plugins that bring native functionality\u000ato the web. More ambitiously, if you can stomach the learning curve,\u000acontribute to open source browsers like WebKit/Chromium and Firefox!</p>\u000a\u000a<h2>Limitation: JS library to spec is uncharted territory</h2>\u000a\u000a<p>One of the missing pieces in the "forward polyfill" approach I advocate\u000afor in this post is that it can still be very difficult to get your\u000avoice heard without being a browser vendor or spec author.</p>\u000a\u000a<p>Anecdotally I've sent a few messages to the <a href="http://goo.gl/hrBvS">www-style@\u000alist</a>, without many tangible results. Other web developers\u000asuch as, roughly following the tactic I described earlier, have\u000aexperienced similar frustrations in the recent discussions <a href="http://www.webmonkey.com/2012/05/browsers-at-odds-with-web-developers-over-adaptive-images/">regarding\u000ahigh DPI images</a>.</p>\u000a\u000a<h2>Conclusion</h2>\u000a\u000a<p>This post is a summary of my thinking around most of my recent web\u000adevelopment work: projects like <a href="https://github.com/borismus/pointer.js">pointer.js</a>, <a href="https://github.com/borismus/physical-units">physical units</a>\u000aand the <a href="https://github.com/borismus/srcset-polyfill/">srcset-polyfill</a> all have the same mission in mind: to\u000acease to exist by having their functionality replaced by the web\u000aplatform itself. The most notable example of such a project is PhoneGap,\u000awhich explicitly states self destruction as its now famous <a href="http://phonegap.com/2012/05/09/phonegap-beliefs-goals-and-philosophy/">second\u000agoal</a>.</p>\u000a\u000a<p>It may seem strange to develop projects with such a nihilistic purpose.\u000aBut keep in mind, that in this case it's not just the code that counts,\u000abut the ideas behind it. We would be better off with many of these\u000athings in the web platform: an implementation of pointer events,\u000aphysical units, and a good set of device APIs.</p>\u000a\u000a<p>I'm not arguing that the current spec-and-browser-first flow should be\u000areplaced. I'm merely suggesting that there is an alternative out\u000athere that involves web developers; one worthy of exploration by\u000athose of us that aren't in the business of writing browsers but\u000acare enough about the web platform to try to make a change.</p>\u000a\u000a<p><strong><em>UPDATE</em></strong>: At Paul Irish's great suggestion, please post your\u000acomments in <a href="https://plus.google.com/115694705577863745195/posts/fMyCkBYvHRi">this Google+ thread</a>.</p>\u000a
p1825
tp1826
Rp1827
sg13
V/how-the-web-should-work
p1828
sg15
Nsg16
I01
sg17
VHow the web should work
p1829
sg20
VWhen developing in a particular environment, say Android or Cocoa, we\u000aare subconsciously aware that the APIs are essentially fixed and beyond\u000aour control.
p1830
sg6
V<p>When developing in a particular environment, say Android or Cocoa, we\u000aare subconsciously aware that the APIs are essentially fixed and beyond\u000aour control. There is no effective mechanism to go and tell Apple that\u000asome method is poorly named, or tell the Android team how much you wish\u000athe Audio APIs were nicer to use.</p>\u000a\u000a<p>The web, however, is built by many different companies and individuals,\u000agiving consumers of the platform (web developers) a unique chance to\u000aalso become contributors to its evolution. Rather than griping about how\u000abroken something on the web is, remember that you can play a part in\u000afixing it!</p>\u000a\u000a
p1831
sg25
g169
sg33
g1828
sg170
(dp1832
g172
S'Sep'
p1833
sg174
S'September 12, 2012'
p1834
sg176
I9
sg177
S'2012-09-12T09:00:00-00:00'
p1835
sg179
I1347465600
sg180
I2012
sg181
I12
ssg65
g182
sg31
S'how-the-web-should-work'
p1836
sS'posted'
p1837
g188
(S'\x07\xdc\t\x0c'
p1838
tp1839
Rp1840
ssg34
S'content/posts/2012/how-the-web-should-work.md'
p1841
sg36
F1347792976.0
sa(dp1842
g2
(dp1843
g4
V Although I love Apple's design aesthetic and ability to consistently churn out amazing hardware, I'm never quite comfortable fully embracing it. The reasons have to do with platform fertility, or how well suited a platform is to incremental platform innovations and new platform creation. Onward!   
p1844
sg28
g7
(g8
g9
V<p>Although I love Apple's design aesthetic and ability to consistently\u000achurn out amazing hardware, I'm never quite comfortable fully embracing\u000ait. The reasons have to do with platform fertility, or how well suited a\u000aplatform is to incremental platform innovations and new platform\u000acreation. Onward!</p>\u000a\u000a<!--more-->\u000a\u000a<h2>Personal computing</h2>\u000a\u000a<p>The evolution of personal computing is generally accepted to look something\u000alike the following:</p>\u000a\u000a<pre><code>Mainframe -&gt; Minicomputer -&gt; Personal computer\u000a</code></pre>\u000a\u000a<p>The above relationship can be interpreted in at least two ways:</p>\u000a\u000a<ol>\u000a<li>The former was disrupted by the latter.</li>\u000a<li>The former was used to design the latter.</li>\u000a</ol>\u000a\u000a<p>Taking the first interpretation, we are in the middle (closer to the end, I\u000areckon) of another disruptive innovation:</p>\u000a\u000a<pre><code>Personal computer -&gt; Tablet\u000a</code></pre>\u000a\u000a<p>Indeed, compared to laptops, tablets seem to be a better way for the\u000ageneral public to use computers. The touch interface is more intuitive,\u000aand the physical form factor is better suited to casual use. For the\u000apurposes of this post, let's assume for argument's sake that tablets\u000aovertake personal computing in the future. The interesting thing is what\u000ahappens next:</p>\u000a\u000a<pre><code>Tablet -&gt; (next disruptive innovation)\u000a</code></pre>\u000a\u000a<p>Let's go back to the second interpretation: <code>b was designed using a</code>\u000a(or, in some biblical sense, b begat a). In other words, designers and\u000aengineers used a mainframe to invent minicomputers, a minicomputer to\u000ainvent PCs, and a PC to build tablets. We can call this "platform\u000afertility" just for fun. Up to now, said professionals could use\u000athe latest general purpose computers to do their job, but this may be\u000achanging.</p>\u000a\u000a<p>The tablet disruption was enabled by its predecessor, the PC, being\u000aflexible and extensible enough, and thus well suited as a prototyping\u000aplatform. Linux, OS X and even Windows are all very flexible platforms,\u000aintended to work with a variety of software toolkits and external\u000adevices. These desktop platforms never imposed restrictions like\u000asoftware signing, and even if they did, you the power user could always\u000aoverride.</p>\u000a\u000a<h2>Peril of a closed platform</h2>\u000a\u000a<p>I don't want to argue about the semantics of the word "open", but\u000aregardless of your religious dispositions, we can all agree that Apple\u000ais not, nor has any pretenses to be associated with, that word. I'm not\u000amaking judgements here, it's just how they roll. The API surface is\u000acarefully designed to give developers the right amount of flexibility,\u000abut not more. AppStore is explicitly a sandbox, and if you don't play by\u000athe rules, you lose your playground privileges.</p>\u000a\u000a<p>So imagine for a minute that iPad swept the tablet market (shouldn't be\u000ahard given the <a href="http://www.appleinsider.com/articles/12/05/04/ipad_tablet_market_share_will_dip_to_50_by_2017_study_says.html">current market distribution</a>). From a pragmatic\u000auser's perspective, this is fine, even good! iOS is a very\u000awell-integrated platform, working across all shiny Apple products, and\u000ausers are generally pretty happy with the interface and overall\u000aexperience. From a curious developer's perspective, however, things\u000aare a bit different.</p>\u000a\u000a<p>As an iOS developer that wants to improve the platform experience,\u000ahowever, you are pretty much stuck with how things are. You can't\u000areplace the lock screen, can't write long-running applications that read\u000ain accelerometer data in the background, can't customize your home\u000ascreen launcher, etc.</p>\u000a\u000a<p>The problem is exacerbated when you set out to try to invent the next\u000athing. How do you interface with your new stereo camera rig? How many\u000ahurdles do you have to overcome to make it possible to interface with\u000ayour new smart watch? How about a pair of smart contact lenses? How do\u000ayou get raw USB access? Bluetooth? Ad-hoc wireless? Granted, the further\u000ayou venture away from the platform core, the less help you would expect\u000afrom it. This is generally where you climb down a layer of abstraction -\u000afor example, to <a href="http://developer.android.com/sdk/ndk/index.html">NDK in Android</a>. Without such an option, you're\u000aleft dead in the water.</p>\u000a\u000a<h2>A Litmus test</h2>\u000a\u000a<p>Unless iPads become more hackable or other, more developer-friendly\u000atablets emerge as serious competitors, laptop computers will become\u000aspecialized tools for software professionals while tablets supplant\u000alaptops for the rest of thepublic.</p>\u000a\u000a<p>Platforms inherently restrict the developer in some sense, placing them\u000ain a box delineated by the APIs that the platform provides. An\u000ainteresting way to examine this box is with this notion of "platform\u000afertility", centering around the question:</p>\u000a\u000a<p><strong>Can this platform beget future platforms?</strong></p>\u000a\u000a<p>Whether your idea of the next platform is incremental (for example, a\u000abetter lock screen) or a fundamental disruptive innovation (for example,\u000asmart glasses), the answer to the above question for the current state\u000aof iOS is a resounding no. Sorry bro!</p>\u000a
p1845
tp1846
Rp1847
sg13
V/platform-fertility
p1848
sg15
Nsg16
I01
sg17
VPlatform fertility: open for innovation?
p1849
sg20
VAlthough I love Apple's design aesthetic and ability to consistently\u000achurn out amazing hardware, I'm never quite comfortable fully embracing\u000ait.
p1850
sg6
V<p>Although I love Apple's design aesthetic and ability to consistently\u000achurn out amazing hardware, I'm never quite comfortable fully embracing\u000ait. The reasons have to do with platform fertility, or how well suited a\u000aplatform is to incremental platform innovations and new platform\u000acreation. Onward!</p>\u000a\u000a
p1851
sg25
g169
sg33
g1848
sg170
(dp1852
g172
S'Jun'
p1853
sg174
S'June 6, 2012'
p1854
sg176
I6
sg177
S'2012-06-06T09:00:00-00:00'
p1855
sg179
I1338998400
sg180
I2012
sg181
I6
ssg65
g182
sg31
S'platform-fertility'
p1856
sS'posted'
p1857
g188
(S'\x07\xdc\x06\x06'
p1858
tp1859
Rp1860
ssg34
S'content/posts/2012/platform-fertility.md'
p1861
sg36
F1338969220.0
sa(dp1862
g2
(dp1863
g4
V  Ember  and  Backbone  are both promising JavaScript frameworks but have completely different philosophies. In this post, I'll compare the two, both from a practical and philosophical perspective. I'll defer to real world experience with Backbone and  SproutCore  (Ember's predecessor), as well as basic experiments with Ember (full disclosure: haven't built a large Ember app yet). I'll also supplement claims with quotes from a fantastic  conversation  from Freenode  #documentcloud  on February 3rd, 2011. For quote context,  wycats  is  Yehuda Katz , one of the lead developers on Ember, and  jashkenas  is  Jeremy Ashkenas , one of the lead developers on Backbone. 
p1864
sg28
g7
(g8
g9
V<p><a href="http://emberjs.com/">Ember</a> and <a href="http://documentcloud.github.com/backbone/">Backbone</a> are both promising JavaScript frameworks but have\u000acompletely different philosophies. In this post, I'll compare the two, both\u000afrom a practical and philosophical perspective. I'll defer to real world\u000aexperience with Backbone and <a href="http://sproutcore.com">SproutCore</a> (Ember's predecessor), as well as\u000abasic experiments with Ember (full disclosure: haven't built a large Ember app\u000ayet). I'll also supplement claims with quotes from a fantastic <a href="http://goo.gl/t7gHG">conversation</a>\u000afrom Freenode <a href="irc://irc.freenode.net/#documentcloud">#documentcloud</a> on February 3rd, 2011. For quote\u000acontext, <code>wycats</code> is <a href="http://yehudakatz.com/">Yehuda Katz</a>, one of the lead developers on Ember, and\u000a<code>jashkenas</code> is <a href="http://ashkenas.com/">Jeremy Ashkenas</a>, one of the lead developers on Backbone.\u000a<!--more--></p>\u000a\u000a<p>Before I go into too much detail, it's pretty clear that both frameworks have\u000athe same goal: to help developers build apps.</p>\u000a\u000a<blockquote>\u000a  <p>1:09 PM <strong>wycats</strong> backbone apps are apps</p>\u000a</blockquote>\u000a\u000a<p>So we have a roughly apples-to-apples comparison. Let me dive in and talk about\u000asome philosophical differences:</p>\u000a\u000a<h2>Backbone at a glance</h2>\u000a\u000a<p>Backbone is a minimalist framework that builds on top of ideas from\u000ajQuery to give some structure to web applications. It introduces\u000aconcepts of views, models, a restful sync interface, routers, etc, in\u000asurprisingly little code. Backbone is the darling of microframework\u000alovers, who highly value small framework size and don't want to commit\u000ato a single "full stack" solution.</p>\u000a\u000a<p>Backbone is incredibly flexible, and doesn't impose how your views and\u000amodels should actually interact. The main benefit is that it adds some\u000astructure to the app and provides convenient ways of listening to DOM\u000aevents and turning them into application events.</p>\u000a\u000a<blockquote>\u000a  <p>1:15 PM <strong>wycats</strong> backbone is 600loc <br />\u000a  1:16 PM <strong>wycats</strong> "here's how you should think about your app structure"</p>\u000a</blockquote>\u000a\u000a<p>Since Backbone is so small, it leaves a lot of decisions up to the\u000adeveloper. This is both a blessing (flexibility, works for me, etc) and\u000aa curse, since support for many things is missing.</p>\u000a\u000a<blockquote>\u000a  <p>12:32 PM <strong>wycats</strong> jashkenas: backbone is 600 lines of code <br />\u000a  12:32 PM <strong>wycats</strong> jashkenas: the idea that there are things missing in it that\u000a  are common should not be controversial</p>\u000a</blockquote>\u000a\u000a<h2>Ember at a glance</h2>\u000a\u000a<p>Ember has a very different history. It's an evolution of SproutCore,\u000awhich is a complete web application solution. Ember takes the core parts\u000aof SproutCore - two-way data binding, computed properties, tight\u000atemplate integration, and strips the rest off into sub-modules. Things\u000athat come in module format are a data serialization layer (via <a href="https://github.com/emberjs/data">data</a>),\u000arouting (via <a href="https://github.com/emberjs-addons/sproutcore-routing">routes</a>). Ember's out of the box functionality is\u000aactually smaller than Backbone's, but it provides a higher level\u000aabstraction.</p>\u000a\u000a<p>I used SproutCore back in 2008 while working as an engineer on\u000aiWork.com. It was a SproutCore pre-1.0 release, and things were a bit\u000arough. Ember seems to have come clean in many ways, presenting a more\u000aconsistent template data binding solution via Handlebars, and being a\u000alot less monolithic SproutCore once was.</p>\u000a\u000a<p>Ember aims to provide a full solution in an opinionated way. Thus,\u000ato get the most of the framework, developers must do things in a certain\u000astyle. While you're not forced to use Handlebars, it's the only way to\u000aget some of the compelling features the framework provides (eg. for\u000atwo-way data binding). Many of the auxillary modules such as ember-data\u000aare designed to fit will into the existing structure of the framework.</p>\u000a\u000a<blockquote>\u000a  <p>1:22 PM <strong>wycats</strong> tbranyen: ember is an end-to-end framework built on\u000a  top of more modular components</p>\u000a</blockquote>\u000a\u000a<p>There is a sentiment from many people, including Backbone's founder that\u000amuch of what Ember provides is over-engineered:</p>\u000a\u000a<blockquote>\u000a  <p>12:28 PM <strong>jashkenas</strong> It's Backbone's take that Ember's more complex\u000a  data binding model, intermediate controllers, run loop etc. ... are\u000a  all interesting approaches, but are <em>not</em> usually helpful in building\u000a  a real site.</p>\u000a</blockquote>\u000a\u000a<h2>Different audiences</h2>\u000a\u000a<blockquote>\u000a  <p>12:30 PM <strong>jashkenas</strong> wycats: and I find quite the opposite --\u000a  self-selecting sample pools, as you'd expect.</p>\u000a</blockquote>\u000a\u000a<p>So to summarize at a high level, there are two camps (and excuse me\u000awhile I generalize a lot).</p>\u000a\u000a<ol>\u000a<li><p>Lovers of micro-frameworks. JavaScript hackers seeking extra\u000astructure in their slightly complex apps. These people are\u000acomfortable mixing and matching frameworks, solving problems as they\u000acome, and just want to get started quickly.</p></li>\u000a<li><p>Software Engineers that are used to a deep abstraction layer and a\u000afull service stack. These people are probably coming from native app\u000adevelopment and want to write very complex applications on the web.</p></li>\u000a</ol>\u000a\u000a<p>Basically, these two developers have different needs, come from\u000adifferent programming cultures, and are maybe even writing different\u000aapplications.</p>\u000a\u000a<blockquote>\u000a  <p>12:31 PM <strong>wycats</strong> jashkenas: so then it's fair to say that for <em>some</em>\u000a  people, Ember's approach is overkill</p>\u000a</blockquote>\u000a\u000a<p>Andrew says it best:</p>\u000a\u000a<blockquote>\u000a  <p>12:36 PM <strong>andrewdeandrade</strong> It's really all about values. Occasionally\u000a  I get frustrated by things backbone.js doesn't do and occasionally I\u000a  get frustrated by things rails does that are hard to undo. My personal\u000a  preference is to have a framework not do something and implement it\u000a  myself than have a framework do something and figure out how to do the\u000a  opposite. That's me. I know people who feel differently</p>\u000a</blockquote>\u000a\u000a<h2>A more detailed comparison</h2>\u000a\u000a<p>After engineering with SproutCore, writing an app with Backbone, and\u000awriting small amounts of Ember (mostly samples to get a feel for data\u000abindings, etc), I've got some sense of the issues that you will run into\u000awhen developing a moderately complex app. I'll go over some, and\u000asupplement them with quotes from framework authors.</p>\u000a\u000a<blockquote>\u000a  <p>1:28 PM <strong>wycats</strong> jashkenas: do you disagree that the pattern "listen\u000a  for these properties, and when any of them changes, trigger observers"\u000a  is very common? <br />\u000a  1:26 PM <strong>jashkenas</strong> that's correct -- you listen for changes to the source data and render computed values. not hard. <br />\u000a  1:26 PM <strong>wycats</strong> jashkenas: yes\u2026 a pattern that happens sufficiently often that it's good to abstract</p>\u000a</blockquote>\u000a\u000a<p>In general, Ember seeks to find common problems that developers face,\u000aand solve them in an opinionated way. Backbone, on the other hand,\u000aleaves it to developers solve their own problems in the way that works\u000afor them.</p>\u000a\u000a<h3>Templates</h3>\u000a\u000a<p>While Ember in theory lets you pick which templating engine to use, you\u000alose a lot of benefits if you're not using Handlebars.</p>\u000a\u000a<blockquote>\u000a  <p>1:29 PM <strong>jashkenas</strong> forcing your users to use logic-less templates is <em>incredibly</em> constraining. <br />\u000a  1:30 PM <strong>tomdale</strong> jashkenas: ember supports any templating language you'd like. you just don't get auto-updating <br />\u000a  1:30 PM <strong>jashkenas</strong> right, exactly.</p>\u000a</blockquote>\u000a\u000a<p>In practice, my last project used Mustache because it was logic-less.\u000aSuch templating engines are my preference, and I'm happy to be\u000aconstrained to a pretty good templating system if that means access to\u000apowerful features, cleaner application code and less boilerplate.</p>\u000a\u000a<p>I don't think that Ember should be toting choice of template as a big\u000abenefit. Like any application framework, Ember is a contract between\u000adeveloper and framework author. You write in our style and we'll give\u000ayou powerful features. In an ideal world, this lets developers focus on\u000atheir app instead of dealing with middleware.</p>\u000a\u000a<h3>Views</h3>\u000a\u000a<p>view decomposition is one of the first tasks a front end developer\u000afaces. They need to decide which parts of their app will be implemented\u000awith what view. Should each list item be its own view? Should the list\u000abe a single monolithic view?</p>\u000a\u000a<p>In my experience writing Backbone apps, views are very primitive and\u000atend to cause issues. There's no support for any sort of view nesting,\u000awhich is totally critical for large applications with complex UIs. In\u000acontrast, Ember provides an easy way of nesting views inside one\u000aanother.</p>\u000a\u000a<p>The other thing you'll notice with Backbone is that there's a lot of\u000amicro-management required when building Backbone views. They need to\u000abe properly cleaned up by hand, otherwise you end up with <a href="http://stackoverflow.com/questions/7125402/backbone-bind-multi-event-to-one-button-after-i-new-view-multi-times">zombie\u000aviews</a> bound to events, or events that <a href="http://stackoverflow.com/questions/7348988/backbone-js-events-not-firing-after-re-render">don't fire at all</a>.</p>\u000a\u000a<p>Some backbone projects, such as the <a href="https://github.com/tbranyen/backbone.layoutmanager">layout manager</a>, aim to remedy\u000asome of these limitations by creating a Layout abstraction, that allows\u000anested views and handles a lot of the rendering. This is a very\u000ainteresting project, but I haven't tried it yet.</p>\u000a\u000a<h3>MVC?</h3>\u000a\u000a<p>Ember is a traditional MVC framework, where it's clear which parts are\u000athe view, the controller and the model. Backbone on the other hand, is\u000aexplicitly not an MVC. It never even claims to be!</p>\u000a\u000a<p>In fact, if you read the <a href="http://documentcloud.github.com/backbone/">main page</a>, you'll notice that\u000athere's no mention of controllers anywhere. Don't get me wrong: Backbone\u000astill gives your applications structure, but has no opinion on the\u000agluing layer between data and presentation. I think Backbone once had\u000acontrollers, but they were renamed to routers, designed primarily for\u000ahandling URLs and history/pushState.</p>\u000a\u000a<p>In my experience, there's very much a need for a controller when writing\u000aeven moderately complex apps. You're presented with several options:</p>\u000a\u000a<ol>\u000a<li>Write controller code in views</li>\u000a<li>Write controller code in models</li>\u000a<li>Write controller code in a router</li>\u000a<li>Write your own controller infrastructure</li>\u000a</ol>\u000a\u000a<p>If you care about separation of concerns, none of these options are\u000areally acceptable.</p>\u000a\u000a<h3>Data and servers</h3>\u000a\u000a<p>Backbone packs a huge punch in a small package. It comes with\u000a<code>Backbone.sync</code>, which lets you fully customize how you want to interact\u000awith the server.</p>\u000a\u000a<p>Following the CRUD pattern, Backbone lets you specify the response\u000aformat of an Read, but unfortunately doesn't allow you to fully\u000acustomize how you would like to serialize Create and Update and Delete\u000apayloads for calling your server-side API. For my last project, I ended\u000aup just completely redefining Backbone.sync, which is very powerful, but\u000aI needed to write a lot of boilerplate to make it work reasonably.</p>\u000a\u000a<p>The problem with Backbone's approach is that it doesn't separate two\u000aparts of data stores: internal collection management and the interface\u000awith the backend API.</p>\u000a\u000a<p>Ember data, on the other hand, has the concept of API adapters, which\u000alet you specify the interface the server. Additionally it has a\u000aDataStore model. I haven't experimented with this in Ember yet, but\u000aSproutCore's version of this worked quite well.</p>\u000a\u000a<h3>Performance considerations</h3>\u000a\u000a<p>When I worked on iWork.com, we definitely had some issues with\u000aSproutCore performance for large amounts of data. The problem was that\u000awe couldn't really optimize our code because we were locked into the way\u000aSproutCore does things, so our way forward was to patch SproutCore\u000aitself to address some of the performance issues, or break out of the\u000aframework and implement the performance intensive part manually.</p>\u000a\u000a<p>Jeremy voices this concern here:</p>\u000a\u000a<blockquote>\u000a  <p>1:33 PM <strong>jashkenas</strong> finally, and perhaps most importantly, embers tight\u000a  coupling of handlebars-ui-with-very-specific-bindings-to-ember-models is\u000a  trouble, performance-wise. You can't build the really intensive parts of\u000a  your UI with that level of binding / dom tweaking.</p>\u000a</blockquote>\u000a\u000a<p>And I agree with him. It's very legitimate concern for large apps, and\u000aYehuda didn't answer adequately in my opinion.</p>\u000a\u000a<blockquote>\u000a  <p>1:34 PM <strong>wycats</strong> jashkenas: exposing the performance question to the\u000a  user is trouble <br />\u000a  1:34 PM <strong>wycats</strong> over the long haul Ember will be able to heuristically\u000a  decide how bulky to update <br />\u000a  1:34 PM <strong>jashkenas</strong> sufficiently smart compiler, eh? <br />\u000a  1:34 PM <strong>wycats</strong> jashkenas: :P  </p>\u000a</blockquote>\u000a\u000a<p>I'm a bit concerned about this point and would love to hear a less\u000ahandwavy answer from Yehuda.</p>\u000a\u000a<h2>Final thoughts (Backbone)</h2>\u000a\u000a<p>My conclusion from writing an app with Backbone, and attending a Bocoup\u000atraining workshop, is that Backbone by itself is not sufficient for\u000abuilding complex web apps. You will invariably go one of two directions:</p>\u000a\u000a<ol>\u000a<li>Engineer a lot of stuff on top of it, or</li>\u000a<li>Use existing Backbone plugins of various maturity and hope they work\u000awell together.</li>\u000a</ol>\u000a\u000a<p>Based on my experience with jQuery, and the mess of ensuing plugins, The\u000alatter seems overly optimistic. So basically, be prepared to write a lot\u000aof extra <a href="https://github.com/tbranyen/backbone-boilerplate">boilerplate</a> code. But if you're a JavaScript developer, you\u000acan handle that!</p>\u000a\u000a<p>In practice, it's very difficult to remain productive if you're writing\u000aboth an app and a framework at the same time. Unfortunately this was my\u000atendency when using Backbone. I hate reinventing the wheel. Especially\u000aif it's kind of lopsided.</p>\u000a\u000a<p>That said, Backbone is fantastic for mid-to-low complexity applications\u000athat want to maintain structured code.</p>\u000a\u000a<h2>Final thoughts (Ember)</h2>\u000a\u000a<p>Ember, on the other hand, forces you into its way of doing things. This\u000ais a framework with opinions that gives you less flexibility. However,\u000aif you grit your teeth a little bit and buy in, you'll be exposed to a\u000awell thought out set of libraries that work well together.</p>\u000a\u000a<p>I really like the features Ember offers, and its philosophy of finding\u000acommon problems developers will face and solving them. One thing I\u000aanticipate is that many of the non-core Ember modules are immature.\u000aHowever, just having these modules that are designed to work together is\u000aa boon for serious application developers.</p>\u000a\u000a<p>I'm still concerned that Ember applications may be stuck if dealing with\u000aparticularly hairy custom view situations, or large amounts of data, but\u000aI'll have a better sense of the limitations soon.</p>\u000a
p1865
tp1866
Rp1867
sg13
V/backbone-and-ember
p1868
sg15
Nsg16
I01
sg17
VBackbone and ember
p1869
sg20
V\u000a\u000a[Ember][] and [Backbone][] are both promising JavaScript frameworks but have\u000acompletely different philosophies.
p1870
sg6
V<p><a href="http://emberjs.com/">Ember</a> and <a href="http://documentcloud.github.com/backbone/">Backbone</a> are both promising JavaScript frameworks but have\u000acompletely different philosophies. In this post, I'll compare the two, both\u000afrom a practical and philosophical perspective. I'll defer to real world\u000aexperience with Backbone and <a href="http://sproutcore.com">SproutCore</a> (Ember's predecessor), as well as\u000abasic experiments with Ember (full disclosure: haven't built a large Ember app\u000ayet). I'll also supplement claims with quotes from a fantastic <a href="http://goo.gl/t7gHG">conversation</a>\u000afrom Freenode <a href="irc://irc.freenode.net/#documentcloud">#documentcloud</a> on February 3rd, 2011. For quote\u000acontext, <code>wycats</code> is <a href="http://yehudakatz.com/">Yehuda Katz</a>, one of the lead developers on Ember, and\u000a<code>jashkenas</code> is <a href="http://ashkenas.com/">Jeremy Ashkenas</a>, one of the lead developers on Backbone.\u000a
p1871
sg25
g169
sg33
g1868
sg170
(dp1872
g172
S'Feb'
p1873
sg174
S'February 15, 2012'
p1874
sg176
I2
sg177
S'2012-02-15T09:00:00-00:00'
p1875
sg179
I1329325200
sg180
I2012
sg181
I15
ssg65
g182
sg31
S'backbone-and-ember'
p1876
sS'categories'
p1877
(lp1878
S'web'
p1879
asS'posted'
p1880
g188
(S'\x07\xdc\x02\x0f'
p1881
tp1882
Rp1883
ssg34
S'content/posts/2012/backbone-and-ember/index.md'
p1884
sg36
F1433825556.0
sa(dp1885
g2
(dp1886
g4
V There are many technical decisions to make when writing web applications. I've come back to writing modern web applications lately, and wanted to consolidate some scattered thoughts that I\u2019ve recorded over the course of my development cycle. In particular, this post is about the set of frameworks that I found to be instrumental in developing my most recent project. I'll go over some of the most important framework types, each of which could be expanded into an article in its own right. This is not meant to be an extensive comparison of existing offerings, just a slice of technologies that I experimented with most recently. 
p1887
sg28
g7
(g8
g9
V<p>There are many technical decisions to make when writing web applications. I've\u000acome back to writing modern web applications lately, and wanted to consolidate\u000asome scattered thoughts that I\u2019ve recorded over the course of my development\u000acycle. In particular, this post is about the set of frameworks that I found to\u000abe instrumental in developing my most recent project. I'll go over some of the\u000amost important framework types, each of which could be expanded into an article\u000ain its own right. This is not meant to be an extensive comparison of existing\u000aofferings, just a slice of technologies that I experimented with most recently.\u000a<!--more--></p>\u000a\u000a<p>Although my focus is on mobile first, I think that this set of technologies can\u000abe applied to web apps in general. All of my decisions and data points were\u000amade with a few requirements in mind:</p>\u000a\u000a<ul>\u000a<li>JavaScript only (CoffeeScript, Dart, are definitely worth a serious look, but\u000acause an explosion in choice which I wanted to avoid)</li>\u000a<li>Must work well in modern browsers (iOS 5, Android 4)</li>\u000a</ul>\u000a\u000a<h2>Picking an MVC</h2>\u000a\u000a<p>The model view controller pattern has been in use in native UI app development\u000afor decades. The basic idea is to separate the data layer (storage,\u000acommunication, data) from the presentation layer (UI, animation, input). There\u000aare other similar patterns such as MVVM (Model View ViewModel), but the main idea\u000ais to have well-defined separation between the presentation and data layers for\u000acleaner code and ultimately long-term velocity:</p>\u000a\u000a<p><img src="separation.jpg" alt="separation" /></p>\u000a\u000a<p>There are tons of offerings of JavaScript model view controller frameworks.\u000aSome, such as <a href="http://backbonejs.org/">Backbone.js</a> and <a href="http://spinejs.com/">Spine.js</a> are written in pure\u000acode, while others like <a href="http://knockoutjs.com/">Knockout.js</a> and <a href="http://angularjs.org/#/">Angular</a> rely on\u000aDOM data attribute binding. Relying on HTML5 data DOM attributes feels wrong\u000afor an MVC system, whose whole point is to separate view and data. This\u000aexcludes the Knockout and Angular frameworks. Spine.js is easier with\u000aCoffeeScript, which we exclude based on my initial requirements.</p>\u000a\u000a<p>Backbone.js has been around for longer than most (except perhaps JavaScriptMVC,\u000aseems like a dead project), and also features a growing open source community.\u000aFor my app stack, I went with Backbone.js. For more information about picking\u000aan MVC, check out <a href="http://addyosmani.github.com/todomvc/">TodoMVC</a>, which implements the same Todo\u000aapplication using different MVC frameworks. Also see this\u000a<a href="http://codebrief.com/2012/01/the-top-10-javascript-mvc-frameworks-reviewed/">MVC framework comparison</a>, which strongly favors the\u000a<a href="http://emberjs.com/">Ember.js</a>, a relative newcomer to the scene. I haven\u2019t yet had a chance\u000ato play with it, but it\u2019s on my list.</p>\u000a\u000a<h2>Picking a templating engine</h2>\u000a\u000a<p>To build a serious application on the web, you inevitably build up large DOM\u000atrees. Rather than using JavaScript APIs to manipulate the DOM, it can be much\u000asimpler and more efficient to write HTML using a string-based template instead.\u000aGenerally speaking JS templates have evolved to use this at-first strange\u000aconvention of embedding the template content inside script tags: <code>&lt;script\u000a  id="my-template" type="text/my-template-language"&gt;...&lt;/script&gt;</code>. The basic\u000apattern of use for all template engines is to load the template as a string,\u000aconstruct template parameters and then run the template and parameters through\u000athe templating engine.</p>\u000a\u000a<p>Backbone.js depends on <a href="http://documentcloud.github.com/underscore/">Underscore.js</a>, which ships with a somewhat\u000alimited templating engine with verbose syntax. There are other options\u000aavailable, including <a href="http://api.jquery.com/category/plugins/templates/">jQuery Templates</a>, <a href="http://handlebarsjs.com/">Handlebars.js</a>,\u000a<a href="http://mustache.github.com/">Mustache.js</a> and many others. jQuery Templates have been deprecated\u000aby the jQuery team, so I did not consider this option. Mustache is a\u000across-language templating system, featuring simplicity and a deliberate\u000adecision to support as little logic as possible. Indeed, the most complex\u000aconstruct in Mustache is a way to iterate an array of objects. Handlebars.js\u000abuilds heavily on Mustache, adding some nice features such as template\u000aprecompilation and in-template expressions. For my purposes I didn\u2019t need these\u000aextra features, and chose Mustache.js as my templating platform.</p>\u000a\u000a<p>In general, my impression is that the existing templating frameworks are quite\u000aminimal and comparable in features, so the decision is largely a matter of\u000apersonal preference.</p>\u000a\u000a<h2>Picking a CSS Framework</h2>\u000a\u000a<p>CSS frameworks are essential tools that extend CSS\u2019s feature set with\u000aconveniences such as variables, a way to create hierarchical CSS selectors, and\u000asome more advanced features. This essentially creates a new language: an\u000aaugmented version of CSS (let\u2019s call it CSS++). For development ease, some\u000aframeworks implement a JavaScript CSS++ interpreter in the browser, while other\u000aframeworks let you monitor a CSS++ file and compile it whenever there are any\u000achanges made. All CSS frameworks should provide command line tool to compile\u000aCSS++ down to CSS for deployment.</p>\u000a\u000a<p>As with templating languages, there are many choices all of which do very\u000asimilar things. My choice was motivated by personal syntax preference, and I\u000aprefer <a href="http://sass-lang.com/">SCSS</a> because it avoids weird syntax like <code>@</code>. One drawback of SCSS is\u000athat it doesn\u2019t ship with a JavaScript interpreter (there is an <a href="https://github.com/bmavity/scss-js">unofficial one</a>\u000athat I haven\u2019t tried), but does come with a command line watcher. Other similar\u000aCSS frameworks include <a href="http://lesscss.org/">LESS</a> and <a href="http://learnboost.github.com/stylus/">Stylus</a>.</p>\u000a\u000a<h2>How to layout views</h2>\u000a\u000a<p>HTML5 provides a variety of ways to layout content, and MVC frameworks provide\u000ano opinion about which of these layout technologies to use, leaving the\u000asometimes difficult decision to you, the developer.</p>\u000a\u000a<p>Generally speaking, relative positioning is appropriate for documents, but\u000afalls apart for apps. Absolute positioning should be avoided, as should tables,\u000aclearly. Many web developers have turned to the float property to align\u000aelements, but this is suboptimal for building application views, since it\u2019s not\u000aoptimized for app-like layouts, which results in many odd problems and <a href="http://stackoverflow.com/questions/8554043/what-actually-is-clearfix">infamous\u000aclearfix hacks</a>.</p>\u000a\u000a<p>After much experimentation with various web layout technologies over the years,\u000aI think that a combination of fixed positioning and flexbox model is ideal for\u000amobile web applications. I use fixed positioning for UI elements that are fixed\u000aon the screen (headers, sidebars, footers, etc). The flex box model is great\u000afor laying out stacked views on the page (horizontally or vertically). It\u2019s the\u000aonly CSS box model explicitly optimized for interface design, quite similar to\u000aAndroid\u2019s LinearLayout manager. For more information about the flexbox model,\u000aread <a href="http://www.html5rocks.com/en/tutorials/flexbox/quick/">Paul's article</a> and note that this spec is being replaced by a\u000a<a href="http://www.w3.org/TR/css3-flexbox/">new, non-backwards compatible version</a>.</p>\u000a\u000a<h3>Adaptive Web Apps</h3>\u000a\u000a<p>One final section on this matter: I\u2019m a strong proponent of creating\u000adevice-specific user interfaces. This means re-writing parts of your view code\u000afor different form factors. Luckily, the MVC pattern makes it relatively easy\u000ato reuse a single model for multiple views (eg. tablet and phone).</p>\u000a\u000a<p>Flipboard for iOS demonstrates this idea very well, giving tablet and phone\u000ausers a highly tailored experience for each device form factor.</p>\u000a\u000a<p><img src="flipboard-phone.jpg" alt="flipboard-phone" />\u000aThe phone UI is optimized for vertical swipes, allowing single hand use.</p>\u000a\u000a<p><img src="flipboard-tablet.jpg" alt="flipboard-tablet" />\u000aTablet UI works well for two hands holding the device on opposite sides.</p>\u000a\u000a<h2>Input considerations</h2>\u000a\u000a<p>On mobile, the main way users interact with your application is by touching the\u000ascreen with their fingers. This is quite different from mouse-based\u000ainteraction, since there are 9 additional points to track on the screen, which\u000ameans developers need to move away from mouse events when writing mobile apps.\u000aIn addition, mouse events on mobile have the problem of clicks being delayed by\u000a300ms (there is a well-known <a href="http://code.google.com/mobile/articles/fast_buttons.html">touch-based workaround</a>). For more information\u000aabout using these events in mobile browsers, see <a href="http://www.html5rocks.com/en/mobile/touch.html">my touch events article</a>.</p>\u000a\u000a<p>It\u2019s not enough to just <code>s/mousedown/touchstart/</code> all of your event handlers.\u000aThere is a completely new set of gestures that users have come to expect on\u000atouch devices, such as swipes to, for example, navigate through lists of\u000aimages. Though Apple has a little-known <a href="http://developer.apple.com/library/safari/#documentation/UserExperience/Reference/GestureEventClassReference/GestureEvent/GestureEvent.html#//apple_ref/doc/uid/TP40009353">gestures API</a>, there is no open spec for\u000adoing gesture detection on the web. We really need a JavaScript library to do\u000agesture detection, for some of the <a href="http://www.lukew.com/touch/TouchGestureGuide.pdf">more common gestures</a>.</p>\u000a\u000a<h2>How to make it work offline</h2>\u000a\u000a<p>For an app to work offline, you need two things to be true:</p>\u000a\u000a<ol>\u000a<li>Assets are available (via AppCache, Filesystem API, etc)</li>\u000a<li>Data is available (via LocalStorage, WebSQL, IndexedDB, etc)</li>\u000a</ol>\u000a\u000a<p>In practice, building offline apps on the web is a difficult problem. Generally\u000aspeaking offline functionality should be built into your app from the\u000abeginning. It\u2019s especially difficult to offline-ify an existing web application\u000awithout significant code rewriting. Additionally, there are often unknown\u000astorage limits for various offline technologies, and undefined behavior for\u000awhat happens when those limits are exceeded. Finally, there are problems with\u000atechnologies in the offline technology stack, most notably AppCache, as I\u000aoutlined in a <a href="http://smus.com/game-asset-loader">previous post</a>.</p>\u000a\u000a<p>A very interesting approach to write truly offline-capable apps is to go\u000a\u201coffline first\u201d. In other words, write everything as if you have no internet\u000aconnection, and implement a syncing layer that synchronizes data when an\u000ainternet connection exists. In the Backbone.js MVC model, this can fit nicely\u000aas a custom <code>Backbone.sync</code> adapter.</p>\u000a\u000a<h2>Unit testing</h2>\u000a\u000a<p>It\u2019s hard to unit test your UI. However, since you\u2019re using an MVC, the model\u000ais completely isolated from the UI and as a result, easy to test. <a href="http://docs.jquery.com/QUnit">QUnit</a> is\u000aquite a nice option, especially because it allows to unit test asynchronous\u000acode using it\u2019s <a href="http://docs.jquery.com/QUnit/start#decrement">start() and stop()</a> methods.</p>\u000a\u000a<h2>Signing off</h2>\u000a\u000a<p>To summarize, I used Backbone.js for MVC, Mustache.js for templating, SCSS for\u000aa CSS framework, CSS Flexbox to render views, custom touch events and QUnit for\u000aunit testing to write my mobile web application. For offline support, I\u2019m still\u000aexperimenting with various technologies and will hopefully follow up with more\u000ainformation a future post. While I strongly believe in the need for each class\u000aof tool (eg. MVC) outlined here, I also believe that many of the specific\u000atechnologies I described here are interchangeable (eg. Handlebars and\u000aMustache).</p>\u000a\u000a<p><strong>One more thing</strong>: yesterday (on January 17th, 2012), <a href="http://walmartlabs.github.com/thorax/">Thorax</a> was\u000aannounced. This is a Backbone-based set of libraries very similar in spirit to\u000awhat I describe in this post. I've yet to investigate it in any depth, but the\u000aname is great :)</p>\u000a\u000a<p>Use a similar set of frameworks? Have a personal favorite? Think I\u2019m missing an\u000aimportant type of framework? Let me know!</p>\u000a
p1888
tp1889
Rp1890
sg13
V/mobile-web-app-tech-stack
p1891
sg15
Nsg16
I01
sg17
VA mobile web application tech stack
p1892
sg20
V\u000aThere are many technical decisions to make when writing web applications.
p1893
sg6
V<p>There are many technical decisions to make when writing web applications. I've\u000acome back to writing modern web applications lately, and wanted to consolidate\u000asome scattered thoughts that I\u2019ve recorded over the course of my development\u000acycle. In particular, this post is about the set of frameworks that I found to\u000abe instrumental in developing my most recent project. I'll go over some of the\u000amost important framework types, each of which could be expanded into an article\u000ain its own right. This is not meant to be an extensive comparison of existing\u000aofferings, just a slice of technologies that I experimented with most recently.\u000a
p1894
sg25
g169
sg33
g1891
sg170
(dp1895
g172
S'Jan'
p1896
sg174
S'January 18, 2012'
p1897
sg176
I1
sg177
S'2012-01-18T09:00:00-00:00'
p1898
sg179
I1326906000
sg180
I2012
sg181
I18
ssg65
g182
sg31
S'mobile-web-app-tech-stack'
p1899
sS'categories'
p1900
(lp1901
S'web'
p1902
aS'mobile'
p1903
asS'posted'
p1904
g188
(S'\x07\xdc\x01\x12'
p1905
tp1906
Rp1907
ssg34
S'content/posts/2012/mobile-web-app-tech-stack/index.md'
p1908
sg36
F1482771640.0
sa(dp1909
g2
(dp1910
g4
V  Update (August 7, 2013): Pointer.js is deprecated. Please use the  PointerEvents polyfill  instead.     Mouse will soon cease to be the dominant input method for computing, though it will likely remain in some form for the forseeable future. Touch is the heir to the input throne, and the web needs to be ready. Unfortunately, the current state of input on the web is... you guessed it: a complete mess! There are two separate issues:      No unified story between mouse, touch, and other spatial input.   Poor support for complex gestures, especially needed for touch.      I'll look at each in a bit of detail, and then show you  pointer.js .    
p1911
sg28
g7
(g8
g9
V<p><strong>Update (August 7, 2013): Pointer.js is deprecated. Please use the\u000a<a href="https://github.com/Polymer/PointerEvents">PointerEvents polyfill</a> instead.</strong></p>\u000a\u000a<p>Mouse will soon cease to be the dominant input method for computing,\u000athough it will likely remain in some form for the forseeable future.\u000aTouch is the heir to the input throne, and the web needs to be ready.\u000aUnfortunately, the current state of input on the web is... you guessed\u000ait: a complete mess! There are two separate issues:</p>\u000a\u000a<ol>\u000a<li>No unified story between mouse, touch, and other spatial input.</li>\u000a<li>Poor support for complex gestures, especially needed for touch.</li>\u000a</ol>\u000a\u000a<p>I'll look at each in a bit of detail, and then show you\u000a<a href="https://github.com/borismus/pointer.js">pointer.js</a>. </p>\u000a\u000a<!--more-->\u000a\u000a<h2>Lack of unified touch and mouse system</h2>\u000a\u000a<p>Most web developers should care about providing a good experience on\u000aboth mouse and touch interfaces. This is increasingly true with\u000acrossover mouse-touch devices like the Transformer prime, and upcoming\u000aWindows 8 laptops.</p>\u000a\u000a<p>Here's what you end up with if you want to support touch and mouse\u000aevents on the web today:</p>\u000a\u000a<pre><code>$(window).mousedown(function(e) { down(e.pageY); });\u000a$(window).mousemove(function(e) { move(e.pageY); });\u000a$(window).mouseup(function() { up(); });\u000a\u000a// Setup touch event handlers.\u000a$(window).bind('touchstart', function(e) {\u000a  e.preventDefault();\u000a  down(e.originalEvent.touches[0].pageY);\u000a});\u000a$(window).bind('touchmove', function(e) {\u000a  e.preventDefault();\u000a  move(e.originalEvent.touches[0].pageY);\u000a});\u000a$(window).bind('touchend', function(e) {\u000a  e.preventDefault();\u000a  up();\u000a});\u000a</code></pre>\u000a\u000a<p>The above is a bunch of boilerplate code that does absolutely nothing!\u000aYou end up having to manually wrangle two completely different models\u000ainto one.</p>\u000a\u000a<p>Microsoft is taking a very smart approach with IE10 to address this\u000aissue by <a href="http://blogs.msdn.com/b/ie/archive/2011/09/20/touch-input-for-ie10-and-metro-style-apps.aspx">introducing pointer events</a>. The idea is to\u000aconsolidate all input that deals with one or more points on the screen\u000ainto a single unified model.</p>\u000a\u000a<p>Unfortunately, it's not being proposed as a standardized spec.\u000aAlso, because it's not universally available, it will be yet another\u000athing developers need to support (if they want to support Windows\u000a8/Metro apps). So now our sample above gets <a href="http://blogs.msdn.com/b/ie/archive/2011/10/19/handling-multi-touch-and-mouse-input-in-all-browsers.aspx">even more\u000aboilerplate</a>, with at least three more calls like the following:</p>\u000a\u000a<pre><code>$(window).bind('MSPointerDown', function(e) {\u000a  // Extract x, y, and call shared handler.\u000a});\u000a$(window).bind('MSPointerMove', function(e) {\u000a  // Extract x, y, and call shared handler.\u000a});\u000a$(window).bind('MSPointerUp', function(e) {\u000a  // Extract x, y, and call shared handler.\u000a});\u000a</code></pre>\u000a\u000a<p>Although their intentions are good, this approach potentially makes the\u000asituation (hopefully temporarily) worse.</p>\u000a\u000a<h2>Touch gestures need to be easy</h2>\u000a\u000a<p>Touch UIs often involve gestures that aren't easy for developers to\u000aimplement, such as pinch-zooming and rotation. However, on the web, due\u000ato the simplicity of <a href="https://dvcs.w3.org/hg/webevents/raw-file/tip/touchevents.html">the touch events</a>, even implementing\u000asomething as simple as a button <a href="http://code.google.com/mobile/articles/fast_buttons.html">is non-trivial</a>.\u000aImplementing more complex gesture recognizers on top of the primitive\u000a<code>touch*</code> events is even less trivial.</p>\u000a\u000a<p>Frameworks like <a href="http://dev.sencha.com/deploy/touch/examples/production/kitchensink/index.html#demo/touchevents">Sencha Touch</a> and <a href="http://eightmedia.github.com/hammer.js/">Hammer.js</a> come to\u000athe rescue to address the lack of gestures, however these both have\u000aproblems. Sencha comes as a complete package, and it's impossible to use\u000atheir gesture recognizer without using their whole framework (or\u000aspending considerable effort trying to pull it out). Hammer.js, on the\u000aother hand, doesn't actually implement gesture recognition for\u000apinchzoom, but instead relies on the touch spec providing non-standard\u000a<code>rotation</code> and <code>scale</code> values <a href="http://developer.apple.com/library/safari/#documentation/UserExperience/Reference/TouchEventClassReference/TouchEvent/TouchEvent.html#//apple_ref/doc/uid/TP40009358">pioneered by Apple</a>.</p>\u000a\u000a<p>Microsoft has a gesture layer on top of their consolidated pointer\u000amodel. This makes sense as an approach to take. True, certain gestures\u000aonly make sense for touch, and it's easy to distinguish the input type\u000ausing the <code>event.pointerType</code> API. That said, with a unified model,\u000athere can be new gestures that span multiple input modalities, like\u000a<a href="#">this research</a> suggests.</p>\u000a\u000a<h2>Pointer.js - A solution to both problems</h2>\u000a\u000a<p>The solution to this problem is to write another library, tag on a <code>.js</code>\u000ato the end of the name, get everyone to use it, prove that it's very\u000auseful, and have browsers and spec implement it natively. Once this\u000ais spec'ed, approved, and widely implemented, it should just be a matter\u000aof removing the script tag!</p>\u000a\u000a<p><img src="pointer.js-architecture.png" alt="Pointer.js architecture." /></p>\u000a\u000a<p>Pointer.js consolidates pointer-like input models across browsers and\u000adevices. It provides the following:</p>\u000a\u000a<ul>\u000a<li>Events: <code>pointerdown, pointermove, pointerup</code></li>\u000a<li>Event payload class: <code>originalEvent, pointerType, getPointerList()</code></li>\u000a<li>Pointer class: <code>x, y, type</code></li>\u000a</ul>\u000a\u000a<p>To use it, simply include <code>pointer.js</code> in your web page. This\u000aautomatically rigs <code>addEventListener</code> with support for <code>pointer*</code> and\u000a<code>gesture*</code> events.</p>\u000a\u000a<p>Try some simple pointer.js demos:</p>\u000a\u000a<ul>\u000a<li><a href="http://borismus.github.com/pointer.js/demos/draw.html">Multi-touch drawing</a></li>\u000a<li><a href="http://borismus.github.com/pointer.js/demos/basic-pointers.html">Pointer event logger</a></li>\u000a<li><a href="http://borismus.github.com/pointer.js/demos/basic-gestures.html">Gesture event logger</a> (supports scale, longpress and doubletap)</li>\u000a</ul>\u000a\u000a<p>For more info about the library, check it out at\u000a<a href="https://github.com/borismus/pointer.js">https://github.com/borismus/pointer.js</a>. Contributions in the form of\u000apull requests are most welcome: more demos using pointer events,\u000aunimplemented gesture recognizers for common gestures, like swipe and\u000arotation, and tweaks to the system itself.</p>\u000a
p1912
tp1913
Rp1914
sg13
V/mouse-touch-pointer
p1915
sg15
Nsg16
I01
sg17
VGeneralized input on the cross-device web
p1916
sg20
V**Update (August 7, 2013): Pointer.
p1917
sg6
V<p><strong>Update (August 7, 2013): Pointer.js is deprecated. Please use the\u000a<a href="https://github.com/Polymer/PointerEvents">PointerEvents polyfill</a> instead.</strong></p>\u000a\u000a<p>Mouse will soon cease to be the dominant input method for computing,\u000athough it will likely remain in some form for the forseeable future.\u000aTouch is the heir to the input throne, and the web needs to be ready.\u000aUnfortunately, the current state of input on the web is... you guessed\u000ait: a complete mess! There are two separate issues:</p>\u000a\u000a<ol>\u000a<li>No unified story between mouse, touch, and other spatial input.</li>\u000a<li>Poor support for complex gestures, especially needed for touch.</li>\u000a</ol>\u000a\u000a<p>I'll look at each in a bit of detail, and then show you\u000a<a href="https://github.com/borismus/pointer.js">pointer.js</a>. </p>\u000a\u000a
p1918
sg25
g169
sg33
g1915
sg170
(dp1919
g172
S'Jun'
p1920
sg174
S'June 14, 2012'
p1921
sg176
I6
sg177
S'2012-06-14T09:00:00-00:00'
p1922
sg179
I1339689600
sg180
I2012
sg181
I14
ssg65
g182
sg31
S'mouse-touch-pointer'
p1923
sS'posted'
p1924
g188
(S'\x07\xdc\x06\x0e'
p1925
tp1926
Rp1927
ssg34
S'content/posts/2012/mouse-touch-pointer/index.md'
p1928
sg36
F1375937485.0
sa(dp1929
g2
(dp1930
g4
V One of my ongoing projects is called smusique, a mobile web-based sheet music viewer. The application was designed for tablets, enabling a rich browsing experience through sheet music. Though some sheet music is encumbered by licenses, most of the classic stuff is legally available online via sheet music databases such as the  Petrucci Library  at  imslp.org . 
p1931
sg28
g7
(g8
g9
V<p>One of my ongoing projects is called smusique, a mobile web-based sheet\u000amusic viewer. The application was designed for tablets, enabling a\u000arich browsing experience through sheet music. Though some sheet\u000amusic is encumbered by licenses, most of the classic stuff is legally\u000aavailable online via sheet music databases such as the <a href="http://imslp.org">Petrucci\u000aLibrary</a> at <a href="http://imslp.org">imslp.org</a>.\u000a<!--more--></p>\u000a\u000a<p>While the focus of my project was to design and develop a delightful\u000atablet-based interface for sheet music, I needed to solve some technical\u000aproblems:</p>\u000a\u000a<ol>\u000a<li>Figure out how to deal with PDFs in the browser.</li>\u000a<li>Create or find a database with a large enough corpus of searchable\u000asheet music.</li>\u000a</ol>\u000a\u000a<p>Read on to find out how I solved these problems using the AppEngine\u000aconversion API and through a Chrome extension that does client-side\u000ascraping.</p>\u000a\u000a<h2>Dealing with PDFs</h2>\u000a\u000a<p>Since digital sheets are largely in PDF format, I needed some solution\u000afor showing PDFs inside a web user interface. Unfortunately\u000ashowing PDFs inline is pretty much impossible using today's web. Ideally,\u000athe browser would have support for something like the following:</p>\u000a\u000a<pre><code>&lt;img src="something.pdf" page="3" /&gt;\u000a</code></pre>\u000a\u000a<p>But this functionality simply doesn't exist in a widely supported\u000afasion. Given how important showing PDFs is for my application, I\u000aneeded a workaround. A few options came to mind:</p>\u000a\u000a<ol>\u000a<li>Attempt to show PDFs in an iframe, and programatically scroll to the\u000aright position based on the page number.</li>\u000a<li>Convert PDFs on the client side using something like <a href="http://andreasgal.com/2011/06/15/pdf-js/">PDFJS</a></li>\u000a<li>Build my own PDF conversion server</li>\u000a</ol>\u000a\u000a<p>I experimented briefly with the first approach and unsurprisingly found\u000amixed levels of browser support for PDF rendering inside iframes. Some mobile\u000abrowsers allowed PDFs to be opened within iframes, but exhibited\u000aunexpected scaling behavior, could not be programmatically scrolled, or\u000aboth.</p>\u000a\u000a<p>Did not seriously consider PDFJS as a solution, judging that running PDF\u000aconversion in a mobile web browser would be prohibitively slow.</p>\u000a\u000a<p>Just as I was bracing to take the plunge and write my own\u000a<a href="http://www.imagemagick.org/">ImageMagick</a>-based conversion API running on Django/Slicehost, I\u000adiscovered the very new <a href="http://code.google.com/appengine/docs/python/conversion/overview.html">AppEngine conversion API</a>.</p>\u000a\u000a<h2>The conversion API</h2>\u000a\u000a<p>Turns out that AppEngine provides a new experimental <a href="http://code.google.com/appengine/docs/python/conversion/overview.html">file format\u000aconversion API</a>. It allows you to map from a variety of input file\u000aformats to a variety of outputs, including PDF to PNGs (one per page),\u000awhich is exactly what I wanted. The full list of conversion paths is\u000aavailable in <a href="http://code.google.com/appengine/docs/python/conversion/overview.html">the docs</a>. Some of the more exciting\u000afeatures include image to text conversions via OCR, and generating images\u000afrom HTML.</p>\u000a\u000a<p>My conversion was really straight forward to implement:</p>\u000a\u000a<pre><code>def convert_pdf(self, pdf_data):\u000a    """Converts PDF to PNG images. Returns an array of PNG data."""\u000a    asset = conversion.Asset('application/pdf', pdf_data, 'sheet.pdf')\u000a    conversion_request = conversion.Conversion(asset, 'image/png')\u000a    result = conversion.convert(conversion_request)\u000a    if result.assets:\u000a        return [asset.data for asset in result.assets]\u000a    else:\u000a        raise Exception('Conversion failed: %d %s'\u000a                % (result.error_code, result.error_text))\u000a</code></pre>\u000a\u000a<p>Note that the API methods recently changed (maybe in AppEngine 1.6?)\u000afrom <code>conversion.ConversionRequest</code> to <code>conversion.Conversion</code>. I should\u000ahave expected breaking changes since the conversion API is still\u000aexperimental, but it stumped me for a little while anyway.</p>\u000a\u000a<p>The API works pretty well, but doesn't provide much meaningful feedback\u000aif running in the dev server. So far I've only managed to overload the\u000aconversion service a few times with very large (eg. ~100 page) PDFs.\u000aThat said, a lot of sheet music is incredibly long, especially in an\u000aorchestral setting. So, if I was doing this for production, I would\u000aprobably need to build a custom solution.</p>\u000a\u000a<p>One other caveat with this approach is that I have no idea how much\u000amoney I would be charged for using this service. Conversion is quite\u000aintensive, and given the recently increased rates, I would be wary.</p>\u000a\u000a<p>Once the images are converted, I upload them to an Amazon S3 instance\u000awhere I keep my data. To do this, I use <a href="http://aws.amazon.com/code/134">S3.py</a>, a really simple\u000alibrary for interacting with Amazon S3:</p>\u000a\u000a<pre><code>def upload_helper(self, path, data, contentType):\u000a    conn = S3.AWSAuthConnection(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)\u000a    options = {'x-amz-acl': 'public-read', 'Content-Type': contentType}\u000a    response = conn.put(DEFAULT_BUCKET, path, S3.S3Object(data), options)\u000a    return URL_FORMAT % {'bucket': DEFAULT_BUCKET, 'path': path}\u000a</code></pre>\u000a\u000a<p>The full code for the AppEngine server is open source on <a href="https://github.com/borismus/smusique-uploader">github</a>.</p>\u000a\u000a<h2>Client-side scraping</h2>\u000a\u000a<p>Unfortunately IMSLP doesn't provide a useful API out of the box. I\u000awasn't ambitious enough to create a full API for it, but needed some\u000ainterim solution. I figured that for a demo, it wouldn't be such a\u000aterrible experience to have to seed the database with the repertoire you\u000awere interested in if it was easy enough to do.</p>\u000a\u000a<p>Ordinarily, one might write a little scraper utility in their favorite\u000ascripting language. However, scraping HTML from python (or any other\u000ascripting language for that matter) is really not my favorite activity.\u000aAdditionally, relying on the command line excludes the target audience\u000a(musicians) for this application.</p>\u000a\u000a<p>As a workaround, I came across a potentially interesting idea:\u000aclient-side scraping with a Chrome extension. Let me explain.</p>\u000a\u000a<p>Suppose you want to scrape part of a corpus of data that's available\u000athrough a website, but want to let the user decide which parts they are\u000ainterested in. Simply use a Chrome extension that injects code into the\u000atarget page, fetches the interesting part of the DOM using selectors and\u000aperhaps jQuery for convenience, and then uploads the data to some\u000aserver. I used <a href="http://code.google.com/chrome/extensions/content_scripts.html">content scripts</a> for this purpose. In the manifest, the\u000aentry looks as follows:</p>\u000a\u000a<pre><code>"content_scripts": [{\u000a  "matches": ["http://imslp.org/wiki/*"],\u000a  "js": ["imslp.js"],\u000a  "css": ["imslp.css"]\u000a}]\u000a</code></pre>\u000a\u000a<p>Then, <code>imslp.js</code> does the scraping, which can happen automatically as a\u000auser navigates through a page, or by adding extra elements to the page.\u000aThis simple IMSLP scraper creates a "send to smusique" button beside\u000aeach PDF on IMSLP:</p>\u000a\u000a<p><img src="http://i.imgur.com/6YhZ2.png" alt="screenshot" /></p>\u000a\u000a<p>Once clicked, <code>imslp.js</code> gets the URL to fetch, extracts all of the meta\u000adata of the current piece via CSS selectors, and makes a cross-domain\u000arequest to the converter with all of this information.</p>\u000a\u000a<p>This approach is much harder to prevent than traditional, server side\u000ascraping. Although setting UserAgent restrictions is futile, many sites\u000ause JavaScript to render their content, and serve up a content-free\u000apage. In contrast, there's very little a content provider can do to\u000aprotect themselves from client side scraping, making this a very\u000apowerful technique. If the content is in the DOM, it can be extracted.</p>\u000a\u000a<p>The source of the extension is somewhat messy but available also on\u000a<a href="https://github.com/borismus/smusique-extension">github</a>.</p>\u000a
p1932
tp1933
Rp1934
sg13
V/pdf-conversion-client-side-scraping
p1935
sg15
Nsg16
I01
sg17
VPDF conversion and client-side scraping
p1936
sg20
V\u000a\u000aOne of my ongoing projects is called smusique, a mobile web-based sheet\u000amusic viewer.
p1937
sg6
V<p>One of my ongoing projects is called smusique, a mobile web-based sheet\u000amusic viewer. The application was designed for tablets, enabling a\u000arich browsing experience through sheet music. Though some sheet\u000amusic is encumbered by licenses, most of the classic stuff is legally\u000aavailable online via sheet music databases such as the <a href="http://imslp.org">Petrucci\u000aLibrary</a> at <a href="http://imslp.org">imslp.org</a>.\u000a
p1938
sg25
g169
sg33
g1935
sg170
(dp1939
g172
S'Feb'
p1940
sg174
S'February 8, 2012'
p1941
sg176
I2
sg177
S'2012-02-08T09:00:00-00:00'
p1942
sg179
I1328720400
sg180
I2012
sg181
I8
ssg65
g182
sg31
S'pdf-conversion-client-side-scraping'
p1943
sS'categories'
p1944
(lp1945
S'web'
p1946
aS'extension'
p1947
asS'posted'
p1948
g188
(S'\x07\xdc\x02\x08'
p1949
tp1950
Rp1951
ssg34
S'content/posts/2012/pdf-conversion-client-side-scraping/index.md'
p1952
sg36
F1433825570.0
sa(dp1953
g2
(dp1954
g4
V There's an increasing variety of devices in use today. Even generally rectangular touch enabled devices vary hugely in their physical sizes, aspect ratios, pixel densities, etc.    One thing that remains constant across these devices are their users. Technologies come and go every year, but people stay the same. Existing form factors:  pads, tabs and boards  still make sense, and will continue to do so for the forseeable future. As a result, ergonomic considerations like touch target sizing, readable text and image size remain constant. Fingers will be fingers and eyes will be eyes! Our bodies are firmly rooted in the physical world, and the interfaces we create should reflect that.   
p1955
sg28
g7
(g8
g9
V<p>There's an increasing variety of devices in use today. Even generally\u000arectangular touch enabled devices vary hugely in their physical sizes,\u000aaspect ratios, pixel densities, etc.</p>\u000a\u000a<p>One thing that remains constant across these devices are their users.\u000aTechnologies come and go every year, but people stay the same. Existing\u000aform factors: <a href="http://en.wikipedia.org/wiki/Smart_device">pads, tabs and boards</a> still make sense, and\u000awill continue to do so for the forseeable future. As a result,\u000aergonomic considerations like touch target sizing, readable text and\u000aimage size remain constant. Fingers will be fingers and eyes will be\u000aeyes! Our bodies are firmly rooted in the physical world, and the\u000ainterfaces we create should reflect that.</p>\u000a\u000a<!--more-->\u000a\u000a<p>Take touch targets, for example. Apple's <a href="http://developer.apple.com/library/ios/#DOCUMENTATION/UserExperience/Conceptual/MobileHIG/Characteristics/Characteristics.html">human interface guidelines\u000arecommend</a> minimum touch target size of 44x44pt for iOS. On\u000aan iPhone, 44pt is about 0.25in. On an iPad, 44pt is about 0.3in, physically\u000alarger because of different device pixel density. Even Apple has such\u000adiscrepancies despite a very simple device landscape. Apple devices only\u000acome in two configurations: iPhone and iPad (and their retina\u000acounterparts).</p>\u000a\u000a<p>The situation gets far more complex when we look at mobile devices in\u000ageneral, which is the world of mobile web developers. In an ideal world,\u000aimages and fonts should be scalable and all units should be physical.\u000aI built a <a href="https://github.com/borismus/physical-units">prototype</a> that illustrates roughly how this\u000amodel would work. Onward for the gory details.</p>\u000a\u000a<h2>Device variance</h2>\u000a\u000a<p>Here's a brief sampling of pixel density and aspect ratio for a few\u000apopular devices (data <a href="http://en.wikipedia.org/wiki/List_of_displays_by_pixel_density">from wikipedia</a>):</p>\u000a\u000a<pre><code>Device          DPI   Aspect\u000a============================\u000aiPhone*         163      3:2\u000aiPad*           132      4:3\u000aNexus S*        117      5:3\u000aGalaxy Nexus*   158     16:9\u000aGalaxy Note*    142      8:5\u000aGalaxy Tab      149      8:5\u000aT-Prime         149      8:5\u000aLumia 900       217     10:6\u000a</code></pre>\u000a\u000a<p>The ones with a <code>*</code> denote double density devices. In other words, the\u000aphysical pixel density is double the value indicated, but the browser\u000agenerally reports the resolution to be this halved value. For more\u000ainformation about this practice, read <a href="http://www.quirksmode.org/blog/archives/2010/04/a_pixel_is_not.html">a pixel is not a pixel</a>.</p>\u000a\u000a<p>Thus, in practice, a 44x44px button rendered with a reasonable viewport:\u000a<code>&lt;meta name="viewport" content="width=device-width"&gt;</code> will vary in\u000adimensions between 0.20in on the Lumia 900 to nearly double, 0.38in on\u000athe Nexus S.</p>\u000a\u000a<p>Further complicating things is a somewhat obscure fact that pixels might\u000anot be square at all. The term for it is <a href="http://en.wikipedia.org/wiki/Pixel_aspect_ratio">pixel aspect\u000aratio</a>. As far as I can tell, this is only a concern when\u000adisplays aren't using their native resolutions, which luckily is a\u000ararity in mobile devices.</p>\u000a\u000a<h2>The problem with pixels</h2>\u000a\u000a<p><a href="http://www.lukew.com/ff/entry.asp?1085">Many HIGs</a> recommend minimum touch target sizes in\u000apixels. What really matters is the size of human fingers, which as we\u000aestablished earlier, don't grow and shrink as a function of the device\u000ayou happen to be using.</p>\u000a\u000a<p>Microsoft, anticipating a more fragmented ecosystem of devices with\u000avaried densities and screen sizes, is recommending a physical size\u000aapproach instead, which makes more sense for touch targets. They suggest\u000aa 9x9mm recommended lower bound, and an absolute minimum of 7x7mm. This\u000ajives well with established research, such as MIT's <a href="http://touchlab.mit.edu/publications/2003_009.pdf">Touch Lab study of\u000aHuman Fingertips to Investigate the Mechanics of Tactile Sense</a>,\u000awhich found that the average human finger pad is 10-14mm and the average\u000afingertip is 8-10mm in diameter.</p>\u000a\u000a<h2>Benefits of physical units</h2>\u000a\u000a<p>As a developer, I'd like to just say "this button is 40x9mm" and have\u000athat actually map to real physical dimensions. If I could ensure usable\u000asizes for all of my touch targets, that solve this problem across all\u000adevices, once and for all!</p>\u000a\u000a<p>Physical units also make feature-based device detection far more\u000areliable. Currently, the best we can do is basically guess what sort of\u000adevice is visiting your site, using heuristics like device resolution\u000a(in magic CSS pixels that automatically get scaled). Using this\u000aapproach, it's hard to distinguish between laptops and tablets, for\u000aexample. It's even more challenging to distinguish phones from 7"\u000atablets, or 7" tablets from 10" ones. Approaches like <a href="https://github.com/borismus/device.js">device.js</a> would\u000abenefit greatly.</p>\u000a\u000a<p>Finally, as you saw, using pixels results in a huge variance in font\u000asize, resulting in bad text readability in general. With physical units,\u000ayou can set a good default baseline size for text to ensure readability.</p>\u000a\u000a<p>Of course, in all of these cases, it's possible to manually rescale the\u000aview in order to get the desired zoom level that's readable and\u000atouchable, but the whole point is to have a system that behaves well by\u000adefault.</p>\u000a\u000a<h2>Scaling well</h2>\u000a\u000a<p>So, in this world of physical sizes for everything, virtually everything\u000aneeds to be scaled to the appropriate size. This includes text, images,\u000aand general layout (eg.  sizes, coordinates, etc). This means having\u000aassets, fonts and a layout engine that's capable of scaling well,\u000awithout loss of visual fidelity.</p>\u000a\u000a<p>General layout is pretty easy to do - all you need is to convert real\u000aunits into pixels for actually rendering. Text gets a bit tricker, since\u000amost fonts are defined only at certain key-sizes, and scaling to\u000afractional sizes might not work perfectly. Scaling images, of course, is\u000aa whole separate topic. Rasters are very difficult to scale down without\u000acompromising quality, and scaling up creates a blurry or pixelated\u000alooking image. The obvious solution is to use scalable image formats. On\u000athe web, this basically means SVG, which is <a href="http://caniuse.com/#search=svg">quite well\u000asupported</a> across browsers, but does have its own\u000aidiosyncrasies.</p>\u000a\u000a<h3>Vectors and rasters</h3>\u000a\u000a<p>Still, even with vector images, there are potential arguments to be made\u000aabout the superiority of pixel-perfect assets, since the designer has\u000aabsolute control to decide their assets at the pixel level.\u000aUnfortunately, the pixel-perfect approach causes a lot of pain for\u000adesigners, who are routinely forced to create multiple versions of the\u000asame asset. On iOS, you specify both the regular and retina asset. On\u000aAndroid, you specify four: <code>ldpi</code>, <code>mdpi</code>, <code>hdpi</code> and <code>xhdpi</code>. These\u000adon't actually get scaled, but the closest one gets served based on the\u000adevice DPI. This means that it's impossible to create assets in an exact\u000aphysical size. Here's an excerpt from the <a href="http://developer.android.com/guide/practices/screens_support.html">Android docs</a>:</p>\u000a\u000a<p><img src="android-dpi.png" alt="Android assets and device DPI" /></p>\u000a\u000a<p>On the web, you can't realistically hope for pixel perfection because of\u000athe vast variety of devices and browsers. Designers need to embrace the\u000amedium, and do as well as they can given the constraints of the web.\u000aScaling the same image (vector or raster) may not be adequate for other\u000areasons. In some cases, especially with icons, you want to specify\u000aassets with different levels of detail depending on the physical size.\u000aCheck out a few <a href="http://mrgan.tumblr.com/post/708404794/ios-app-icon-sizes">great</a> <a href="http://www.pushing-pixels.org/2011/11/04/about-those-vector-icons.html">posts</a> on this subject.</p>\u000a\u000a<blockquote>\u000a  <p>It\u2019s simply not possible to create excellent, detailed icons which can\u000a  be arbitrarily scaled to very small dimensions while preserving clarity.</p>\u000a</blockquote>\u000a\u000a<p><img src="http://farm7.static.flickr.com/6224/6311957505_6f15b6f925.jpg" alt="Varying levels of details depending on size" /></p>\u000a\u000a<p>That said, there's no reason why icons with appropriate levels of detail\u000afor smaller and larger sized screens shouldn't use the same scalable,\u000aphysical approach. In the above example, using scaled assets would let\u000athe designer create 4 instead of 5 separate assets, since the 128px and\u000a64px versions are the same, just scaled.</p>\u000a\u000a<p>It's not all flowers and sausages, though. Vectors are inherently less\u000aefficient to deal with, since they require the intermediate step of\u000arasterization before they can be blitted to the screen buffer. This is\u000aexpensive, but not a show stopper. Optimizing this issue is a topic that\u000aprobably warrants a whole article on its own, but one major win could be\u000ato rasterize once depending on your device DPI, and then cache the\u000arasterized asset so that you don't need to rasterize the vector every\u000atime you render.</p>\u000a\u000a<h2>Doing it on the web</h2>\u000a\u000a<p>If you want to figure out your phone's exact physical dimensions in a\u000abrowser based application, you're out of luck. Even units that share a\u000aname with physical units (like <code>inch</code>, <code>cm</code>, etc) don't actually render\u000athat way. Here's a <a href="http://lewisnyman.github.com/Where-are-our-absolute-units--Demo/">test page</a> that renders some text with\u000aCSS inches. If you measure the actual output on your laptop or phone,\u000ayou'll notice that it's not a real inch.</p>\u000a\u000a<p>Now, you could imagine doing some monkey patching to enable physical\u000aunits using JavaScript, but that's also impossible to do cleanly since\u000ayou need to know either the true device DPI or the physical size of\u000athe screen, neither of which is queriable in any way on the web.\u000aJavaScript does provide <code>window.devicePixelRatio</code>, but that only reports\u000athe scale factor for pixel density, which is used for retina and other\u000adisplays where CSS pixels differ from physical pixels.</p>\u000a\u000a<h2>How does it feel?</h2>\u000a\u000a<p>Even though it's impossible to get physical size or DPI from a browser,\u000aI wanted to try it out, to see if this would work in practice. Here's a\u000a<a href="https://github.com/borismus/physical-units">prototype library</a> that uses entirely physical units\u000afor everything, and fully scalable assets. All objects shown on <a href="http://borismus.github.com/physical-units/controls.html">this\u000acontrols page</a> should render to the correct physical size on\u000athe following devices: iPhone, Galaxy Nexus, iPad, MacBook Air 13". Go\u000aahead and pull out a ruler (I did) and measure - the sizes should be\u000aexact across the supported devices.</p>\u000a\u000a<p>Here's a screenshot of a <a href="http://borismus.github.com/physical-units/sample.html">basic phone UI</a> built with only physical\u000aunits and percentages on an iPhone 4S and Galaxy Nexus:</p>\u000a\u000a<p><img src="android-ios-physical.png" alt="Phones with a UI built using physical units" /></p>\u000a\u000a<h3>Implementation details</h3>\u000a\u000a<p>As mentioned, there's no way to get DPI in the browser. My prototype\u000aworks because I've hard-coded the physical DPI for each supported device\u000abased on user agent matching. It's a terrible hack and not a great\u000aoption for production, even with more complete coverage.</p>\u000a\u000a<p>The prototype currently only works by specifying sizes in absolute\u000aunits. It currently only supports inline styles, and not CSS rules. For\u000aexample, one can now write the DOM like:</p>\u000a\u000a<pre><code>&lt;button style="width: 9mm;"&gt;Nice touch target&lt;/button&gt;\u000a</code></pre>\u000a\u000a<p>I ran into a few small implementation details along the way that I\u000athought might be worth mentioning.</p>\u000a\u000a<p>Firstly, by default, nginx doesn't serve SVG with the correct mime type.\u000aSo, visiting the asset URL just downloads the asset rather than\u000arendering it, and embedding the asset in an <code>img</code> tag doesn't work. The\u000afix is <a href="http://stackoverflow.com/questions/3695409/nginx-offers-of-downoload-svg-instead-of-showing-it">simple and documented</a>: add the correct mime type to an nginx\u000aconfiguration file.</p>\u000a\u000a<p>Secondly, I was a bit disappointed not to find many pre-downloadable SVG\u000aicon sets, but did discover <a href="http://raphaeljs.com/icons/">this resource</a> which has SVG\u000aicons as pathes. To use them it's just a matter of pasting the path\u000aspecification into an SVG <code>path</code> element, as follows:</p>\u000a\u000a<pre><code>&lt;svg width="32" height="32" xmlns="http://www.w3.org/2000/svg"&gt;\u000a  &lt;path d="your-pathspec-goes-here" /&gt;\u000a&lt;/svg&gt;\u000a</code></pre>\u000a\u000a<p>Though unstated, the icons on the above site seem to all be 32x32 units\u000a(Note: not pixels).</p>\u000a\u000a<p>Lastly, since CSS has units that look like physical ones (cm, mm, in,\u000aetc), I wanted to create a new unit, say truemm, truein, etc.\u000aUnfortunately, Chrome (and perhaps other browsers, haven't tested)\u000adoesn't support units that it doesn't recognize. As a result, I had to\u000apiggyback and override existing units. This approach would break the\u000abehavior of sites that use for some unknown-to-me reason, but\u000aintroducing new units prefixed by <code>true</code> is verbose and confusing for\u000abeginners. If you know of any reasons why a developer today would be\u000ausing CSS mms/inches, please let me know.</p>\u000a\u000a<h2>Angular units</h2>\u000a\u000a<p>Rob <a href="http://lists.w3.org/Archives/Public/www-style/2012Feb/0948.html">writes</a>:</p>\u000a\u000a<blockquote>\u000a  <p>The only use-cases for truemm are when you need content matching the\u000a  size of some real-world object, e.g. a ruler, or a life-size image, or\u000a  a human fingertip. That's it.</p>\u000a</blockquote>\u000a\u000a<p>I disagree. It so happens that generally touch devices are used at a\u000arelatively fixed distance from screen to user. This makes physical units\u000auseful for not just touch targets, but also text and image readability.</p>\u000a\u000a<p>But in the same breath, Rob makes a great point:</p>\u000a\u000a<blockquote>\u000a  <p>Ask yourself, "do I really want this content to be the same physical\u000a  size on a phone and a wall projector?" If the answer is yes, use\u000a  truemm, otherwise don't.</p>\u000a</blockquote>\u000a\u000a<p>Indeed, there are cases where physical units aren't ideal. The\u000aconditions for this are roughly:</p>\u000a\u000a<ol>\u000a<li>Unknown distance from viewer to screen.</li>\u000a<li>Unknown screen size.</li>\u000a</ol>\u000a\u000a<p>In this case, the reasonable thing to do is adopt an angular unit, which\u000awould be able to scale depending on viewing distance from the screen\u000a(and of course DPI too).  This might mean that you use degrees, <a href="http://en.wikipedia.org/wiki/Minute_of_arc">minutes\u000aof arc</a>, or some arbitrary angular unit "au" (not to be confused\u000awith <a href="http://en.wikipedia.org/wiki/Astronomical_unit">AU</a>).</p>\u000a\u000a<p>The idea of angular units isn't new. In fact, surprisingly, the CSS spec\u000afeatures it quite prominently in the definition of CSS pixels. A <a href="http://inamidst.com/stuff/notes/csspx">recent\u000aarticle</a> on this topic reminded that pixels are actually\u000aspecified to be angular in nature.</p>\u000a\u000a<blockquote>\u000a  <p>The reference pixel is the visual angle of one pixel on a device with\u000a  a pixel density of 96dpi and a distance from the reader of an arm's\u000a  length. For a nominal arm's length of 28 inches, the visual angle is\u000a  therefore about 0.0213 degrees. For reading at arm's length, 1px thus\u000a  corresponds to about 0.26 mm (1/96 inch). -- <a href="http://www.w3.org/TR/CSS21/syndata.html#length-units">CSS spec</a></p>\u000a</blockquote>\u000a\u000a<p>There are a couple of problem with this.</p>\u000a\u000a<ol>\u000a<li><p>Poor naming: calling such a unit a pixel is incredibly confusing.</p></li>\u000a<li><p>Lack of flexibility: this approach assumes a 96dpi display and a\u000afixed viewing distance. As previously discussed, the former is a bad\u000aassumption, and in the case of large format viewing, so is the latter.</p></li>\u000a</ol>\u000a\u000a<p>Introducing true angular units that can be configured based on display\u000adistance from the observer would be a boon for designers and engineers\u000aworking on software for TVs and other large-format devices. Some default\u000aviewing distance could be provided, but developers should have a means\u000aof specifying a custom viewing distance (potentially inferred from\u000adevice hardware, as described in <a href="http://www.chrisharrison.net/index.php/Research/LeanAndZoom">this research</a>).</p>\u000a\u000a<h2>Next steps</h2>\u000a\u000a<p>Browser vendors need to start supporting true physical units. One path\u000amight be creating all new units (eg. <code>truemm</code>, <code>truein</code>) or redefining\u000athe old ones to be use absolute values, which in many cases may be what\u000athe developers actually intended.</p>\u000a\u000a<p>These units should also be usable in media queries, so that it's\u000apossible to do things like:</p>\u000a\u000a<pre><code>(max-device-width: 4in)\u000a</code></pre>\u000a\u000a<p>This will enable a more robust way to do device detection via media\u000aqueries a la <a href="https://github.com/borismus/device.js">device.js</a>.</p>\u000a\u000a<p>There appear to be some efforts from Mozilla to have a media query for\u000aDPI, via <a href="https://developer.mozilla.org/en/CSS/resolution">resolution</a>. This is a good start, but there should\u000aalso be a way to get DPI directly via JavaScript, otherwise enterprising\u000adevelopers will need to resort to binary searching through possible DPIs\u000avia <code>window.matchMedia</code>, which is just ridiculous.</p>\u000a\u000a<p>The introduction of a true angular unit with a configurable viewing\u000adistance would be amazing for a web that works well on large-format\u000adisplays.</p>\u000a\u000a<p>Ultimately, UI developers need to understand the serious problems with\u000apixels because this problem isn't going away any time soon.</p>\u000a
p1956
tp1957
Rp1958
sg13
V/physical-units
p1959
sg15
Nsg16
I01
sg17
VLet's get physical (units)
p1960
sg20
VThere's an increasing variety of devices in use today.
p1961
sg6
V<p>There's an increasing variety of devices in use today. Even generally\u000arectangular touch enabled devices vary hugely in their physical sizes,\u000aaspect ratios, pixel densities, etc.</p>\u000a\u000a<p>One thing that remains constant across these devices are their users.\u000aTechnologies come and go every year, but people stay the same. Existing\u000aform factors: <a href="http://en.wikipedia.org/wiki/Smart_device">pads, tabs and boards</a> still make sense, and\u000awill continue to do so for the forseeable future. As a result,\u000aergonomic considerations like touch target sizing, readable text and\u000aimage size remain constant. Fingers will be fingers and eyes will be\u000aeyes! Our bodies are firmly rooted in the physical world, and the\u000ainterfaces we create should reflect that.</p>\u000a\u000a
p1962
sg25
g169
sg33
g1959
sg170
(dp1963
g172
S'May'
p1964
sg174
S'May 2, 2012'
p1965
sg176
I5
sg177
S'2012-05-02T09:00:00-00:00'
p1966
sg179
I1335974400
sg180
I2012
sg181
I2
ssg65
g182
sg31
S'physical-units'
p1967
sS'posted'
p1968
g188
(S'\x07\xdc\x05\x02'
p1969
tp1970
Rp1971
ssg34
S'content/posts/2012/physical-units/index.md'
p1972
sg36
F1433825576.0
sa(dp1973
g2
(dp1974
g4
V I re-designed this site using the  PT Sans font , aiming for appealing typography for optimal readability. Interestingly,        PT Sans is based on Russian sans serif types of the second part of the   20th century, but at the same time has distinctive features of   contemporary humanistic designs.      Since visitors are increasingly coming from a variety of devices, I also created three variants of the site for small, medium and large screens via media queries:   
p1975
sg28
g7
(g8
g9
V<p>I re-designed this site using the <a href="http://www.google.com/webfonts/specimen/PT+Sans">PT Sans font</a>, aiming for\u000aappealing typography for optimal readability. Interestingly,</p>\u000a\u000a<blockquote>\u000a  <p>PT Sans is based on Russian sans serif types of the second part of the\u000a  20th century, but at the same time has distinctive features of\u000a  contemporary humanistic designs.</p>\u000a</blockquote>\u000a\u000a<p>Since visitors are increasingly coming from a variety of devices, I also\u000acreated three variants of the site for small, medium and large screens\u000avia media queries:</p>\u000a\u000a<!--more-->\u000a\u000a<pre><code>@media screen and (max-width: $small) {/* small */}\u000a@media screen and (min-width: $small) and (max-width: $large) {/* medium */}\u000a@media screen and (min-width: $large) {/* large */}\u000a</code></pre>\u000a\u000a<p>Note that the above is valid <a href="http://sass-lang.com/">SCSS</a> (as of version 3.2)! To try it out,\u000ayou may need to install the prerelease via <code>gem install sass --pre</code>.</p>\u000a\u000a<p>As usual, the layout of this site is largely inspired by designers far\u000amore skilled than I, including: <a href="http://viljamis.com/">http://viljamis.com/</a> and\u000a<a href="http://www.markboulton.co.uk/">http://www.markboulton.co.uk/</a>.</p>\u000a\u000a<h2>New engine</h2>\u000a\u000a<p>This blog can now also double as a micro blog, where I can easily <a href="/archive/links">post\u000alinks</a> in a social network-agnostic way. Similarly, it now hosts\u000a<a href="/archive/talks">all of my talks</a>, which used to live on\u000a<a href="http://smustalks.appspot.com">http://smustalks.appspot.com</a>. It's also a lot easier for me to create\u000anew posts.</p>\u000a\u000a<p>All of these structural changes are largely due to me building a\u000acompletely new static site generator which addresses a lot of my pains\u000ausing other static generators. I'll write a separate article about this\u000aengine when I'm ready to release it to the public.</p>\u000a\u000a<h2>No more comments</h2>\u000a\u000a<p>Since the start of this blog, I've struggled with the concept of\u000acomments on my site and in on blogs in general. Even with services like\u000adisqus, which try to integrate your blog commentator personality into\u000aone place, I feel that in practice, commenting on the web is very hard\u000ato keep in one place, and the most interesting discussions end up\u000adistributed in many pockets such as Hacker News, twitter, etc. Having\u000athe in-blog comments as well only makes things messier.</p>\u000a\u000a<p>Other problems include:</p>\u000a\u000a<ul>\u000a<li>Leaving a comment right after reading content encourages unconsidered\u000aresponses, since readers haven't had time to process the content.</li>\u000a<li>Lately my blog has been generating a lot of spam comments that I need\u000ato moderate. I don't have time for that.</li>\u000a<li>People tend to ask for technical support in the comments, which adds\u000avery little value for other readers.</li>\u000a<li>Disqus slows down page load time, and presents a UI that's not easily\u000askinnable.</li>\u000a</ul>\u000a\u000a<p>For a more in-depth analysis about blog comments, see Matt Gemmell's\u000athorough post <a href="http://mattgemmell.com/2011/11/29/comments-off/">on this subject</a>.</p>\u000a\u000a<p>I've disabled comments on all posts. That said, I still want to hear\u000ayour ideas and feedback, and engage in discussion around topics I'm\u000aobviously interested in (enough to write about!)</p>\u000a\u000a<h2>Evolving designs</h2>\u000a\u000a<p>This is the fourth iteration of my blog's design. I apparently do a\u000are-design every year, since I started in 2008.</p>\u000a\u000a<p><style>\u000aarticle img { border: 1px solid gray; }\u000a</style></p>\u000a\u000a<p><img src="v1.png" alt="original un-design" /></p>\u000a\u000a<p>This version was a small modification done to an existing wordpress\u000atheme.</p>\u000a\u000a<p><img src="v2.png" alt="first iteration" /></p>\u000a\u000a<p>I really liked this version for a long time, since it was so clean and\u000aminimal. Unfortunately the header took way too much space, and the\u000atypography left something to be desired.</p>\u000a\u000a<p><img src="v3.png" alt="second iteration" /></p>\u000a\u000a<p>This was the first "responsive" version. It had too many gimmicky,\u000anon-standard design elements.</p>\u000a\u000a<p><img src="v4.png" alt="current iteration" /></p>\u000a\u000a<p>I'm reasonably happy with the current version, especially the typography\u000ain the main body.</p>\u000a\u000a<p>And strangely excited about the engine that powers this blog too\u000a(codenamed <em>Lightning</em>, which generates a lot of static, and is also\u000avery fast! Ha, get it?). Once released, I will write another meta-blog\u000apost about it. For now, though, I wrote a bit about Lightning\u000a<a href="/site">here</a>.</p>\u000a
p1976
tp1977
Rp1978
sg13
V/redesign-2012
p1979
sg15
Nsg16
I01
sg17
VNew design
p1980
sg20
VI re-designed this site using the [PT Sans font][ptsans], aiming for\u000aappealing typography for optimal readability.
p1981
sg6
V<p>I re-designed this site using the <a href="http://www.google.com/webfonts/specimen/PT+Sans">PT Sans font</a>, aiming for\u000aappealing typography for optimal readability. Interestingly,</p>\u000a\u000a<blockquote>\u000a  <p>PT Sans is based on Russian sans serif types of the second part of the\u000a  20th century, but at the same time has distinctive features of\u000a  contemporary humanistic designs.</p>\u000a</blockquote>\u000a\u000a<p>Since visitors are increasingly coming from a variety of devices, I also\u000acreated three variants of the site for small, medium and large screens\u000avia media queries:</p>\u000a\u000a
p1982
sg25
g169
sg33
g1979
sg170
(dp1983
g172
S'Mar'
p1984
sg174
S'March 30, 2012'
p1985
sg176
I3
sg177
S'2012-03-30T09:00:00-00:00'
p1986
sg179
I1333123200
sg180
I2012
sg181
I30
ssg65
g182
sg31
S'redesign-2012'
p1987
sS'posted'
p1988
g188
(S'\x07\xdc\x03\x1e'
p1989
tp1990
Rp1991
ssg34
S'content/posts/2012/redesign-2012/index.md'
p1992
sg36
F1433825581.0
sa(dp1993
g2
(dp1994
g4
V These days, I write a lot more code, and my projects have increased in complexity. Often, a single application brings many kinds of data sources and bleeding edge web features together. On the other hand, most of what I build are prototypes, which need to be churned out quickly, work reliably in demos, and look/feel good.    MVC frameworks help write UI code much more quickly, but there are drawbacks too: there are too many to choose from, they don't interoperate with one another, and if you want to release parts of your code to the open source community, only those developers that use the same framework will benefit. The solution is to create a clear separation between core application logic and MVC UI code. This way you can reuse a lot of code and reduce switching costs.   
p1995
sg28
g7
(g8
g9
V<p>These days, I write a lot more code, and my projects have increased in\u000acomplexity. Often, a single application brings many kinds of data\u000asources and bleeding edge web features together. On the other hand, most\u000aof what I build are prototypes, which need to be churned out quickly,\u000awork reliably in demos, and look/feel good.</p>\u000a\u000a<p>MVC frameworks help write UI code much more quickly, but there are\u000adrawbacks too: there are too many to choose from, they don't\u000ainteroperate with one another, and if you want to release parts of your\u000acode to the open source community, only those developers that use the\u000asame framework will benefit. The solution is to create a clear\u000aseparation between core application logic and MVC UI code. This way you\u000acan reuse a lot of code and reduce switching costs.</p>\u000a\u000a<!--more-->\u000a\u000a<p>Before jumping in, here's a diagram of the approach:</p>\u000a\u000a<p><img src="mvc-js-layers.svg" alt="diagram" /></p>\u000a\u000a<p>The rest of the post is about interop problems with JS MVC frameworks,\u000aand a closer look reasons for taking the above approach.</p>\u000a\u000a<h2>MVC frameworks are great</h2>\u000a\u000a<p>One of my key requirements is to be able to build user interfaces\u000a<strong>quickly</strong>. This means that writing in pure JavaScript is out of the\u000aquestion, because it simply doesn't provide a high enough layer of\u000aabstraction for highly interactive user interfaces, and reduces to\u000aspaghetti code in just a couple of days. Instead of raw JS, use an model\u000aview controller (MVC) framework.</p>\u000a\u000a<p>My current weapon of choice is <a href="http://emberjs.com/">Ember.js</a>, mostly because it\u000amakes many decisions that I agree with, providing easy two-way binding,\u000aa sane templating syntax, observers and computed properties. All-in-all,\u000ait is pretty effective for whipping up consistent UIs quickly.</p>\u000a\u000a<p>But there are problems with buying into an MVC framework.</p>\u000a\u000a<h2>JS libraries are like insects</h2>\u000a\u000a<p>JavaScript frameworks are like insects. There are thousands of them,\u000athey move very quickly, and generally have very short life spans.</p>\u000a\u000a<p>If you've ever taken a hiatus from client-side web development, you were\u000aprobably overwhelmed with the amount of new stuff available when you\u000areturned. The flipside is that many of the frameworks you were familiar\u000awith could have easily disappeared. To be fair, some winners have\u000aemerged in the past, most notably in the utility frameworks, where\u000ajQuery has risen to the top, and Prototype has fallen to obscurity. In\u000aMVC frameworks, however, there are many interesting contenders and, as I\u000awrote earlier, still <a href="http://smus.com/backbone-and-ember/">no clear winner</a>.</p>\u000a\u000a<p>So at this stage, going all-in in on a framework may be a bad idea,\u000abecause what if the community moves on to something else? What if the\u000aframework developers get bored, stop caring or cease maintenance for\u000asome other reasons? I have this feeling all the time, despite framework\u000aauthors promises to the contrary. And this is the case even though I\u000abuild mostly prototypes with relatively short life spans.</p>\u000a\u000a<h2>Switching cost between MVC frameworks is high</h2>\u000a\u000a<p>Once you bite the bullet and decide to invest in a framework, you often\u000ahave no easy way to move your code out of it. If you pick Backbone, but\u000adecide mid-cycle that it's not for you, you are in for a world of hurt:</p>\u000a\u000a<p>Not only do your Models and Views not share the same base classes, they\u000adon't even use the same <strong>class system</strong>. Backbone and Ember provide\u000atheir own class systems that are not compatible. This is a ridiculous\u000aproblem to have, and one unique to JavaScript, which provides a\u000aprototypal inheritance system which is so inconvenient, there are about\u000a<a href="http://goo.gl/hqAHD">a million libraries</a> that add OO-style classes to the\u000alanguage.</p>\u000a\u000a<ol>\u000a<li>Those who use or invent a custom class system in JavaScript that\u000alooks more like traditional OO.</li>\u000a<li>Those who don't believe in classes, or think that JavaScript provides\u000aenough through prototypal inheritance.</li>\u000a</ol>\u000a\u000a<p>For the reasons outlined above, I'm very much in favor of the former\u000aoption: having a language-level class abstraction. This seems to be\u000a<a href="http://h3manth.com/content/classes-javascript-es6">coming soon in ECMAScript 6</a>, and will basically provide\u000asyntactic sugar on top of prototypal inheritance. Having a consistent\u000aclass and module system is one of the main reasons why languages like\u000a<a href="http://www.dartlang.org/">Dart</a>, <a href="http://www.typescriptlang.org/">TypeScript</a> and <a href="http://coffeescript.org/">Coffeescript</a> are increasingly\u000aappealing to me.</p>\u000a\u000a<h2>Coding to a framework restricts your audience</h2>\u000a\u000a<p>Because of a lack of interoperability between frameworks and their class\u000asystems, if you write non-UI code using a framework, only users of that\u000aframework will use your code. It's very unlikely that someone building\u000aan Ember application will want to use your library that uses Backbone\u000aobjects. </p>\u000a\u000a<p>Often your collaborators may have varied tastes and prefer one framework\u000aover another, but including multiple MVC frameworks in the same\u000aapplication gets messy quickly. If you have core functionality that you\u000awant to release, release it in pure JavaScript, not as a jQuery plugin,\u000aor Ember module. Of course use prototypal inheritance and proper\u000aabstraction (or at least, as proper as JS can provide).</p>\u000a\u000a<h2>Solution: defensive architecture</h2>\u000a\u000a<p>To avoid framework and class-system lock-in, I have taken a slightly\u000adifferent approach to developing with JavaScript MVC frameworks. It\u000aaffords the convenience of building a UI with MVC, but keeps the core\u000aof the application flexible.</p>\u000a\u000a<p>The basic idea is to separate the core functionality of the application\u000afrom the user interface into two separate layers. With this separation,\u000ayou can implement the two layers differently:</p>\u000a\u000a<ol>\u000a<li><p>Build the base layer using pure JavaScript prototypal inheritance.\u000aThis is the part you write with the intention of keeping for later.\u000aThis base layer will need an API that you will want to spend a bit of\u000atime honing.  To make the separation crystal clear, you can think of the\u000aUI as a client that uses this API as if it were on the server. This way\u000ayou can avoid creating leaky abstractions.</p></li>\u000a<li><p>Use an MVC framework to implement the UI, and call into the base\u000alayer directly. This lets you move quickly and focus entirely on writing\u000athe user interface. This architecture lets you build your UI on a solid\u000afoundation and avoid getting stuck.</p></li>\u000a</ol>\u000a\u000a<h2>Benefits of this approach</h2>\u000a\u000a<p>You get many benefits by taking this approach:</p>\u000a\u000a<ol>\u000a<li><p>If you want to scrap your existing UI and write a new one very\u000aquickly, you can easily do this and still reuse large chunks of your\u000alogic.</p></li>\u000a<li><p>Clear layer separation leads to more maintainable code.</p></li>\u000a<li><p>Easy to ship the underlying core functionality as a library or\u000astandalone module.</p></li>\u000a<li><p>Easy to write unit tests for the core functionality of the\u000aapplication. Unit tests aren't well suited to user interface code\u000aanyway.</p></li>\u000a</ol>\u000a\u000a<h2>This is why we can't have nice things</h2>\u000a\u000a<p>The lack of a widely used class system is ridiculous. The sheer number\u000aof different JS class systems is a clear signal that this is a big\u000aomission in the language. Similarly for MVC frameworks. A renewed\u000ainterest in JavaScript MVC shows that the web platform needs something\u000abuilt-in to address this problem.</p>\u000a\u000a<p>All other widely used programming languages provide a consistent class\u000asystem, and popular platforms provide a framework for separating\u000aapplication logic and user interface. Until these things come to the\u000aweb, I'll continue to have second thoughts about embracing any\u000aparticular MVC framework or custom class system.</p>\u000a
p1996
tp1997
Rp1998
sg13
V/reusable-js-mvc-frameworks
p1999
sg15
Nsg16
I01
sg17
VReusable JavaScript for MVC frameworks
p2000
sg20
VThese days, I write a lot more code, and my projects have increased in\u000acomplexity.
p2001
sg6
V<p>These days, I write a lot more code, and my projects have increased in\u000acomplexity. Often, a single application brings many kinds of data\u000asources and bleeding edge web features together. On the other hand, most\u000aof what I build are prototypes, which need to be churned out quickly,\u000awork reliably in demos, and look/feel good.</p>\u000a\u000a<p>MVC frameworks help write UI code much more quickly, but there are\u000adrawbacks too: there are too many to choose from, they don't\u000ainteroperate with one another, and if you want to release parts of your\u000acode to the open source community, only those developers that use the\u000asame framework will benefit. The solution is to create a clear\u000aseparation between core application logic and MVC UI code. This way you\u000acan reuse a lot of code and reduce switching costs.</p>\u000a\u000a
p2002
sg25
g169
sg33
g1999
sg170
(dp2003
g172
S'Nov'
p2004
sg174
S'November 6, 2012'
p2005
sg176
I11
sg177
S'2012-11-06T09:00:00-00:00'
p2006
sg179
I1352221200
sg180
I2012
sg181
I6
ssg65
g182
sg31
S'reusable-js-mvc-frameworks'
p2007
sS'posted'
p2008
g188
(S'\x07\xdc\x0b\x06'
p2009
tp2010
Rp2011
ssg34
S'content/posts/2012/reusable-js-mvc-frameworks/index.md'
p2012
sg36
F1433825589.0
sa(dp2013
g2
(dp2014
g4
V About a year ago, I wrote an overview of many of the different  responsive image approaches  in an HTML5Rocks article, all of which try to solve the fundamental problem:     Serve the optimal image to the device.     Sounds simple, but the devil's in the details. For the purposes of this here discussion, I will focus on optimal image size and fidelity, and much to your chagrin, will completely ignore the art direction component of the problem.    Even for tackling screen density, a lot of the solutions out there involve a lot of extra work for web developers. I'll go into two solutions (client and server side) on the horizon that serve the right density images. In both cases, all you need to do is:     &lt;img src="img.jpg"/&gt;     
p2015
sg28
g7
(g8
g9
V<p>About a year ago, I wrote an overview of many of the different <a href="http://www.html5rocks.com/en/mobile/high-dpi/">responsive\u000aimage approaches</a> in an HTML5Rocks article, all of which try to solve the\u000afundamental problem:</p>\u000a\u000a<p><strong>Serve the optimal image to the device.</strong></p>\u000a\u000a<p>Sounds simple, but the devil's in the details. For the purposes of this here\u000adiscussion, I will focus on optimal image size and fidelity, and much to your\u000achagrin, will completely ignore the art direction component of the problem.</p>\u000a\u000a<p>Even for tackling screen density, a lot of the solutions out there involve a\u000alot of extra work for web developers. I'll go into two solutions (client and\u000aserver side) on the horizon that serve the right density images. In both cases,\u000aall you need to do is:</p>\u000a\u000a<pre><code>&lt;img src="img.jpg"/&gt;\u000a</code></pre>\u000a\u000a<!--more-->\u000a\u000a<h2>Nobody cares about responsive images (that much)</h2>\u000a\u000a<p>Let me start with an underlying problem: for one reason or another, most developers\u000adon't really care that much about responsive images. Even if left unsolved,\u000athe images still get to their destination, they're just a little crummier than\u000athey should be. If fidelity doesn't matter much to you and your app, then no\u000abig deal.</p>\u000a\u000a<p>Others may not even know about the problem. If you're not a high density screen\u000auser, you may have not been disappointed by the gulf in quality between crisp\u000aimages in native apps and blurry images in web apps. Some applications may prioritize\u000aperformance over fidelity, and want to deliberately send low resolution images.</p>\u000a\u000a<p><strong>A vast majority of devs know about the problem, but are just waiting for a\u000asolution that works well</strong>. We're all inherently lazy and in my opinion, a\u000areasonable solution is one that requires little to no extra work.</p>\u000a\u000a<h2>Good solutions require almost no extra work</h2>\u000a\u000a<p>How can we serve the optimal image to the device with as little work as\u000apossible?  One approach is to always serve a highly compressed but high density\u000aimage, as I outlined in <a href="http://www.html5rocks.com/en/mobile/easy-high-dpi-images/">Easy High DPI Images</a> on HTML5Rocks. This\u000aapproach is better than nothing, but isn't really optimal since you end up\u000asending high density images to low density screens.</p>\u000a\u000a<p>Two promising standards are on the horizon to wider adoption: the <code>srcset</code>\u000aattribute for <code>img</code> elements, and the <code>CH</code> client hint header.</p>\u000a\u000a<h3>Solution 1: Client-side build step with srcset &amp; friends</h3>\u000a\u000a<p>The <code>srcset</code> attribute <a href="https://www.webkit.org/blog/2910/improved-support-for-high-resolution-displays-with-the-srcset-image-attribute/">recently landed in WebKit</a>, and it looks like\u000aothers will follow. Though it's more terse than <code>&lt;picture&gt;</code> and friends, <code>srcset</code>\u000astill requires quite a bit of extra work to implement:</p>\u000a\u000a<pre><code>&lt;img src="img.jpg" srcset="img-1.5x.jpg 1.5x, img-2x.jpg 2x, img-3x.jpg 3x"&gt;\u000a</code></pre>\u000a\u000a<p><a href="http://www.w3.org/TR/css4-images/#image-set-notation"><code>image-set</code></a> is the CSS equivalent, and looks quite similar.\u000aUnfortunately it requires even more work:</p>\u000a\u000a<pre><code>selector {\u000a  background-image: url(img.jpg);\u000a  background-image: -webkit-image-set(\u000a      url(img-1.5x.jpg) 1.5x, url(img-2x.jpg) 2x, url(img-3x.jpg) 3x);\u000a  background-image: -moz-image-set(\u000a      url(img-1.5x.jpg) 1.5x, url(img-2x.jpg) 2x, url(img-3x.jpg) 3x);\u000a  background-image: -ms-image-set(\u000a      url(img-1.5x.jpg) 1.5x, url(img-2x.jpg) 2x, url(img-3x.jpg) 3x);\u000a  background-image: -o-image-set(\u000a      url(img-1.5x.jpg) 1.5x, url(img-2x.jpg) 2x, url(img-3x.jpg) 3x);\u000a  /* Hehe, moar prefixes! */\u000a}\u000a</code></pre>\u000a\u000a<p>Phew! After you have exploded your markup, you need to generate multiple images\u000aof different sizes and decide on appropriate compression levels for each.</p>\u000a\u000a<p>You'll notice that this extra work is very formulaic. It almost looks like it\u000acould be automated! Let's skip the busywork and write our web pages like we do\u000atoday, specifying a very high quality asset (eg. 3x), and running a build\u000ascript. In your markup, all you need to do is:</p>\u000a\u000a<pre><code>&lt;img src="img.jpg" /&gt;\u000a</code></pre>\u000a\u000a<p>or</p>\u000a\u000a<pre><code>selector {\u000a  background-image: url(img.jpg);\u000a}\u000a</code></pre>\u000a\u000a<p>This magic time-saving script would need to do two things. First, it\u000agenerates images:</p>\u000a\u000a<ol>\u000a<li>Find all image files on the site.</li>\u000a<li>Downsize all image files to the right size depending on desired density breakpoints (eg. <code>1x, 1.5x, 2x, 3x</code>).</li>\u000a<li>Name the images according to some convention (eg. <code>${image}-${density}.${format}</code>).</li>\u000a</ol>\u000a\u000a<p>Image resizing already has a <a href="http://addyosmani.com/blog/generate-multi-resolution-images-for-srcset-with-grunt/">grunt-based solution</a>, and many\u000aothers will surely follow. The second part is rewriting the HTML and\u000aCSS. Here's how it works:</p>\u000a\u000a<ol>\u000a<li>Parse all image references from HTML (eg. <code>img</code>) and CSS (eg. <code>background</code>,\u000a<code>background-image</code>).</li>\u000a<li>Augment all HTML <code>img</code> elements with the right srcset. Augment all CSS\u000a<code>background</code> and <code>background-image</code> properties with the right (and prefixed)\u000aimage-set value.</li>\u000a</ol>\u000a\u000a<p>Now we're talking! And all you need to do is provide one set of high quality\u000aimage assets and add this script to your build step (you have a build step,\u000aright?). Keep writing those <code>&lt;img src&gt;</code>s!</p>\u000a\u000a<h3>Solution 2: Server-side build step with Client-Hints</h3>\u000a\u000a<p>The <a href="http://tools.ietf.org/html/draft-grigorik-http-client-hints-00">Client-Hints proposal</a> (CH) is another promising (read: minimal\u000adeveloper effort required) future direction that would help solve the\u000aresponsive image problem on the server. Ilya Grigorik goes into much\u000amore detail in <a href="http://www.igvita.com/2013/08/29/automating-dpr-switching-with-client-hints/">his post</a>.</p>\u000a\u000a<p>Currently, the main thing a server has to identify a client is its\u000aUser-Agent (UA) header. The UA header is insufficient to infer basic\u000athings like display density, even in conjunction with a <a href="http://en.wikipedia.org/wiki/WURFL">UA\u000adatabase</a>. CH is a new header used to pass information to the\u000aserver about the user agent.  With it, you can specify the\u000a<code>devicePixelRatio</code> (DPR) of your device explicitly:</p>\u000a\u000a<pre><code>CH: dpr=2\u000a</code></pre>\u000a\u000a<p>Once browsers send this CH header, you can imagine some really simple\u000aserver-side logic to serve the best asset for the DPR specified. You will need\u000aeither a smart image generator (and cache) on the server, or a build script for\u000agenerating images at different densities. Luckily this build script is the same\u000aas the first half of solution 2, so less work for us! Once the images are\u000agenerated, it's just a matter of producing the right redirects based on the CH\u000aheader, which Ilya provides his <a href="http://www.igvita.com/2013/08/29/automating-dpr-switching-with-client-hints/">article</a>.</p>\u000a\u000a<p>One benefit of solving this problem server side is that it's universal\u000aand completely transparent to the client. A drawback to the first\u000a(client-side) solution is that it will not work when setting <code>&lt;img src&gt;</code>\u000awith JavaScript, although this can be remedied easily with a loader that\u000ayou use to specify the image asset. In practice, instead of specifying\u000athe image asset directly, you would need to go through a small image URL\u000arewriter. Imagine something like this:</p>\u000a\u000a<pre><code>var imagePath = images.get('img.jpg');\u000a// imagePath is now img-2x.jpg if on a 2x display.\u000aimageEl.src = imagePath;\u000a</code></pre>\u000a\u000a<p>Another benefit of the server-side approach is that there's no need for\u000aparsing HTML and CSS (the second part of the build step) which can be\u000atricky and error prone.</p>\u000a\u000a<h2>Both solutions are good</h2>\u000a\u000a<p>In summary, both solutions have merit, and since <code>srcset</code> has <a href="https://www.webkit.org/blog/2910/improved-support-for-high-resolution-displays-with-the-srcset-image-attribute/">momentum\u000aalready</a>, it should be standardized and broadly supported as soon as\u000apossible. Many designers may not have access to server side configuration, so\u000afor them the client-side build script would make sense. Conversely, many\u000adevelopers that have access to server-side image generators and advanced\u000acaching techniques should take advantage of Client-Hints once it's\u000aavailable, which <a href="https://groups.google.com/a/chromium.org/forum/#!topic/blink-dev/c38s7y6dH-Q">may be soon</a>!</p>\u000a\u000a<p>Now, to write that build script... Any volunteers?</p>\u000a
p2016
tp2017
Rp2018
sg13
V/responsive-image-workflow
p2019
sg15
Nsg16
I01
sg17
VResponsive image workflow
p2020
sg20
VAbout a year ago, I wrote an overview of many of the different [responsive\u000aimage approaches][h5r1] in an HTML5Rocks article, all of which try to solve the\u000afundamental problem:\u000a\u000a**Serve the optimal image to the device.
p2021
sg6
V<p>About a year ago, I wrote an overview of many of the different <a href="http://www.html5rocks.com/en/mobile/high-dpi/">responsive\u000aimage approaches</a> in an HTML5Rocks article, all of which try to solve the\u000afundamental problem:</p>\u000a\u000a<p><strong>Serve the optimal image to the device.</strong></p>\u000a\u000a<p>Sounds simple, but the devil's in the details. For the purposes of this here\u000adiscussion, I will focus on optimal image size and fidelity, and much to your\u000achagrin, will completely ignore the art direction component of the problem.</p>\u000a\u000a<p>Even for tackling screen density, a lot of the solutions out there involve a\u000alot of extra work for web developers. I'll go into two solutions (client and\u000aserver side) on the horizon that serve the right density images. In both cases,\u000aall you need to do is:</p>\u000a\u000a<pre><code>&lt;img src="img.jpg"/&gt;\u000a</code></pre>\u000a\u000a
p2022
sg25
g169
sg33
g2019
sg170
(dp2023
g172
S'Sep'
p2024
sg174
S'September 9, 2013'
p2025
sg176
I9
sg177
S'2013-09-09T09:00:00-00:00'
p2026
sg179
I1378742400
sg180
I2013
sg181
I9
ssg65
g182
sg31
S'responsive-image-workflow'
p2027
sS'posted'
p2028
g188
(S'\x07\xdd\t\t'
p2029
tp2030
Rp2031
ssg34
S'content/posts/2013/responsive-image-workflow.md'
p2032
sg36
F1378651351.0
sa(dp2033
g2
(dp2034
g4
V I just got back from Scotland, where I had the pleasure of attending UIST 2013 in St. Andrews. This was my second time attending, and again it was incredibly engaging and interesting content. I was impressed enough to take notes just like  my last UIST in 2011 . What follows are my favorite talks with demo videos. I grouped them into topics of interest: gestural interfaces, tangibles and GUIs.   
p2035
sg28
g7
(g8
g9
V<p>I just got back from Scotland, where I had the pleasure of attending\u000aUIST 2013 in St. Andrews. This was my second time attending, and again\u000ait was incredibly engaging and interesting content. I was impressed\u000aenough to take notes just like <a href="http://smus.com/uist-2011/">my last UIST in 2011</a>. What\u000afollows are my favorite talks with demo videos. I grouped them into\u000atopics of interest: gestural interfaces, tangibles and GUIs.</p>\u000a\u000a<!--more-->\u000a\u000a<h3>Quadrotor Tricks</h3>\u000a\u000a<p>UIST kicked off with a very compelling demos from Rafaello D'Andrea,\u000aprofessor at ETH, co-founder of Kiva. He currently works on the <a href="http://www.flyingmachinearena.org/">flying\u000amachine arena</a>, a lab at ETH working on quadrotor control systems.</p>\u000a\u000a<p>I really liked the flight assembled architecture idea: a building\u000aassembled by quadrotors.</p>\u000a\u000a<iframe width="560" height="315" src="//www.youtube.com/embed/JnkMyfQ5YfY" frameborder="0" allowfullscreen></iframe>\u000a\u000a<p>Rafaello also showed off a kinect controlled quadrotor. A pointing\u000ainterface to control quadrotors. Other highlights included the ability\u000ato place the quadrotor with your hand, and simulating environments like\u000acontrolled gravity, virtual walls, springs, and damped oscillations.</p>\u000a\u000a<h3>Mime: Compact, Low Power 3D Gesture Sensing</h3>\u000a\u000a<p>An MIT Media Lab group presented a pretty neat approach for gesture\u000atracking combining time-of-flight and RGB cameras. The approach is\u000acompact enough to be embedded on a HUD device like Google Glass.</p>\u000a\u000a<p>The specs are impressive: 100 FPS, sub-centimeter resolution, low-power\u000a(45 mW). Showed glasses hardware with 3 cameras (baseline = face) and an\u000aIR LED. Here's roughly how it works:</p>\u000a\u000a<ol>\u000a<li>Illuminate scene with IR. Backscatter light captured by cameras.</li>\u000a<li>Time-of-flight approach. Source <code>s(t)</code> and response <code>r_n(t)</code>. Look\u000afor time-shifted waveforms.</li>\u000a<li>...Lots of crazy math reducing to convex optimization...</li>\u000a</ol>\u000a\u000a<p>Applications presented were a bit limited, mostly focused on in-air\u000awriting and drawing. They also presented some cringe-worthy menu\u000anavigation. The last and most obvious application was games.</p>\u000a\u000a<h3>Gaze Locking: Passive Eye Contact Detection for Human\u2013Object Interaction</h3>\u000a\u000a<p>Surprisingly insightful project from Columbia based on a simple idea:\u000agaze tracking is hard. Knowing WHERE the user is looking is very\u000adifficult, but knowing IF the user is looking is much easier. I loved\u000athe approach of <a href="http://blog.kenperlin.com/?p=13296">solving the simpler problem</a>.</p>\u000a\u000a<p>Detector approach:</p>\u000a\u000a<ol>\u000a<li>Eye corner detection</li>\u000a<li>Geometric rectification</li>\u000a<li>Mask eye area</li>\u000a<li>Extract features from 96x26px rectangle.</li>\u000a<li>PCA + MDA compression</li>\u000a<li>Binary classifier (gaze locked or not).</li>\u000a</ol>\u000a\u000a<p>They also generated a Gaze Data set (6K images). The detector actually\u000adoes better than human vision. Works well from 18m away, though the\u000apresenter claimed there was no degradation as a function of distance,\u000awhich was very suspicious.</p>\u000a\u000a<p>They also presented a series of compelling applications:</p>\u000a\u000a<ul>\u000a<li>Human-object interaction (very cool video of iPads powering on based\u000aon gaze).</li>\u000a<li>Ad analytics (wow, incredible potential for Google/Signs team).</li>\u000a<li>Sort/filter images by eye contact (as a measure of photo quality).</li>\u000a<li>Gaze-triggered photography (when everyone is looking at the camera).</li>\u000a</ul>\u000a\u000a<p>More info on <a href="http://www.cs.columbia.edu/CAVE/projects/gaze_locking/index.php">the lab's site</a>.</p>\u000a\u000a<h3>BodyAvatar: Creating 3-D Avatars with Your Body and Imagination</h3>\u000a\u000a<p>Setting your avatar in video games is annoying. You basically go through\u000aa wizard based on a GUI. This delightful implementation from Microsoft\u000aResearch uses your body to build your character's avatar. Creation\u000abegins from the first person, as you create a general skeleton for the\u000aavatar. Then the perspective changes to third person as you add\u000acustomizations using gestures. The final stage lets you paint your\u000aavatar from the third person perspective.</p>\u000a\u000a<iframe width="560" height="315" src="//www.youtube.com/embed/yU2Ai18tft4" frameborder="0" allowfullscreen></iframe>\u000a\u000a<p>They also showed some impressive demos of stepping into limbs for\u000aparticularily complex models (eg. butterfly with 6 limbs). Super cool!</p>\u000a\u000a<h3>Sauron: Embedded Single-Camera Sensing of Printed Physical User Interfaces</h3>\u000a\u000a<p>Excellent work from Berkeley showing how a single camera can drive a\u000awhole printed physical UI. The idea is that you 3D print an object,\u000ainsert a camera and have a fully functional input device.</p>\u000a\u000a<p>Sauron simulates full motions of all components, ensures that everything\u000ais visible via ray casting. One problem is that you can't always see the\u000awhole interior. So Sauron modifies the design by extruding inputs,\u000aadding mirrors.</p>\u000a\u000a<p>A good question was asked about doing the same for output. Using a\u000atransparent material you might also be able to light up specific areas\u000aof the prototype, but apparently 3d printers can't print\u000atransparent/translucent plastics. Cool future work might be to design\u000amobile tangibles that snap to a phone and use the phone's camera.</p>\u000a\u000a<iframe width="560" height="315" src="//www.youtube.com/embed/GNdCnmm-cw8" frameborder="0" allowfullscreen></iframe>\u000a\u000a<p>Ok, that's all for vision and gestures. Now on to tangibles:</p>\u000a\u000a<h3>PneUI: pneumatically activated soft materials</h3>\u000a\u000a<p>Ishii's group presented nature-inspired interfaces that are\u000atransformative and responsive. Using mostly air pockets, they set out to\u000acreate tangible UIs inspired by soft marine organisms. Some examples of\u000athe applications:</p>\u000a\u000a<ol>\u000a<li><p>Curvature: folding wristband/phone. Wraps up when placed on wrist.\u000aUnwraps when used as a tablet. Pulsates shape changes to indicate\u000aincoming calls.</p></li>\u000a<li><p>Volume-change based interfaces with underlying origami substructure.\u000aApplication: origami accordion with variable height and input.</p></li>\u000a<li><p>Micro + macro elastomers to create transformable textures.\u000aApplication: "feel" GPS on the steering wheel rather than see/hear.</p></li>\u000a</ol>\u000a\u000a<iframe src="//player.vimeo.com/video/63591283" width="500" height="281" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>\u000a\u000a<p>Mind = blown.</p>\u000a\u000a<h3>Paper Generators: Harvesting Energy from Touching, Rubbing and Sliding</h3>\u000a\u000a<p>Disney research presented a way of harvesting energy from interaction,\u000aprimarily for popup book-type applications. Based mostly on static\u000aelectricity, they used teflon, which has low electron affinity. Rubbing\u000ait on paper causes a discharge. Rubbing generates 500 A, 1200 V.\u000aTapping generates 60 mW.</p>\u000a\u000a<p>The approach is easy to build, printable with conductive ink cartridges.\u000aIn addition to rubbing, showed a bunch of different widgets that can\u000agenerate electricity - buttons, cranks, </p>\u000a\u000a<p>Approach 1: direct energy usage. (eg. animations on e-ink displays.)\u000aApproach 2: store and release if more energy is needed. (eg. actuate servos.)</p>\u000a\u000a<iframe width="560" height="315" src="//www.youtube.com/embed/4WaUcXSfPTg" frameborder="0" allowfullscreen></iframe>\u000a\u000a<h3>Touch &amp; Activate: Adding Interactivity via Active Acoustic Sensing</h3>\u000a\u000a<p>Tsukuba University presented a very cool paper on adding acoustic\u000asensing to hard objects using contact mics and speakers. The basic idea\u000ais that touching an object changes its bounding conditions, depending on\u000ahow it is touched. </p>\u000a\u000a<p>The way it works is they vibrate objects at a wide frequency range and\u000acapture the response.</p>\u000a\u000a<ol>\u000a<li>Attach contact speaker and microphone.</li>\u000a<li>Make the object vibrate, doing a sweep signal from 20-40 KHz (inaudible).</li>\u000a<li>Vibration response determined by object properties.</li>\u000a<li>Extract features via FFT</li>\u000a<li>Classify via SVM</li>\u000a</ol>\u000a\u000a<p>Applications:</p>\u000a\u000a<ul>\u000a<li>Simple music player based on duplo blocks.</li>\u000a<li>Interactive animal body</li>\u000a<li>Grasp recognition system for phone using a case.</li>\u000a</ul>\u000a\u000a<iframe width="560" height="315" src="//www.youtube.com/embed/XgxXi6w8IQc" frameborder="0" allowfullscreen></iframe>\u000a\u000a<p>How cool is that? Anyway, now for something a bit more traditional:</p>\u000a\u000a<h3>Transmogrifiers: Casual Manipulations of Visualizations</h3>\u000a\u000a<p>University of Calgary presented their awesome visualization toolkit.\u000aTheir goal is to enable exploration and manipulation of data that is\u000astored in images with no underlying data.</p>\u000a\u000a<p>The idea is to pick a "lens" shape which acts as a template and is\u000aplaced on an image. Also provide an output shape to serve as the target.</p>\u000a\u000a<p>Applications:</p>\u000a\u000a<ul>\u000a<li><a href="http://upload.wikimedia.org/wikipedia/commons/5/5a/1862_Johnson_and_Ward_Map_or_Chart_of_the_World%27s_Mountains_and_Rivers_-_Geographicus_-_MtsRvrs-j-1861.jpg">Tracing rivers to 1D</a> to compare their lengths.</li>\u000a<li>Mutate data chart types (eg. ring chart ==&gt; bar chart)</li>\u000a</ul>\u000a\u000a<iframe width="560" height="315" src="//www.youtube.com/embed/S1Roi2NOmx8" frameborder="0" allowfullscreen></iframe>\u000a\u000a<h3>Content-based Tools for Editing Audio Stories</h3>\u000a\u000a<p>A Berkeley PhD student showed his project, which aims to edit audio\u000astories (radio shows, podcasts, audio books) at a semantic level much\u000ahigher than the current industry standard (waveforms). Not that\u000atechnically challenging, just a really cool idea. Might be a very\u000acompelling product.</p>\u000a\u000a<p>Cool interactions:</p>\u000a\u000a<ul>\u000a<li>Edit speech (eg. copy, paste) in a text editor.</li>\u000a<li>Lets you pick sentences from a list of takes.</li>\u000a<li>Insert breaths and pauses where needed.</li>\u000a<li>Retarget music by segmenting song by beats and automatically finding music change points.</li>\u000a<li>Specify speech emphasis points manually, and use them as alignment points to music change points.</li>\u000a</ul>\u000a\u000a<iframe width="560" height="315" src="//www.youtube.com/embed/RHtI4G5L31w" frameborder="0" allowfullscreen></iframe>\u000a\u000a<p>Here's to <a href="https://twitter.com/ACMUIST/status/390958095939407872">next UIST</a>. Hang loose!</p>\u000a
p2036
tp2037
Rp2038
sg13
V/uist-2013
p2039
sg15
Nsg16
I01
sg17
VUIST 2013 highlights
p2040
sg20
VI just got back from Scotland, where I had the pleasure of attending\u000aUIST 2013 in St.
p2041
sg6
V<p>I just got back from Scotland, where I had the pleasure of attending\u000aUIST 2013 in St. Andrews. This was my second time attending, and again\u000ait was incredibly engaging and interesting content. I was impressed\u000aenough to take notes just like <a href="http://smus.com/uist-2011/">my last UIST in 2011</a>. What\u000afollows are my favorite talks with demo videos. I grouped them into\u000atopics of interest: gestural interfaces, tangibles and GUIs.</p>\u000a\u000a
p2042
sg25
g169
sg33
g2039
sg170
(dp2043
g172
S'Oct'
p2044
sg174
S'October 25, 2013'
p2045
sg176
I10
sg177
S'2013-10-25T09:00:00-00:00'
p2046
sg179
I1382716800
sg180
I2013
sg181
I25
ssg65
g182
sg31
S'uist-2013'
p2047
sS'posted'
p2048
g188
(S'\x07\xdd\n\x19'
p2049
tp2050
Rp2051
ssg34
S'content/posts/2013/uist-2013.md'
p2052
sg36
F1433825443.0
sa(dp2053
g2
(dp2054
g4
V As usual, I want two conflicting things. Firstly, I want to own the content I write, and control how it is authored. My weapon of choice is MacVim and  Lightning , a static blog engine I wrote to address my  very specific requirements :          Secondly, I want people to read the things I write and follow the stories that I link to, since it feels good, and sometimes generates interesting discussions. I wrote a Mac GUI that automates link blogging and  POSSE style  cross-posting to social networks.   
p2055
sg28
g7
(g8
g9
V<p>As usual, I want two conflicting things. Firstly, I want to own the\u000acontent I write, and control how it is authored. My weapon of choice is\u000aMacVim and <a href="https://github.com/borismus/lightning">Lightning</a>, a static blog engine I wrote to\u000aaddress my <a href="http://smus.com/site/">very specific requirements</a>: </p>\u000a\u000a<p><img src="composing-long-post.png" alt="Composing a blog post with Lightning" /></p>\u000a\u000a<p>Secondly, I want people to read the things I write and follow the\u000astories that I link to, since it feels good, and sometimes generates\u000ainteresting discussions. I wrote a Mac GUI that automates link blogging\u000aand <a href="http://indiewebcamp.com/POSSE">POSSE style</a> cross-posting to social networks.</p>\u000a\u000a<!--more-->\u000a\u000a<p>Before I carry on, let me take you back to a simpler time. A time before\u000aJustin Bieber, Black Eyed Peas and social networks. Here's how blogging\u000aused to work: writers wrote to their blogs, and readers subscribed to\u000athem. They would read blog posts from RSS feed readers. Some people ran\u000alink blogs that were kind of like twitter -- short updates discussing a\u000alink. It's was a very clean, federated model. Unfortunately that's not\u000ahow things work today. I have already lamented this fact in a <a href="http://smus.com/really-simple-social-syndication/">previous\u000ablog post</a>.</p>\u000a\u000a<p>For this site, RSS feed readership is a tiny fraction of the inbound\u000atraffic for new posts. Most of non-search traffic comes from social\u000anetworks. As it turns out, the value of these networks is (wait for it)\u000ain their network effect! It only takes a few prominent reshares to have\u000aa post become relatively widely read. By not seeding your content to\u000asocial networks, you lose that benefit.</p>\u000a\u000a<h2>Cross-posting to social networks is hard</h2>\u000a\u000a<p>To seed content to social networks, we look to POSSE. From the\u000a<a href="http://indiewebcamp.com/POSSE">indiewebcamp.com</a> definition:</p>\u000a\u000a<blockquote>\u000a  <p>POSSE is an acronym/abbreviation for Publish (on your) Own Site,\u000a  Syndicate Elsewhere. It's a Syndication Model where the flow involves\u000a  posting your content on your own domain first, then syndicating out\u000a  copies to 3rd party services with perma(short)links back to the\u000a  original version.</p>\u000a</blockquote>\u000a\u000a<p>The problem is that posting to social networks is a pain in the rear.\u000aEach network has its own limitations (eg. 140 characters on Twitter,\u000anetwork-specific markup on G+). There are services which cross-post to\u000athese networks, but they tend to sweep the subtle differences between\u000athe networks under the carpet. For example, services like\u000a<a href="http://manageflitter.com/">ManageFlitter</a> and <a href="http://friendsplus.me/">Friends+Me</a> can cross-post from G+ to\u000aTwitter, but if the post is too long to fit in 140 chars, they include a\u000alink back to the original G+ post. I find cross-linking between social\u000anetworks to be questionable, so I have stopped using such tools.</p>\u000a\u000a<h2>Making link-style updates easier</h2>\u000a\u000a<p>A while ago, I realized that my use of social networks is remarkably\u000aclose to a link blog. While I'll sometimes @reply/comment and +1/star\u000athings, I hardly ever post broadcast-style updates without a URL.</p>\u000a\u000a<p>About a year ago, I added a special "link" type of content on this blog,\u000aspecially for this purpose. This type of content is just like a post,\u000aexcept one link is prominently shown as the title of the post, and the\u000apost itself is focused on commentary about the link. My plan is to use\u000athis type of content more, whenever I want to broadcast a URL and have\u000asomething to say about it.</p>\u000a\u000a<p>So to scratch my itch, I made a little GUI that automates creating a\u000alink entry on the local static blog (with commentary). After the link is\u000aposted and deployed publicly, it also broadcasts the content to\u000asupported social networks, sometimes linking back to the link page on\u000athis site.</p>\u000a\u000a<p><img src="lightning-link.png" alt="Lightning linker Mac GUI" /></p>\u000a\u000a<p>As you can see, there are three fields: the URL, a title (pre-populated\u000afrom the <code>&lt;body&gt;&lt;title&gt;</code> of the URL), and the body of the post (in\u000aMarkdown), all of which are clearly visible in the link page.\u000aTechnically, posting to social networks is easy enough. If they provide\u000aa write API, it's just a matter of doing the OAuth dance and hanging on\u000ato an access token to authorize requests. A more interesting question is\u000ahow to re-arrange the above three fields to form social network updates.</p>\u000a\u000a<h2>Posting to other networks</h2>\u000a\u000a<p>Once the link is posted on the site, Lightning Link has a set of\u000aheuristics to decide what to post to each supported social network. This\u000acan be quite challenging if you are faced with a strict character limit.</p>\u000a\u000a<p>Here are some options I considered for Twitter:</p>\u000a\u000a<ol>\u000a<li>Truncated plaintext body, followed by the URL</li>\u000a<li>Title colon space, the truncated plaintext body, and then the URL</li>\u000a<li>Title and URL</li>\u000a</ol>\u000a\u000a<p>With Option 1, the truncated plaintext body doesn't necessarily reflect\u000athe main idea of the commentary on the link, since the body can be much\u000alonger than 140 chars, and I might just be warming up :) Option 2 leaves\u000avery little room for the body at all, except for about half of a\u000asentence. I went with Option 3, which lends itself well to short,\u000aTwitter-style updates.</p>\u000a\u000a<p>Posting to G+ is relatively easy: take the title and slap on the\u000aplaintextified (from markdown) body, while attaching the URL. </p>\u000a\u000a<p>The other question is about URLs. There are two URLs of relevance in\u000aeach link blog post:</p>\u000a\u000a<ol>\u000a<li>The one on the link blog (eg. <a href="http://smus.com/link/2013/not-terrible-javascript-modules/">http://smus.com/link/2013/not-terrible-javascript-modules/</a>), and</li>\u000a<li>The linked material (eg. <a href="http://github.com/substack/node-browserify">http://github.com/substack/node-browserify</a>).</li>\u000a</ol>\u000a\u000a<p>My approach is to use the linked material directly (2) if the comments\u000acan fit entirely into the space alotted by the social network, falling\u000aback to the link blog URL (1).</p>\u000a\u000a<p>The Lightning Link app isn't general enough for me to recommend unless\u000ayou either like pain, use lightning already, or have a static blog very\u000asetup similar to mine, with a "link" type of post. If you'd still like\u000ato try it, the <a href="https://github.com/borismus/lightning/tree/master/link">code is on github</a>. If you have a similar\u000alink-blogging approach with automatic syndication to social networks,\u000atell me about it!</p>\u000a
p2056
tp2057
Rp2058
sg13
V/easier-link-blogging
p2059
sg15
Nsg16
I01
sg17
VEasier link blogging
p2060
sg20
VAs usual, I want two conflicting things.
p2061
sg6
V<p>As usual, I want two conflicting things. Firstly, I want to own the\u000acontent I write, and control how it is authored. My weapon of choice is\u000aMacVim and <a href="https://github.com/borismus/lightning">Lightning</a>, a static blog engine I wrote to\u000aaddress my <a href="http://smus.com/site/">very specific requirements</a>: </p>\u000a\u000a<p><img src="composing-long-post.png" alt="Composing a blog post with Lightning" /></p>\u000a\u000a<p>Secondly, I want people to read the things I write and follow the\u000astories that I link to, since it feels good, and sometimes generates\u000ainteresting discussions. I wrote a Mac GUI that automates link blogging\u000aand <a href="http://indiewebcamp.com/POSSE">POSSE style</a> cross-posting to social networks.</p>\u000a\u000a
p2062
sg25
g169
sg33
g2059
sg170
(dp2063
g172
S'Jul'
p2064
sg174
S'July 29, 2013'
p2065
sg176
I7
sg177
S'2013-07-29T09:00:00-00:00'
p2066
sg179
I1375113600
sg180
I2013
sg181
I29
ssg65
g182
sg31
S'easier-link-blogging'
p2067
sS'posted'
p2068
g188
(S'\x07\xdd\x07\x1d'
p2069
tp2070
Rp2071
ssg34
S'content/posts/2013/easier-link-blogging/index.md'
p2072
sg36
F1433825541.0
sa(dp2073
g2
(dp2074
g4
V Imagine this: you start conducting as if you were in front of a great orchestra, and music fades in out of thin air, matching your tempo and time signature. Your nuanced gestures can indicate changes in intensity, and of course affect the speed of the piece. You'd first need to learn some basic conducting patterns, like these:        
p2075
sg28
g7
(g8
g9
V<p>Imagine this: you start conducting as if you were in front of a great\u000aorchestra, and music fades in out of thin air, matching your tempo and time\u000asignature. Your nuanced gestures can indicate changes in intensity, and of\u000acourse affect the speed of the piece. You'd first need to learn some\u000abasic conducting patterns, like these:</p>\u000a\u000a<p><img src="conduct-time-signatures.png" alt="Conducting patterns for various time signatures" /></p>\u000a\u000a<!--more-->\u000a\u000a<p>You would also have to wait for me to finish this project, which uses a LEAP\u000amotion device (or your trackpad), the Web Audio API, and some signal processing\u000ato achieve a scaled back version of the idea described above.</p>\u000a\u000a<h2>Prototype</h2>\u000a\u000a<p>The current prototype lets you control the tempo of a song called "Phantom"\u000afrom the excellent <a href="http://www.parovstelar.com/">Parov Stelar</a>. You can do this by making simple\u000aconducting patterns, similar to the 2/4 pattern above. In practice, you can use\u000aany pattern in which your hand oscillates between two points in space to play\u000awith this prototype. You can even use your mouse instead of a LEAP motion\u000adevice. Just click in to enable pointer lock. This will ensure that your mouse\u000awill always be focused inside your browser.</p>\u000a\u000a<p>I built a visualizer which is an 8-bit inspired frequency graph which\u000aalso shows directional changes as pulsating red dots, and clusters which\u000aflash to the beat.</p>\u000a\u000a<p><a href="http://borismus.github.io/gestural-music-direction/"><img src="screenshot.png" alt="Screenshot of leap conductor" /></a></p>\u000a\u000a<p>If you'd like to try it live, the <a href="http://borismus.github.io/gestural-music-direction/">demo lives here</a>.</p>\u000a\u000a<h2>Handling input</h2>\u000a\u000a<p>With a LEAP device plugged in, the prototype maps the palm's 3D center\u000ato 2D. It works just as well with just a trackpad or mouse attached to\u000ayour computer, which directly outputs 2D coordinates. The input handling\u000aalgorithm then uses the resulting (x, y) pairs to do roughly the\u000afollowing:</p>\u000a\u000a<ul>\u000a<li><p>First, track positions and first (velocity) and second (acceleration)\u000aorder history, including times. Store in a ring buffer, which is\u000aimplemented in <a href="https://github.com/borismus/gestural-music-direction/blob/master/js/ring-buffer.js">ring-buffer.js</a>.</p></li>\u000a<li><p>Extract sudden changes of direction based on heuristics related to\u000avelocity and acceleration history.</p></li>\u000a<li><p>Cluster directional changes using K-means or similar clustering\u000aalgorithm which is implemented in <a href="https://github.com/borismus/gestural-music-direction/blob/master/js/clusterizer.js">clusterizer.js</a>. I run this\u000aK-means implementation with 3 values of k in [2, 3, 4] and pick the\u000aone with the lowest error. I've also build a standalone\u000a<a href="http://borismus.github.io/gestural-music-direction/cluster.html">clustering test page</a> with the following output:</p></li>\u000a</ul>\u000a\u000a<p><img src="cluster.png" alt="Clustering algorithm visualization" /></p>\u000a\u000a<ul>\u000a<li><p>To calculate tempo, pick a cluster and calculate mode of the deltas\u000abetween adjacent points.</p></li>\u000a<li><p>The time signature is just the number of clusters over 4 (for the\u000asimple 2/4, 3/4 and 4/4 patterns).</p></li>\u000a</ul>\u000a\u000a<h2>Changing tempo in real-time</h2>\u000a\u000a<p>Once we have an idea of what pattern the user is creating with their hands, we\u000aneed to match up the song to the pattern, and continuously adapt the song's\u000aplayback rate to the user's motions.</p>\u000a\u000a<p>The Web Audio API makes it dead simple to change the playback rate of an audio\u000abuffer for a source node'ss entire duration. However, things get a bit\u000atrickier if this rate changes continuously over time. Chris Wilson\u000adescribes a scheduling technique which addresses this exact problem in\u000ahis <a href="http://www.html5rocks.com/en/tutorials/audio/scheduling/">"Tale of Two Clocks" HTML5Rocks article</a>. You can also see\u000aa simple version of it inaction in his <a href="http://www.html5rocks.com/en/tutorials/audio/scheduling/goodmetronome.html">metronome demo</a>.</p>\u000a\u000a<p>I used this idea to do a bit of granular synthesis on an audio buffer. I\u000aschedule a bit of the buffer into the future, at the current tempo. As the\u000atempo changes, new bits of the buffer are scheduled at a different\u000aplaybackRate. I keep track of how far into the buffer we've gone and use that\u000aas the grainOffset. Here's some code that illustrates this (but see the\u000a<a href="https://github.com/borismus/gestural-music-direction/blob/master/js/music-player.js">variable rate music player</a> for the full code):</p>\u000a\u000a<pre><code>MusicPlayer.prototype.loop_ = function() {\u000a  // Schedule the next bar if it's not yet scheduled.\u000a  while (this.nextNoteTime &lt; audioContext.currentTime + this.scheduleAheadTime) {\u000a    this.scheduleSegment_(this.grainOffset, this.nextNoteTime);\u000a    this.nextNote_();\u000a  }\u000a}\u000a\u000aMusicPlayer.prototype.scheduleSegment_ = function(grainOffset, time) {\u000a  // Get the part of the buffer that we're going to play.\u000a  var source = audioContext.createBufferSource();\u000a  source.buffer = this.buffer;\u000a  source.connect(audioContext.destination);\u000a\u000a  var rate = this.getPlaybackRate_();\u000a  source.playbackRate.value = rate;\u000a\u000a  var secondsPerBeat = 60.0 / this.tempo;\u000a  source.noteGrainOn(time, grainOffset, secondsPerBeat * rate);\u000a}\u000a\u000aMusicPlayer.prototype.nextNote_ = function() {\u000a  // Advance current note and time by a 16th note...\u000a  var secondsPerBeat = 60.0 / this.tempo;\u000a  // Notice this picks up the CURRENT tempo value to calculate beat length.\u000a  this.nextNoteTime += secondsPerBeat;\u000a  // Get the next grain.\u000a  var rate = this.getPlaybackRate_();\u000a  this.grainOffset += secondsPerBeat * rate;\u000a}\u000a</code></pre>\u000a\u000a<p>In practice, I'm unfortunately hitting some rounding errors, so the\u000agrains aren't stitched together as seamlessly as I wanted. You can\u000asometimes hear artifacts if you slow the tempo way down.</p>\u000a\u000a<h2>A work in progress</h2>\u000a\u000a<p>My initial idea was to use <a href="http://developer.echonest.com/">The Echo Nest</a> to pick the right song\u000a(based on time signature and tempo), and then stream that song from some\u000astreaming music service. Unfortunately it's quite hard to get at PCM versions\u000aof tracks from Rdio and Spotify. That said, it can be <a href="https://github.com/oampo/AmbientCloud">done with\u000aSoundcloud</a>. Long story short, the prototype currently only\u000asupports one song.</p>\u000a\u000a<p>A time signature recognizer is mainly useful for classical music, since so much\u000aof popular music is in common time (with <a href="http://twentytwowords.com/2011/05/18/6-pop-songs-in-unusual-time-signatures/">rare exceptions of popular music with\u000acomplex time signatures</a>). But applying simple transformations\u000alike changing the tempo just feels wrong for complex music without a very\u000aobvious rhythmic structure.</p>\u000a\u000a<p>Lastly, LEAP's palm tracking is still quite noisy (even after drastic\u000aimprovements to palm tracking as of <a href="https://developer.leapmotion.com/blog/sdk-0-7-7-released-new-palm-tracking-and-gesture-settings">SDK 0.7.7</a>). Also, the\u000abay windows in my living room lets in tons of infrared light which often puts\u000athe device into a low fidelity tracking mode.</p>\u000a\u000a<p>As always, let me know what you think, and of course, feel free to fork\u000aand evolve on <a href="https://github.com/borismus/gestural-music-direction">github</a>.</p>\u000a
p2076
tp2077
Rp2078
sg13
V/gestural-music-direction
p2079
sg15
Nsg16
I01
sg17
VGestural music direction
p2080
sg20
VImagine this: you start conducting as if you were in front of a great\u000aorchestra, and music fades in out of thin air, matching your tempo and time\u000asignature.
p2081
sg6
V<p>Imagine this: you start conducting as if you were in front of a great\u000aorchestra, and music fades in out of thin air, matching your tempo and time\u000asignature. Your nuanced gestures can indicate changes in intensity, and of\u000acourse affect the speed of the piece. You'd first need to learn some\u000abasic conducting patterns, like these:</p>\u000a\u000a<p><img src="conduct-time-signatures.png" alt="Conducting patterns for various time signatures" /></p>\u000a\u000a
p2082
sg25
g169
sg33
g2079
sg170
(dp2083
g172
S'May'
p2084
sg174
S'May 3, 2013'
p2085
sg176
I5
sg177
S'2013-05-03T09:00:00-00:00'
p2086
sg179
I1367596800
sg180
I2013
sg181
I3
ssg65
g182
sg31
S'gestural-music-direction'
p2087
sS'posted'
p2088
g188
(S'\x07\xdd\x05\x03'
p2089
tp2090
Rp2091
ssg34
S'content/posts/2013/gestural-music-direction/index.md'
p2092
sg36
F1433825538.0
sa(dp2093
g2
(dp2094
g4
V   article img.border {   margin: 0 auto;   max-width: 100%;   box-shadow: inset 0 0 10px #999 }         Have you seen the  extensible web manifesto ? It's the formalization of a recent trend in web standards: a tendency towards lower level APIs. Lower levels of abstraction enable developers to build more on top of a solid foundation. By going down a level of abstraction in the web platform, web developers can contribute to the platform itself in a more fundamental way, working along with browser vendors and spec writers. This is  how the web should work .    But there is a big missing piece in the extensible web vision. Our beloved platform is stuck in a constrictive security sandbox. The "drive by" web's security philosophy is that users of the web should be able to feel safe on any webpage they visit. While very important for the well being of web denizens, it prevents developers from using increasingly important features enjoyed by native platforms such as access to contacts, TCP/UDP sockets, interfaces to external USB/Bluetooth devices. Breaching this sandbox is a huge barrier for the web as a compelling application platform.    Some recent features, such as  getUserMedia , which gives web developers access to the audio and video streams of your device's camera and microphone, have started to break out of the sandbox. There are two approaches to this problem today: (a) infobars and (b) packaged apps. In the rest of this post I'll describe why these are bad solutions, deconstruct them down into small pieces and then glue the pieces back together. The ultimate goal is a modest proposal for installable web apps. Read on for my take on the background of the problem, or skip ahead to read my  illustrated proposal  for fixing it.   
p2095
sg28
g7
(g8
g9
V<p><style>\u000aarticle img.border {\u000a  margin: 0 auto;\u000a  max-width: 100%;\u000a  box-shadow: inset 0 0 10px #999\u000a}\u000a</style>\u000a<a name="problem"></a></p>\u000a\u000a<p>Have you seen the <a href="http://extensiblewebmanifesto.org/">extensible web manifesto</a>? It's the\u000aformalization of a recent trend in web standards: a tendency towards\u000alower level APIs. Lower levels of abstraction enable developers to build\u000amore on top of a solid foundation. By going down a level of abstraction\u000ain the web platform, web developers can contribute to the platform\u000aitself in a more fundamental way, working along with browser vendors and\u000aspec writers. This is <a href="/how-the-web-should-work/">how the web should work</a>.</p>\u000a\u000a<p>But there is a big missing piece in the extensible web vision. Our\u000abeloved platform is stuck in a constrictive security sandbox. The "drive\u000aby" web's security philosophy is that users of the web should be able to\u000afeel safe on any webpage they visit. While very important for the well\u000abeing of web denizens, it prevents developers from using increasingly\u000aimportant features enjoyed by native platforms such as access to\u000acontacts, TCP/UDP sockets, interfaces to external USB/Bluetooth devices.\u000aBreaching this sandbox is a huge barrier for the web as a compelling\u000aapplication platform.</p>\u000a\u000a<p>Some recent features, such as <code>getUserMedia</code>, which gives web developers\u000aaccess to the audio and video streams of your device's camera and\u000amicrophone, have started to break out of the sandbox. There are two\u000aapproaches to this problem today: (a) infobars and (b) packaged apps. In\u000athe rest of this post I'll describe why these are bad solutions,\u000adeconstruct them down into small pieces and then glue the pieces back\u000atogether. The ultimate goal is a modest proposal for installable web\u000aapps. Read on for my take on the background of the problem, or skip\u000aahead to read my <a href="#solution">illustrated proposal</a> for fixing it.</p>\u000a\u000a<!--more-->\u000a\u000a<h2>The extensible web is a good idea</h2>\u000a\u000a<p>There are many recent examples of the extensible web philosophy across\u000amany areas of the web. Because of the low level nature of WebGL and Web\u000aAudio, these technologies open up a wide variety of applications to be\u000abuilt. Under the hood, these APIs are relatively thin wrappers around\u000aunderlying native technology, not compromising performance (much) while\u000aadding developer usability. General purpose low level computing\u000atechnologies like <a href="http://asmjs.org/">asm.js</a> and <a href="https://developers.google.com/native-client/">NaCl</a> enable computationally\u000aintensive algorithms to run far more efficiently. </p>\u000a\u000a<p>Finally, frameworks like <a href="http://www.polymer-project.org/">Polymer</a> use other kinds of low level\u000aAPIs like <a href="http://www.youtube.com/watch?v=fqULJBBEVQE">web components</a> and <a href="https://github.com/Polymer/mdv">model-driven views</a> to let\u000adevelopers invent new types of HTML elements with custom functionality. </p>\u000a\u000a<h2>Sandbox vs. low level APIs</h2>\u000a\u000a<p>Restrictive web security makes a lot of sense. You should never have to\u000aworry about malicious or careless developers erasing files from your\u000alocal filesystem, even if you frequent the most notorious <code>.ru</code> domains!\u000aThat said, if the web is to be a viable application platform that stands\u000aa chance against native platform, it needs to have access to certain\u000adata that is sensitive.</p>\u000a\u000a<p>Many of the APIs that align with the extensible web philosophy have\u000aalready tested the bounds of the web's sandbox. Some have resulted in\u000asecurity vulnerabilities, such as 2011's <a href="http://blog.chromium.org/2011/07/using-cross-domain-images-in-webgl-and.html">cross-domain WebGL texture\u000aattack</a>. Others have required extending the web platform\u000awith additional levels of security. The earliest of these is probably\u000athe geolocation API. More recent additions include <code>getUserMedia</code>, which\u000agives developers a stream of the microphone and camera. These APIs could\u000aobviously lead to very serious privacy breaches if turned on by default\u000aon all web pages. I don't want the Russians knowing where I live, or\u000aeavesdropping on my conversations (NSA already knows).</p>\u000a\u000a<p>The "drive-by" web solves this problem through infobars. I will explain\u000alater why this is a terrible idea. The other solution is packaged web\u000aapps. Packaging circumvent the web completely by copying the\u000adistribution model of native apps, bundling your whole application into\u000aa locally downloaded zip. This model also features an installation step\u000awhich sometimes also grants additional permissions up front. Both of\u000athese so-called solutions reduce the likelihood of new low level APIs\u000afrom coming to the web platform.</p>\u000a\u000a<h2>Infobars are a bad user experience</h2>\u000a\u000a<p>First, look at this:</p>\u000a\u000a<p><img src="infobar-apocalypse.png"/></p>\u000a\u000a<p>This gem, courtesy of <a href="http://persistent.info">Mihai Parparita</a>, is my favorite\u000aexplanation for why infobars suck. You can immediately see several\u000aproblems stemming from the obvious fact that the infobar model does not\u000ascale well. In the infobar world, each feature requires its own\u000apermission, leading to far too many stacked dialogs that are just ugly.\u000aFrom a usability perspective, your users have to click through each one\u000aof the Allow/OK dialogs before they can do anything with your\u000aapplication. If you then reload the page, many of these infobars will\u000aagain return to haunt you, forcing you to click OK five more times\u000abefore being able to use the webapp.</p>\u000a\u000a<p>In some cases, your browser might remember that you accepted an infobar,\u000aand choose not to show it to you again. For example, this happens if you\u000agrant <code>getUserMedia</code> access on an HTTPS site, selecting the "save this\u000apreference" option. This is remembered on a per-domain basis, and in\u000aChrome, is available via <code>Preferences -&gt; Content Settings</code>. In general,\u000aconditions for when exactly the browser remembers how you responded to\u000aan infobar are unclear and underdefined.</p>\u000a\u000a<p>There are also some less obvious issues with the infobar model. Because\u000ainfobars are non-modal, users often don't realize that they have to\u000aaccept them before they can use the webapp's functionality. For example,\u000aif you have a <a href="http://webcamtoy.com">photo booth application</a>, it will be\u000acompletely useless until you accept the "access to video stream"\u000ainfobar, yet many of your users may not notice the infobar at the top of\u000ayour browser window. If you attempt to draw your users' attention to the\u000ainfobar via some illustration, you may end up pointing to the Deny\u000abutton by accident because of variations in placement across various\u000abrowsers and browser versions.</p>\u000a\u000a<p>To summarize, infobars are broken in the following ways:</p>\u000a\u000a<ol>\u000a<li>Does not scale with number of permissions.</li>\u000a<li>Visually jarring at scale. Sometimes not visually obvious enough.</li>\u000a<li>Permission granting should often be modal.</li>\u000a<li>Inconsistent persistence, poor management.</li>\u000a</ol>\u000a\u000a<p>Part of the problem might be addressable via a <a href="https://code.google.com/p/chromium/issues/detail?id=250797">visual\u000arefresh</a> of infobars (as Paul Neave suggests), but I\u000asuspect that a broader rethink of the problem is in order. Until this is\u000aresolved, many new low level APIs will increases Mihai's stack of\u000aapocalyptic infobars, reducing their chance of coming to the platform in\u000athe first place.</p>\u000a\u000a<h2>Packaged apps...</h2>\u000a\u000a<p>Packaged apps are an odd marriage between native app distribution and\u000aweb technologies. The packaged web app model consists of a few moving\u000aparts:</p>\u000a\u000a<ul>\u000a<li>A directory for discovering and installing apps (eg. Firefox\u000aMarketplace, Chrome Web Store).</li>\u000a<li>A set of platform-specific APIs built on top of the web platform for\u000ause in these apps.</li>\u000a<li>A manifest describing each app, which can specify permissions to\u000aenable either the above platform specific APIs or restricted open web APIs.</li>\u000a</ul>\u000a\u000a<p>There are certainly benefits to this approach, such as a sane offline\u000astory, since all of the assets of the application can be packaged\u000atogether into a bundle that is downloaded at install time, circumventing\u000apainful technology like <a href="appcache.png">AppCache</a>. There is a clear install\u000astep, during which you can grant an application permissions beyond the\u000ascope of the open web platform. Also, it's very easy to add features to\u000apackaged apps, since there are no annoying standards to worry about,\u000aamirite?</p>\u000a\u000a<h2>...are bad for the web</h2>\u000a\u000a<p>Unfortunately, packaged web apps provide the worst of both worlds,\u000acombining relatively poor web developer ergonomics with the longer\u000adevelopment and distribution cycle of native apps. Also, many of the\u000adrawbacks of packaging are at odds with the philosophy of the open web\u000aplatform.</p>\u000a\u000a<p>The first and most obvious problem is the lack of URLs for packaged\u000aapps. URLs are critically important as unique identifiers for content\u000afound on the web. They are great for sharing content, indexing, and\u000abookmarking. Secondly, there is no standard packaged app format across\u000aplatforms, which means that the packaging formats and APIs available are\u000acompletely different between Chrome, Firefox, and other packaged app\u000aproviders. This cross-platform aspect is the main economic reason to\u000adevelop for the web. Another drawback is that each of these packaged app\u000avendors has its own app store, sometimes complete with approval\u000aprocesses similar to the much reviled App Store approval flow. </p>\u000a\u000a<p>Lastly, once a browser vendor has a packaged app model, it's very\u000atempting for them to just implement new low level features there and not\u000aon the open web. This effectively lifts the pressure for browser vendors\u000ato go through the pain of standardization. The standard response can now\u000abe "just go build a packaged app". A summary of these issues with packaged\u000aapps:</p>\u000a\u000a<ol>\u000a<li>No URLs</li>\u000a<li>Not cross platform</li>\u000a<li>Dependent on centralized directories</li>\u000a<li>Vendors have an excuse to punt on adding new features to the web\u000aplatform.</li>\u000a</ol>\u000a\u000a<p>Packaged apps are at odds with the web. To the unintiated, it feels as\u000aif their inventors slapped web technology on top of the Apple app store\u000amodel. I know that there are some legitimate, security-motivated reasons\u000afor their decisions, but believe that these are surmountable.</p>\u000a\u000a<h2>Installable web apps</h2>\u000a\u000a<p>If you have an iOS device at your disposal, take a look at\u000a<a href="http://forecast.io/">forecast.io</a>. Forecast.io is an example of an <a href="http://blog.forecast.io/its-not-a-web-app-its-an-app-you-install-from-the-web/">app you install\u000afrom the web</a>. This approach is interesting because it combines\u000athe best of both worlds. On one hand, you retain the benefits of the\u000aweb: URLs, cross-linking, lack of centralized control. On the other, you\u000aget the benefit of elevated permissions.</p>\u000a\u000a<p>A benefit of this approach is that there is a clear install step during\u000awhich you can request additional permissions, which is a natural place\u000afor breaking out of the web's sandbox in a user-friendly manner. The\u000aresult of installation is a homescreen shortcut, which is both a launch\u000aconvenience, and a way of managing permissions. Removing that shortcut\u000acan also mean revoking special permissions for that application.</p>\u000a\u000a<p>Another benefit is that there is no centralized appstore - you can\u000adiscover apps in the same way that you discover the web today - through\u000asearch engines, links in your email inbox, feed readers and through any\u000aother URL-based sharing scheme. There is no reason to conflate\u000ainstallation with the presence of a centralized directory. Google search\u000ais already revealing apps in search results. If you search for "Angry\u000aBirds", you will find both the iOS and Android versions on the first\u000apage.</p>\u000a\u000a<p><a name="solution"></a></p>\u000a\u000a<h1>Proposal: apps you can install from the web</h1>\u000a\u000a<p>So far I've described the <a href="#problem">problem</a>: a major barrier to the\u000avision of the extensible web: there is no good way of getting outside of\u000athe sandbox. I have been complaining a lot without providing any\u000aconstructive answers.</p>\u000a\u000a<p>In order to keep things constructive, the second half of the post\u000aproposes a solution to get us out of the sandbox. There's a whole world\u000aout there! Here is my birds-eye-view of the install-from-the-web world:</p>\u000a\u000a<p><img src="flow.png" alt="flow" /></p>\u000a\u000a<p>This diagram is intended to be general enough to work across\u000aoperating systems and device types, but the mocks themselves will be\u000asketched out with a phone form factor in mind. We'll be installing\u000a<code>app.io</code>, a mobile app that lets you leave audio notes.</p>\u000a\u000a<p><img src="screen1.png" class="border" /></p>\u000a\u000a<p><em>Screen 1: App.io example.</em></p>\u000a\u000a<h2>An API for installing webapps</h2>\u000a\u000a<p>This can be done with an iOS-style approach (and corresponding Chrome\u000afor Android <a href="https://code.google.com/p/chromium/issues/detail?id=153066">feature request</a>), which presents a generic UI\u000afor adding apps to the homescreen (see Screen 2).</p>\u000a\u000a<p><img src="screen2.png" class="border" /></p>\u000a\u000a<p><em>Screen 2: Add via browser button.</em></p>\u000a\u000a<p>There are trade-offs between having a button or an opt-in developer\u000a<strong>API for installing web apps</strong>. With a button, any URL can be added to\u000athe home screen, which may not make sense. But with an API, the\u000adeveloper has to provide an explicit call to action for you to install\u000atheir app. The button UX will always be consistent, since it's part of\u000athe browser. An API-based install path may be ugly or spammy. However,\u000aan API can also provide a consistent experience across browsers without\u000athe need for guessing where each browser places the button. Many\u000aforecast.io-style apps on iOS have callouts on the page pointing to the\u000abutton in the browser chrome which would be broken if another browser\u000ahad a different method of adding to homescreen.</p>\u000a\u000a<p>My opinion is that button- and API-based approaches both have a place.\u000aFor webapps that make more sense installed, the API can be a nice touch.\u000aOther pages might be useful as webapps without their developer realizing\u000ait, so the button-based approach is useful there.</p>\u000a\u000a<p>How would the installation API look like? A JavaScript-based API only\u000acallable on user action, similar to how audio playback in mobile\u000abrowsers prevents the annoying situation where visiting a page\u000aautomatically prompts you to install it. Installing a webapp should\u000acome with a default set of permissions above and beyond what the web\u000aplatform provides.</p>\u000a\u000a<pre><code>var button = document.querySelector('button#install');\u000abutton.addEventListener('click', window.app.requestInstall);\u000a</code></pre>\u000a\u000a<p>You should also be able to request additional permissions at\u000ainstall-time. For example, to request installation and audio capture,\u000athe following code should work:</p>\u000a\u000a<pre><code>window.app.requestInstall({permissions: ['audioCapture']});\u000a</code></pre>\u000a\u000a<p>This action should also result in a standard browser-specific dialog to\u000aaccept installation, showing which permissions have been requested\u000a(Screen 3).</p>\u000a\u000a<p><img src="screen3.png" class="border" /></p>\u000a\u000a<p><em>Screen 3: Confirm installation.</em></p>\u000a\u000a<p>Once accepted, a launcher shortcut should be created (Screen 4). Two\u000apieces of metadata are necessary for this launcher:</p>\u000a\u000a<ol>\u000a<li><p>Icon, which should first look for a large enough version of the\u000a<a href="http://en.wikipedia.org/wiki/Favicon#HTML5_recommendation_for_icons_in_multiple_sizes">multiresolution favicon</a> as determined by the UA. If none\u000aexists, it should look for the <a href="http://goo.gl/6Qdi3">apple-touch-icon</a> in the\u000a<code>&lt;head&gt;</code>. If none is specified, a screenshot of the page can be used\u000aas in iOS.</p></li>\u000a<li><p>Title, which can be extracted from the <code>&lt;title&gt;</code> element in the head.\u000aIf none is specified, the user can be prompted to input their own\u000atitle.</p></li>\u000a</ol>\u000a\u000a<p><img src="screen4.png" class="border" /></p>\u000a\u000a<p><em>Screen 4: New launcher added to the home screen.</em></p>\u000a\u000a<h2>Launching in standalone mode</h2>\u000a\u000a<p>iOS already has an <strong>API to know if a webapp was launched in standalone\u000amode</strong> (ie. from the launcher) or if it was opened from a browser. This\u000afunctionality is available via <code>window.navigator.standalone</code>. It also\u000aopens the app in full-screen mode.</p>\u000a\u000a<p>Other vendors should standardize and implement similar functionality.\u000aFor example, something like <code>window.app.standalone</code>, if only for naming\u000aconsistency could be implemented, and a polyfill provided for the Apple\u000aspec. It would also make sense to launch homescreen apps in full screen,\u000aproviding the same UX as the full-screen API (Screen 5):</p>\u000a\u000a<p><img src="screen5.png" class="border" /></p>\u000a\u000a<p><em>Screen 5: App launched in standalone mode.</em></p>\u000a\u000a<h2>Requesting additional permissions</h2>\u000a\u000a<p>Apps might need additional permissions that go beyond the default\u000abaseline of permissions granted to the app at install time. Access to\u000ayour camera would fall into this bucket. An <strong>API call to request extra\u000apermissions</strong> might look like the following:</p>\u000a\u000a<pre><code>window.app.requestExtraPermissions(['videoCapture']);\u000a</code></pre>\u000a\u000a<p>Running this command would also require user-initiation and prompt a\u000amodal optional permissions dialog (Screen 6) similar to the one seen at\u000ainstallation. After granting it, the associated API call (in this case,\u000a<code>getUserMedia</code>) can be invoked without incurring any infobars.</p>\u000a\u000a<p><img src="screen6.png" class="border" /></p>\u000a\u000a<p><em>Screen 6: Additional permissions request.</em></p>\u000a\u000a<h2>Removing installed web apps and extra permissions</h2>\u000a\u000a<p>If the installed webapp has a native launcher, removing the launcher can\u000ado this implicitly. There should also be a browser- or system- UI\u000asimilar to existing app management interfaces that lets you remove\u000ainstalled apps, or revoke granted permissions.</p>\u000a\u000a<h2>That's it folks</h2>\u000a\u000a<p>So there you have it: my strawman fixing the security model of the web,\u000awhich, as I outlined at the <a href="#problem">beginning of this post</a>, is\u000acritically important to address for the continued success of the web.\u000aTo recap, the solution consists of an API surface in the <code>window.app</code>\u000anamespace, and a number of new screens that are part of the installation\u000aprocess.</p>\u000a\u000a<p>If tackled, this could solve one of the most important issues on the web\u000atoday. Otherwise, we may find ourselves in a place where the web\u000aplatform is irrelevant to application developers, who will just build\u000afor packaged platforms.</p>\u000a\u000a<p>I'm not silly enough to think that this proposal is the ultimately\u000acorrect and best solution for elevated priveledges on the open web.\u000aThere is a huge amount of work required to refine the flow, think of all\u000aof the edge cases, implement it across browsers, etc. The above is just\u000aa draft to re-ignite the web permissions discussion that died several\u000ayears ago. Please blog something in response or in the worst case, tweet\u000aor email your opinion. Looking forward to hearing from you.</p>\u000a\u000a<h1>Update: important links</h1>\u000a\u000a<p><strong>July 18, 2013</strong>: Several people have pointed out that I've missed some\u000aimportant links.  My public apologies!</p>\u000a\u000a<p><a href="https://developers.google.com/chrome/apps/docs/developers_guide">Chrome hosted apps</a> are a somewhat similar concept,\u000abut suffered from security issues that still need to be resolved to make\u000athis proposal a reality. There was even an effort to make hosted web\u000aapps installable from the web, called <a href="http://blog.persistent.info/2011/07/theres-web-app-for-that-site.html">CRX-less web\u000aapps</a> (preserved on Mihai's blog), which today is little\u000amore than a <a href="http://code.google.com/intl/en-US/chrome/apps/docs/no_crx.html">broken link</a>.</p>\u000a\u000a<p>To my knowledge, the most active project along the lines of this\u000aproposal is the <a href="https://developer.mozilla.org/en-US/docs/Web/Apps">Open Web Apps</a> work from Firefox OS. </p>\u000a
p2096
tp2097
Rp2098
sg13
V/installable-webapps
p2099
sg15
Nsg16
I01
sg17
VInstallable webapps: extend the sandbox
p2100
sg20
V<style>\u000aarticle img.
p2101
sg6
V<p><style>\u000aarticle img.border {\u000a  margin: 0 auto;\u000a  max-width: 100%;\u000a  box-shadow: inset 0 0 10px #999\u000a}\u000a</style>\u000a<a name="problem"></a></p>\u000a\u000a<p>Have you seen the <a href="http://extensiblewebmanifesto.org/">extensible web manifesto</a>? It's the\u000aformalization of a recent trend in web standards: a tendency towards\u000alower level APIs. Lower levels of abstraction enable developers to build\u000amore on top of a solid foundation. By going down a level of abstraction\u000ain the web platform, web developers can contribute to the platform\u000aitself in a more fundamental way, working along with browser vendors and\u000aspec writers. This is <a href="/how-the-web-should-work/">how the web should work</a>.</p>\u000a\u000a<p>But there is a big missing piece in the extensible web vision. Our\u000abeloved platform is stuck in a constrictive security sandbox. The "drive\u000aby" web's security philosophy is that users of the web should be able to\u000afeel safe on any webpage they visit. While very important for the well\u000abeing of web denizens, it prevents developers from using increasingly\u000aimportant features enjoyed by native platforms such as access to\u000acontacts, TCP/UDP sockets, interfaces to external USB/Bluetooth devices.\u000aBreaching this sandbox is a huge barrier for the web as a compelling\u000aapplication platform.</p>\u000a\u000a<p>Some recent features, such as <code>getUserMedia</code>, which gives web developers\u000aaccess to the audio and video streams of your device's camera and\u000amicrophone, have started to break out of the sandbox. There are two\u000aapproaches to this problem today: (a) infobars and (b) packaged apps. In\u000athe rest of this post I'll describe why these are bad solutions,\u000adeconstruct them down into small pieces and then glue the pieces back\u000atogether. The ultimate goal is a modest proposal for installable web\u000aapps. Read on for my take on the background of the problem, or skip\u000aahead to read my <a href="#solution">illustrated proposal</a> for fixing it.</p>\u000a\u000a
p2102
sg25
g169
sg33
g2099
sg170
(dp2103
g172
S'Jun'
p2104
sg174
S'June 25, 2013'
p2105
sg176
I6
sg177
S'2013-06-25T09:00:00-00:00'
p2106
sg179
I1372176000
sg180
I2013
sg181
I25
ssg65
g182
sg31
S'installable-webapps'
p2107
sS'posted'
p2108
g188
(S'\x07\xdd\x06\x19'
p2109
tp2110
Rp2111
ssg34
S'content/posts/2013/installable-webapps/index.md'
p2112
sg36
F1433825531.0
sa(dp2113
g2
(dp2114
g4
V Many new devices come with unexpected connectivity - often a WiFi connection that enables them to connect to a hotspot and the larger internet.  Nest , a smart thermostat, was one of the first commercial products to do this. Many more indie projects are following suit, with an explosion of kickstarters like this  teleoperated light ,  connected scale  or this  general purpose connected sensor . The idea of an Internet of Things, in which every appliance and object is somehow connected, has long been popular in academic circles, and this time around it feels like we're actually close.    If we think of these physical devices/appliances as web services with APIs, we can mash them up just like we did in the early days of the web, creating applications that are more useful than the sum of their parts. In this post I argue for using the web as the medium to tie everything together, describe a simple architecture for building networked physical devices and build a web lamp controlled by an arduino.   
p2115
sg28
g7
(g8
g9
V<p>Many new devices come with unexpected connectivity - often a WiFi\u000aconnection that enables them to connect to a hotspot and the larger\u000ainternet. <a href="http://www.nest.com/">Nest</a>, a smart thermostat, was one of the first\u000acommercial products to do this. Many more indie projects are following\u000asuit, with an explosion of kickstarters like this <a href="http://www.kickstarter.com/projects/limemouse/lifx-the-light-bulb-reinvented">teleoperated\u000alight</a>, <a href="http://www.withings.com/">connected scale</a> or this <a href="http://supermechanical.com/twine/">general purpose\u000aconnected sensor</a>. The idea of an Internet of Things, in which\u000aevery appliance and object is somehow connected, has long been popular\u000ain academic circles, and this time around it feels like we're actually\u000aclose.</p>\u000a\u000a<p>If we think of these physical devices/appliances as web services with\u000aAPIs, we can mash them up just like we did in the early days of the web,\u000acreating applications that are more useful than the sum of their parts.\u000aIn this post I argue for using the web as the medium to tie everything\u000atogether, describe a simple architecture for building networked physical\u000adevices and build a web lamp controlled by an arduino.</p>\u000a\u000a<!--more-->\u000a\u000a<h2>Web meets ubiquitous computing</h2>\u000a\u000a<p>The projects I mentioned above don't use proprietary home automation\u000aprotocols like X10.  Instead, they establish connections via the\u000ainternet. Nest registers itself with a central server (operated by Nest\u000ainc) over WiFi. You can then control your thermostat by logging into\u000atheir website at nest.com.</p>\u000a\u000a<p>There's no need for heavyweight ISO666 standardization efforts. This is\u000aa very good thing, because it's really hard for anyone (including our\u000aindustry) to agree on something new. Instead, we reuse a proven protocol\u000athat everyone's already agreed on: HTTP.</p>\u000a\u000a<h2>An Internet of too many things</h2>\u000a\u000a<p>Nest also provides Android and iOS apps to control your thermostat. But\u000awhat happens when you have your thermostat, light bulbs, and home\u000atheater, scanner and <a href="http://www.google.com/cloudprint/learn/">printer</a> all connected up to the internet in\u000athis fashion? Too many apps! Imagine you have all of these different\u000adevices, perhaps multiple of each in some cases:</p>\u000a\u000a<p><img src="many-smart-things.png" alt="Many smart things" /></p>\u000a\u000a<p>It pains me to imagine an app for each of these different services -\u000athere are simply too many things to juggle. I would not want to install\u000aapps for each of these devices for reasons similar to why I would never\u000ainstall the United app: there is significant overhead to managing apps\u000aon your smart phone. They take up space on your home screens, you need\u000ato keep them up to date. <a href="http://jenson.org/">Scott Jenson</a> writes eloquently about\u000athis problem on his blog.</p>\u000a\u000a<p>If each hardware manufacturer releases their device with a central\u000aregistry and public API, however, third parties might be able to build\u000aon top of the services that these devices provide, and certainly some\u000ainteresting and pleasant user experiences will arise in this nascent\u000aarea.</p>\u000a\u000a<h2>A minimum viable physical web architecture</h2>\u000a\u000a<p>There are many discovery protocols (eg. UPnP, zeroconf, ...), and many\u000amessaging protocols (eg. TCP, HTTP, ...), and I won't argue for one over\u000aanother. The technical tradeoffs of each of these is outside of the\u000ascope for this discussion. My approach was to devise the simplest\u000aproof-of-concept that would work anywhere without fancy protocols, only\u000ausing widely available features available on the web, to enable a\u000aplatform for prototyping, the goal being to establish useful and\u000ainteresting interactions first, and implement the underlying technology\u000alater.</p>\u000a\u000a<p>In general, we want to be able to interact with devices that are nearby\u000aand also with devices that are far away. With this constraint, even the\u000asimplest approach requires a server component. Here's a sketch:</p>\u000a\u000a<p><img src="lamp-arch.png" alt="A simple WiFi lamp" /></p>\u000a\u000a<p>In this setup, the controller listens to the server for what it should\u000ado. At the same time, the remote sends commands to the server based on\u000auser interaction. The server keeps track of the state of all of the\u000adevices it manages.</p>\u000a\u000a<h2>Example: cloud lamp</h2>\u000a\u000a<p>The above approach is to simply have a RESTful server. The remote can\u000asend commands to via POST requests, and the controller can poll for its\u000anew state periodically with GET requests. A simple API for a lamp might\u000alook like this:</p>\u000a\u000a<ul>\u000a<li><code>GET /</code>: Returns 1 if the lamp is on. Otherwise returns 0.</li>\u000a<li><code>POST /on</code>: Turns the lamp on.</li>\u000a<li><code>POST /off</code>: Turns the lamp off.</li>\u000a</ul>\u000a\u000a<p>This API and the three components (server, client and device) above are\u000aenough to get a prototype off the ground. A simple implementation of all\u000athree pieces can be found in the <a href="http://github.com/borismus/hello-lamp">Hello Lamp on github</a>.</p>\u000a\u000a<p>The server-side is written on AppEngine, and simply tracks state. The\u000ahandlers all use <a href="http://en.wikipedia.org/wiki/Cross-origin_resource_sharing">CORS</a>, which lifts the cross-domain security policy\u000aof the web, making it possible to POST to the lamp from any web page. </p>\u000a\u000a<p>The client is a really simple web page with two ways of controlling the\u000alamp: the toggle switch and <a href="https://dvcs.w3.org/hg/speech-api/raw-file/tip/speechapi.html">speech recognition</a> (available\u000abehind a flag in Chrome), which enables simple "lamp, turn off" and "on"\u000acommands.</p>\u000a\u000a<p><img src="client.png" alt="Simple client" /></p>\u000a\u000a<p>Finally, the device itself is an Arduino Uno with a <a href="http://arduino.cc/blog/2012/08/16/the-arduino-wifi-shield-is-now-available/">WiFi\u000ashield</a>. One of the pins is connected to a relay which controls\u000aa power socket with the plugged in lamp. The program itself is nearly\u000aidentical to the existing sample apps available for the WiFi shield.</p>\u000a\u000a<p><img src="arduino.png" alt="Arduino WiFi shield" /></p>\u000a\u000a<h2>Practical considerations</h2>\u000a\u000a<p>The solution described here is definitely not one to use in production\u000afor many reasons, including robustness, performance and security.</p>\u000a\u000a<p>In practice, we'd probably want a server to control multiple devices, so\u000aeach request should specify an ID representing the device. Also,\u000acontinuous polling is a very inefficient approach, since every HTTP\u000arequest has a lot of overhead. It's better to maintain open connection\u000aserver-controller and server-remote connections to enable pushing of\u000adata directly. This connection should be bidirectional to let the device\u000afeed back to the remote as well as enabling control.</p>\u000a\u000a<p>A promising startup working in the direction of making vision more real\u000ais <a href="http://electricimp.com">electricimp</a>. These guys provide\u000aconnectivity through a SD card form factor they call an "imp". Though\u000athey have a proprietary communication protocol between their imps and\u000atheir cloud (mainly for performance), they ultimately provide a RESTful\u000aweb API to control each imp. Definitely looking forward to playing with\u000atheir offerings.</p>\u000a\u000a<h2>Mash it up</h2>\u000a\u000a<p>Now we have a device registering with a public web server. The server\u000aprovides an API for developers to access that device. Developers can\u000athen build applications around that API which interact with the device.\u000aThis is no different from a regular web API, and at this point, all we\u000ahave built is a glorified remote control that uses the web instead of IR\u000afor transmitting messages.</p>\u000a\u000a<p>However! One of the great things about web APIs is that they can easily\u000abe mixed with one another to create something greater than the sum of\u000athe parts. This is known on the internet as a "mashup". The first\u000amashups emerged about decades years ago, but are still an important part\u000aof the web landscape. I recently used a really nice one called\u000a<a href="http://livelovely.com/">lovely</a>, which is a house hunting mashup combining Google Maps and\u000acraigslist. Mashup making services like <a href="http://pipes.yahoo.com/pipes/">Yahoo! Pipes</a> and\u000a<a href="https://ifttt.com/">IFTTT</a> have existed for a while. Notably, IFTTT includes support\u000afor some of the "smart" physical devices I mentioned in the beginning of\u000athis post.</p>\u000a\u000a<p>To me, the truly interesting question around this physical web is the\u000auser's experience. Each of the various promises of\u000adevice/appliance/thing control or automation is not terribly compelling.\u000aWhy would I unlock my phone, find and run the lamp app, and find the\u000aright fixture? The alternative of walking up to the light switch and\u000aflipping it seems much more lucrative. That said, I'm optimistic that\u000athere are compelling user interfaces to be discovered, but they will\u000aonly emerge on top of this physical web framework.</p>\u000a\u000a<h2>A plea for physical APIs</h2>\u000a\u000a<p>As we move into a world connected devices, we need to have a solid,\u000awidely available platform for communication between these devices. This\u000ainfrastructure can and should ultimately be the web.</p>\u000a\u000a<p>This opinion of mine is not zealotry, but completely practical: I do not\u000awant every device in my home to be stuck in an networked ecosystem\u000aprovided by some single company. Whether it's an Android app, an iOS\u000aapp, a web app, a universal dashboard on a smart phone, a voice\u000acontrolled system in the living room, or (more likely) something\u000acompletely different, any of these systems should be able to easily\u000ainterface with any device in the world.</p>\u000a\u000a<p>The recent stink around <a href="http://www.technologyreview.com/view/508666/twitter-instagram-and-the-internet-of-disconnected-things/">twitter no longer embedding instagram\u000aphotos</a> is just one example of important chunks of the\u000ainternet become segregated from one another. This wall-building trend is\u000aalarming and dangerous to the integrity of the web platform, which is\u000abased on interoperability: cross-linking, cross-embedding, and API\u000amashups. While this new web of things is still in its infancy, it's\u000aespecially important that we nurture the same principles that made the\u000aearly web so great.</p>\u000a
p2116
tp2117
Rp2118
sg13
V/mashup-of-things
p2119
sg15
Nsg16
I01
sg17
VInternet mashup of things
p2120
sg20
VMany new devices come with unexpected connectivity - often a WiFi\u000aconnection that enables them to connect to a hotspot and the larger\u000ainternet.
p2121
sg6
V<p>Many new devices come with unexpected connectivity - often a WiFi\u000aconnection that enables them to connect to a hotspot and the larger\u000ainternet. <a href="http://www.nest.com/">Nest</a>, a smart thermostat, was one of the first\u000acommercial products to do this. Many more indie projects are following\u000asuit, with an explosion of kickstarters like this <a href="http://www.kickstarter.com/projects/limemouse/lifx-the-light-bulb-reinvented">teleoperated\u000alight</a>, <a href="http://www.withings.com/">connected scale</a> or this <a href="http://supermechanical.com/twine/">general purpose\u000aconnected sensor</a>. The idea of an Internet of Things, in which\u000aevery appliance and object is somehow connected, has long been popular\u000ain academic circles, and this time around it feels like we're actually\u000aclose.</p>\u000a\u000a<p>If we think of these physical devices/appliances as web services with\u000aAPIs, we can mash them up just like we did in the early days of the web,\u000acreating applications that are more useful than the sum of their parts.\u000aIn this post I argue for using the web as the medium to tie everything\u000atogether, describe a simple architecture for building networked physical\u000adevices and build a web lamp controlled by an arduino.</p>\u000a\u000a
p2122
sg25
g169
sg33
g2119
sg170
(dp2123
g172
S'Jan'
p2124
sg174
S'January 11, 2013'
p2125
sg176
I1
sg177
S'2013-01-11T09:00:00-00:00'
p2126
sg179
I1357923600
sg180
I2013
sg181
I11
ssg65
g182
sg31
S'mashup-of-things'
p2127
sS'posted'
p2128
g188
(S'\x07\xdd\x01\x0b'
p2129
tp2130
Rp2131
ssg34
S'content/posts/2013/mashup-of-things/index.md'
p2132
sg36
F1433825522.0
sa(dp2133
g2
(dp2134
g4
V All good things must come to an end. VPS hosting paid for by my former university is no exception! Ever since the University of Madeira-provided credit card paying for the account expired, I began wondering whether it's worth paying for a VPS that I hardly use. Combined with two consecutive 10-minute stretches of downtime last week, I had my answer.    I run this blog, my mother's site and a handful of mini-sites, all of which are inherentily static content. Today, I moved them all away from my VPS completely. I migrated the relatively complex sites to the  lightning  engine, and updated the engine with a couple of nice features: fixed content links in list pages and feeds, and support for publishing to S3.   
p2135
sg28
g7
(g8
g9
V<p>All good things must come to an end. VPS hosting paid for by my former\u000auniversity is no exception! Ever since the University of\u000aMadeira-provided credit card paying for the account expired, I began\u000awondering whether it's worth paying for a VPS that I hardly use.\u000aCombined with two consecutive 10-minute stretches of downtime last week,\u000aI had my answer.</p>\u000a\u000a<p>I run this blog, my mother's site and a handful of mini-sites, all of\u000awhich are inherentily static content. Today, I moved them all away from\u000amy VPS completely. I migrated the relatively complex sites to the\u000a<a href="https://github.com/borismus/lightning">lightning</a> engine, and updated the engine with a couple of nice\u000afeatures: fixed content links in list pages and feeds, and support for\u000apublishing to S3.</p>\u000a\u000a<!--more-->\u000a\u000a<h2>System administration</h2>\u000a\u000a<p>In my early Linux days, I ran an AMD Athlon server off my parents'\u000ainternet connection. I took pride in configuration, maintenance,\u000aadministration, endlessly recompiling updates and dealing with broken\u000adependencies. I enjoyed the challenge and got very good at it. By\u000asinking enough time into any problem, I was confident that I would\u000aultimately solve it. Sometimes I contributed an ebuild or two to\u000aportage. I learned a lot, and eventually my web server outgrew my\u000aparents' internet connection.</p>\u000a\u000a<p>So I turned to managed hosting. Several years later, sick of the crappy\u000amanagement UI, and yearning to flex some sysadmin muscle, I jumped on\u000aVPS opportunity for performance reasons. While clearly overkill for\u000astatic sites, it was appealing from a "what if?" perspective: what if\u000asuddenly I wanted to run a complex webapp? No problem, VPS was ready!</p>\u000a\u000a<h2>Except system administration sucks</h2>\u000a\u000a<p>My VPS slice was running Ubuntu 8. Since Ubuntu 12 was released, I was\u000agreeted with a "48 packages are out of date" message upon logging into\u000athe machine. </p>\u000a\u000a<p>Long ago, this message would have sent me down a rabbit hole of emerging\u000aall of the outdated packages, resolving dependencies and rewriting\u000aconfig files. It was gratifying to be on the bleeding edge, to have a\u000aclean system with all of the daemons dancing to your tune in perfect\u000aharmony.</p>\u000a\u000a<p>These days, I could care less about being up-to-date. In fact, I\u000aactively dislike upgrading. An upgrade is a risk, likely to lead to\u000asomething breaking, likely without me noticing at first. So rather than\u000athe "ooh, new shiny" feeling I used to have when Apache needed an\u000aupdate, I actively dread needing to update anything. I don't want to\u000aneed to tweak configurations, especially because I've forgotten a lot of\u000athe domain-specific config languages. </p>\u000a\u000a<h2>S3 for static hosting, PaaS for everything else</h2>\u000a\u000a<p>Happily, all of my sites are currently static. Blogs and mini-sites all\u000alend themselves very well to being hosted on S3.</p>\u000a\u000a<p>For the hypothetical case that I require a dynamic web server on\u000athe internet, I'll turn to a Platform-as-a-service solution like\u000a<a href="http://nodejitsu.com/">Nodejitsu</a> or <a href="https://developers.google.com/appengine/">AppEngine</a> to avoid doing rote configuration\u000atasks.</p>\u000a\u000a<p>Being a sysadmin is not a part time job.</p>\u000a
p2136
tp2137
Rp2138
sg13
V/moved-to-s3
p2139
sg15
Nsg16
I01
sg17
VFrom VPS to static hosting
p2140
sg20
VAll good things must come to an end.
p2141
sg6
V<p>All good things must come to an end. VPS hosting paid for by my former\u000auniversity is no exception! Ever since the University of\u000aMadeira-provided credit card paying for the account expired, I began\u000awondering whether it's worth paying for a VPS that I hardly use.\u000aCombined with two consecutive 10-minute stretches of downtime last week,\u000aI had my answer.</p>\u000a\u000a<p>I run this blog, my mother's site and a handful of mini-sites, all of\u000awhich are inherentily static content. Today, I moved them all away from\u000amy VPS completely. I migrated the relatively complex sites to the\u000a<a href="https://github.com/borismus/lightning">lightning</a> engine, and updated the engine with a couple of nice\u000afeatures: fixed content links in list pages and feeds, and support for\u000apublishing to S3.</p>\u000a\u000a
p2142
sg25
g169
sg33
g2139
sg170
(dp2143
g172
S'Jan'
p2144
sg174
S'January 16, 2013'
p2145
sg176
I1
sg177
S'2013-01-16T09:00:00-00:00'
p2146
sg179
I1358355600
sg180
I2013
sg181
I16
ssg65
g182
sg31
S'moved-to-s3'
p2147
sS'posted'
p2148
g188
(S'\x07\xdd\x01\x10'
p2149
tp2150
Rp2151
ssg34
S'content/posts/2013/moved-to-s3/index.md'
p2152
sg36
F1433825517.0
sa(dp2153
g2
(dp2154
g4
V I've been thinking about this for a while, but the recent  sunset announcement of Google Reader  made me revisit this topic. Google Reader isn't the only thing that's dead. RSS (aka. Really Simple Syndication) has long been proclaimed dead as well. In fact most people never even knew what RSS was. That said, it was a very useful tool for me and many others that like to stay up-to-date in their areas of interest. Increasingly, I've been getting my dose of news through social networks. However, social networks contain a lot of noise that I care little for. I want to rebuild the RSS spirit using modern social networks. This post describes one possible approach, which I refer to as  Really Simple Social Syndication (RSSS) .   
p2155
sg28
g7
(g8
g9
V<p>I've been thinking about this for a while, but the recent <a href="http://googlereader.blogspot.ca/2013/03/powering-down-google-reader.html">sunset\u000aannouncement of Google Reader</a> made me revisit this topic.\u000aGoogle Reader isn't the only thing that's dead. RSS (aka. Really Simple\u000aSyndication) has long been proclaimed dead as well. In fact most people\u000anever even knew what RSS was. That said, it was a very useful tool for\u000ame and many others that like to stay up-to-date in their areas of\u000ainterest. Increasingly, I've been getting my dose of news through social\u000anetworks. However, social networks contain a lot of noise that I care\u000alittle for. I want to rebuild the RSS spirit using modern social\u000anetworks. This post describes one possible approach, which I refer to as\u000a<strong>Really Simple Social Syndication (RSSS)</strong>.</p>\u000a\u000a<!--more-->\u000a\u000a<h2>Really simple syndication: the good old days</h2>\u000a\u000a<p>Here's what the content flow used to be with blogs and RSS:</p>\u000a\u000a<p><img src="rss-flow.png" alt="RSS-based content syndication" /></p>\u000a\u000a<p>It was simple. Really simple, actually!</p>\u000a\u000a<h2>Social syndication: today</h2>\u000a\u000a<p>I don't care much for social networks. I mostly see them as a two-way\u000autility for ultimately connecting content creators to content consumers.</p>\u000a\u000a<p>One way, social networks like Facebook, Twitter, G+, etc are just\u000avehicles for finding out what to read based on your interests. I spend\u000atoo much time checking them individually for my news, and this is\u000aunfortunate. </p>\u000a\u000a<p>The other way, social networks make it easier to have your content read\u000aby a bunch of the right people. In my case, the vast majority of\u000anon-organic search traffic comes from social sources (mostly twitter). I\u000awaste a bit of time tweeting, and G+ing new posts on my blog as they\u000acome out, but sometimes they are picked up by others and I don't\u000aactually need to do that.</p>\u000a\u000a<p>As long as the content itself stays outside of the walled gardens of the\u000asocial networks, I think we can come to a syndication solution that\u000amight rival the old RSS-based one, but enjoy the benefits of having some\u000aextra signals from all of the social network junk. Here's the model I'm\u000athinking of:</p>\u000a\u000a<p><img src="social-flow.png" alt="Social-based content syndication" /></p>\u000a\u000a<p>Now for a little bit more about Pub and Sub.</p>\u000a\u000a<h2>Sub: Requirements for consumption</h2>\u000a\u000a<ul>\u000a<li><p><strong>Reuse existing feeds</strong>. I don't want to re-create my sources. Use\u000aexisting people you follow on Twitter, from G+ circles, Facebook\u000afriends.</p></li>\u000a<li><p><strong>De-duplicated content</strong>. Some people post the same link on multiple\u000anetworks. Some popular posts are reshared by everyone. I only want to\u000asee a post once.</p></li>\u000a<li><p><strong>Content centric</strong>. Show me the content in some standard, readable\u000away. Hide the social stuff unless I explicitly ask. Most of the time I\u000adon't care where it came from, don't care how many people liked it, or\u000awhat they wrote in the comments. Sometimes I'm curious and want a way\u000ato trace it back.</p></li>\u000a<li><p><strong>Social signals as a metric</strong>. If many of my sources share something,\u000aI probably should at least take a skim.</p></li>\u000a<li><p><strong>Web based service</strong> so that I can use it anywhere.</p></li>\u000a<li><p>Set a <strong>volume-based daily quota</strong>. If I'm busy today I'd like to only\u000asee the top N articles, sorted by some transparent metric of my\u000achoosing.</p></li>\u000a</ul>\u000a\u000a<p>Flipboard, Pulse, Feedly all sort of fit into this class of readers.\u000aIdeally this would be an API that just lets me connect a few social\u000aaccounts, and get back a filtered feed of content. This could be the API\u000afor the product - a Really Simple Social Syndication (RSSS) feed.</p>\u000a\u000a<p>Anyone could then build a UI on top of it for their favorite platform\u000a(Google Reader-like).</p>\u000a\u000a<h2>Pub: Requirements for content production</h2>\u000a\u000a<ul>\u000a<li><p>Automatically post content to a bunch of social services. </p></li>\u000a<li><p>Intelligent shortening of links and content (eg. for twitter, to fit\u000ain 140 chars).</p></li>\u000a</ul>\u000a\u000a<p>Wordpress plugins, Tumblr and others provide ways to automatically tweet\u000aand otherwise post new updates to your content. There needs to be some\u000aother way that works in general for any type of content. Such a service\u000acould be similar to feedburner, in that it would take an existing RSS\u000afeed and socialize it.</p>\u000a\u000a<h2>Social networks, let's be friends!</h2>\u000a\u000a<p>I'm not religious about Google Reader or RSS. It was a good solution for\u000acontent syndication at the time, but I'm ready to accept that perhaps\u000ait's time to move on. Hopefully with tools like the above, we can have\u000asomething that comes close to the utility of RSS feeds.</p>\u000a\u000a<p>Social networks could do some evil stuff which would preclude RSSS from\u000ahappening. Economically, they are incentivized own content, create\u000awalled gardens, insert advertisements, and prevent access to their\u000afeeds. I'm hoping that some human element will prevent that from\u000ahappening.</p>\u000a\u000a<p>Here is a short list of what we need from the social networks:</p>\u000a\u000a<ul>\u000a<li><p>The content itself must be free from walled gardens (eg. paywalls,\u000alogin walls, etc)</p></li>\u000a<li><p>Social network feeds are available to read in full, as is, without\u000amagical suggestions, collaborative filtering, etc.</p></li>\u000a<li><p>Social networks provide some programmatic way to post content.</p></li>\u000a</ul>\u000a\u000a<p>Once we have a good flow for the production and consumption of content,\u000ausing social networks as a delivery mechanism, I will be very happy to\u000aminimize the amount of time spent on social networks directly, and focus\u000aon consuming and producing interesting things. Also, happy &pi; day!</p>\u000a
p2156
tp2157
Rp2158
sg13
V/really-simple-social-syndication
p2159
sg15
Nsg16
I01
sg17
VReally simple social syndication
p2160
sg20
VI've been thinking about this for a while, but the recent [sunset\u000aannouncement of Google Reader][sunset] made me revisit this topic.
p2161
sg6
V<p>I've been thinking about this for a while, but the recent <a href="http://googlereader.blogspot.ca/2013/03/powering-down-google-reader.html">sunset\u000aannouncement of Google Reader</a> made me revisit this topic.\u000aGoogle Reader isn't the only thing that's dead. RSS (aka. Really Simple\u000aSyndication) has long been proclaimed dead as well. In fact most people\u000anever even knew what RSS was. That said, it was a very useful tool for\u000ame and many others that like to stay up-to-date in their areas of\u000ainterest. Increasingly, I've been getting my dose of news through social\u000anetworks. However, social networks contain a lot of noise that I care\u000alittle for. I want to rebuild the RSS spirit using modern social\u000anetworks. This post describes one possible approach, which I refer to as\u000a<strong>Really Simple Social Syndication (RSSS)</strong>.</p>\u000a\u000a
p2162
sg25
g169
sg33
g2159
sg170
(dp2163
g172
S'Mar'
p2164
sg174
S'March 14, 2013'
p2165
sg176
I3
sg177
S'2013-03-14T09:00:00-00:00'
p2166
sg179
I1363276800
sg180
I2013
sg181
I14
ssg65
g182
sg31
S'really-simple-social-syndication'
p2167
sS'posted'
p2168
g188
(S'\x07\xdd\x03\x0e'
p2169
tp2170
Rp2171
ssg34
S'content/posts/2013/really-simple-social-syndication/index.md'
p2172
sg36
F1433825504.0
sa(dp2173
g2
(dp2174
g4
V Largely because of the plummeting price and thickness of touch screens, these devices are increasingly ubiquitous. One of the latest trends is touch screen laptops, spearheaded by  Surface  devices and the recently announced  Chromebook Pixel . In this post I'll dive into some experiements around this new form factor. The main goal is to try to convince myself that this form factor makes sense for reasons other than economic ones.    In exploring the interaction design angle of these new devices, I came across a couple of what I think are a couple of interesting ideas that I'd like to share with you:  responsive input  and  simultaneous interactions  using both mouse/trackpad and touchscreen. I wrote some demos that illustrate these ideas.  A touchscreen laptop is required for these demos to work properly .       Auto scaling in response to input type .    Mouse-to-map and touch-to-mark .    Multimodal transform demo .     
p2175
sg28
g7
(g8
g9
V<p>Largely because of the plummeting price and thickness of touch screens,\u000athese devices are increasingly ubiquitous. One of the latest trends is\u000atouch screen laptops, spearheaded by <a href="http://www.microsoft.com/Surface/en-US">Surface</a> devices and the\u000arecently announced <a href="http://chrome.blogspot.com/2013/02/the-chromebook-pixel-for-whats-next.html">Chromebook Pixel</a>. In this post I'll dive into\u000asome experiements around this new form factor. The main goal is to try\u000ato convince myself that this form factor makes sense for reasons other\u000athan economic ones.</p>\u000a\u000a<p>In exploring the interaction design angle of these new devices, I came\u000aacross a couple of what I think are a couple of interesting ideas that\u000aI'd like to share with you: <strong>responsive input</strong> and <strong>simultaneous\u000ainteractions</strong> using both mouse/trackpad and touchscreen. I wrote some\u000ademos that illustrate these ideas. <em>A touchscreen laptop is required for\u000athese demos to work properly</em>.</p>\u000a\u000a<ul>\u000a<li><a href="http://borismus.github.com/touch-laptop-experiments/responsive">Auto scaling in response to input type</a>.</li>\u000a<li><a href="http://borismus.github.com/touch-laptop-experiments/map">Mouse-to-map and touch-to-mark</a>.</li>\u000a<li><a href="http://borismus.github.com/touch-laptop-experiments/transform">Multimodal transform demo</a>.</li>\u000a</ul>\u000a\u000a<!--more-->\u000a\u000a<p>Because you probably don't have a touch screen laptop, I recorded a\u000arough video showing some of these interactions:</p>\u000a\u000a<iframe width="640" height="360" src="http://www.youtube.com/embed/rcE2z9tudGw" frameborder="0" allowfullscreen></iframe>\u000a\u000a<p>Hopefully this gives you a better sense of what I mean by responsive\u000ainput and simultaneous touch and mouse interactions.</p>\u000a\u000a<h2>Responsive input</h2>\u000a\u000a<p>The touch laptop class of device has a two main interaction modes:</p>\u000a\u000a<ol>\u000a<li>As a regular laptop with trackpad (or external mouse) and keyboard.</li>\u000a<li>As a touch tablet with a keyboard.</li>\u000a</ol>\u000a\u000a<p>These two interaction modes differ fundamentally in many ways. The\u000afollowing are some examples of these differences:</p>\u000a\u000a<ul>\u000a<li>Touch has no hover state.</li>\u000a<li>Touch is less precise than mouse and requires bigger targets.</li>\u000a<li>Touch requires that you are closer to the screen.</li>\u000a</ul>\u000a\u000a<p><img src="touch-laptop.png" alt="Chrome Pixel" /></p>\u000a\u000a<p>Ideally, you want to provide optimal experiences for both cases. For the\u000amouse case, this means taking advantage of hover states and a finer\u000apointer. For the touch case, this means ensuring that touch targets are\u000abig enough to be tapped, not relying on hover at all.</p>\u000a\u000a<p>So I explored a user interface concept that adapts touch laptop\u000ainterfaces to the user's current input mode. The tricky bit is detecting\u000athe user's current input mode. Several adaptation options are possible:</p>\u000a\u000a<ol>\u000a<li>Immediately transform to the mouse-style UI as soon as the input mode\u000achanges (simplest, but can cause transitions to fire too rapidly\u000abetween the two modes, which may be jarring).</li>\u000a<li>Transform only after some period of not using the other input mode\u000a(eg. go to touch mode only if the user is actively using touch, and\u000anot touching the mouse at all).</li>\u000a<li>Transform based on some external criteria, like whether or not the\u000ascreen is docked to a mouse, or based on input from sensors other\u000athan mouse/touchscreen.</li>\u000a</ol>\u000a\u000a<p>The first approach is problematic in that your first touch transforms\u000athe page. If this transformation causes your target to move away from\u000ait's initial position, you will miss it entirely. This can be mitigated\u000aby having intelligent resizing which does not affect anything directly\u000aunder the touch point, but may result in a lopsidedly zoomed interface.</p>\u000a\u000a<p>The second approach is problematic since the mode switching will happen\u000aautomatically after some period of inactivity, which may be jarring. The\u000alast approach is either obvious (eg. mouse removed), or an area of\u000aresearch (eg. predicting when the user will touch based on camera).</p>\u000a\u000a<p>I wrote a <a href="http://borismus.github.com/touch-laptop-experiments/responsive">demo of auto scaling in response to input type</a>.\u000aIf you use the mouse, click targets will decrease in size. If you use\u000ayour finger, touch targets increase in size. (Of course, this will only\u000awork on a touchscreen laptop).</p>\u000a\u000a<h2>Simultaneous touchscreen + mouse/trackpad interactions</h2>\u000a\u000a<p>In the above section, I described an automatic way to switch between\u000atouch and mouse mode However, there is a middle ground between the two:\u000amultimodal interactions that involve both touchscreen and\u000amouse/trackpad. Simultaneous bimodal interaction is already common. For\u000aexample, using mouse and keyboard simultaneously makes a very efficient\u000ainterface for FPS games, with the movement via the WASD keys, and\u000amouse-look.</p>\u000a\u000a<p>One experiment involves using the mouse or trackpad as a navigation\u000adevice and using the touch screen as a way to input positional data.\u000aThis is illustrated through Google maps. You pan and zoom the map using\u000amouse events, and place markers on the map using the touch screen. Try\u000aout this demonstration of <a href="http://borismus.github.com/touch-laptop-experiments/map">mouse-to-map and touch-to-mark</a> (again,\u000athis requires a touchscreen laptop).</p>\u000a\u000a<p>Another experiment involves manipulating geometric objects on the\u000ascreen. The idea here was to use the touch screen to select objects, and\u000ause the trackpad/mouse as way of manipulating the selected object. In\u000athis demo, you can manipulate the object in a number of ways:</p>\u000a\u000a<ol>\u000a<li>Move it by simply dragging it around on the screen with touch.</li>\u000a<li>Rotate by selecting the object on the touchscreen, and then\u000aperforming a mousemove (either by moving a mouse or dragging one\u000afinger on a trackpad). The rotation happens around the point where\u000ayou touched the object, which acts as a fulcrum. </li>\u000a<li>Scale it in the same fashion as rotation (selecting object and\u000atransformation origin with the touchscreen), except with a two-finger\u000adrag on the trackpad, or using the mousewheel if a mouse is attached.</li>\u000a</ol>\u000a\u000a<p>With no selection, the canvas itself can be zoomed and panned with the\u000amouse/trackpad directly. Try out this <a href="http://borismus.github.com/touch-laptop-experiments/transform">multimodal transform\u000ademo</a> (requires touchscreen laptop).</p>\u000a\u000a<h2>Missing pieces</h2>\u000a\u000a<p>Like any brave new world, the one of multimodal input has its own set of\u000achallenges.</p>\u000a\u000a<p>It's currently impossible to distinguish a touch laptop from any other\u000atouch screen. Notably, this means that you should never assume that\u000atouch support implies no mouse support. In practice, make sure that you\u000aalways bind to mouse events. If you also have touch event handlers, just\u000ause <code>event.preventDefault()</code> there to ensure that you aren't handling\u000aone event in multiple handlers. If you're interested in this, follow the\u000adiscussion at <a href="http://crbug.com/174553">http://crbug.com/174553</a>.</p>\u000a\u000a<p>As a generalization of the above, there is currently no way to determine\u000awhich kinds of input are available in the browser. A fully fledged Input\u000aAvailability API might seem like overkill, but there are already some\u000acases beyond touch laptops that are relevant. For example, detecting the\u000apresence of a physical keyboard would be very useful. Further, detecting\u000ahardware features like an attached camera and microphone could fall into\u000athe same bucket rather than relying on exception handling from APIs like\u000a<code>getUserMedia</code>. Lastly, having such an API would allow websites to react\u000adynamically to changes in input (eg. a tablet gets docked to a physical\u000akeyboard, or a mouse is attached).</p>\u000a\u000a<p>The final missing piece is that dealing with two different event models\u000a(mouse and touch) is definitely clunky. I have already <a href="http://smus.com/mouse-touch-pointer/">written\u000aextensively about pointer events</a> and a <a href="https://github.com/borismus/pointer.js">pointer event\u000apolyfill</a>. In this particular case, pointer events would be\u000agreat, because although they provide a consolidated model for input,\u000ait's very easy and natural to distinguish between the two modalities.</p>\u000a\u000a<p>These experiments are all available <a href="https://github.com/borismus/touch-laptop-experiments">on github</a>. </p>\u000a\u000a<h2>Your turn!</h2>\u000a\u000a<p>Do you have thoughts or demos around new types of interactions using\u000atouch laptops? Please share them below.</p>\u000a
p2176
tp2177
Rp2178
sg13
V/touch-laptop-experiments
p2179
sg15
Nsg16
I01
sg17
VInteractive touch laptop experiments
p2180
sg20
V\u000aLargely because of the plummeting price and thickness of touch screens,\u000athese devices are increasingly ubiquitous.
p2181
sg6
V<p>Largely because of the plummeting price and thickness of touch screens,\u000athese devices are increasingly ubiquitous. One of the latest trends is\u000atouch screen laptops, spearheaded by <a href="http://www.microsoft.com/Surface/en-US">Surface</a> devices and the\u000arecently announced <a href="http://chrome.blogspot.com/2013/02/the-chromebook-pixel-for-whats-next.html">Chromebook Pixel</a>. In this post I'll dive into\u000asome experiements around this new form factor. The main goal is to try\u000ato convince myself that this form factor makes sense for reasons other\u000athan economic ones.</p>\u000a\u000a<p>In exploring the interaction design angle of these new devices, I came\u000aacross a couple of what I think are a couple of interesting ideas that\u000aI'd like to share with you: <strong>responsive input</strong> and <strong>simultaneous\u000ainteractions</strong> using both mouse/trackpad and touchscreen. I wrote some\u000ademos that illustrate these ideas. <em>A touchscreen laptop is required for\u000athese demos to work properly</em>.</p>\u000a\u000a<ul>\u000a<li><a href="http://borismus.github.com/touch-laptop-experiments/responsive">Auto scaling in response to input type</a>.</li>\u000a<li><a href="http://borismus.github.com/touch-laptop-experiments/map">Mouse-to-map and touch-to-mark</a>.</li>\u000a<li><a href="http://borismus.github.com/touch-laptop-experiments/transform">Multimodal transform demo</a>.</li>\u000a</ul>\u000a\u000a
p2182
sg25
g169
sg33
g2179
sg170
(dp2183
g172
S'Feb'
p2184
sg174
S'February 21, 2013'
p2185
sg176
I2
sg177
S'2013-02-21T09:00:00-00:00'
p2186
sg179
I1361466000
sg180
I2013
sg181
I21
ssg65
g182
sg31
S'touch-laptop-experiments'
p2187
sS'posted'
p2188
g188
(S'\x07\xdd\x02\x15'
p2189
tp2190
Rp2191
ssg34
S'content/posts/2013/touch-laptop-experiments/index.md'
p2192
sg36
F1433825466.0
sa(dp2193
g2
(dp2194
g4
V The phone in your pocket is an amazing, fluid, multi-functional tool. When it comes to talking to other devices, such as your TV or laptop, the user experience drops off sharply. Bill Buxton  speaks eloquently  on the subject, describing three stages of high tech evolution:      Device works: feature completeness and stability   Device flows: good user experience   Many devices work together      But connecting devices is a pain and we have been squarely at stage 2 since the release of the iPhone. There are many competing approaches to do this: Bluetooth, Bluetooth LE, WiFi direct, discovery over the same local WiFi network, and many many others. This post is dedicated to attacking this problem from an unexpected angle: using ultrasound to broadcast and receive data between nearby devices. Best of all, the approach uses the Web Audio API, making it viable for pure web applications:       
p2195
sg28
g7
(g8
g9
V<p>The phone in your pocket is an amazing, fluid, multi-functional tool.\u000aWhen it comes to talking to other devices, such as your TV or laptop,\u000athe user experience drops off sharply. Bill Buxton <a href="http://www.youtube.com/watch?v=ZQJIwjlaPCQ&amp;feature=youtu.be&amp;t=21m00">speaks\u000aeloquently</a> on the subject, describing three stages of\u000ahigh tech evolution:</p>\u000a\u000a<ol>\u000a<li>Device works: feature completeness and stability</li>\u000a<li>Device flows: good user experience</li>\u000a<li>Many devices work together</li>\u000a</ol>\u000a\u000a<p>But connecting devices is a pain and we have been squarely at stage 2\u000asince the release of the iPhone. There are many competing approaches to\u000ado this: Bluetooth, Bluetooth LE, WiFi direct, discovery over the same\u000alocal WiFi network, and many many others. This post is dedicated to\u000aattacking this problem from an unexpected angle: using ultrasound to\u000abroadcast and receive data between nearby devices. Best of all, the\u000aapproach uses the Web Audio API, making it viable for pure web\u000aapplications:</p>\u000a\u000a<iframe width="640" height="360" src="//www.youtube.com/embed/w6lRq5spQmc" frameborder="0" allowfullscreen="true"></iframe>\u000a\u000a<!--more-->\u000a\u000a<h2>A device tower of babel</h2>\u000a\u000a<p><a href="http://www.apple.com/airplay/">Airplay</a> and <a href="http://www.google.com/chromecast">Chromecast</a> are great approaches to a subset of the\u000aproblem for devices within the same ecosystem (eg. Apple, or Google),\u000abut the general problem remains hard to solve.</p>\u000a\u000a<p>Because there are so many possible technical approaches, chances are\u000athat the pair of devices that you happen to be using don't have a common\u000alanguage to speak. Even if both devices have Bluetooth, one of them may\u000arequire a profile the other doesn't support, or support a different\u000aversion of the standard. This is especially common with Bluetooth today,\u000awhere many devices have the hardware to support Bluetooth 4.0 (aka\u000aBTLE), but many devices don't currently support the new protocol for\u000avarious reasons.</p>\u000a\u000a<p>On the web, the problem is even worse, since low level device connection\u000aAPIs aren't exposed for <a href="http://smus.com/installable-webapps/">security sandbox reasons</a>. Because of\u000ahow slowly the web evolves, it's hard to imagine this changing any time\u000asoon.</p>\u000a\u000a<h2>Transmitting data in interesting ways</h2>\u000a\u000a<p><a href="http://www.youtube.com/watch?v=sVWlQNzU4Ak">Blinkup from Electric Imp</a> is an interesting approach to\u000across-device communication. It uses a series of blinks to transfer\u000aconfiguration data between a smart phone and an Imp, a small SD-card\u000ashaped device with a light sensor.</p>\u000a\u000a<p>Dial-up modems did a similar thing. They encoded and decoded digital\u000adata onto an analog phone line. Remember those annoying connection\u000anoises? Dial-up modems would turn on their speaker to give the user an\u000aidea of how the handshake is progressing. If you don't remember, here's\u000aa <a href="http://www.windytan.com/2012/11/the-sound-of-dialup-pictured.html">refresher</a>. Even today on analog phones, the sounds you hear\u000awhen pressing numbers on a dialer correspond to the frequencies the\u000aphone system uses for analog-to-digital conversion. The conversion\u000ahappens using <a href="http://en.wikipedia.org/wiki/Dual-tone_multi-frequency_signaling">Dual-tone multi-frequency signaling (DTMF)</a>.</p>\u000a\u000a<p>Your phone and a lot of other devices around you has a speaker and a\u000amicrophone. These two pieces of hardware can be used for sending and\u000areceiving data using sounds, similar to how modems did it over phone\u000alines. Better yet, if the OS supports high enough frequency sending and\u000areceiving, we can create an inaudible data channel.</p>\u000a\u000a<h2>Transmitting data using sound</h2>\u000a\u000a<p>I should note that encoding data in sound is not new. The idea of <a href="http://en.wikipedia.org/wiki/Audio_watermark">audio\u000awatermarking</a> is to encode a signature into music that is not\u000adiscernable by the listener (due to the way humans hear), but can be\u000apicked up by a machine. This is used as a clever piracy detection\u000ascheme. </p>\u000a\u000a<p>Most commodity speakers are capable of producing sound with a 44.1KHz\u000asample rate (resulting in a maximum frequency of about 22KHz by the\u000a<a href="http://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem">Nyquist-Shannon sampling theorem</a>). This lets us encode data\u000anot just as sound, but as sound that adults can't hear. Children and\u000anon-human animals are still susceptible, though :)</p>\u000a\u000a<p>One technical caveat is that microphones are sometimes not as capable as\u000aspeakers, especially in phones, since they are often optimized for human\u000aspeech, which sounds fine with a lower sample rate. In other cases,\u000aeven though the hardware is capable, the firmware runs at a lower sample\u000arate for energy efficiency. If this is the case, one of the devices will\u000anot be able to receive the wave and the sound-based connection will be\u000aone-way only.</p>\u000a\u000a<h2>Sonicnet.js, a web audio implementation</h2>\u000a\u000a<p>To illustrate these concepts, I built a <a href="https://github.com/borismus/sonicnet.js">JavaScript library</a>\u000athat can send and receive data as sounds. My approach is and not nearly\u000aas sophisticated as the audio watermarking technique, and even simpler\u000athan the DTMF approach. Basically, you can specify a range of\u000afrequencies to use, and an alphabet of characters that can be\u000atransmitted. The frequency spectrum is split into ranges corresponding\u000ato the specified alphabet and start/end codes, with each character/code\u000acorresponding to a part of the full frequency range.</p>\u000a\u000a<p>The sending side converts each character of the word to be sent into the\u000acenter of the corresponding frequency range, and transmits that\u000afrequency for a certain duration. The receiving side does a continuous\u000afourier transform of the signal and looks for peaks in the specified\u000afrequency range. Upon finding a peak for a significant duration, it does\u000athe conversion back from frequency to character. This is essentially\u000aa <a href="http://en.wikipedia.org/wiki/Selective_calling#Tone_burst_or_single_tone">single-tone multi-frequency signaling (STMF)</a> scheme.</p>\u000a\u000a<p>There is a timing issue: on the sending side, how long should each\u000acharacter be transmitted for, and on the receiving side, how long should\u000athe listened for? An easy workaround for this is to disallow adjacently\u000arepeated characters.</p>\u000a\u000a<p>I built a socket-like API for sonic networking. Client code\u000alooks like this:</p>\u000a\u000a<pre><code>ssocket = new SonicSocket({alphabet: '0123456789'});\u000afunction onButton() {\u000a  ssocket.send('31415');\u000a}\u000a</code></pre>\u000a\u000a<p>And the server can look like this:</p>\u000a\u000a<pre><code>sserver = new SonicServer({alphabet: '0123456789'});\u000asserver.on('message', function(message) {\u000a  // Expect message to be '31415'.\u000a  console.log(message);\u000a});\u000asserver.start();\u000a</code></pre>\u000a\u000a<p>The library is available for use on <a href="https://github.com/borismus/sonicnet.js/tree/master/lib">github</a>.</p>\u000a\u000a<p>Of course, using it requires a Web Audio implementation (mostly\u000a<code>OscillatorNode</code> on the sending side, and <code>AnalyserNode</code> on the\u000areceiving side) and good enough hardware. I have experimented with\u000aChrome-to-Chrome transmission on Mac Books, as well as between Chrome\u000afor Android (beta) and Chrome for Mac.</p>\u000a\u000a<p>I wrote a couple of demos to illustrate the idea. These appear in the\u000a<a href="http://www.youtube.com/watch?v=w6lRq5spQmc">video I embedded at the top of the post</a>. The first demo lets\u000ayou <a href="http://borismus.github.io/sonicnet.js/emoticons">send emoticons</a> from one device to the other. It uses a\u000asmall alphabet of just 6 characters - one for each emoticon. You pick\u000aone of 6 emoticons, and the corresponding character is sent over the\u000asonic network, received and shown prominently on the other end.</p>\u000a\u000a<p>A more realistic use for sonicnet.js is this <a href="http://borismus.github.io/sonicnet.js/chat-pair">chat\u000aapplication</a>, which generates a non-repeating 5-digit token\u000aand uses it to create connections between two devices. This is done with\u000athe help of a pairing server, which helps establish a proxied connection\u000abetween the two devices, over a websocket. Once the connection is\u000aestablished, the chats themselves are sent through the websocket. The\u000a<a href="https://github.com/borismus/sonicnet.js/tree/master/server">server code</a> is hosted on <a href="https://www.nodejitsu.com/">nodejitsu</a>.</p>\u000a\u000a<h2>Conclusions and a request</h2>\u000a\u000a<p>It's great to see that the Web Audio API has come far enough that\u000aapplications like these are possible. I'm fascinated by the implications\u000aof sonicnet.js for the Web of Things. It is a pure web technology that\u000acan be used to pair devices together. Because of the ubiquity of web\u000abrowsers and audio hardware, the combination can be a huge win, even\u000aamong commodity hardware, without having to wait for Bluetooth and other\u000aclose-range connectivity technology to become available to the web\u000aplatform.</p>\u000a\u000a<p>If this post has piqued your interest and you are interested in helping,\u000atry writing an app using sonicnet.js. As I mentioned earlier, receiving\u000ahigh frequency sounds does not work on all devices because of\u000afirmware/hardware limitations so I'd love to know which devices it does\u000aand does not work on. My expectation is that most phones should be able\u000ato send only, and that most laptops should be able to both send and\u000areceive. Please fill out <a href="https://docs.google.com/forms/d/1dAgNdVdhss-QR-Owm556RZch-MV_ntnAMP8_ZJi5XLA/viewform">this form</a> once you try the <a href="http://borismus.github.io/sonicnet.js/emoticons">emoticons\u000ademo</a> on your own hardware. At the time of writing, <a href="http://crbug.com/242894">live\u000ainput is not supported</a> in Chrome for Android Beta, so sending\u000adata from mobile device to laptop is the only possible configuration.</p>\u000a
p2196
tp2197
Rp2198
sg13
V/ultrasonic-networking
p2199
sg15
Nsg16
I01
sg17
VUltrasonic networking on the web
p2200
sg20
VThe phone in your pocket is an amazing, fluid, multi-functional tool.
p2201
sg6
V<p>The phone in your pocket is an amazing, fluid, multi-functional tool.\u000aWhen it comes to talking to other devices, such as your TV or laptop,\u000athe user experience drops off sharply. Bill Buxton <a href="http://www.youtube.com/watch?v=ZQJIwjlaPCQ&amp;feature=youtu.be&amp;t=21m00">speaks\u000aeloquently</a> on the subject, describing three stages of\u000ahigh tech evolution:</p>\u000a\u000a<ol>\u000a<li>Device works: feature completeness and stability</li>\u000a<li>Device flows: good user experience</li>\u000a<li>Many devices work together</li>\u000a</ol>\u000a\u000a<p>But connecting devices is a pain and we have been squarely at stage 2\u000asince the release of the iPhone. There are many competing approaches to\u000ado this: Bluetooth, Bluetooth LE, WiFi direct, discovery over the same\u000alocal WiFi network, and many many others. This post is dedicated to\u000aattacking this problem from an unexpected angle: using ultrasound to\u000abroadcast and receive data between nearby devices. Best of all, the\u000aapproach uses the Web Audio API, making it viable for pure web\u000aapplications:</p>\u000a\u000a<iframe width="640" height="360" src="//www.youtube.com/embed/w6lRq5spQmc" frameborder="0" allowfullscreen="true"></iframe>\u000a\u000a
p2202
sg25
g169
sg33
g2199
sg170
(dp2203
g172
S'Aug'
p2204
sg174
S'August 8, 2013'
p2205
sg176
I8
sg177
S'2013-08-08T09:00:00-00:00'
p2206
sg179
I1375977600
sg180
I2013
sg181
I8
ssg65
g182
sg31
S'ultrasonic-networking'
p2207
sS'posted'
p2208
g188
(S'\x07\xdd\x08\x08'
p2209
tp2210
Rp2211
ssg34
S'content/posts/2013/ultrasonic-networking/index.md'
p2212
sg36
F1433825460.0
sa(dp2213
g2
(dp2214
g4
V I wrote a short book about the Web Audio API. The book is meant as an introduction to the web audio API, as well as some audio basics for web developers with little audio experience. It is  available for free on Chimera , a web-based book viewer, which presents a nicely laid out page and lets you leave per-paragraph comments. The online version also includes inline samples from  webaudioapi.com . If you don't like reading on the web, you can also  buy a physical copy or an ebook  from O'Reilly.   
p2215
sg28
g7
(g8
g9
V<p>I wrote a short book about the Web Audio API. The book is meant as an\u000aintroduction to the web audio API, as well as some audio basics for web\u000adevelopers with little audio experience. It is <a href="http://chimera.labs.oreilly.com/books/1234000001552/">available for free on\u000aChimera</a>, a web-based book viewer, which presents a nicely laid\u000aout page and lets you leave per-paragraph comments. The online version\u000aalso includes inline samples from <a href="http://webaudioapi.com">webaudioapi.com</a>. If you don't\u000alike reading on the web, you can also <a href="http://shop.oreilly.com/product/0636920025948.do">buy a physical copy or an\u000aebook</a> from O'Reilly.</p>\u000a\u000a<!--more-->\u000a\u000a<p>So I got this cool bat on the cover. I am told that technically\u000aspeaking, it is a brown long-eared bat (<a href="http://en.wikipedia.org/wiki/Brown_long-eared_bat">Plecotus auritus</a>). Though\u000ait's no orca (an ideal O'Reilly cover, don't you think?), I'm very happy\u000athat it's a <a href="http://en.wikipedia.org/wiki/Animal_echolocation">sound related animal</a>.</p>\u000a\u000a<p><img src="cover.jpg" alt="Web Audio Book Cover" /></p>\u000a\u000a<p>The book was written on my laptop in a Google Doc. I hand-drew some\u000aillustrations in a notebook and brought them into the doc. Once I was\u000aready for feedback, I sent the doc around to my technical reviewers, who\u000aleft feedback in comments. After incorporating their feedback, I got\u000asome editorial feedback, still in the doc. Once the draft was more or\u000aless ready to go, I created an <a href="https://www.odesk.com/">oDesk</a> task to convert the Google\u000aDoc into docbook.xml format. The contractor did a great job and charged\u000ame about $100. This was my first time paying anyone to do work for me.</p>\u000a\u000a<p>Thanks to all of the reviewers, editors, illustrators and organizers for\u000ahelping. Also, thanks to <a href="http://www.kevincennis.com/">Kevin Ennis</a> who kindly donated the\u000a<a href="http://webaudioapi.com">webaudioapi.com</a> domain which I'm currently using to host the\u000asamples.</p>\u000a
p2216
tp2217
Rp2218
sg13
V/webaudio-book
p2219
sg15
Nsg16
I01
sg17
VWeb Audio book
p2220
sg20
VI wrote a short book about the Web Audio API.
p2221
sg6
V<p>I wrote a short book about the Web Audio API. The book is meant as an\u000aintroduction to the web audio API, as well as some audio basics for web\u000adevelopers with little audio experience. It is <a href="http://chimera.labs.oreilly.com/books/1234000001552/">available for free on\u000aChimera</a>, a web-based book viewer, which presents a nicely laid\u000aout page and lets you leave per-paragraph comments. The online version\u000aalso includes inline samples from <a href="http://webaudioapi.com">webaudioapi.com</a>. If you don't\u000alike reading on the web, you can also <a href="http://shop.oreilly.com/product/0636920025948.do">buy a physical copy or an\u000aebook</a> from O'Reilly.</p>\u000a\u000a
p2222
sg25
g169
sg33
g2219
sg170
(dp2223
g172
S'Mar'
p2224
sg174
S'March 18, 2013'
p2225
sg176
I3
sg177
S'2013-03-18T09:00:00-00:00'
p2226
sg179
I1363622400
sg180
I2013
sg181
I18
ssg65
g182
sg31
S'webaudio-book'
p2227
sS'posted'
p2228
g188
(S'\x07\xdd\x03\x12'
p2229
tp2230
Rp2231
ssg34
S'content/posts/2013/webaudio-book/index.md'
p2232
sg36
F1433825453.0
sa(dp2233
g2
(dp2234
g4
V It is human nature to create taxonomies for everything: people, places, and things.  Without such a system of reference, we become lost and disoriented.  Imagine your city with street names and addresses blanked out. Finding your favorite cafe, meeting up with your friend on the weekend, even locating your own parked car would become incredibly difficult. Travel outside your city would become far more challenging.    The web's defining property is addressability. URLs on the web are like street names and addresses in the physical world. This makes sharing and cross-linking easy. Non-web platforms are a little bit like our city with blanked out street names and addresses. There's no good way of talking about where you currently are, or how to get somewhere else. These platforms typically give users a crutch to help with the issue, such as a share button or dialog. But these create an inherently inferior experience, since addressability is no longer built-in. Addressability becomes a burden on the app developer, and as a result, the platform is no longer navigable.    In light of the success of Android and iOS, and given a potential explosion in new types of lower power computing (wearables, IoT, etc), it's unclear if  browsers will be as ubiquitous  as they are today (at least in the near term). I'm very interested in seeing if and how non-web platforms can embrace URLs.  How closely coupled are URLs to HTML, and do they make sense without a presentation layer?   
p2235
sg28
g7
(g8
g9
V<p>It is human nature to create taxonomies for everything: people, places,\u000aand things.  Without such a system of reference, we become lost and\u000adisoriented.  Imagine your city with street names and addresses blanked\u000aout. Finding your favorite cafe, meeting up with your friend on the\u000aweekend, even locating your own parked car would become incredibly\u000adifficult. Travel outside your city would become far more\u000achallenging.</p>\u000a\u000a<p>The web's defining property is addressability. URLs on the web are like\u000astreet names and addresses in the physical world. This makes sharing\u000aand cross-linking easy. Non-web platforms are a little bit like our\u000acity with blanked out street names and addresses. There's no good\u000away of talking about where you currently are, or how to get somewhere\u000aelse. These platforms typically give users a crutch to help with the\u000aissue, such as a share button or dialog. But these create an\u000ainherently inferior experience, since addressability is no longer\u000abuilt-in. Addressability becomes a burden on the app developer, and\u000aas a result, the platform is no longer navigable.</p>\u000a\u000a<p>In light of the success of Android and iOS, and given a potential\u000aexplosion in new types of lower power computing (wearables, IoT, etc),\u000ait's unclear if <a href="http://smus.com/ebb-of-the-web/">browsers will be as ubiquitous</a> as they are\u000atoday (at least in the near term). I'm very interested in seeing if and\u000ahow non-web platforms can embrace URLs.  How closely coupled are URLs to\u000aHTML, and do they make sense without a presentation layer?</p>\u000a\u000a<!--more-->\u000a\u000a<h2>Not all URLs are created equal</h2>\u000a\u000a<p>The modern URL can host several very different kinds of entities:</p>\u000a\u000a<ol>\u000a<li><strong>Data</strong>: text files, images, audio, movies, JSON, etc.</li>\u000a<li><strong>Hypertext</strong> (content-program hybrid): HTML that can reference\u000acontent.</li>\u000a<li><strong>Program</strong>: webapps designed to deliver a bundle of JavaScript that\u000athen constructs the HTML dynamically from other URLs.</li>\u000a</ol>\u000a\u000a<p><img src="data-vs-program.png" alt="Program vs. data: evolution of web" /></p>\u000a\u000a<p>Without an HTML renderer, hypertext and program URLs cannot be\u000ainterpreted. Only one of these types of entities makes sense: data. Data\u000aURLs are seen everywhere on the web: whenever you include an <code>&lt;img&gt;</code> tag\u000aon your page, or embed a <code>&lt;video&gt;</code>, reference some CSS, or make an XHR\u000ato fetch some JSON, you are using a data URL.</p>\u000a\u000a<p>Apps on other platforms use data URLs too, though not as much. Images\u000aare typically included as part of the app itself, but all API access is\u000adone in exactly the same fashion as on the web: using HTTP requests to\u000atext or binary data.</p>\u000a\u000a<p>The similarity isn't entirely superficial. Any sort of web-connected app\u000acan be seen as just a view on top of a series of data URLs (APIs).\u000aHowever, data URLs are typically hidden from the user. The only types of\u000aURLs that users see are hypertext and program URLs. These are the ones\u000athat are being shared around. But both of these types of URLs ultimately\u000amap to HTML, sometimes via JavaScript. The underlying data URLs  are\u000aconcealed inside the page, and aren't exposed to the user.</p>\u000a\u000a<h2>The "URL in, URL out" principle</h2>\u000a\u000a<p>A user need not understand schemes, domain names, DNS, HTTP or GET\u000arequests. They don't need to think about conceptual distinctions\u000abetween URL types to know that a URL is an address that gets you to the\u000asame thing you're looking at right now. Whether it's Android/Java,\u000aPolymer/JS or <em>InsertPlatform/InsertLanguage</em> underneath, the only thing\u000athey want to be able to do is to continue reading their book on whatever\u000adevice they happen to be transitioning to. They want to share it with\u000atheir friend too, and have them enjoy a good read.</p>\u000a\u000a<p>To make a platform URL-friendly, it should satisfy two simple\u000arequirements:</p>\u000a\u000a<ol>\u000a<li>The platform should provide a way for apps to reveal the underlying\u000aURL for the view.</li>\u000a<li>Given a URL, the platform should open it in a way that yields the\u000abest available user experience.</li>\u000a</ol>\u000a\u000a<p><img src="url-in-url-out.png" alt="Platforms handle content URLs and provide them on demand." /></p>\u000a\u000a<p>However, to bring URL friendliness to a platform retroactively takes a\u000alot of effort. Taking a quick look at today's trending web-alternatives,\u000ait's plain to see that Android has some form of URL in (via intent\u000afilters), but no URL out. iOS has neither in, nor out (you're stuck). To\u000aaddress this lack of URL out in Android, you can imagine all Android\u000aactivities having to implement a <code>URLReporter</code> interface like this:</p>\u000a\u000a<pre><code>class TwitterProfileViewer extends Activity implements URLReporter {\u000a  @Override\u000a  String reportURL() {\u000a    return String.format("http://twitter.com/%s", username);\u000a  }\u000a}\u000a</code></pre>\u000a\u000a<p>Of course, there is the not-unimportant UX question of how to then\u000areveal the URL and transfer it to other devices and people. But this\u000aquestion will for now be left unanswered. With this API, a very\u000atasty carrot, and a very painful stick to force developers to implement\u000ait (and a bit more UX thinking), we can make Android URL-friendly. </p>\u000a\u000a<h2>But URLs aren't just identifiers</h2>\u000a\u000a<p>You can look at URLs in one of two ways:</p>\u000a\u000a<ol>\u000a<li>As a <strong>universal identifier</strong>. The same URL is also the universal and\u000acanonical way of getting to content that you are reading now.</li>\u000a<li>As a <strong>web address</strong>. A URL like <code>http://smus.com/addressable-apps</code>\u000acan be viewed as instructions for getting to a certiain place:\u000aresolve the <code>smus.com</code> to <code>205.251.243.108</code>, connect to port 80 over\u000aTCP, perform a <code>GET /addressable-apps</code> request.</li>\u000a</ol>\u000a\u000a<p>The real power of URLs is in both aspects combined. When only one facet\u000ais used, the system is broken. I can't quite put my finger on it, but\u000asomething feels wrong in the cases, where the addressability aspect (2)\u000aof URLs are not taken into account:</p>\u000a\u000a<ul>\u000a<li>AppLinks: these aren't universal identifiers, but fragile shortcuts to\u000athe platform-specific apps. (violates 1)</li>\u000a<li>Android intent filters: when you register an URL indent filter for\u000ayour activity, you aren't actually hosting anything at that URL.\u000a(violates 2)</li>\u000a<li>History API: a hack allowing developers to set the path of the URL to\u000aanything they want. (violates 2)</li>\u000a</ul>\u000a\u000a<p>The History API emerged from a trend on the web: highly imperative\u000aapplications. These apps have grown so far from being a collection of\u000ahyperlinked markup that they no longer have a natural URL-to-HTML-page\u000amapping. Because they are so script heavy, they need to be able to\u000apretend to respond to URLs. The History API is the webapp's hack for\u000aURL-out.</p>\u000a\u000a<h2>Mixed feelings</h2>\u000a\u000a<p>This is my umpteenth attempt at finishing a post on the complicated\u000asubject of URLs in non-web apps. And you have had the pleasure of\u000areading it not because the ideas in my head have crystallized into a\u000asomething coherent, but because I feel that the topic is difficult and\u000afundamentally unresolvable. In light of that, this post contains more\u000aquestions than answers. Sorry to disappoint :)</p>\u000a\u000a<p>I'm still torn between maintaining the ideological and benefits of the\u000a<a href="http://www.polymer-project.org/">mostly-declarative web</a> and the practicality of <a href="http://smus.com/installable-webapps/">jumping out\u000aof the web's sandbox</a>. While I would be happy to see more\u000anative platforms embrace URL in URL out, I don't think that the solution\u000ais a clean one, nor do I think that first-class URLs are likely to\u000aemerge in any platform after-the-fact. Unfortunately I don't think that\u000athere is a clean solution.</p>\u000a\u000a<p>However, as an optimist, I must believe in the long-term victory of the\u000aweb, though not in the sense that the web will rule over all other\u000auser-facing platforms unopposed. Instead, platforms will continue to\u000arise and fall; the web's influence will ebb and flow as well. But the\u000aweb must be the one that wins out the most, keeping the idea of\u000aaddressability alive.</p>\u000a\u000a<p>That URLs become a universal address that works across all platforms,\u000aand not just the web, is a proposition worth considering.</p>\u000a
p2236
tp2237
Rp2238
sg13
V/addressable-apps
p2239
sg15
Nsg16
I01
sg17
VAddressable apps
p2240
sg20
VIt is human nature to create taxonomies for everything: people, places,\u000aand things.
p2241
sg6
V<p>It is human nature to create taxonomies for everything: people, places,\u000aand things.  Without such a system of reference, we become lost and\u000adisoriented.  Imagine your city with street names and addresses blanked\u000aout. Finding your favorite cafe, meeting up with your friend on the\u000aweekend, even locating your own parked car would become incredibly\u000adifficult. Travel outside your city would become far more\u000achallenging.</p>\u000a\u000a<p>The web's defining property is addressability. URLs on the web are like\u000astreet names and addresses in the physical world. This makes sharing\u000aand cross-linking easy. Non-web platforms are a little bit like our\u000acity with blanked out street names and addresses. There's no good\u000away of talking about where you currently are, or how to get somewhere\u000aelse. These platforms typically give users a crutch to help with the\u000aissue, such as a share button or dialog. But these create an\u000ainherently inferior experience, since addressability is no longer\u000abuilt-in. Addressability becomes a burden on the app developer, and\u000aas a result, the platform is no longer navigable.</p>\u000a\u000a<p>In light of the success of Android and iOS, and given a potential\u000aexplosion in new types of lower power computing (wearables, IoT, etc),\u000ait's unclear if <a href="http://smus.com/ebb-of-the-web/">browsers will be as ubiquitous</a> as they are\u000atoday (at least in the near term). I'm very interested in seeing if and\u000ahow non-web platforms can embrace URLs.  How closely coupled are URLs to\u000aHTML, and do they make sense without a presentation layer?</p>\u000a\u000a
p2242
sg25
g169
sg33
g2239
sg170
(dp2243
g172
S'May'
p2244
sg174
S'May 21, 2014'
p2245
sg176
I5
sg177
S'2014-05-21T09:00:00-00:00'
p2246
sg179
I1400688000
sg180
I2014
sg181
I21
ssg65
g182
sg31
S'addressable-apps'
p2247
sS'posted'
p2248
g188
(S'\x07\xde\x05\x15'
p2249
tp2250
Rp2251
ssg34
S'content/posts/2014/addressable-apps/index.md'
p2252
sg36
F1402287753.0
sa(dp2253
g2
(dp2254
g4
V Tech pundits like to lament that the web has  no viable future , while web idealists hold that in fact the web is totally fine, with a "too big to fail"  sort of attitude .    At the root of this disagreement are poorly defined terms. The web can mean many different things to different people. Though it started from a pretty abstract notion of a series of interlinked documents, it has now evolved to refer to a very specific technology stack of hyperlinked HTML documents styled with CSS, enhanced with JavaScript, all served on top of HTTP. In light of an increasing movement away from desktop-style computing, we've seen a big shift away from the web in mobile platforms.     Let's take apart this gob of web technology in light of the increasingly complex landscape of computing and try to make sense of what the web is and where it's going.        
p2255
sg28
g7
(g8
g9
V<p>Tech pundits like to lament that the web has <a href="http://cdixon.org/2014/04/07/the-decline-of-the-mobile-web/">no viable\u000afuture</a>, while web idealists hold that in fact the web is\u000atotally fine, with a "too big to fail" <a href="http://schepers.cc/the-recline-of-the-mobile-web">sort of attitude</a>.</p>\u000a\u000a<p>At the root of this disagreement are poorly defined terms. The web can\u000amean many different things to different people. Though it started from a\u000apretty abstract notion of a series of interlinked documents, it has now\u000aevolved to refer to a very specific technology stack of hyperlinked HTML\u000adocuments styled with CSS, enhanced with JavaScript, all served on top\u000aof HTTP. In light of an increasing movement away from desktop-style\u000acomputing, we've seen a big shift away from the web in mobile platforms. </p>\u000a\u000a<p>Let's take apart this gob of web technology in light of the increasingly\u000acomplex landscape of computing and try to make sense of what the web is\u000aand where it's going.</p>\u000a\u000a<p><img src="webiness.png" alt="A framework for webiness" /></p>\u000a\u000a<!--more-->\u000a\u000a<h2>Webiness: how far down the rabbit hole?</h2>\u000a\u000a<p>I want to introduce the concept of webiness, a framework for evaluating\u000ahow deeply an application embraces "the web":</p>\u000a\u000a<p>Games are typical examples of apps that are <strong>pure native</strong> and not\u000awebby at all (barring high score servers, etc). Because many are\u000aplayable offline, with no added benefit of having an internet\u000aconnection, there is obviously no room for web. They are built entirely\u000aon native APIs, benefiting from being as close to the hardware as\u000apossible for performance reasons.</p>\u000a\u000a<p>Another class of native apps are Twitter, Facebook and the like, which\u000aessentially act as <strong>specialized browsers</strong>. They largely use HTTP to\u000aaccess RESTful endpoints that serve up JSON, which is rendered by the\u000anative clients. Because a specialized browser is designed for a specific\u000ause case in mind (eg. interacting with Twitter), it can be streamlined\u000afor that purpose and doesn't need to deal with processing the web's\u000apresentation layer (HTML, CSS, JavaScript). Therefore it presents a\u000amore focused experience and benefits from being faster at start-up and\u000aduring interaction, offline support, and a generally better experience.</p>\u000a\u000a<p><strong>Embedded browsers</strong> (also called hybrid apps) embed a webview into the\u000auser interface of the page to use the web's presentation layer for part\u000aof the user interface. How much of the user interface is created using\u000aweb technologies varies widely. This approach is beneficial because it\u000aallows web developers to be featured in app stores, and also gives them\u000aaccess to native APIs that are not available on the open web.</p>\u000a\u000a<p>Lastly, <strong>browser-based</strong> pages and apps are ones where even the code\u000aitself is fetched over HTTP. The app is written in a cross-browser way\u000ato ensure that regardless of which browser loads the page, the user is\u000apresented with a reasonable experience. The idea of this is to be able\u000ato write once, deploy everywhere. Like the other types of apps on our\u000aspectrum, these also use HTTP to interact with the server. All of the\u000acontent is rendered using HTML/CSS/JavaScript. Additionally, the HTML is\u000aoften hyperlinked.</p>\u000a\u000a<h2>The web as greatest common divisor</h2>\u000a\u000a<p>In mathematics, the greatest common divisor (GCD) between multiple\u000adigits is the largest positive integer that divides the numbers without\u000aa remainder. Observe that the GCD of a set of numbers (S) is by\u000adefinition less than or equal to each number. Furthermore, if the\u000anumbers in S are not multiples of one another, the inequality\u000abecomes strict. So <code>foreach n in S, gcd(S) &lt; n</code>.</p>\u000a\u000a<p>In the webbiest case above, where we write once for all browsers, the\u000aweb user interface becomes the GCD for all existing interfaces. It is\u000aguaranteed to be slower, less featureful, etc, than each individual\u000anative platform, but when the web was conceived, the benefits outweighed\u000athe costs. As the web evolved in the 90s, native platforms evolved\u000aalongside it. Computing at the time was very desktop-centric, requiring\u000aa physical keyboard, mouse, and relatively large display at, let's say\u000a1024x768 pixels. At best, variance was between operating systems. The\u000acomputer geeks were on the Linux fringe. The art and music geeks used\u000aMacs. But the hardware was pretty much the same.</p>\u000a\u000a<p>Because of this uniformity, it was easy to standardize on a set of input\u000aand outputs that would work reasonably well across a bunch of existing\u000acomputer configurations and operating systems. Computer hardware was all\u000avery similar, just off by some factor. And the GCD of this orderly set\u000awas pretty large: <code>gcd(200, 300, 400) = 100</code>.</p>\u000a\u000a<h2>We're not in Kansas anymore</h2>\u000a\u000a<p>Contrast this uniformity to today, when our mots du jour are "mobile\u000afirst", or even "mobile only". But even these notions are becoming\u000apass, as our day-to-day tech encounters start including wearable\u000asensors for health, chips embedded in your appliances, shoes, and\u000acomputers on your face. Even with the most conservative notion of what\u000amobile means - small screens and touch input - we've really thrown a\u000awrench into the big-screen, mouse-and-keyboard web. Scott Jenson is\u000aabsolutely right in remarking in an <a href="https://www.youtube.com/watch?v=6u03xYkwMVI">Edge conf panel</a>, that the\u000aweb hasn't even recovered from the fact that screens have gotten\u000asmaller. With today's extended notion of computing, the GCD of all\u000aof the devices and use cases the web is trying to support becomes very\u000asmall: <code>gcd(100, 200, 300, 50, 99, 198, 33) = 1</code>.</p>\u000a\u000a<p>At the same Edge conf panel on the future of the web, somebody\u000aasked the question of how the web would work on hardware without a\u000adisplay. Answers from the panel were incoherent, but it's unclear how\u000athis would be built into today's frankenweb, which is already a snowball\u000aof many, often redundant technologies. And this is largely because of the\u000anotion of <strong>THE WEB</strong> as a single platform. This is both its greatest\u000astrength, and ultimately its tragic flaw, as the legacy of the early 90s\u000acauses the singular web to cave in on itself, as we are experiencing\u000atoday.</p>\u000a\u000a<p>We can no longer have a one-web-for-all approach. We need to focus on\u000ahaving many different webs, each specializing on a particular subset of\u000aour universe of devices. Taking our set above, we can split it in two\u000asubsets, <code>S1 = {100, 200, 300, 50}</code>, and <code>S2 = {99, 198, 33}</code>.  Imagine\u000aS1 are the desktop-like devices, and S2 are the phone-like devices. Now,\u000awe have pretty okay GCDs: <code>gcd(S1) = 50</code>, and <code>gcd(S2) = 33</code>. Our worst\u000acase GCD is now 33, which is a lot better than 1!</p>\u000a\u000a<h2>The web is dead, long live the web!</h2>\u000a\u000a<p>I'm pretty sure that HTTP is here to stay. Our desire for content is\u000auniversal, and that content needs to live somewhere. Regardless of how\u000athat content is presented to us, it is likely to be served to us through\u000athe cloud, over HTTP in the forseeable future.</p>\u000a\u000a<p>What is indisputably being downplayed in the vibrant and variant near\u000afuture of computing, is the web's presentation layer - HTML, JavaScript\u000aand CSS. These comprise the lingua franca of the web, and have no real\u000acompetition. However, this browser-served presentation layer of the web\u000awill become less relevant as fewer things are done through the browser,\u000aespecially on mobile platforms.</p>\u000a\u000a<p>To me, the critical thing is that content be addressable by URL, and\u000across-linkable in some reasonable way. This is conventionally achieved\u000awith HTML's <code>&lt;a&gt;</code> elements, but can also be done with JavaScript (eg. a\u000abutton that runs <code>javascript:window.location.href = myUrl;</code>, or a\u000a<code>&lt;form&gt;</code> that creates a POST request to some other resource. This can\u000aeven be done without HTML at all. As long as we continue using HTTP, we\u000aare guaranteed to have content that is available at a given URL. And as\u000along as we can guarantee that there's some handler for that content, the\u000aspirit of the web lives on.</p>\u000a\u000a<h2>Specialized browsers are a good solution</h2>\u000a\u000a<p>RSS readers are good examples of specialized browsers focused on\u000apresenting a good reading experience to the user. They are a single\u000aentry point for all of the interesting things on the internet for the\u000auser to read.</p>\u000a\u000a<p>The Twitter app on your mobile device is an example of a specialized\u000abrowser designed for reading and sending tweets. This specialized\u000abrowser relies on the web's ability to link to various kinds of content\u000aaddressable by URLs. Tweets typically include an article or an image\u000awhich are served up using HTTP, and sometimes require HTML to render.\u000aTwitter is all about hyperlinked content, without ever using an <code>&lt;a&gt;</code>\u000atag.</p>\u000a\u000a<p>Apple also has several projects that are specialized browsers in spirit,\u000athough they rarely link out to the wild west of the world wide\u000aweb. Generally, these specialized browsers focus on giving a great\u000aexperience for the user aiming to do something specific. Similar in\u000afunction to an RSS reader, Newsstand is an entry point to the magazines\u000aand newspapers you read. Passbook is a specialized browser for tracking\u000aevent tickets, boarding passes and coupons. From a developer\u000aperspective, both of these browsers require you to setup a server and\u000awrite some iOS code. (Unfortunately you're then stuck in Apple's\u000aecosystem forever.)</p>\u000a\u000a<p>By identifying common patterns of functionality, Apple is able to\u000asuccessfully introduce a specialized browser notion that spans across\u000amultiple services, unifying it with a consistent user experience. This\u000abegins to address the concern that there are <a href="http://designmind.frogdesign.com/blog/mobile-apps-must-die.html">too many apps for\u000aeverything</a>. Of course you don't want separate apps for\u000aNYTimes, WSJ, USA Today, LA Times, etc. And of course you don't want\u000aseparate apps for each event booking service, airline and coupon\u000acompany.</p>\u000a\u000a<p>The problem is endemic to the web community. The standards process\u000abehemoth is slow, heavy and change-averse. Its focus is on a monolothic\u000aweb, <strong>THE WEB</strong>. Imagine if we focused on use cases in the same way\u000athat Apple does, and created specialized sub-webs for various related\u000athings. The next big thing is wearable health. Imagine a sub-web for\u000athat, where we get to define the way all of these devices can talk to\u000aone another. There would be a browser for that sub-web, providing a good\u000aexperience to see historical data, analyze trends, and plot data over\u000atime. This is not the stuff of a web browser, but a completely different\u000abeast.</p>\u000a\u000a<p>Android's intent filters are a step in the direction of a specialized\u000abrowser. Intent filters let apps register to handle specific URL\u000apatterns. If the URL pattern is opened by the user, she is presented\u000awith a dialog of all of the possible handlers for that resource, which\u000amay include Chrome, other web browsers, and specialized browsers that\u000asubscribe to that URL. Once you're in an Android app, however, you're\u000astuck. Unless the app provides the ability to share content, there's no\u000away to send your state to someone else. In contrast, with a web browser,\u000ayou just take the URL and send it to your friend and if they have access\u000ato that content, they get to see it.</p>\u000a\u000a<h2>The web, the good parts</h2>\u000a\u000a<p>When tech pundits hail the ebb of the web, they mean that mobile native\u000aapps are eating away at the presentation layer on the mobile web and\u000abeyond. After some contemplation, I have made peace with this\u000apossible future. HTML is not the best possible way of creating content,\u000anor is CSS a reasonable way of laying out content. JavaScript may be\u000acommonly used, but that does not make it a very good language. The\u000apresentation layer is optimized for content consumption using pointers\u000aand keyboard input, or if you're really adventurous, a smaller touch\u000ascreen.</p>\u000a\u000a<p>Apps are another story. Frameworks like iOS and Android provide a much\u000amore modern, cogent way of developing apps for mobile platforms. But\u000aunfortunately they aren't linkable or portable across platforms. As I\u000amentioned earlier, there's no need for <code>&lt;a&gt;</code> elements, or anything from\u000athe presentation layer to preserve benefits of linkability. But what\u000adoes the web look like once we've gutted the presentation layer?  Here\u000aare some characteristics that we should preserve:</p>\u000a\u000a<ul>\u000a<li>The ability to take any content that is currently being shown and\u000aserialize it into a URL.</li>\u000a<li>The ability to open a URL with the right specialized browser of the\u000auser's choosing.</li>\u000a<li>If no specialized browser is installed, some way of presenting the\u000auser with a list of good browsers.</li>\u000a</ul>\u000a\u000a<p>By dropping the notion of <strong>THE WEB</strong> (singular), and ushering an era of\u000aspecialized browsers, we can split our universe of devices into subsets\u000aand increase the baseline greatest common denominator. Trying to extend\u000athe web to work for every possible case will lead to even more feature\u000acreep in a web platform that is already keeling over.</p>\u000a\u000a<p>The web community should take a look at verticals and spec and build\u000aspecialized browsers for them. How would a web for boarding passes,\u000aconcert tickets and coupons look like? How about a web for personal\u000ahealth tracking data? A web for content that should be consumed on an\u000aaudio-only device?</p>\u000a\u000a<p>This has been my slightly edited brain dump on the future of the web.\u000aThanks for reading, I eagerly await your thoughts :)</p>\u000a
p2256
tp2257
Rp2258
sg13
V/ebb-of-the-web
p2259
sg15
Nsg16
I01
sg17
VThe ebb of the web
p2260
sg20
VTech pundits like to lament that the web has [no viable\u000afuture][no-viable], while web idealists hold that in fact the web is\u000atotally fine, with a "too big to fail" [sort of attitude][recline].
p2261
sg6
V<p>Tech pundits like to lament that the web has <a href="http://cdixon.org/2014/04/07/the-decline-of-the-mobile-web/">no viable\u000afuture</a>, while web idealists hold that in fact the web is\u000atotally fine, with a "too big to fail" <a href="http://schepers.cc/the-recline-of-the-mobile-web">sort of attitude</a>.</p>\u000a\u000a<p>At the root of this disagreement are poorly defined terms. The web can\u000amean many different things to different people. Though it started from a\u000apretty abstract notion of a series of interlinked documents, it has now\u000aevolved to refer to a very specific technology stack of hyperlinked HTML\u000adocuments styled with CSS, enhanced with JavaScript, all served on top\u000aof HTTP. In light of an increasing movement away from desktop-style\u000acomputing, we've seen a big shift away from the web in mobile platforms. </p>\u000a\u000a<p>Let's take apart this gob of web technology in light of the increasingly\u000acomplex landscape of computing and try to make sense of what the web is\u000aand where it's going.</p>\u000a\u000a<p><img src="webiness.png" alt="A framework for webiness" /></p>\u000a\u000a
p2262
sg25
g169
sg33
g2259
sg170
(dp2263
g172
S'Apr'
p2264
sg174
S'April 15, 2014'
p2265
sg176
I4
sg177
S'2014-04-15T09:00:00-00:00'
p2266
sg179
I1397577600
sg180
I2014
sg181
I15
ssg65
g182
sg31
S'ebb-of-the-web'
p2267
sS'posted'
p2268
g188
(S'\x07\xde\x04\x0f'
p2269
tp2270
Rp2271
ssg34
S'content/posts/2014/ebb-of-the-web/index.md'
p2272
sg36
F1433826756.0
sa(dp2273
g2
(dp2274
g4
V When the world wide web was first conceived, it was as a collection of interlinked textual documents. Today's web is full of rich media. YouTube and other video sites alone consume an enormous 53% of all internet traffic. Web denizens often have an open audio player in one of their tabs. Web-based photo sharing services such as Flickr are the most common way of enjoying photos on our computers. The remote control, foundations of which are attributed to everyone's favorite inventor Nikola Tesla in  patent US613809 , has been the preferred way of controlling media for over half a century.    Yet the only way we can control all of this web media is via the on-screen user interfaces that the websites provide. The web has no remote control, and this is a big usability problem. Many use the desktop versions of streaming services like Spotify and Rdio rather than their web player, exclusively because of mac media key support. For scenarios where you're far from the screen, like showing friends a slideshow of photos on a TV, the lack of remote controllability is a non-starter.    This post is a concrete proposal for what a remote controls for the web should be like. To get a sense for how it might feel, try a  rough prototype .   
p2275
sg28
g7
(g8
g9
V<p>When the world wide web was first conceived, it was as a collection of\u000ainterlinked textual documents. Today's web is full of rich media.\u000aYouTube and other video sites alone consume an enormous 53% of all\u000ainternet traffic. Web denizens often have an open audio player in one of\u000atheir tabs. Web-based photo sharing services such as Flickr are the most\u000acommon way of enjoying photos on our computers. The remote control,\u000afoundations of which are attributed to everyone's favorite inventor\u000aNikola Tesla in <a href="https://www.google.com/patents/US613809">patent US613809</a>, has been the preferred way of\u000acontrolling media for over half a century.</p>\u000a\u000a<p>Yet the only way we can control all of this web media is via the\u000aon-screen user interfaces that the websites provide. The web has no\u000aremote control, and this is a big usability problem. Many use the\u000adesktop versions of streaming services like Spotify and Rdio rather than\u000atheir web player, exclusively because of mac media key support. For\u000ascenarios where you're far from the screen, like showing friends a\u000aslideshow of photos on a TV, the lack of remote controllability is a\u000anon-starter.</p>\u000a\u000a<p>This post is a concrete proposal for what a remote controls for the web\u000ashould be like. To get a sense for how it might feel, try a <a href="http://borismus.github.io/media-control-prototype/">rough\u000aprototype</a>.</p>\u000a\u000a<!--more-->\u000a\u000a<p><img src="inputs.png" alt="Ways of controlling media: dedicated keyboard buttons,headphone\u000aremotes, hardware remote controls, second-screen remote controls,\u000acamera-based gestures, voice commands" /></p>\u000a\u000a<h2>Related attempts to solve this problem</h2>\u000a\u000a<p>Many platforms, especially Android, Mac and iOS, do a pretty good job of\u000asupporting some of the inputs from the above image. The web, one of the\u000amost common platforms for consuming media, supports none of them. The\u000aonly exception, of course, is the mouse and keyboard, but only when the\u000aplayer tab is in the foreground.</p>\u000a\u000a<p>On the web, there have been a number of proposals and half-solutions to\u000aaddress this problem. Back in 2011, I shared <a href="http://smus.com/chrome-media-keys-revisited/">KeySocket</a>, a\u000aMenu Bar app for OS X that handles media keys on the mac keyboard and\u000asends them to a companion Chrome extension that injects content scripts\u000ainto web-based media players. A similar project, <a href="http://sway.fm/">Sway.fm</a> built\u000asupport for media keys as an NPAPI plugin (a now <a href="http://blog.chromium.org/2013/09/saying-goodbye-to-our-old-friend-npapi.html">deprecated</a>\u000atechnology). The <a href="https://flutterapp.com/">Flutter app</a> takes a similar approach (native\u000aapp and companion extension), but enables webcam-based gestures for\u000acontrolling media.</p>\u000a\u000a<p>Recently, I contributed the Mac implementation to the new <a href="https://codereview.chromium.org/60353008/">global\u000akeyboard shortcuts API</a> for Chrome Apps and\u000aExtensions. This API lets developers bind to any global shortcut,\u000aincluding media keys. This is a good start since it brings the media key\u000ahandling infrastructure into Chrome, but does not address the problem\u000afor the web in general.</p>\u000a\u000a<h2>Starting with a good user experience</h2>\u000a\u000a<p>Since we have a blank slate when it comes to controlling media on the\u000aweb, how should media controls behave? Let's start with some\u000asub-optimal behaviors. Here's one: all media events to get routed to all\u000aopen tabs capable of handling them. Imagine the case with many YouTube\u000atabs open, and the ensuing cacophony! Another bad approach is to route\u000acommands only to the foreground tab, since a very common case for\u000aneeding media controls occurs when music is playing in the background.</p>\u000a\u000a<p><a href="http://borismus.github.io/media-control-prototype/">This rough prototype</a> illustrates what I think is a pretty\u000agood experience. It follows a focus-based model inspired by mobile\u000aoperating systems like iOS and Android. However, the web is messier than\u000athe app model and edge cases like multiple sources of media playing\u000asimultaneously (eg. music player and YouTube video) are likely to\u000ahappen, so we need to be careful.</p>\u000a\u000a<p>Here is what happens when a user issues a play/pause command. I'll\u000adefine the bold terms in a second.</p>\u000a\u000a<ol>\u000a<li>If any media is <strong>playing</strong> in a <strong>background tab</strong>, it should pause.</li>\u000a<li>Otherwise, if the <strong>foreground tab</strong> supports <strong>media events</strong>, it\u000ashould receive the media control and be pushed to the <strong>media focus\u000astack</strong>.</li>\u000a<li>Otherwise, if the <strong>media focus stack</strong> is non-empty, the event\u000ashould be routed to the tab at the top of the stack.</li>\u000a<li>Otherwise (if the stack is empty), find the first open tab\u000asupporting <strong>media events</strong>, relay the event to that page and push it\u000aon the <strong>media focus stack</strong>.</li>\u000a<li>If there are no open tabs supporting <strong>media events</strong>, do nothing.\u000aOptionally alert the user with a non-modal notification (eg. audible\u000achime).</li>\u000a</ol>\u000a\u000a<p>When a next or previous control is issued, the command should be routed\u000ato the tab with <strong>media focus</strong>. If there are no tabs with <strong>media\u000afocus</strong> and none capable of media control, we drop the event.</p>\u000a\u000a<p>If a tab closes, remove it from the <strong>media focus stack</strong> and ensure\u000athat <strong>media focus</strong> is granted to the tab at the top of the stack.</p>\u000a\u000a<p>To clarify the description above, here are a few terms:</p>\u000a\u000a<ul>\u000a<li><strong>Foreground</strong>: the active tab of the foreground browser window.</li>\u000a<li><strong>Background</strong>: every tab that is not in the foreground.</li>\u000a<li><strong>Media events</strong>: a new event type that a page can listen to,\u000aindicating how to interpret media controls (see the next section).</li>\u000a<li><strong>Playing tab</strong>: a tab that is currently playing audio or video\u000acontent.</li>\u000a<li><strong>Media focused tab</strong>: the tab which is the default receiver of media\u000acontrol events.</li>\u000a<li><strong>Media focus stack</strong>: a stack of tabs where the top-most tab\u000ais the one that currently has media focus. If that tab is\u000apopped off the stack, the next one gets media focus.</li>\u000a</ul>\u000a\u000a<p>The dry description above and <a href="http://borismus.github.io/media-control-prototype/">the prototype</a> should give a\u000asense of what tab should handle basic media controls, regardless of\u000atheir origin: keyboard, remote control hardware, gesture, etc.</p>\u000a\u000a<p>Now, when a command comes in, how does the page know how to interpret\u000ait? That's up to the web developer, and is done through <code>media</code> events,\u000adescribed in the next section.</p>\u000a\u000a<h2>Enabling media controls using media events</h2>\u000a\u000a<p>A fundamental missing piece so far is a way for a web page to indicate\u000athat it can receive media controls, and a way for it to specify how it\u000awants to handle them. The solution to this is to create a new type of\u000aevent, the <code>media</code> event, which are defined on a page-level, bound to\u000athe window object. This suggestion is not new, and first (as far as I\u000acan tell) came up in this <a href="http://paulrouget.com/e/mediaevents/">blog post by Paul Rouget</a>. Here's\u000ahow media events work:</p>\u000a\u000a<pre><code>// Subscribing to media events.\u000awindow.addEventListener('media', function(e) {\u000a  if (e.data == e.MEDIA_PLAY) {\u000a    myPlayer.play();\u000a  } else if (e.data == e.MEDIA_PAUSE) {\u000a    myPlayer.pause();\u000a  } else if (e.data == e.MEDIA_NEXT_TRACK) {\u000a    myPlayer.next();\u000a  } else if (e.data == e.MEDIA_PREVIOUS_TRACK) {\u000a    myPlayer.previous();\u000a  }\u000a});\u000a</code></pre>\u000a\u000a<p>This code tells the browser that this page can accept media controls,\u000aand what this page should do when a particular media control is\u000areceived.</p>\u000a\u000a<h2>Determining user-initiated media playback change</h2>\u000a\u000a<p>Another missing piece in the narrative so far is how to populate the\u000afocus stack. So far, we know that a closed tab should be popped from the\u000astack, and that play/pause sometimes causes a tab to be pushed onto the\u000astack. But this is not enough, since the user can still interact with\u000amedia using the UI of the player. For example, if I start listening to\u000amusic through a Spotify tab, and then switch tabs, media commands should\u000aobviously go to the Spotify tab, despite me never having issued any\u000amedia controls.</p>\u000a\u000a<p>One option is to, as the user navigates between tabs, push any tab with\u000asupporting media events onto the stack. This approach fails for the case\u000awhere you are listening to music in the background, and then change\u000atabs, passing an open YouTube video on the way. In this case, that\u000aYouTube video would become focused and there would be no way to control\u000athe music (until you close the YouTube tab). What we actually need is to\u000abe able to <strong>track when the user interacts with the page using the media\u000aplayer UI</strong>, in order to then push page to the media focus stack.</p>\u000a\u000a<p>A browser can distinguish user-initiated events (like clicks and\u000akeyboard presses) from programatic ones (like a timer firing, or a page\u000aloading). <a href="https://developer.apple.com/library/safari/documentation/AudioVideo/Conceptual/Using_HTML5_Audio_Video/Device-SpecificConsiderations/Device-SpecificConsiderations.html">iOS does this</a> to prevent annoying pages from\u000aautoplaying music (remember the 90s?). Using the same idea, browsers can\u000aalso track when a media player's playback state changes due to user\u000ainput.</p>\u000a\u000a<p>Even so, there may be special cases that aren't perfect. For example,\u000aimagine a music app with media controls and a video ad on the side. If\u000athe user then clicks the video ad, it doesn't mean that the page should\u000anow have media focus. There are other tricky cases such as a page full\u000aof videos. Here, if a user starts playing a particular video, and then\u000awants to stop it using media controls, the expectation is that the same\u000avideo pauses. If the web developer does not handle this case gracefully,\u000aanother video may start playing concurrently.</p>\u000a\u000a<h2>Response at FOMS was positive</h2>\u000a\u000a<p>Pitching this idea at <a href="http://www.foms-workshop.org/foms2013/pmwiki.php/Main/MediaFocus">FOMS 2013</a> a few months ago, folks seemed\u000areceptive. There was an understanding that a lack of media controls on\u000athe web is a genuine problem. Additionally, I got good feedback on the\u000asolution, which helped to iterate and get to this stage. This is all\u000avery encouraging, and I've written this post to keep the discussion\u000aalive and keep the momentum going. To make remote controls for the web a\u000areality, we need is a critical mass of interested browser implementers.</p>\u000a\u000a<p>As always, thanks for reading, and let me know if you have thoughts or\u000asuggestions on this topic, especially if you make browsers for a living\u000aand want to help standardize!</p>\u000a
p2276
tp2277
Rp2278
sg13
V/remote-controls-web-media
p2279
sg15
Nsg16
I01
sg17
VRemote controls for web media
p2280
sg20
VWhen the world wide web was first conceived, it was as a collection of\u000ainterlinked textual documents.
p2281
sg6
V<p>When the world wide web was first conceived, it was as a collection of\u000ainterlinked textual documents. Today's web is full of rich media.\u000aYouTube and other video sites alone consume an enormous 53% of all\u000ainternet traffic. Web denizens often have an open audio player in one of\u000atheir tabs. Web-based photo sharing services such as Flickr are the most\u000acommon way of enjoying photos on our computers. The remote control,\u000afoundations of which are attributed to everyone's favorite inventor\u000aNikola Tesla in <a href="https://www.google.com/patents/US613809">patent US613809</a>, has been the preferred way of\u000acontrolling media for over half a century.</p>\u000a\u000a<p>Yet the only way we can control all of this web media is via the\u000aon-screen user interfaces that the websites provide. The web has no\u000aremote control, and this is a big usability problem. Many use the\u000adesktop versions of streaming services like Spotify and Rdio rather than\u000atheir web player, exclusively because of mac media key support. For\u000ascenarios where you're far from the screen, like showing friends a\u000aslideshow of photos on a TV, the lack of remote controllability is a\u000anon-starter.</p>\u000a\u000a<p>This post is a concrete proposal for what a remote controls for the web\u000ashould be like. To get a sense for how it might feel, try a <a href="http://borismus.github.io/media-control-prototype/">rough\u000aprototype</a>.</p>\u000a\u000a
p2282
sg25
g169
sg33
g2279
sg170
(dp2283
g172
S'Jan'
p2284
sg174
S'January 27, 2014'
p2285
sg176
I1
sg177
S'2014-01-27T09:00:00-00:00'
p2286
sg179
I1390842000
sg180
I2014
sg181
I27
ssg65
g182
sg31
S'remote-controls-web-media'
p2287
sS'posted'
p2288
g188
(S'\x07\xde\x01\x1b'
p2289
tp2290
Rp2291
ssg34
S'content/posts/2014/remote-controls-web-media/index.md'
p2292
sg36
F1433825428.0
sa(dp2293
g2
(dp2294
g4
V A live-input spectrogram written using  Polymer  using the  Web Audio API .         If you're running Chrome or Firefox,  see it in action . Once the spectrogram is running, see if you can make a pattern with your speech or by whistling. You can also click anywhere on the page to turn on the oscillator. For a mind-blowing effect,  load this  in a parallel tab.   
p2295
sg28
g7
(g8
g9
V<p>A live-input spectrogram written using <a href="http://polymer-project.org">Polymer</a> using the <a href="http://webaudioapi.com">Web\u000aAudio API</a>.</p>\u000a\u000a<p><img src="screenshot.png" alt="Screenshot of spectrogram" /></p>\u000a\u000a<p>If you're running Chrome or Firefox, <a href="http://borismus.github.io/spectrogram">see it in action</a>. Once the\u000aspectrogram is running, see if you can make a pattern with your speech\u000aor by whistling. You can also click anywhere on the page to turn on the\u000aoscillator. For a mind-blowing effect, <a href="https://www.youtube.com/watch?v=M9xMuPWAZW8&amp;t=5m30s">load this</a> in a parallel\u000atab.</p>\u000a\u000a<!--more-->\u000a\u000a<h2>Why?</h2>\u000a\u000a<p>Having a spectrogram is incredibly handy for a lot of the work I've been\u000adoing recently. So a while ago, I built one that satisfies my needs. It\u000aruns in a full-screen, using the microphone input as the source.</p>\u000a\u000a<p>It also includes an oscillator, which plays a sine wave at the frequency\u000aof your pointer. It also shows you the frequency that it plays back, and\u000aplots a short buffer of pointer positions. This is handy for measuring\u000ainternal latency:</p>\u000a\u000a<p><img src="latency.png" alt="Latency estimation" /></p>\u000a\u000a<p>Having the oscillator built-in is also pretty fun. You can <a href="sounds/morse.wav">send morse\u000acode</a> (short short short, long long, short short long, short\u000ashort short), <a href="sounds/radio.wav">scan for radio stations</a>, make 8-bit character\u000a<a href="sounds/sfx.wav">dying sound effects</a>, simulate <a href="sounds/ghosts.wav">aliens, ghosts and\u000atheremins</a>, and annoy <a href="sounds/dogs.wav">small, annoying dogs</a>.</p>\u000a\u000a<p>I use the tool mostly in Chrome, but it also works in Firefox.\u000aUnfortunately no other browser currently has both <code>getUserMedia</code> and Web\u000aAudio API support.</p>\u000a\u000a<h2>Configuration parameters</h2>\u000a\u000a<p>The following are HTML attributes of the <code>g-spectrogram</code> component. Many\u000aof them are also configurable via the spectrogram controls component,\u000awhich shows up if the <code>controls</code> attribute is set to true.</p>\u000a\u000a<ul>\u000a<li><code>controls</code> (boolean): shows a config UI component.</li>\u000a<li><code>log</code> (boolean): enables y-log scale (linear by default).</li>\u000a<li><code>speed</code> (number): how many pixels to move past for every frame.</li>\u000a<li><code>labels</code> (boolean): enables y-axis labels.</li>\u000a<li><code>ticks</code> (number): how many y labels to show.</li>\u000a<li><code>color</code> (boolean): turns on color mode (grayscale by default).</li>\u000a<li><code>oscillator</code> (boolean): enables an oscillator overlay component. When\u000ayou click anywhere in the spectrogram, a sine wave plays corresponding\u000ato the frequency you click on.</li>\u000a</ul>\u000a\u000a<h2>Using the Polymer component</h2>\u000a\u000a<p>If you are inclined to embed this component somewhere, you can,\u000asince it's implemented in Polymer, which, by the way, is an\u000aawesome framework. Once you've <a href="http://www.polymer-project.org/docs/start/getting-the-code.html">gotten started</a>, here's\u000athe simplest possible version:</p>\u000a\u000a<pre><code>&lt;g-spectrogram/&gt;\u000a</code></pre>\u000a\u000a<p>Enable controls:</p>\u000a\u000a<pre><code>&lt;g-spectrogram controls&gt;&lt;/g-spectrogram&gt;\u000a</code></pre>\u000a\u000a<p>Pass parameters to the component:</p>\u000a\u000a<pre><code>&lt;g-spectrogram log labels ticks="10"&gt;&lt;/g-spectrogram&gt;\u000a</code></pre>\u000a\u000a<h2>Future work ideas</h2>\u000a\u000a<p>It would be great to add a few things to this tool. If you're interested\u000ain helping, submit your changes as a pull request <a href="https://github.com/borismus/spectrogram">on github</a>.\u000aSome ideas for things that can be done:</p>\u000a\u000a<ul>\u000a<li>Improved axis labeling.</li>\u000a<li>Make it work in mobile browsers.</li>\u000a<li>Loading/saving of traces.</li>\u000a<li>Loading audio data from a file.</li>\u000a<li>Zoom support.</li>\u000a<li>Higher precision FFT results (would require writing a custom FFT\u000arather than using the one built into Web Audio API.)</li>\u000a</ul>\u000a
p2296
tp2297
Rp2298
sg13
V/spectrogram-and-oscillator
p2299
sg15
Nsg16
I01
sg17
VSpectrogram and oscillator
p2300
sg20
VA live-input spectrogram written using [Polymer][polymer] using the [Web\u000aAudio API][wapi].
p2301
sg6
V<p>A live-input spectrogram written using <a href="http://polymer-project.org">Polymer</a> using the <a href="http://webaudioapi.com">Web\u000aAudio API</a>.</p>\u000a\u000a<p><img src="screenshot.png" alt="Screenshot of spectrogram" /></p>\u000a\u000a<p>If you're running Chrome or Firefox, <a href="http://borismus.github.io/spectrogram">see it in action</a>. Once the\u000aspectrogram is running, see if you can make a pattern with your speech\u000aor by whistling. You can also click anywhere on the page to turn on the\u000aoscillator. For a mind-blowing effect, <a href="https://www.youtube.com/watch?v=M9xMuPWAZW8&amp;t=5m30s">load this</a> in a parallel\u000atab.</p>\u000a\u000a
p2302
sg25
g169
sg33
g2299
sg170
(dp2303
g172
S'Jun'
p2304
sg174
S'June 9, 2014'
p2305
sg176
I6
sg177
S'2014-06-09T09:00:00-00:00'
p2306
sg179
I1402329600
sg180
I2014
sg181
I9
ssg65
g182
sg31
S'spectrogram-and-oscillator'
p2307
sS'posted'
p2308
g188
(S'\x07\xde\x06\t'
p2309
tp2310
Rp2311
ssg34
S'content/posts/2014/spectrogram-and-oscillator/index.md'
p2312
sg36
F1433825421.0
sa(dp2313
g2
(dp2314
g4
V This year's  UIST  was held in Waikiki, Honolulu, the undisputed tourist capital of Hawaii. I've stuck to my now three year old habit of taking notes and  posting my favorite work . Since last year, the conference has grown an extra track. The split was generally OK for me, with my track mostly dedicated to user interface innovation (sensors, etc) and another more concerned with crowdsourcing, visualization, and more traditional UIs.    My overall feeling was that the research was mostly interesting from a tech perspective, but focused on solving the wrong problem. For example, at least 5 papers/posters/demos were focused on typing on smartwatches. The keynotes were very thought provoking, especially when juxtaposed with one another.   
p2315
sg28
g7
(g8
g9
V<p>This year's <a href="http://www.acm.org/uist/uist2014/">UIST</a> was held in Waikiki, Honolulu, the undisputed\u000atourist capital of Hawaii. I've stuck to my now three year old habit of\u000ataking notes and <a href="http://smus.com/uist-2013">posting my favorite work</a>. Since last year,\u000athe conference has grown an extra track. The split was generally OK for\u000ame, with my track mostly dedicated to user interface innovation\u000a(sensors, etc) and another more concerned with crowdsourcing,\u000avisualization, and more traditional UIs.</p>\u000a\u000a<p>My overall feeling was that the research was mostly interesting from a\u000atech perspective, but focused on solving the wrong problem. For example,\u000aat least 5 papers/posters/demos were focused on typing on smartwatches.\u000aThe keynotes were very thought provoking, especially when juxtaposed\u000awith one another.</p>\u000a\u000a<!--more-->\u000a\u000a<h2>Focused Ultrasonic Arrays</h2>\u000a\u000a<p>I got to play with a holographic display with touch-feedback. Sounds\u000acrazy, and it is. HaptoMime uses an array of ultrasonic transducers to\u000abeam-form focused ultrasound to a specific target. The touch feedback\u000afeels like a little electric shock, but it's incredible that it works.\u000aThe field of view of the holographic screen is a bit limited:</p>\u000a\u000a<iframe width="600" height="339" src="//www.youtube.com/embed/uARGRlpCWg8" frameborder="0" allowfullscreen></iframe>\u000a\u000a<p>As a kid, I loved drawing patterns on my grandma's rug with my finger.\u000aThis research team was clearly inspired by the same activity, and they\u000acreated several ways of automating the process: using a roller device, a\u000apen, and an focused ultrasonic array:</p>\u000a\u000a<iframe width="600" height="339" src="//www.youtube.com/embed/L0hrETGddLQ" frameborder="0" allowfullscreen></iframe>\u000a\u000a<h2>Multi-device interactions</h2>\u000a\u000a<p>GaussStones built on a bunch of other "Gauss"-prefixed previous work\u000afrom the same lab, showing an array of hall sensors used to sense a\u000avariety of shielded magnetic tokens, which can encode an ID using field\u000astrength. You could play physical chess, or even combine magnetic tokens\u000ato create more complex interactions, like a slider or button unit:</p>\u000a\u000a<iframe width="600" height="339" src="//www.youtube.com/embed/qlr-15Oto6s" frameborder="0" allowfullscreen></iframe>\u000a\u000a<p>Another nice example of multi-device interaction came from MIT, where\u000aa group used this extremely clever way of tracking the phone's position\u000arelative to a laptop, using a 2D gradient, where the color of each pixel\u000amaps to a position in space:</p>\u000a\u000a<iframe width="600" height="339" src="//www.youtube.com/embed/hFH6hJLDoLE" frameborder="0" allowfullscreen></iframe>\u000a\u000a<p>The awkwardly named Vibkinesis shows a smartphone case which is\u000aequipped with two vibrator motors which give a phone the ability to\u000atranslate and rotate on a flat surface. In one example,\u000anotifications caused the phone to rotate by 90 degrees, which had the added benefit of\u000anotifying the user of a notification even if the battery runs out of\u000ajuice. This is apparently funny from a Japanese culture perspective,\u000awhere characters often die under strange circumstances, leaving no clue\u000abut a "dying message" on or around their person. Another example\u000ainvolved a fish-eye lens on the front-facing camera to detect the\u000aposition of the user's hand (based on skin color), and then physically\u000anudging the user to get their attention:</p>\u000a\u000a<iframe width="600" height="339" src="//www.youtube.com/embed/UlFwVUHotrU" frameborder="0" allowfullscreen></iframe>\u000a\u000a<h2>Awesome fabrication techniques</h2>\u000a\u000a<p>I'm a huge fan of subtractive techniques (eg. laser cutting) rather than\u000aadditive ones (eg. 3D printing). FlatFitFab is a CAD tool for easily\u000acreating balsa dinosaur-style models, and evaluating their stability and\u000afeasibility. Super cool work:</p>\u000a\u000a<iframe width="600" height="339" src="//www.youtube.com/embed/HeFQw0chSJY" frameborder="0" allowfullscreen></iframe>\u000a\u000a<p>Rather than creating PCBs in something like Eagle, why not just sketch\u000athem with a conductive pen instead? ShrinkyCircuits does just this,\u000afollowing the principles of Shrinky Dinks, which shrinks when heated.\u000aBecause the whole board shrinks, it improves conductivity of the\u000aconductive ink, and the contact points with electronics components.</p>\u000a\u000a<iframe width="600" height="339" src="//www.youtube.com/embed/4p-l374rb8M" frameborder="0" allowfullscreen></iframe>\u000a\u000a<h2>Spatial AR</h2>\u000a\u000a<p>Research from Microsoft showed rooms instrumented with multiple\u000aProjector+Depth Camera rigs, which allowed for some interesting\u000amulti-user interactions:</p>\u000a\u000a<iframe width="600" height="339" src="//www.youtube.com/embed/ILb5ExBzHqw" frameborder="0" allowfullscreen></iframe>\u000a\u000a<p>Of course, the setup above doesn't allow perspective-corrected scenes.\u000aTo remedy this, they had a companion project which split the room in\u000atwo, creating head-tracked scenes for two participants. Pretty cool,\u000athough it does not generalize to more than two people, nor does it\u000asupport stereo:</p>\u000a\u000a<iframe width="600" height="339" src="//www.youtube.com/embed/Df7fZAYVAIE" frameborder="0" allowfullscreen></iframe>\u000a\u000a<p>Still, entering either of these VR rooms feels a lot less dorky than\u000ahaving to don a VR headset.</p>\u000a\u000a<h2>Keynotes</h2>\u000a\u000a<p>UIST was punctuated by three keynotes, from Ken Perlin, Mark\u000aBolas and Bret Victor, all of which were thought provoking and sometimes\u000afrightening, but unfortunately not recorded. When I used to be in\u000aDeveloper Relations, we would be hardpressed to show up at a conference\u000aif the talk was not recorded because so much of the engagement happened\u000aafter the fact online. <strong>Academia needs the same culture</strong>.</p>\u000a\u000a<p>Ken Perlin kicked off the co-located conference, SUI (Symposium for Spatial\u000aInteraction), off with a nice talk about making computer use more like\u000aperforming a music, and less like writing a musical composition. As he\u000agave the talk, he very effectively used a tool he created called Chalk\u000aTalk, which lets you sketch objects with behaviors in short hand - very\u000ameta. Unfortunately I've been unable to find anything published about\u000athe tool, as it would be interesting to play with. Ken envisioned a\u000aworld where you could do something conceptually similar to Chalk Talk\u000awithout a computer in the way. If this "virtual chalk" capability was\u000aavailable to all humans, it would transform the way we communicate. I\u000awasn't completely convinced. When I'm discussing something with\u000acolleagues, we only use a whiteboard for only very specialized things\u000alike drawing a diagram of multiple objects. So there are two things that\u000aneed to happen:</p>\u000a\u000a<ol>\u000a<li>This virtual chalk needs to be <strong>easier to access</strong> than a whiteboard\u000awhile in a meeting room.</li>\u000a<li><strong>Expand the set of concepts</strong> that can be expressed with virtual chalk.\u000aText and speech is <a href="http://graydon.livejournal.com/196162.html">pretty powerful</a>.</li>\u000a</ol>\u000a\u000a<p>Mark Bolas started UIST with a pretty terrifying keynote on virtual\u000areality. His premise was that "we are headed into a virtual future,\u000awhether we want it or not". Terrifyingly, Mark seemed to be okay with\u000athis inevitability, even going as far as discounting augmented reality,\u000asince by the time we've built VR, we'll just want to stay in our\u000ahelmets. The real world isn't that great anyway. One thing I liked was\u000ahis call for creating more <strong>surreal experiences</strong> in VR rather than\u000atrying to emulate the real world. These types of simulations are\u000aconspicuously missing from <a href="https://share.oculusvr.com/category/all">existing VR demos</a>.</p>\u000a\u000a<p>Bret Victor ended the conference with a much needed humanist\u000acounterpoint to Mark' vision. I cannot do the talk justice, and eagerly\u000aawait a recording of it to try to understand all of the nuances. The big\u000aidea of the talk was that "knowledge work" which started with the\u000aprinting press is tyrannical, reducing all of our senses and abilities\u000ato manipulating symbols on a sheet of paper. So many other things that\u000aevolution has designed for us, like hearing, smell, sense of space,\u000atouch, etc, are all thrown out of the window. This problem only gets\u000acompounded as we move to virtualize everything with touch screens. Bret\u000athinks we're poised to design the next great "dynamic" medium after the\u000aprinting press, something that is always interactive and multimodal, and\u000atakes advantage of a wide array of human capabilities.</p>\u000a\u000a<p>According to Mark Bolas, the real world is flawed, and we should build a\u000abetter virtual one. Bret Victor's vision is that humans are perfect,\u000ahaving evolved over thousands of years. Rather than changing what it\u000ameans to be human, we should build a new medium that adapts to our\u000ainherent strengths and weaknesses. Ken Perlin's "virtual chalk" is a\u000agreat example application for this dynamic medium.</p>\u000a\u000a<h2>Tracks I missed</h2>\u000a\u000a<p>Because UIST has become a multi-track conference, I inevitably missed\u000ainteresting parts. In particular, the collaboration track had some\u000a<a href="https://www.youtube.com/watch?v=QtyO-oFlzGg">awesome</a> <a href="https://www.youtube.com/watch?v=jMH_qQF0vKg">work</a>, and there was one <a href="https://www.youtube.com/watch?v=YMfzAstvij0">music-related paper</a>\u000apaper. It was great to have had a good excuse to go this year, showing\u000aCardboard to the academic community. Looking forward to next year,\u000aalthough it is to be held in a somewhat <a href="http://uist.acm.org/about">less glamorous location</a>.</p>\u000a\u000a<p><img src="sunset.jpg" alt="Hawaii sunset." /></p>\u000a
p2316
tp2317
Rp2318
sg13
V/uist-2014
p2319
sg15
Nsg16
I01
sg17
VUIST 2014 highlights
p2320
sg20
VThis year's [UIST][uist] was held in Waikiki, Honolulu, the undisputed\u000atourist capital of Hawaii.
p2321
sg6
V<p>This year's <a href="http://www.acm.org/uist/uist2014/">UIST</a> was held in Waikiki, Honolulu, the undisputed\u000atourist capital of Hawaii. I've stuck to my now three year old habit of\u000ataking notes and <a href="http://smus.com/uist-2013">posting my favorite work</a>. Since last year,\u000athe conference has grown an extra track. The split was generally OK for\u000ame, with my track mostly dedicated to user interface innovation\u000a(sensors, etc) and another more concerned with crowdsourcing,\u000avisualization, and more traditional UIs.</p>\u000a\u000a<p>My overall feeling was that the research was mostly interesting from a\u000atech perspective, but focused on solving the wrong problem. For example,\u000aat least 5 papers/posters/demos were focused on typing on smartwatches.\u000aThe keynotes were very thought provoking, especially when juxtaposed\u000awith one another.</p>\u000a\u000a
p2322
sg25
g169
sg33
g2319
sg170
(dp2323
g172
S'Oct'
p2324
sg174
S'October 14, 2014'
p2325
sg176
I10
sg177
S'2014-10-14T09:00:00-00:00'
p2326
sg179
I1413302400
sg180
I2014
sg181
I14
ssg65
g182
sg31
S'uist-2014'
p2327
sS'posted'
p2328
g188
(S'\x07\xde\n\x0e'
p2329
tp2330
Rp2331
ssg34
S'content/posts/2014/uist-2014/index.md'
p2332
sg36
F1433825417.0
sa(dp2333
g2
(dp2334
g4
V Sensors found in smartphones define the mobile experience. GPS and the magnetometer enable the fluid experience of maps; motion sensing enables activity recognition and games, and of course the camera and microphone allow whole categories of rich media applications. Beyond these now obvious examples, sensors can also enable clever inventions, such as  Cycloramic , which used the vibrator motor in iPhones (4 and 5) to rotate the phone and take a panorama,  pushup counters  which use the proximity sensor to count repetitions, and  Send Me To Heaven , which uses the accelerometer to determine flight time of a phone thrown vertically as high as possible. I've had some experience using and abusing sensors too, most recently for the  Cardboard magnet button .        However, over the last couple of years, I've had to step away from the web as a development platform, in part because of the poor state of sensor APIs.  In this post, I will describe some of the problems, take a look at sensor APIs on iOS and Android, and suggest a solution in the spirit of the  extensible web manifesto .   
p2335
sg28
g7
(g8
g9
V<p>Sensors found in smartphones define the mobile experience. GPS and the\u000amagnetometer enable the fluid experience of maps; motion sensing enables\u000aactivity recognition and games, and of course the camera and microphone\u000aallow whole categories of rich media applications. Beyond these now\u000aobvious examples, sensors can also enable clever inventions, such as\u000a<a href="http://www.cycloramic.com/">Cycloramic</a>, which used the vibrator motor in iPhones (4 and 5) to\u000arotate the phone and take a panorama, <a href="https://play.google.com/store/apps/details?id=com.runtastic.android.pushup.pro">pushup counters</a> which\u000ause the proximity sensor to count repetitions, and <a href="https://play.google.com/store/apps/details?id=com.carrotpop.www.smth">Send Me To\u000aHeaven</a>, which uses the accelerometer to determine flight time of\u000aa phone thrown vertically as high as possible. I've had some experience\u000ausing and abusing sensors too, most recently for the <a href="http://smus.com/talk/2014/io14/">Cardboard magnet\u000abutton</a>.</p>\u000a\u000a<iframe width="640" height="360" src="https://www.youtube.com/embed/l5775qwORk4" frameborder="0" allowfullscreen></iframe>\u000a\u000a<p>However, over the last couple of years, I've had to step away from the\u000aweb as a development platform, in part because of the poor state of\u000asensor APIs.  In this post, I will describe some of the problems, take a\u000alook at sensor APIs on iOS and Android, and suggest a solution in the\u000aspirit of the <a href="https://extensiblewebmanifesto.org/">extensible web manifesto</a>.</p>\u000a\u000a<!--more-->\u000a\u000a<h2>Existing sensor APIs are underspecified</h2>\u000a\u000a<p>One of the most popular sensor APIs on the web is the <a href="http://w3c.github.io/deviceorientation/spec-source-orientation.html#devicemotion">DeviceMotion event\u000aAPI</a>, which is basically always just an opaque abstraction around the\u000aaccelerometer. The web, as always, tries to solve the problem in the\u000amost general way possible:</p>\u000a\u000a<blockquote>\u000a  <p>This specification provides several new DOM events for obtaining\u000a  information about the physical orientation and movement of the hosting\u000a  device. The information provided by the events is not raw sensor data,\u000a  but rather high-level data which is agnostic to the underlying source\u000a  of information. Common sources of information include gyroscopes,\u000a  compasses and accelerometers.</p>\u000a</blockquote>\u000a\u000a<p>This could be fine in theory, except the specs end up being so vague in\u000atheir attempt to please everybody, that they under-specify the behavior\u000aof events such as <code>DeviceOrientation</code>. Throw in some rogue implementers,\u000aand you end up with huge discrepancies in browsers, as <a href="http://www.html5rocks.com/en/tutorials/device/orientation/">Pete found back\u000ain 2011</a>:</p>\u000a\u000a<blockquote>\u000a  <p>For most browsers, alpha returns the compass heading, so when the\u000a  device is pointed north, alpha is zero. With Mobile Safari, alpha is\u000a  based on the direction the device was pointing when device orientation\u000a  was first requested. The compass heading is available in the\u000a  webkitCompassHeading parameter.</p>\u000a</blockquote>\u000a\u000a<p>A useful sensor abstraction would be to build a compass on top of the\u000amagnetometer (and maybe gyro) sensors, and then expose that as a high\u000alevel Compass API. Unfortunately many web sensor APIs give us a\u000amid-level of abstraction. They don't map reliably to particular hardware\u000asensors, nor do they provide much use. Sensors allow many applications\u000athat were not originally envisioned by the spec writers. By choosing\u000apoorly specified ivory-tower abstractions, the web limits what can be\u000adone on the platform.</p>\u000a\u000a<h2>Low level sensor APIs don't exist</h2>\u000a\u000a<p>While you can work around the insanity of <code>Device*</code> style events on the\u000aweb with platform-specific shims, you cannot work around missing sensor\u000aAPIs. Magnetometers, pressure sensors, proximity, light, temperature,\u000abattery, etc. These are mostly missing, and the ones that are specified\u000aare specified in a very narrow way that does not generalize across to\u000aother types of sensors (eg. <a href="http://www.w3.org/TR/2013/CR-ambient-light-20131001/">DeviceLightEvent</a>).</p>\u000a\u000a<p>Unfortunately it seems that previous attempts to push for a general low\u000alevel sensor API <a href="http://lists.w3.org/Archives/Public/public-geolocation/2011Oct/0000.html">haven't really gotten much traction</a>. In\u000afact, it's a bit unclear whether or not the <a href="http://www.w3.org/2009/dap/">Device API working\u000agroup</a>, is even the right place for sensor APIs, since their\u000amandate is supposedly more about services than sensors:</p>\u000a\u000a<blockquote>\u000a  <p>[To] enable the development of Web Applications and Web Widgets that\u000a  interact with devices services such as Calendar, Contacts, Camera,\u000a  etc.</p>\u000a</blockquote>\u000a\u000a<p>Except <a href="https://dvcs.w3.org/hg/dap/raw-file/default/sensor-api/Overview.html">here's a sensor API</a> from the same group, which\u000aseems to be abandoned... I don't even</p>\u000a\u000a<p>There are more recent voices (circa 2014) that seem to be pushing in a\u000ageneric sensor API direction, from folks like <a href="https://github.com/rwaldron/sensors">Rick Waldron</a>\u000aand <a href="http://lists.w3.org/Archives/Public/public-device-apis/2014Sep/0024.html">Tim Volodine</a>. Many of these ideas are still working within\u000athe confines of a sensor API for each type of sensor. This does not\u000ascale well for the web, which tends to take a long time for any new web\u000astandard, but this renewed interest is very exciting and promising!</p>\u000a\u000a<h2>Sensors on other platforms</h2>\u000a\u000a<p>The web is woefully behind native platforms in almost every regard (with\u000apossibly the exception of audio). Sensors on iOS and Android have a rich\u000ahistory, and ended up in a pretty similar place as the two platforms\u000ahave scrambled to converge. Let's take a look.</p>\u000a\u000a<p>iOS started off with a <a href="https://developer.apple.com/LIBRARY/ios/documentation/UIKit/Reference/UIAccelerometer_Class/index.html">UIAccelerometer API</a>, which was\u000areplaced by <a href="https://developer.apple.com/LIBRARY/ios/documentation/CoreMotion/Reference/CoreMotion_Reference/index.html">CoreMotion</a> in iOS 5. Rather than providing a\u000aseries of specific APIs for each type of sensor API as it had before,\u000aCoreMotion provides a unified framework for sensor events. Each data\u000atype inherits from a common base class <code>CMLogItem</code>, and most of the API is\u000aencapsulated in <code>CMMotionManager</code>, which explicitly lists accelerometer,\u000agyroscope and magnetometer-related APIs. iOS went from specific to\u000ageneric, which makes it super easy to add new types of sensor data. That\u000asaid, the API is generic only for motion sensors, which excludes a bunch\u000aof sensors not directly related to motion like temperature, humidity,\u000aetc.</p>\u000a\u000a<p>Android started off right, and hasn't had to change much, providing a\u000a<a href="http://developer.android.com/reference/android/hardware/SensorEvent.html">generic API for sensors</a> since API level 3. Android's API\u000ais accessed through a SensorManager, which provides a somewhat overly\u000aabstract API, because of its support for multiple sensors of one type\u000a(eg. two accelerometers) in the same device. Still, the idea is good,\u000aand all of the low level sensor data are well specified (per sensor\u000atype)so the hardware/firmware vendor knows what data format their sensor\u000ashould stream. Of course there are still rogue implementations that\u000adon't follow the spec, but that is a perennial problem for any open-ish\u000aecosystem.</p>\u000a\u000a<p>Android also has a <a href="http://developer.android.com/guide/topics/sensors/sensors_overview.html">distinction</a> between software-based sensors and\u000ahardware-based ones. The idea is that the same framework can provide\u000aboth the low level data coming directly from the hardware, as well as\u000auseful higher level data obtained through <a href="http://en.wikipedia.org/wiki/Sensor_fusion">sensor fusion</a>. As of\u000aAPI level 19, Android also provides <a href="http://developer.android.com/reference/android/hardware/SensorManager.html#flush(android.hardware.SensorEventListener)">batch mode</a> for sensor data, which\u000ais very useful for conserving battery and CPU for applications where\u000asome delay is acceptable.</p>\u000a\u000a<p>One nice advantage of an iOS style API is that each sensor type has its\u000aown structure (rather than just an amorphous array of floats, as in\u000aAndroid), which is quite a bit easier to parse. The downside is that\u000aadding new sensor types introduces more overhead, since each one\u000arequires a new structure to be defined and agreed upon. Since we are\u000atalking about web standards, which evolve at a glacial pace, we should\u000aerr on a simple API that works well without spec modifications.</p>\u000a\u000a<h2>Great artists steal</h2>\u000a\u000a<p>There is no need for the web to reinvent the wheel. The wheel has\u000aalready been invented by iOS and Android. All we need to do is take the\u000agood parts from these successful sensor platforms, and integrate them\u000ainto the web in a way that makes sense. The web is not the place for\u000ainnovation, but for standardization.</p>\u000a\u000a<p>Conceptually, a sensor provides a stream of data. The developer should\u000abe able to configure the rate at which new data comes in, as well as\u000abatching the data in windows of sensor data (as is customarily done with\u000aaudio data, for example). In Android, because of a plurality of devices,\u000ait's important to be able to check if a particular sensor is available.\u000aThe same concept maps well to the web.</p>\u000a\u000a<h2>Toward A Web Sensor API</h2>\u000a\u000a<p>In general, here are the requirements for a Web Sensor API that works:</p>\u000a\u000a<ul>\u000a<li>A specification defining the format of the data, similar to\u000a<a href="http://developer.android.com/reference/android/hardware/SensorEvent.html">Android</a>.</li>\u000a<li>A way to feature detect for the existence of a particular sensor.</li>\u000a<li>A way to request (and revoke) a stream of sensor data.</li>\u000a<li>A way to specify how often to poll the sensor.</li>\u000a<li>Bonus: A way to request sensor data in batch form.</li>\u000a</ul>\u000a\u000a<p>While bringing an API like this to the web is a huge undertaking, there\u000ais a silver lining. The sensors we're talking about are all considered\u000a(at least for now) low-security, in the sense that on native platforms,\u000athere is no extra permission required to access them. This makes it\u000apossible to simply propose an API, convince everybody of it's worth, and\u000athen have it implemented across the web!</p>\u000a\u000a<p>I don't have a strong opinion about how the API itself looks like as\u000along as it fulfils the above requirements. Here's a simple strawman\u000awhich should satisfy them:</p>\u000a\u000a<pre><code>// Check for magnetometer support.\u000aif (sensors.Magnetometer === undefined) {\u000a  console.error('No magnetometer found');\u000a}\u000a\u000a// Start listening for changes to the sensor.\u000avar magnetometer = sensors.Magnetometer;\u000amagnetometer.addEventListener('changed', onMagnetometer, {\u000a  sample_rate: sensors.POLL_FAST, // In hertz, eg. POLL_FAST == 100\u000a  batch: 1 // Number of data points to provide in a single poll.\u000a});\u000a\u000a// Handle sensor events.\u000afunction onMagnetometer(event) {\u000a  var data = event.data[0];\u000a  // Get the timestamp (in millis).\u000a  var t = data.timestamp;\u000a  // Get the data (in this case T, as per spec).\u000a  var x = data.values[0];\u000a  var y = data.values[1];\u000a  var z = data.values[2];\u000a  // Process the data.\u000a  superAdvancedSensorFusionThing.addData(t, x, y, z);\u000a}\u000a\u000a// Stop listening.\u000amagnetometer.removeEventListener('changed', onMagnetometer);\u000a</code></pre>\u000a\u000a<h2>Conclusion</h2>\u000a\u000a<p>If you aren't yet convinced that we need access to low level sensors on\u000athe web, recall web developers scoffing at device pixel ratio (DPR),\u000areally questioning the need for to ever go above 2x. Now that <a href="https://developers.google.com/cardboard/">some\u000ascreens</a> are ending up 5cm from your face, the current\u000ageneration of 4x displays isn't enough. The same exact thing applies to\u000asensors. The need is there, but it is not seen as enough of a priority\u000aby the web community.</p>\u000a\u000a<p>By enabling low level sensor access, we can allow new experiences never\u000abefore possible on the web. <a href="https://play.google.com/store/apps/details?id=com.runtastic.android.pushup.pro">Pushup rep counters</a>, the <a href="http://smus.com/talk/2014/io14/">magnet\u000abutton</a> in Cardboard, and myriads more applications\u000ayet to be concieved could all be built on the web platform, eliminating\u000aa big reason why the web is increasingly losing its relevance on mobile\u000adevices. Providing low level sensor access is critical and aligns\u000aperfectly with the <a href="https://extensiblewebmanifesto.org/">extensible web vision</a>.</p>\u000a\u000a<p><em>Update (Nov 14, 2014): There was a <a href="http://lists.w3.org/Archives/Public/public-device-apis/2014Nov/0018.html">W3C call</a> about this very\u000atopic yesterday! Kicking off efforts in <a href="https://github.com/w3c/sensors">this github repo</a>.\u000aJoin us!</em></p>\u000a
p2336
tp2337
Rp2338
sg13
V/web-sensor-api
p2339
sg15
Nsg16
I01
sg17
VWeb Sensor API: raw and uncut
p2340
sg20
VSensors found in smartphones define the mobile experience.
p2341
sg6
V<p>Sensors found in smartphones define the mobile experience. GPS and the\u000amagnetometer enable the fluid experience of maps; motion sensing enables\u000aactivity recognition and games, and of course the camera and microphone\u000aallow whole categories of rich media applications. Beyond these now\u000aobvious examples, sensors can also enable clever inventions, such as\u000a<a href="http://www.cycloramic.com/">Cycloramic</a>, which used the vibrator motor in iPhones (4 and 5) to\u000arotate the phone and take a panorama, <a href="https://play.google.com/store/apps/details?id=com.runtastic.android.pushup.pro">pushup counters</a> which\u000ause the proximity sensor to count repetitions, and <a href="https://play.google.com/store/apps/details?id=com.carrotpop.www.smth">Send Me To\u000aHeaven</a>, which uses the accelerometer to determine flight time of\u000aa phone thrown vertically as high as possible. I've had some experience\u000ausing and abusing sensors too, most recently for the <a href="http://smus.com/talk/2014/io14/">Cardboard magnet\u000abutton</a>.</p>\u000a\u000a<iframe width="640" height="360" src="https://www.youtube.com/embed/l5775qwORk4" frameborder="0" allowfullscreen></iframe>\u000a\u000a<p>However, over the last couple of years, I've had to step away from the\u000aweb as a development platform, in part because of the poor state of\u000asensor APIs.  In this post, I will describe some of the problems, take a\u000alook at sensor APIs on iOS and Android, and suggest a solution in the\u000aspirit of the <a href="https://extensiblewebmanifesto.org/">extensible web manifesto</a>.</p>\u000a\u000a
p2342
sg25
g169
sg33
g2339
sg170
(dp2343
g172
S'Nov'
p2344
sg174
S'November 13, 2014'
p2345
sg176
I11
sg177
S'2014-11-13T09:00:00-00:00'
p2346
sg179
I1415898000
sg180
I2014
sg181
I13
ssg65
g182
sg31
S'web-sensor-api'
p2347
sS'posted'
p2348
g188
(S'\x07\xde\x0b\r'
p2349
tp2350
Rp2351
ssg34
S'content/posts/2014/web-sensor-api/index.md'
p2352
sg36
F1433868666.0
sa(dp2353
g2
(dp2354
g4
V It's been over three years since the design of this site has been updated. Time to change that!         This is the fifth revision of this site's design. Looking over  previous designs , I've been happier with minimal designs, especially  this one  from 2012. I was inspired by many excellent designs such as  Butterick's Practical Typography ,  Teehan+Lax ,  Erik Johansson ,  Medium  and  Frank Chimero .    The new design is visually cleaner. I  use flexbox  in many places, which makes the CSS far more intuitive. The responsive parts are very simple, consisting of just ten CSS declarations.   
p2355
sg28
g7
(g8
g9
V<p>It's been over three years since the design of this site has been\u000aupdated. Time to change that!</p>\u000a\u000a<p><img src="0days.jpg" class="floatright"/></p>\u000a\u000a<p>This is the fifth revision of this site's design. Looking over <a href="/redesign-2012">previous\u000adesigns</a>, I've been happier with minimal designs,\u000aespecially <a href="/redesign-2012/v2.png">this one</a> from 2012. I was inspired\u000aby many excellent designs such as <a href="http://practicaltypography.com/typography-in-ten-minutes.html">Butterick's Practical\u000aTypography</a>,\u000a<a href="http://www.teehanlax.com/">Teehan+Lax</a>, <a href="http://www.erikjohanssonphoto.com/">Erik\u000aJohansson</a>,\u000a<a href="https://medium.com/designing-medium/death-to-typewriters-9b7712847639">Medium</a>\u000aand <a href="http://frankchimero.com/writing/other-halves">Frank Chimero</a>.</p>\u000a\u000a<p>The new design is visually cleaner. I <a href="http://caniuse.com/#feat=flexbox">use\u000aflexbox</a> in many places, which makes\u000athe CSS far more intuitive. The responsive parts are very simple,\u000aconsisting of just ten CSS declarations.</p>\u000a\u000a<!--more-->\u000a\u000a<p>Rather than subjecting readers to my face on every page, I have a simple\u000astipple background on the <a href="/about">about page</a>, which I created using the\u000acomplex but functional <a href="http://www.evilmadscientist.com/2012/stipplegen2/">StippleGen</a>.</p>\u000a\u000a<p>Also, I've started working on a self-hosted visual link blog that you\u000acan check out in under <a href="/inspiration">inspiring clippings</a>. I've\u000aimplemented a companion Chrome extension that makes it super easy to\u000aclip inspiring content from anywhere on the web and bring it to that\u000apage.</p>\u000a\u000a<p><strong>Typography:</strong> I'm continuing to use Google fonts, which seems to be so\u000amuch simpler to use than various competitors. I have not completely\u000aoptimized my selection of fonts, but this is satisfactory given my\u000abelief that no design is ever finished. <a href="http://alistapart.com/article/improving-ux-through-front-end-performance">Performance is UX</a> too,\u000aand aesthetic decisions need to be counterbalanced by mundane\u000aconsiderations like page load time.  Unfortunately <a href="https://www.google.com/fonts/specimen/Dosis">Dosis</a> didn't\u000amake the cut.</p>\u000a\u000a<p><strong>Infrastructure:</strong> This site is still built using the <a href="https://github.com/borismus/lightning">lightning static\u000ablog</a> engine, which I'm continuing to improve. On that front,\u000aI've dropped the ambitious goal of being able to edit content from any\u000adevice using dropbox, since in practice I always author on my laptop.\u000aInstead, the focus has been on optimizing the edit flow for the local\u000aoffline case, and I have built <a href="https://github.com/lepture/python-livereload">livereload</a> into the local preview\u000aserver. As far as hosting, I have conceded to GitHub Pages, and have\u000amigrated away from using S3 directly.</p>\u000a\u000a<p><strong>Thanks:</strong> to <a href="https://twitter.com/mikemartin604">Mike</a>,\u000a<a href="https://twitter.com/paul_irish">Paul</a>,\u000a<a href="https://twitter.com/smattyang">Seungho</a>,\u000a<a href="https://twitter.com/scottjenson">Scott</a>,\u000a<a href="https://twitter.com/mahemoff">Michael</a>, and other awesome friends that\u000agave me excellent design suggestions and found bugs!</p>\u000a\u000a<p>While I appreciate companies like <a href="http://medium.com">Medium</a> and\u000a<a href="http://svbtle.com/">Svbtle</a> advancing the aesthetics of the web, I\u000acompletely <a href="http://practicaltypography.com/billionaires-typewriter.html">agree with Matthew Butterick</a>'s view, and will\u000acontinue self-hosting my writings for as long as possible. Long live the\u000aplurality of the web!</p>\u000a
p2356
tp2357
Rp2358
sg13
V/design-v5
p2359
sg15
Nsg16
I01
sg17
VSite redesign, version five
p2360
sg20
VIt's been over three years since the design of this site has been\u000aupdated.
p2361
sg6
V<p>It's been over three years since the design of this site has been\u000aupdated. Time to change that!</p>\u000a\u000a<p><img src="0days.jpg" class="floatright"/></p>\u000a\u000a<p>This is the fifth revision of this site's design. Looking over <a href="/redesign-2012">previous\u000adesigns</a>, I've been happier with minimal designs,\u000aespecially <a href="/redesign-2012/v2.png">this one</a> from 2012. I was inspired\u000aby many excellent designs such as <a href="http://practicaltypography.com/typography-in-ten-minutes.html">Butterick's Practical\u000aTypography</a>,\u000a<a href="http://www.teehanlax.com/">Teehan+Lax</a>, <a href="http://www.erikjohanssonphoto.com/">Erik\u000aJohansson</a>,\u000a<a href="https://medium.com/designing-medium/death-to-typewriters-9b7712847639">Medium</a>\u000aand <a href="http://frankchimero.com/writing/other-halves">Frank Chimero</a>.</p>\u000a\u000a<p>The new design is visually cleaner. I <a href="http://caniuse.com/#feat=flexbox">use\u000aflexbox</a> in many places, which makes\u000athe CSS far more intuitive. The responsive parts are very simple,\u000aconsisting of just ten CSS declarations.</p>\u000a\u000a
p2362
sg25
g169
sg33
g2359
sg170
(dp2363
g172
S'Jun'
p2364
sg174
S'June 10, 2015'
p2365
sg176
I6
sg177
S'2015-06-10T09:00:00-00:00'
p2366
sg179
I1433952000
sg180
I2015
sg181
I10
ssg65
g182
sg31
S'design-v5'
p2367
sS'posted'
p2368
g188
(S'\x07\xdf\x06\n'
p2369
tp2370
Rp2371
ssg34
S'content/posts/2015/design-v5/index.md'
p2372
sg36
F1434414169.0
sa(dp2373
g2
(dp2374
g4
V Despite free access to information via the Internet and an increasingly global world, people still seem to have all sorts of divergent ideas about how the world works. For example, did you know that eating hot bread and pastries is incredibly unhealthy? Indeed, it can often even lead to complete bowel obstruction! I learned this fact as a kid, while growing up in the Soviet Union. Understandably, I have been very careful to avoid eating hot baked goods. That is, until recently, when my American girlfriend questioned the validity of my belief and I began to harbor some doubts. I decided to check if it was actually true, and asked Google. The results were very clear: I had fallen prey to an old wives tale. My worldview, shattered.    Incredulous, I searched for the same thing in Russian and arrived at the opposite conclusion. "What's up with that?" I thought, and wrote this post.   
p2375
sg28
g7
(g8
g9
V<p>Despite free access to information via the Internet and an increasingly global\u000aworld, people still seem to have all sorts of divergent ideas about how the world\u000aworks. For example, did you know that eating hot bread and pastries is\u000aincredibly unhealthy? Indeed, it can often even lead to complete bowel\u000aobstruction! I learned this fact as a kid, while growing up in the Soviet Union.\u000aUnderstandably, I have been very careful to avoid eating hot baked goods.\u000aThat is, until recently, when my American girlfriend questioned the validity of my\u000abelief and I began to harbor some doubts. I decided to check if it was actually\u000atrue, and asked Google. The results were very clear: I had fallen prey to an old\u000awives tale. My worldview, shattered.</p>\u000a\u000a<p>Incredulous, I searched for the same thing in Russian and arrived at the\u000aopposite conclusion. "What's up with that?" I thought, and wrote this post.</p>\u000a\u000a<!--more-->\u000a\u000a<h2>Asking in different languages</h2>\u000a\u000a<p>I searched Google for "hot bread unhealthy", and tallied up the top 5 results:</p>\u000a\u000a<table>\u000a<tr><th>Domain</th><th>Unhealthy?</th></tr>\u000a<tr><td>davidwalbert.com</td><td>No</td></tr>\u000a<tr><td>chestofbooks.com</td><td>Maybe</td></tr>\u000a<tr><td>gurumagazine.org</td><td>For some</td></tr>\u000a<tr><td>lthforum.com</td><td>No</td></tr>\u000a<tr><td>answers.yahoo.com</td><td>No</td></tr>\u000a</table>\u000a\u000a<p>I then compared it to an equivalent Russian search string: "\u0433\u043e\u0440\u044f\u0447\u0438\u0439 \u0445\u043b\u0435\u0431\u000a\u0432\u0440\u0435\u0434\u0435\u043d". The following are my results in English:</p>\u000a\u000a<table>\u000a<tr><th>Domain</th><th>Unhealthy?</th></tr>\u000a<tr><td>useful-food.ru</td><td>Yes</td></tr>\u000a<tr><td>foodblogger.ru</td><td>Yes</td></tr>\u000a<tr><td>hlebopechka.ru</td><td>Yes</td></tr>\u000a<tr><td>otvet.mail.ru</td><td>Maybe</td></tr>\u000a<tr><td>otvet.mail.ru</td><td>Yes</td></tr>\u000a</table>\u000a\u000a<p>My <a href="https://goo.gl/ltPefm">working spreadsheet</a> contains more colorful details\u000aif you are interested.</p>\u000a\u000a<h2>Language shapes your... search results?</h2>\u000a\u000a<p>No English language site suggested that eating hot bread was unhealthy. Three of\u000athe top five results explicitly point it out as an old wives tale. The first hit,\u000a<a href="http://goo.gl/Cj9jKS">the most skeptical of the bunch</a> even cites articles from\u000athe 18th and 19th centuries which have since been refuted.</p>\u000a\u000a<p>In stark contrast, no Russian language site suggested that eating fresh\u000abread was totally fine. Four of five of the top results explicitly said that it\u000awas unhealthy, suggesting that fresh bread is difficult to digest, encourages\u000aswallowing without chewing, and eating it leads to all sorts of gastrointestinal\u000atrouble like stomach pain, inflammation, constipation and full on bowel\u000aobstruction. Oh my!</p>\u000a\u000a<p>One possibility is that the environments of the Russian and English speaker are\u000ain fact completely different. The bread making processes in Russia could differ\u000afrom other places in the world. Many Russians favor rye bread, which takes some\u000aeffort to find in North America, for example. The main reason for unhealthiness\u000aof fresh bread seems to be related to it being undercooked, with the yeast still\u000abeing active until it cools. Maybe rye better protects the yeast, or takes less\u000atime or heat to cook? </p>\u000a\u000a<p>This and other theories are possible, though not likely. My intuition suggests a\u000asimpler explanation.</p>\u000a\u000a<h2>Nothing Is True and Everything Is Possible</h2>\u000a\u000a<p>There is an expression in Russian: "\u0443\u043c\u043e\u043c \u0420\u043e\u0441\u0441\u0438\u044e \u043d\u0435 \u043f\u043e\u043d\u044f\u0442\u044c", which roughly\u000atranslates as "Russia cannot be understood with the mind". There is a certain \u000amystery deeply ingrained in the national character which, fascinatingly, has\u000aalways been a point of pride.  The heading of this section is actually taken\u000afrom the title of a <a href="http://www.amazon.com/Nothing-Is-True-Everything-Possible/dp/1610394550">book about modern Russia</a>, subtitled "The Surreal\u000aHeart of the New Russia". In Russia, rationalism and skepticism is on the\u000adecline, in favor of traditionalism and magical thinking. Given a rich tradition\u000aof <a href="https://en.wikipedia.org/wiki/Russian_traditions_and_superstitions">traditions, superstitions, and beliefs</a> in Russian culture, there is a\u000alarge pool of absurdity to pick from.</p>\u000a\u000a<p>Given that, and my recent search history, you can imagine what I now believe\u000aabout the harmful effects of eating freshly baked bread. I don't much care\u000awhether or not eating fresh bread is healthy, especially since as a card\u000acarrying Celiac, I can't even enjoy the delicious kind. The fascinating\u000aconclusion from my multilingual sojourn is this:</p>\u000a\u000a<p><strong>Having searched for the same thing in their native languages, a Russian\u000aspeaker and an English speaker would have arrived at a completely different\u000aworld-view.</strong></p>\u000a\u000a<p>The Russian language <a href="https://en.wikipedia.org/wiki/List_of_territorial_entities_where_Russian_is_an_official_language">maps closely</a> to Russia and Russian culture, certainly\u000amore so than <a href="https://en.wikipedia.org/wiki/List_of_territorial_entities_where_English_is_an_official_language">English does</a> to any particular country and culture. The\u000aresult is that queries in Russian are suspect to a very natural echo chamber,\u000aechoing and amplifying deeply held beliefs with the help of our supposedly\u000anormalizing open Internet. </p>\u000a\u000a<h2>Translated foreign pages</h2>\u000a\u000a<p>There are hundreds of other examples of queries that when translated will yield\u000adramatically different results much like "hot bread unhealthy"/"\u0433\u043e\u0440\u044f\u0447\u0438\u0439 \u0445\u043b\u0435\u0431\u000a\u0432\u0440\u0435\u0434\u0435\u043d". There's a simple formula for finding more. Pick a language and write a\u000aquery string, translate it into another language, perform both searches and\u000aanalyze the top results.</p>\u000a\u000a<p>This sounds a lot like something that can be automated. Indeed, Google used to\u000aautomatically translate queries, perform searches with translated queries, and\u000asurface them to the user. Unfortunately this "Translated foreign pages" feature\u000awas <a href="https://productforums.google.com/forum/#!topic/websearch/tYo0LpcVobI/discussion">removed several years ago</a>, due to lack of usage. Also, there are\u000adifficulties with automating the process. The Google Translation of "hot bread\u000aunhealthy" is "\u0433\u043e\u0440\u044f\u0447\u0438\u0439 \u0445\u043b\u0435\u0431 \u043d\u0435\u0437\u0434\u043e\u0440\u043e\u0432\u044b\u0439", which in Russian sounds like the bread\u000aitself is ill, and yields less relevant search results.</p>\u000a\u000a<p>It's surprising how clearly this cultural difference can be seen through the\u000asimple example of warm bread and a search engine. The initial surprise can be\u000aeasily explained though, since the search engine crawls a naturally insular\u000acorpus of articles in the same language. Many of the search results in English\u000acite the same sources. The same is true for search results in Russian. The key\u000apoint, though, is that there is very little shared linking between the English\u000aand Russian sites, especially since <a href="https://en.wikipedia.org/wiki/List_of_countries_by_English-speaking_population">only 5% of Russians speak English</a>.\u000aThe language corpuses seem to be almost completely insulated from one another.\u000aInevitably, confirmation bias kicks in and you end up with the polarized world\u000awe live in today.</p>\u000a\u000a<p>I'd love to see what similar analyses on other search queries. For instance,\u000athere is a Russian gadget called a <a href="http://www.amazon.com/Dark-Blue-Lamp-Minin-Reflector/dp/B00RPG6UTW">Minin Reflector</a>, which consists of a\u000alamp with a blue filter. You simply shine it onto the part of your body that\u000aails you, and presto, instant pain relief... sigh!</p>\u000a\u000a<p>Wrapping up this blog, I am enjoying some delicious, fresh from the oven, hot\u000amuffins. I'll keep you posted with the definitive truth!</p>\u000a
p2376
tp2377
Rp2378
sg13
V/hot-bread-delicious-deadly
p2379
sg15
Nsg16
I01
sg17
VHot bread: delicious or deadly?
p2380
sg20
VDespite free access to information via the Internet and an increasingly global\u000aworld, people still seem to have all sorts of divergent ideas about how the world\u000aworks.
p2381
sg6
V<p>Despite free access to information via the Internet and an increasingly global\u000aworld, people still seem to have all sorts of divergent ideas about how the world\u000aworks. For example, did you know that eating hot bread and pastries is\u000aincredibly unhealthy? Indeed, it can often even lead to complete bowel\u000aobstruction! I learned this fact as a kid, while growing up in the Soviet Union.\u000aUnderstandably, I have been very careful to avoid eating hot baked goods.\u000aThat is, until recently, when my American girlfriend questioned the validity of my\u000abelief and I began to harbor some doubts. I decided to check if it was actually\u000atrue, and asked Google. The results were very clear: I had fallen prey to an old\u000awives tale. My worldview, shattered.</p>\u000a\u000a<p>Incredulous, I searched for the same thing in Russian and arrived at the\u000aopposite conclusion. "What's up with that?" I thought, and wrote this post.</p>\u000a\u000a
p2382
sg25
g169
sg33
g2379
sg170
(dp2383
g172
S'Sep'
p2384
sg174
S'September 23, 2015'
p2385
sg176
I9
sg177
S'2015-09-23T09:00:00-00:00'
p2386
sg179
I1443024000
sg180
I2015
sg181
I23
ssg65
g182
sg31
S'hot-bread-delicious-deadly'
p2387
sS'posted'
p2388
g188
(S'\x07\xdf\t\x17'
p2389
tp2390
Rp2391
ssg34
S'content/posts/2015/hot-bread-delicious-deadly/index.md'
p2392
sg36
F1443391056.0
sa(dp2393
g2
(dp2394
g4
V It's easy to do, just follow these steps:      Cut two holes in a box   Put your phone in that box   Look inside the box       And that's the way you do it .    Your smartphone is now in a box, so how do you do input? Now that we have a  paper  accepted to  ISWC 2015 , I can tell you!   
p2395
sg28
g7
(g8
g9
V<p>It's easy to do, just follow these steps:</p>\u000a\u000a<ol>\u000a<li>Cut two holes in a box</li>\u000a<li>Put your phone in that box</li>\u000a<li>Look inside the box</li>\u000a</ol>\u000a\u000a<p><a href="https://www.youtube.com/watch?v=ABrSYqiqvzc&amp;t=1m42s">And that's the way you do it</a>.</p>\u000a\u000a<p>Your smartphone is now in a box, so how do you do input? Now that we have a\u000a<a href="mimvr-iswc-2015.pdf">paper</a> accepted to <a href="http://iswc.net/">ISWC 2015</a>, I can tell you!</p>\u000a\u000a<!--more-->\u000a\u000a<h2>It's not easy being in a box</h2>\u000a\u000a<p>Let me remind you: your smartphone is still in a box. This means that your\u000afingers can't reach the touch screen or the volume buttons. Let's consider a\u000acouple of input alternatives:</p>\u000a\u000a<ul>\u000a<li>Cameras and microphones require extra app permissions, are inefficient to keep\u000aalways on, and may face many false positives.</li>\u000a<li>External electronic devices cost money. Plugging them in and out is a\u000ausability nightmare.</li>\u000a</ul>\u000a\u000a<p>Ok, let's nix those. How about permanent magnets? They are inexpensive, robust,\u000arequire no power to operate, and do not degrade over time. The vast majority of\u000asmartphones have a magnetometer, which is used for the compass. Intriguing...</p>\u000a\u000a<h2>Fun with permanent magnets</h2>\u000a\u000a<p>In 2009, Chris Harrison and Scott Hudson published <a href="http://www.chrisharrison.net/index.php/Research/Abracadabra">Abracadabra</a>, a\u000amagnetic ring form factor for finger interactions with small devices:</p>\u000a\u000a<p><img src="abra.jpg" alt="Abracadabra" /></p>\u000a\u000a<p>In 2010, Hamed Ketabdar and others published <a href="https://www.facebook.com/MagiTact">Magitact</a>, instead using\u000aa magnetic rod for more varied interactions near smartphones:</p>\u000a\u000a<p><img src="magitact.jpg" alt="Magitact" /></p>\u000a\u000a<p>In 2011, Daniel Ashbrook and others published <a href="http://dl.acm.org/citation.cfm?id=1979238">Nenya</a>, which is similar\u000ato Abracadabra, but focused more on the eyes-free input aspects:</p>\u000a\u000a<p><img src="nenya.jpg" alt="Nenya" /></p>\u000a\u000a<p>In 2013, Sungjae Hwang and others published <a href="https://www.youtube.com/watch?v=_sSgp0hD-jk">Maggetz</a>, which used\u000apassive magnets to build all sorts of widgets around the device:</p>\u000a\u000a<p><img src="maggetz.jpg" alt="Maggetz" /></p>\u000a\u000a<h2>Fucking magnets: how do they work?</h2>\u000a\u000a<p>Magnets affect the magnetometer in an <a href="https://www.quora.com/Why-does-the-magnetic-field-obey-an-inverse-cube-law">inverse-cubic relationship</a>, so\u000adistance between magnet and magnetometer really makes a dramatic difference in\u000asignal strength. We empirically determined that in most phones, the sensor is\u000aplaced at the top of the device, near the earpiece:</p>\u000a\u000a<table>\u000a<tr><th>Smartphone Model</th><th>Sensor Location</th></tr>\u000a<tr><td>Moto X</td><td>Top</td></tr>\u000a<tr><td>Nexus 4</td><td>Top</td></tr>\u000a<tr><td>Nexus 5</td><td>Top</td></tr>\u000a<tr><td>Samsung S4</td><td>Top</td></tr>\u000a<tr><td>Galaxy Nexus</td><td>Top</td></tr>\u000a<tr><td>Samsung S3</td><td>Bottom</td></tr>\u000a<tr><td>Moto G</td><td>Bottom</td></tr>\u000a</table>\u000a\u000a<p>Some magnetometers are really screwy, like the one found in the first revision\u000aof the HTC M7, or broken, like in some models of the Galaxy Nexus we tested\u000awith. There's little that we can do in these cases, but luckily they are quite\u000arare.</p>\u000a\u000a<p><img src="calibration.png" class="floatright" title="Plot of calibration events"/></p>\u000a\u000a<p>The way you access the magnetometer on Android is via the sensor stack,\u000arequesting the <code>TYPE_MAGNETIC_FIELD</code> sensor. This is a calibrated sensor, since\u000ait's primarily used to determine the direction of magnetic north for the\u000acompass. Calibration means that somewhere deep inside Android, software and\u000ahardware periodically calibrates the output of the sensor. When calibration\u000aoccurs, magnetometer readings effectively reset to some new coordinate system.</p>\u000a\u000a<p>Calibration can happen at any point, and the calibration pattern can look quite\u000adifferent depending on the device. In some cases, it's a gradual calibration,\u000anot a sudden spike as above. This limitation restricts what we can reliably\u000adetect, which is why we chose a pull-and-release interaction. Android already\u000ahas provisions for an uncalibrated magnetometer via\u000a<code>TYPE_MAGNETIC_FIELD_UNCALIBRATED</code>, but this sensor is not nearly as ubiquitous\u000aas its calibrated cousin. Even so, we should be robust to phone insertions and\u000aremovals from Cardboard, which can also look like calibration events.</p>\u000a\u000a<h2>Magnetic input for VR</h2>\u000a\u000a<iframe width="853" height="480" src="https://www.youtube.com/embed/a53a-9FLdL8" frameborder="0" allowfullscreen></iframe>\u000a\u000a<p><img src="mechanism.png" class="floatright" title="Interaction mechanism"/></p>\u000a\u000a<p>As you can see, the interaction involves pulling the magnet downward, and\u000areleasing it. The magnetic ring automatically returns to its rest position\u000abecause of the force exerted on it by an internal magnet. The external magnet is\u000aalso held in-place by the same force, and while it's possible to pull the magnet\u000aoff the cardboard side, it takes concerted effort to do so. The motion of the\u000amagnet is constrained by a cardboard indentation, so it can only move downward.\u000aThe thing I find most elegant about this design is that both the digital signal\u000ato the smartphone and the physical mechanism itself relies on the same\u000aprinciple: magnetism.</p>\u000a\u000a<p>We collected a bunch of data for this pull-and-release interaction from many\u000adevices. We found that most devices behave predictably well. Here's a combined\u000aplot of normalized, superimposed positives and negatives from all phones which\u000awe collected data from, with each dimension of the magnetometer vector plotted\u000aseparately.</p>\u000a\u000a<p><img src="all_features.png" alt="Image of the true positives and negatives from all phones." /></p>\u000a\u000a<p>The detector we built was not based on a template learned from all of the data\u000aabove, but a simpler state machine based on thresholding. The thresholds\u000athemselves were learned empirically. Here's the simple state machine:</p>\u000a\u000a<p><img src="state_machine.png" alt="State machine of the detector" /></p>\u000a\u000a<p>The basic idea is that we take a sliding window approach, normalizing all of the\u000adata relative to the last value in the window. For each window, we calculate <code>min_1</code>,\u000awhich is the smallest value of the first half of the window, and <code>max_2</code>, the\u000alargest value in the second half. Next, we compare to empirically determined\u000athresholds perform the appropriate transition in the state machine. I won't bore\u000ayou with details of normalizing the data, etc but you can find all of the\u000adetails in <a href="mimvr-iswc-2015.pdf">the paper</a>. Oh, and all of the code is also available <a href="https://github.com/dodger487/MIST">on\u000agithub</a>.</p>\u000a\u000a<h2>What's next?</h2>\u000a\u000a<p><img src="joystick.jpg" class="floatright" title="Hypothetical magnetic joystick"/></p>\u000a\u000a<p>A lot more can be done using passive magnetic input. With uncalibrated\u000amagnetometers, there is no fear of calibration events, so we could implement a\u000afaster detector based on just the down motion of the magnet. We could reliably\u000adetect long presses and double clicks. Alternatively, extensions to the existing\u000ainput can be implemented by simply changing the geometry of the physical\u000aconstraints, such as a joystick form factor.</p>\u000a\u000a<p>I'm incredibly happy that Cardboard has been doing so well. Thanks to the great\u000ateam working so hard on it, there are now <a href="http://techcrunch.com/2015/05/28/google-has-shipped-over-1-million-cardboard-vr-units/">over 1 million units shipped</a>.\u000aThe press has been happy with it too, with kind reviews from many tech\u000apublications.</p>\u000a\u000a<p><a href="http://techcrunch.com/2014/06/25/hands-on-with-googles-incredibly-clever-cardboard-virtual-reality-headset/">Techcrunch</a> said:</p>\u000a\u000a<blockquote>\u000a  <p>This funny little cardboard faux-Rift has something even the original Rift\u000a  itself does not: a built-in button. Your phone is able to sense the magnet\u2019s\u000a  movement, allowing it to act as a ridiculously clever little button. Yeesh.</p>\u000a</blockquote>\u000a\u000a<p><a href="http://www.techradar.com/news/phone-and-communications/mobile-phones/google-cardboard-everything-you-need-to-know-1277738">Techradar</a> said:</p>\u000a\u000a<blockquote>\u000a  <p>What's also somewhat amazing is the magnet on the side. [\u2026] The little magnet\u000a  on the side is actually a quite ingenious design aspect of Google Cardboard.\u000a  It's a button!</p>\u000a</blockquote>\u000a\u000a<p><a href="http://www.engadget.com/2014/12/10/google-cardboard/">Engadget</a> said:</p>\u000a\u000a<blockquote>\u000a  <p>One of the things I liked most was a switch located on the left temple, which\u000a  consists of just a couple of magnets and a metal ring.</p>\u000a</blockquote>\u000a\u000a<p><a href="http://www.google.com/get/cardboard/downloads/wwgc_manufacturers_kit_v2.0.zip">Future versions of Cardboard</a> are switching to a different input method\u000ausing a conductive button which brings your body's capacitance to the screen,\u000asimilar to how a touch stylus works. It's cheaper without them, and the new\u000ainput works well, but I'll definitely miss the magnets!</p>\u000a
p2396
tp2397
Rp2398
sg13
V/magnetic-input-mobile-vr
p2399
sg15
Nsg16
I01
sg17
VMagnetic Input for Mobile VR
p2400
sg20
VIt's easy to do, just follow these steps:\u000a\u000a1.
p2401
sg6
V<p>It's easy to do, just follow these steps:</p>\u000a\u000a<ol>\u000a<li>Cut two holes in a box</li>\u000a<li>Put your phone in that box</li>\u000a<li>Look inside the box</li>\u000a</ol>\u000a\u000a<p><a href="https://www.youtube.com/watch?v=ABrSYqiqvzc&amp;t=1m42s">And that's the way you do it</a>.</p>\u000a\u000a<p>Your smartphone is now in a box, so how do you do input? Now that we have a\u000a<a href="mimvr-iswc-2015.pdf">paper</a> accepted to <a href="http://iswc.net/">ISWC 2015</a>, I can tell you!</p>\u000a\u000a
p2402
sg25
g169
sg33
g2399
sg170
(dp2403
g172
S'Sep'
p2404
sg174
S'September 7, 2015'
p2405
sg176
I9
sg177
S'2015-09-07T09:00:00-00:00'
p2406
sg179
I1441641600
sg180
I2015
sg181
I7
ssg65
g182
sg31
S'magnetic-input-mobile-vr'
p2407
sS'posted'
p2408
g188
(S'\x07\xdf\t\x07'
p2409
tp2410
Rp2411
ssg34
S'content/posts/2015/magnetic-input-mobile-vr/index.md'
p2412
sg36
F1441632755.0
sa(dp2413
g2
(dp2414
g4
V VR on the web threatens to cleave the web platform in twain, like mobile did before it. The solution then and the solution now is  Responsive Web Design , which websites to scale well for all form factors. Similarly, for VR to succeed on the web, we need to figure out how to make VR experiences that work both in any VR headset, and also without a VR headset at all.          WebVR boilerplate  is a new starting point for building responsive web VR experiences that work on popular VR headsets and degrace gracefully on other platforms. Check out a couple of demos,  a simple one  and one  ported from MozVR .   
p2415
sg28
g7
(g8
g9
V<p>VR on the web threatens to cleave the web platform in twain, like mobile\u000adid before it. The solution then and the solution now is <a href="http://en.wikipedia.org/wiki/Responsive_web_design">Responsive Web\u000aDesign</a>, which websites to scale well for all form factors.\u000aSimilarly, for VR to succeed on the web, we need to figure out how to\u000amake VR experiences that work both in any VR headset, and also without a\u000aVR headset at all.</p>\u000a\u000a<p><img src="hmds.png" alt="Various head mounted displays." /></p>\u000a\u000a<p><a href="https://github.com/borismus/webvr-boilerplate">WebVR boilerplate</a> is a new starting point for building\u000aresponsive web VR experiences that work on popular VR headsets and\u000adegrace gracefully on other platforms. Check out a couple of demos, <a href="http://borismus.github.io/webvr-boilerplate/">a\u000asimple one</a> and one <a href="http://borismus.github.io/sechelt">ported from MozVR</a>.</p>\u000a\u000a<!--more-->\u000a\u000a<h2>Preview the VR experience for everyone</h2>\u000a\u000a<p>Say you visit a webpage, and it opens up in split-screen mode barrel\u000adistortion, chromatic aberration correction, personalized interpupillary\u000adistance, and provides 6 DOF tracking. <a href="https://www.youtube.com/watch?v=PkFyGNjaQ8k">Whoa</a>. You reach for your\u000aVR headset only to find that you forgot it at work! How disappointing!\u000aThe vast majority of normal people with no head mounted display lying\u000aaround will surely be even more disappointed.</p>\u000a\u000a<p>Responsive web design promises content which automatically adapts to\u000ayour viewing environment by using fluid layouts, flexible images,\u000aproportional grids; a cocktail of modern web technologies. Similarly,\u000aWebVR experiences need to work even without VR hardware. This has two\u000aobvious advantages:</p>\u000a\u000a<ol>\u000a<li>The vast majority of people that don't have VR hardware can still get\u000aa feeling for the experience.</li>\u000a<li>Even if you have VR gear, donning it is a pain. This preview lets you\u000aquickly evaluate whether or not wearing is worth the hassle.</li>\u000a</ol>\u000a\u000a<p>What are some reasonable fallbacks to the in-helmet VR experience? The\u000amain question boils down to emulating head tracking without wearing\u000aanything on your head. On mobile phones, the obvious answer is to use\u000athe gyroscope, for a <a href="http://googlespotlightstories.com/">Spotlight Stories</a>-type experience. On\u000adesktop, we use the mouse to free-look, and also support turning using\u000athe arrow keys. This covers enough of the 3DOF orientation that all HMDs\u000aprovide. Clearly missing are the three translational degrees of freedom,\u000abut these are provided only by some VR headsets, and we can imagine some\u000a<a href="http://topheman.github.io/parallax/">interesting fallbacks</a> for those too.</p>\u000a\u000a<h2>Write once, run in any VR headset</h2>\u000a\u000a<p>Remember the old "write once, run anywhere" promise? The web is the\u000aclosest thing we have to fulfilling it, but what it actually delivers is\u000aoften far from this ideal. The latest VR wave has barely begun and\u000aalready the web VR world is fragmented. Case in point,\u000a<a href="http://vr.chromeexperiments.com/">vr.chromeexperiments.com</a> don't work on Oculus, and\u000a<a href="http://mozvr.com/">mozvr.com</a> demos don't work in Cardboard.  The promise of WebVR\u000ais that once it lands, all will be well in the world. However, this\u000ameans that we need to wait for WebVR to become fully baked. In other\u000awords, we are blocked on the valiant efforts of our dear <a href="https://twitter.com/vvuk">WebVR</a>\u000a<a href="https://twitter.com/Tojiro">implementers</a> (as well as the heavyweight browser development\u000aprocess consisting of spec authors, security reviews, binary size, etc).</p>\u000a\u000a<p>To speed up the process, we need a polyfill for WebVR which uses web\u000aAPIs to provide functionality to the WebVR specification (currently, in\u000a<a href="https://github.com/vvuk/gecko-dev/blob/oculus/dom/webidl/VRDevice.webidl">IDL form</a>). In the absence of a WebVR implementation, the polyfill\u000akicks in and supports mobile VR headsets like Cardboard and Durovis\u000aDive, which are passive contraptions that just piggyback on the\u000asmartness found in your smartphone.</p>\u000a\u000a<h2>Introducing: WebVR Boilerplate</h2>\u000a\u000a<p>The <a href="https://github.com/borismus/webvr-boilerplate">WebVR boilerplate project</a> is on github, and consists of\u000atwo parts. Firstly, the <a href="https://github.com/borismus/webvr-polyfill">WebVR polyfill</a> provides WebVR\u000asupport for Cardboard-compatible devices, and orientation tracking\u000afallbacks where no headset is available. The WebVR polyfill can also be\u000ainstalled from npm (available via <code>npm install webvr-polyfill</code>).</p>\u000a\u000a<ol>\u000a<li>A <code>CardboardHMDDevice</code>, provides a reasonable default for\u000ainterpupillary distance and field of view for cardboard-like devices.</li>\u000a<li>On mobile devices, a <code>GyroPositionSensorVRDevice</code>, which provides\u000aorientation through the <code>DeviceOrientationEvent</code>.</li>\u000a<li>On PCs, a <code>MouseKeyboardPositionSensorVRDevice</code>, which provides\u000aorientation through keyboard and mouse events.</li>\u000a</ol>\u000a\u000a<p>It is designed to be used with <a href="http://threejs.org/">THREE.js</a> plugins that are built\u000afor the WebVR API: <a href="https://github.com/mrdoob/three.js/blob/master/examples/js/controls/VRControls.js">VRControls.js</a> and\u000a<a href="https://github.com/mrdoob/three.js/blob/master/examples/js/effects/VREffect.js">VREffect.js</a>, but of course any valid usage of the WebVR API\u000ashould work (modulo bugs).</p>\u000a\u000a<p>Secondly, the <a href="https://github.com/borismus/webvr-boilerplate/blob/master/js/webvr-manager.js">WebVR manager</a> surfaces VR compatibility\u000ausing consistent iconography and simplifies transitioning in and out of\u000afull VR mode. It also contains some of the best practices for making VR\u000awork on the web, for example, using orientation lock to keep the phone\u000ain landscape orientation, and a means of keeping the phone screen on. If\u000ayou're ready to dive in, the <a href="https://github.com/borismus/webvr-boilerplate">github has all of the technical\u000ainformation</a> available.</p>\u000a\u000a<p>WebVR boilerplate is meant to make it easy to develop immersive\u000aexperiences that run on all VR hardware, including Oculus and Cardboard,\u000aand also provide reasonable fallbacks when no specialized viewer is\u000aavailable.</p>\u000a\u000a<h2>WebVR boilerplate in action</h2>\u000a\u000a<p><img src="sechelt.png" alt="Screenshot of the mozvr.com Sechelt demo." /></p>\u000a\u000a<p>I really liked Mozilla's <a href="http://mozvr.com/projects/sechelt/">Sechelt demo</a>, inspired by the\u000aeponymous town on British Columbia's beautiful Sunshine Coast. I've\u000a<a href="http://borismus.github.io/sechelt">ported it</a> to WebVR Boilerplate. The result is the same\u000ademo, which works in Cardboard, as well as continuing to work on desktop\u000aand mobile devices, and on the Oculus Rift via <a href="https://nightly.mozilla.org/">Firefox Nightly</a> and\u000a<a href="https://drive.google.com/folderview?id=0BzudLt22BqGRbW9WTHMtOWMzNjQ&amp;usp=sharing#list">Brandon's WebVR Chrome builds</a>. It also yielded less confusing\u000aboilerplate code and a <a href="https://github.com/borismus/sechelt/commit/671cff9fc283b58d2ebce0ae6f0dfa3580050aab">simplified code base</a> since\u000athere is no longer need for an unweildy conditional to determine whether\u000ato use <code>DeviceOrientationControls</code>, <code>OrbitControls</code>, or <code>VRControls</code>,\u000aand decide between <code>VREffect</code> and <code>StereoEffect</code>.</p>\u000a\u000a<p>As always, <a href="https://twitter.com/borismus">let me know what you think</a> and feel free to point\u000aout (or fix!) my mistakes on <a href="https://github.com/borismus/webvr-boilerplate">the github</a>.</p>\u000a
p2416
tp2417
Rp2418
sg13
V/responsive-vr
p2419
sg15
Nsg16
I01
sg17
VResponsive WebVR, headset optional
p2420
sg20
VVR on the web threatens to cleave the web platform in twain, like mobile\u000adid before it.
p2421
sg6
V<p>VR on the web threatens to cleave the web platform in twain, like mobile\u000adid before it. The solution then and the solution now is <a href="http://en.wikipedia.org/wiki/Responsive_web_design">Responsive Web\u000aDesign</a>, which websites to scale well for all form factors.\u000aSimilarly, for VR to succeed on the web, we need to figure out how to\u000amake VR experiences that work both in any VR headset, and also without a\u000aVR headset at all.</p>\u000a\u000a<p><img src="hmds.png" alt="Various head mounted displays." /></p>\u000a\u000a<p><a href="https://github.com/borismus/webvr-boilerplate">WebVR boilerplate</a> is a new starting point for building\u000aresponsive web VR experiences that work on popular VR headsets and\u000adegrace gracefully on other platforms. Check out a couple of demos, <a href="http://borismus.github.io/webvr-boilerplate/">a\u000asimple one</a> and one <a href="http://borismus.github.io/sechelt">ported from MozVR</a>.</p>\u000a\u000a
p2422
sg25
g169
sg33
g2419
sg170
(dp2423
g172
S'Feb'
p2424
sg174
S'February 2, 2015'
p2425
sg176
I2
sg177
S'2015-02-02T09:00:00-00:00'
p2426
sg179
I1422896400
sg180
I2015
sg181
I2
ssg65
g182
sg31
S'responsive-vr'
p2427
sS'posted'
p2428
g188
(S'\x07\xdf\x02\x02'
p2429
tp2430
Rp2431
ssg34
S'content/posts/2015/responsive-vr/index.md'
p2432
sg36
F1433825386.0
sa(dp2433
g2
(dp2434
g4
V A major technical challenge for VR is to make head tracking as good as possible. The metric that matters is called  motion-to-photon latency . For mobile VR purposes, this is the time that it takes for a user's head rotation to be fully reflected in the rendered content.         The simplest way to get up-and-running with head tracking on the web today is to use the  deviceorientation  events, which are generally well supported across most browsers. However, this approach suffers from several drawbacks which can be remedied by implementing our own sensor fusion. We can do even better by predicting head orientation from the gyroscope.    I'll dig into these techniques and their open web implementations.  Everything discussed in this post is implemented and available open source as part of the  WebVR Polyfill  project. If you want to skip ahead, check out the  latest head tracker  in action, and play around with this  motion sensor visualizer .   
p2435
sg28
g7
(g8
g9
V<p>A major technical challenge for VR is to make head tracking as good as possible.\u000aThe metric that matters is called <strong>motion-to-photon latency</strong>. For mobile VR\u000apurposes, this is the time that it takes for a user's head rotation to be fully\u000areflected in the rendered content.</p>\u000a\u000a<p><img src="latency-chain.jpg" alt="Motion to photon pipeline" /></p>\u000a\u000a<p>The simplest way to get up-and-running with head tracking on the web today is\u000ato use the <code>deviceorientation</code> events, which are generally well supported across\u000amost browsers. However, this approach suffers from several drawbacks which can\u000abe remedied by implementing our own sensor fusion. We can do even better by\u000apredicting head orientation from the gyroscope.</p>\u000a\u000a<p>I'll dig into these techniques and their open web implementations.  Everything\u000adiscussed in this post is implemented and available open source as part of the\u000a<a href="https://github.com/borismus/webvr-polyfill">WebVR Polyfill</a> project. If you want to skip ahead, check out\u000athe <a href="http://borismus.github.io/webvr-boilerplate/">latest head tracker</a> in action, and play around with this <a href="http://borismus.github.io/sensor-fusion/">motion\u000asensor visualizer</a>.</p>\u000a\u000a<!--more-->\u000a\u000a<h2>The trouble with device orientation</h2>\u000a\u000a<p>The web provides an easy solution for head tracking through the\u000a<code>deviceorientation</code> event, which gives Euler angles corresponding to your\u000aphone's 3-DOF orientation in space. This orientation is calculated through an\u000aundisclosed algorithm. Until very recently, <a href="http://w3c.github.io/deviceorientation/spec-source-orientation.html">the spec</a> didn't\u000aeven specify whether or not these events should give your phone's orientation in\u000arelation to north or not. However, recently <a href="https://github.com/w3c/deviceorientation/pull/22">accepted spec\u000achanges</a> make this behavior more standard across\u000abrowsers.</p>\u000a\u000a<p>In Android, the JavaScript <code>deviceorientation</code> event was implemented using\u000a<code>Sensor.TYPE_ORIENTATION</code> in Android, which fuses accelerometer, gyroscope and\u000amagnetometer sensors together to give a North-aligned orientation. The trouble\u000ais that the magnetometer's estimate of magnetic North is easily affected by\u000aexternal metallic objects. On many devices, the North estimate continually\u000achanges, even when you are not looking around. This breaks the correspondence\u000abetween motion and display, a recipe for disaster.</p>\u000a\u000a<p>Another issue in some implementations is that the <code>deviceorientation</code> sensor\u000aramps up and down in firing rate depending on the speed of the phone's rotation.\u000aTry opening up <a href="http://jsbin.com/device-inertial-sensor-diagnostics">this diagnostic page</a> on Android. This variation in\u000asensor update rate is not good for maintaining a reliable head track.</p>\u000a\u000a<p>To top it off, a <a href="http://crbug.com/540629">recent regression in Android M</a> broke\u000a<code>deviceorientation</code> for Nexus 5s. Why do bad bugs happen to good people?</p>\u000a\u000a<h3>What is to be done?</h3>\u000a\u000a<p>We implement our own sensor fusion with <code>devicemotion</code>, which provides lower\u000alevel accelerometer &amp; gyroscope events. These fire at a regular rate. When you\u000asearch for "sensor fusion", jumping into the rabbit hole will quickly take you\u000ainto the realm of Kalman Filters. This is a bit more firepower than we will need\u000afor the moment, although I did finally get a better sense of the concept with\u000athe help of a <a href="https://www.youtube.com/watch?v=18TKA-YWhX0">boring but understandable explanation</a>.</p>\u000a\u000a<p>Luckily, there are simpler alternatives such as the Complementary Filter, which\u000ais what we'll talk about next.</p>\u000a\u000a<h2>Your sensing smartphone</h2>\u000a\u000a<p>Let us start with the basics: sensors. There are three fundamental motion\u000atracking sensors in your smartphone. </p>\u000a\u000a<p>Accelerometers measure any acceleration, returning a vector in the phone's\u000areference frame. Usually this vector points down, towards the center of the\u000aearth, but other accelerations (eg. linear ones as you move your phone) are also\u000acaptured. The output from an accelerometer is quite noisy by virtue of how the\u000asensor works. Here's a plot of the rotation around the X-axis according to an\u000aaccelerometer:</p>\u000a\u000a<p><img src="accel.gif" alt="Animation of X-axis accelerometer output with a phone turning around the X axis" /></p>\u000a\u000a<p>Gyroscopes measure rotations, returning an angular rotation vector also in the\u000aphone's reference frame. Output from the gyro is quite smooth, and very\u000aresponsive to small rotations. The gyro can be used to estimate pose by keeping\u000atrack of the current pose and adjusting it every timestep, with every new gyro\u000areading. This integration works well, but suffers from drift. If you were to\u000aplace your phone flat and capture it's gyro-based position, then pick it up,\u000arotate it a bunch, and place it flat again, its integrated gyro position might\u000abe quite different from what it was before due to the accumulation of errors\u000afrom the sensor. Rotation around the X-axis according to a gyroscope:</p>\u000a\u000a<p><img src="gyro.gif" alt="Animation of X-axis gyroscope output with a phone turning around the X axis" /></p>\u000a\u000a<p>Magnetometers measure magnetic fields, returning a vector corresponding to the\u000acumulative magnetic field due to any nearby magnets (including the Earth). This\u000asensor acts like a compass, giving an orientation estimate of the phone. This is\u000aincredibly useful combined with the accelerometer, which provides no information\u000aabout the phone's yaw. Magnetometers are affected not by the Earth, but by\u000aanything with a magnetic field, including <a href="http://smus.com/magnetic-input-mobile-vr/">strategically placed permanent\u000amagnets</a> and also ferromagnetic metals which are often found in substantial\u000aquantities in certain environments.</p>\u000a\u000a<h2>Intuition: why do we need sensor fusion?</h2>\u000a\u000a<p>Each sensor has its own strengths and weaknesses. Gyroscopes have no idea where\u000athey are in relation to the world, while accelerometers are very noisy and can\u000anever provide a yaw estimate. The idea of sensor fusion is to take readings from\u000aeach sensor and provide a more useful result which combines the strengths of\u000aeach. The resulting fused stream is greater than the sum of its parts. </p>\u000a\u000a<p>There are many ways of fusing sensors into one stream. Which sensors you fuse,\u000aand which algorithmic approach you choose should depend on the usecase.\u000aThe accelerometer-gyroscope-magnetometer sensor fusion provided by the\u000asystem tries really hard to generate something useful. But as it turns out, it\u000ais not great for VR head tracking. The selected sensors are the wrong ones, and\u000athe output is not sensitive enough to small head movements.</p>\u000a\u000a<p>In VR, drifting away from true north is often fine since you aren't looking at\u000athe real world anyway. So there's no need to fuse with magnetometer. Reducing\u000aabsolute drift is, of course, still desirable in some cases. If you are sitting\u000ain an armchair, maintaining alignment with the front of your chair is critical,\u000aotherwise you will find yourself having to crank your neck too much just to\u000acontinue looking forward in the virtual world. For the time being, we ignore\u000athis problem.</p>\u000a\u000a<h2>Building a complementary filter</h2>\u000a\u000a<p>The complementary filter takes advantage of the long term accuracy of the\u000aaccelerometer, while mitigating the noise in the sensor by relying on the\u000agyroscope in the short term. The filter is called complementary because\u000amathematically, it can be expressed as a weighted sum of the two sensor streams:</p>\u000a\u000a<p><img src="filter-equation.png" class="center" \u000a    title="Filter equation" /></p>\u000a\u000a<p>This approach relies on the gyroscope for angular updates to head orientation,\u000abut corrects for gyro drift by taking into account where measured gravity is\u000aaccording to the accelerometer.</p>\u000a\u000a<p>Initially inspired by <a href="http://www.pieter-jan.com/node/11">Pieter's explanation</a>, I built this filter by\u000acalculating roll and pitch from the accelerometer and gyroscope, but quickly ran\u000ainto issues with <a href="https://en.wikipedia.org/wiki/Gimbal_lock">gimbal lock</a>. A better approach is to use quaternions\u000ato represent orientation, which do not suffer from this problem, and are ideal\u000afor thinking about rotations in 3D. Quaternions are complex (ha!) so I won't go\u000ainto much detail here beyond linking to a <a href="http://www.3dgep.com/understanding-quaternions/">decent primer</a> on the\u000atopic. Happily, quaternions are a useful tool even without fully understanding\u000athe theory, and many implementations exist. For this filter, I used <a href="http://threejs.org/docs/#Reference/Math/Quaternion">the\u000aone</a> found in THREE.js.</p>\u000a\u000a<p>The first task is to express the accelerometer vector as a quaternion rotation,\u000awhich we use to initialize the orientation estimate (see\u000a<a href="https://github.com/borismus/webvr-polyfill/blob/master/src/complementary-filter.js"><code>ComplementaryFilter.accelToQuaternion_</code></a>).</p>\u000a\u000a<pre><code>quat.setFromUnitVectors(new THREE.Vector3(0, 0, -1), normAccel);\u000a</code></pre>\u000a\u000a<p>Every time we get new sensor data, calculate the instantaneous change in\u000aorientation from the gyroscope. Again, we convert to a quaternion, as follows\u000a(see: <a href="https://github.com/borismus/webvr-polyfill/blob/master/src/complementary-filter.js"><code>ComplementaryFilter.gyroToQuaternionDelta_</code></a>):</p>\u000a\u000a<pre><code>quat.setFromAxisAngle(gyroNorm, gyro.length() * dt);\u000a</code></pre>\u000a\u000a<p>Now we update the orientation estimate with the quaternion delta. This is a\u000aquaternion multiplication:</p>\u000a\u000a<pre><code>this.filterQ.copy(this.previousFilterQ);\u000athis.filterQ.multiply(gyroDeltaQ);\u000a</code></pre>\u000a\u000a<p>Next, calculate the estimated gravity from the current orientation and compare\u000ait to the gravity from the accelerometer, getting the quaternion delta.</p>\u000a\u000a<p><img src="complementary-filter.png" class="center" \u000a    title="Complementary filter visual illustration" /></p>\u000a\u000a<pre><code>deltaQ.setFromUnitVectors(this.estimatedGravity, this.measuredGravity);\u000a</code></pre>\u000a\u000a<p>Now we can calculate the target orientation based on the measured gravity, and\u000athen perform a <a href="https://en.wikipedia.org/wiki/Slerp">spherical linear interpolation (SLERP)</a>. How much to\u000aslerp depends on that constant I mentioned before. If we don't slerp at all, we\u000awill end up only using the gyroscope. If we slerp all the way to the target, we\u000awill end up ignoring the gyroscope completely and only using the accelerometer.\u000aIn THREE parlance:</p>\u000a\u000a<pre><code>this.filterQ.slerp(targetQ, 1 - this.kFilter);\u000a</code></pre>\u000a\u000a<p>Sanity checking the result, we expect the filter output to be roughly parallel\u000ato the gyroscope readings, but to align with the accelerometer reading over the\u000along term. Below, you can see the accelerometer and gyroscope (green and blue)\u000aand compare them to the fused output (orange):</p>\u000a\u000a<p><img src="fusion.gif" alt="Complementary filter output" /></p>\u000a\u000a<h2>Predicting the future</h2>\u000a\u000a<p>As your program draws each frame of rendered content, there is delay between\u000athe time you move your head and the time the content actually appears on the\u000ascreen. It takes time for the sensors to fire, for firmware and software to\u000aprocess sensor data, and for a scene to be generated based on that sensor data.</p>\u000a\u000a<p>In Android, this latency is often on the order of 50-100 ms with sensors firing\u000aon all cylinders (the technical term for 200 Hz) and some nice graphics\u000aoptimizations. The web suffers a strictly worse fate since sensors often fire\u000aslower (60 Hz in Safari and Firefox), and there are more hoops of abstraction to\u000ajump through. Reducing motion-to-photon latency can be done by actually reducing\u000aeach step in the process, with faster sensor processing, graphics optimizations,\u000aand better algorithms. It can also be reduced by cheating!</p>\u000a\u000a<p>We can rely on a <a href="https://en.wikipedia.org/wiki/Dead_reckoning#Directional_dead_reckoning">dead reckoning</a> inspired approach, but rather\u000athan predicting position based on velocity, we predict in the angular domain.\u000aOnce we predict the orientation of the head in the (near) future, use that\u000aorientation to render the scene. We predict based on angular velocity, assuming\u000athat your head will keep rotating at the same rate. More complex schemes are\u000apossible to imagine too, using acceleration (2nd order) or Nth order prediction,\u000abut these are more complex, and so more expensive to calculate, and don't\u000anecessarily yield better results.</p>\u000a\u000a<pre><code>var deltaT = timestampS - this.previousTimestampS;\u000avar predictAngle = angularSpeed * this.predictionTimeS;\u000a</code></pre>\u000a\u000a<p>The way this works is pretty straight forward, using angular speed from the\u000agyroscope, we can predict a little bit into the future to yield results like\u000athis:</p>\u000a\u000a<p><img src="prediction.gif" alt="Predicted vs. sensor fusion." /></p>\u000a\u000a<p>Notice that the predicted signal (in red) is somewhat ahead of the fused one (in\u000aorange). This is what we'd expect based on the motion prediction approach taken.\u000aThe downside of this is that there is noticeable noise, since sometimes we\u000aover-predict, and are forced to return back to the original heading.</p>\u000a\u000a<h2>Plotting graphs</h2>\u000a\u000a<p>Although still in very active development, <a href="https://gitgud.io/unconed/mathbox/">Mathbox2</a> is already a\u000aformidable visualization toolkit. It is especially well suited to output in 3D,\u000awhich I used actively to debug and visualize the filter.</p>\u000a\u000a<p>I also used Mathbox2 to generate plots featured earlier in this blog post. I\u000awrote a live-plotting tool that can compare gyroscope, accelerometer, fused and\u000apredicted streams on each axis, and also let you tweak the filter coefficient\u000aand how far into the future to predict.</p>\u000a\u000a<p><img src="plot-options.png" class="center"\u000a    title="Preview of the options available in the plot"/></p>\u000a\u000a<p>You too can <a href="http://borismus.github.io/sensor-fusion/">try the plots live on your phone</a>. After all, it's just a\u000amobile webpage! Many thanks to <a href="https://twitter.com/pierregeorgel">Pierre\u000aFite-Georgel</a> and <a href="https://github.com/jkammerl">Julius\u000aKammerl</a> for lending their incredible\u000afilter-building skills to this project.</p>\u000a
p2436
tp2437
Rp2438
sg13
V/sensor-fusion-prediction-webvr
p2439
sg15
Nsg16
I01
sg17
VSensor fusion and motion prediction
p2440
sg20
VA major technical challenge for VR is to make head tracking as good as possible.
p2441
sg6
V<p>A major technical challenge for VR is to make head tracking as good as possible.\u000aThe metric that matters is called <strong>motion-to-photon latency</strong>. For mobile VR\u000apurposes, this is the time that it takes for a user's head rotation to be fully\u000areflected in the rendered content.</p>\u000a\u000a<p><img src="latency-chain.jpg" alt="Motion to photon pipeline" /></p>\u000a\u000a<p>The simplest way to get up-and-running with head tracking on the web today is\u000ato use the <code>deviceorientation</code> events, which are generally well supported across\u000amost browsers. However, this approach suffers from several drawbacks which can\u000abe remedied by implementing our own sensor fusion. We can do even better by\u000apredicting head orientation from the gyroscope.</p>\u000a\u000a<p>I'll dig into these techniques and their open web implementations.  Everything\u000adiscussed in this post is implemented and available open source as part of the\u000a<a href="https://github.com/borismus/webvr-polyfill">WebVR Polyfill</a> project. If you want to skip ahead, check out\u000athe <a href="http://borismus.github.io/webvr-boilerplate/">latest head tracker</a> in action, and play around with this <a href="http://borismus.github.io/sensor-fusion/">motion\u000asensor visualizer</a>.</p>\u000a\u000a
p2442
sg25
g169
sg33
g2439
sg170
(dp2443
g172
S'Nov'
p2444
sg174
S'November 5, 2015'
p2445
sg176
I11
sg177
S'2015-11-05T09:00:00-00:00'
p2446
sg179
I1446742800
sg180
I2015
sg181
I5
ssg65
g182
sg31
S'sensor-fusion-prediction-webvr'
p2447
sS'posted'
p2448
g188
(S'\x07\xdf\x0b\x05'
p2449
tp2450
Rp2451
ssg34
S'content/posts/2015/sensor-fusion-prediction-webvr/index.md'
p2452
sg36
F1461107001.0
sa(dp2453
g2
(dp2454
g4
V Last summer I visited Austria, the capital of classical music. I had the pleasure of hearing the  Vespers of 1610  in the great  Salzburger Dom (photosphere) . The most memorable part of the piece was that the soloists movedbetween movements, so their voices and instruments emanated from surprising parts of the great hall. Inspired, I returned to the west coast and eventually came around to building a spatial audio prototypes like this one:           Spatial audio is an important part of any good VR experience, since the more senses we simulate, the more compelling it feels to our sense fusing mind. WebVR, WebGL, and WebAudio all act as complementary specs to enable this necessary experience. As you would expect, because it uses the  WebVR boilerplate , this demo can be viewed on mobile, desktop, in Cardboard or an Oculus Rift. In all cases, you will need headphones :)   
p2455
sg28
g7
(g8
g9
V<p>Last summer I visited Austria, the capital of classical music. I had the\u000apleasure of hearing the <a href="http://en.wikipedia.org/wiki/Vespro_della_Beata_Vergine">Vespers of 1610</a> in the great\u000a<a href="http://goo.gl/e3WBB2">Salzburger Dom (photosphere)</a>. The most memorable part of\u000athe piece was that the soloists movedbetween movements, so their voices\u000aand instruments emanated from surprising parts of the great hall.\u000aInspired, I returned to the west coast and eventually came around to\u000abuilding a spatial audio prototypes like this one:</p>\u000a\u000a<p><a href="http://borismus.github.io/moving-music"><img src="collage_small.jpg" alt="Screenshot of a demo" /></a></p>\u000a\u000a<p>Spatial audio is an important part of any good VR experience, since the\u000amore senses we simulate, the more compelling it feels to our sense\u000afusing mind. WebVR, WebGL, and WebAudio all act as complementary specs\u000ato enable this necessary experience. As you would expect, because it\u000auses the <a href="https://github.com/borismus/webvr-boilerplate">WebVR boilerplate</a>, this demo can be viewed on\u000amobile, desktop, in Cardboard or an Oculus Rift. In all cases, you will\u000aneed headphones :)</p>\u000a\u000a<!--more-->\u000a\u000a<h2>Early spatial music</h2>\u000a\u000a<p>One of the things that made my acoustic experience in the Salzburg Dom\u000aso memorable was the beauty of the space in which it was performed. The\u000apotential for awesome sound was staggering, with one massive organ at\u000athe back, and four smaller organs surrounding the nave. During the\u000aperformance of the vespers, the thing that struck me the most was that\u000aas the piece transitioned from movement to movement, choreographed\u000asoloists also moved around the cathedral, resulting in haunting acoustic\u000aeffects. Sometimes, a voice would appear quietly from the far end of the\u000acloister, sounding distant and muffled. Other times, it would come from\u000athe balcony behind the audience, full of unexpected reverb. It was a\u000atruly unique acoustic experience that I will never forget, and it made\u000ame wonder about the role of space in music.</p>\u000a\u000a<p>As it turns out, there is a rich history on the <a href="http://en.wikipedia.org/wiki/Spatial_music">topic of spatialization\u000ain music</a> going back to the 16th century. For the\u000apurposes of this blog, I am more interested in the present day. In\u000aparticular, given the <a href="http://www.w3.org/TR/webaudio/">excellent state of audio APIs</a> on the\u000aweb, what follows is a foray into spatial audio with WebVR.</p>\u000a\u000a<h2>Experiments in spatial audio</h2>\u000a\u000a<p>How does music sound if in addition to pitch, rhythm and timbre, we\u000acould tweak position and velocity as additional expressive dimension?\u000aMy demo places you into a virtual listening space, that you look\u000aaround into (using whatever means you have available: mouse and\u000akeyboard, mobile phone gyroscope, or cardboard-like device) -- thanks to\u000a<a href="https://github.com/borismus/webvr-boilerplate">WebVR boilerplate</a>. Each track is visualized as a blob of\u000aparticles. These animate according to the instantaneous amplitude of the\u000atrack, serving as a per-track visualizer and indicating where the track\u000ais in space.</p>\u000a\u000a<p>There is a surprising amount of multi-track music out there, such as\u000a<a href="http://www.cambridge-mt.com/ms-mtk.htm">Cambridge Music Technology's raw recordings archive</a> for aspiring\u000aaudio engineers and <a href="https://soundcloud.com/search/sets?q=stems">soundcloud stems</a> which are typically\u000arecorded separately in a studio and then released publicly for <a href="http://www.remixcomps.com/">remix\u000acontests</a>. In the end, I went with a few different sets just to\u000aget a feeling for spatializing a variety of tracks:</p>\u000a\u000a<ul>\u000a<li>Multiple people <a href="http://borismus.github.io/moving-music/?set=speech">speaking (demo)</a> simultaneously (the <a href="http://en.wikipedia.org/wiki/Cocktail_party_effect">cocktail party effect</a>).</li>\u000a<li>Multi-track <a href="http://borismus.github.io/moving-music/?set=jazz">live (demo)</a> recording with many mic'ed instruments.</li>\u000a<li>Multi-track <a href="http://borismus.github.io/moving-music/?set=phoenix">studio recording (demo)</a> with cleanly recorded tracks (A <a href="http://en.wikipedia.org/wiki/Phoenix_%28band%29">Phoenix</a> track).</li>\u000a</ul>\u000a\u000a<p>In addition to selecting the sounds to spatialize, the demo supports\u000alaying out the tracks in various formations. To cycle between these\u000amodes, hit space on desktop, or tap the screen on mobile:</p>\u000a\u000a<ul>\u000a<li>Sources are <a href="http://borismus.github.io/moving-music/?mode=0">clumped together (demo)</a> in one place.</li>\u000a<li>Sources are <a href="http://borismus.github.io/moving-music/?mode=1">positioned around the observer (demo)</a>.</li>\u000a<li>Sources are <a href="http://borismus.github.io/moving-music/?mode=2">moving around the observer (demo)</a>.</li>\u000a</ul>\u000a\u000a<p>Given that the code is <a href="https://github.com/borismus/moving-music">open sourced on github</a>, it's pretty\u000aeasy to try your own tracks, implement new trajectories or change the\u000avisualizer. Please fork away!</p>\u000a\u000a<h2>Implementation details</h2>\u000a\u000a<p>In an attempt to eat my own dogfood, this project partly serves as a way\u000ato test the <a href="https://github.com/borismus/webvr-boilerplate">WebVR boilerplate project</a> to make sure that\u000ait is usable, and provides the functionality that it purports to. I've\u000amade a bunch of changes to the boilerplate in parallel, fixing browser\u000acompatibility issues and resolving bugs. Notable improvements since\u000ainception include the ability to mouselook via <a href="http://www.html5rocks.com/en/tutorials/pointerlock/intro/">pointer\u000alock</a> in regular desktop mode and improved support for iOS\u000aand Firefox nightly. Thanks to <a href="http://www.antoniocosta.eu/">Antonio</a>, my awesome designer\u000acolleague, the WebVR boilerplate has a new icon!</p>\u000a\u000a<p>This project relies heavily on audio, but requires the page to be\u000arunning in the foreground for you to enjoy the immersive nature of the\u000aexperience. Browsers, especially on mobile devices, can have some weird\u000abehaviors when it comes to backgrounded tabs. It's a safe bet to just\u000aprevent this from happening altogether, so I've been using the <a href="http://www.smashingmagazine.com/2015/01/20/creating-sites-with-the-page-visibility-api/">page\u000avisibility API</a> to mute the music when the tab goes out of\u000afocus, and then resume it when it's back in focus. This works super well\u000aacross browsers I've tested in and prevents the page-hunt where you're\u000atrying to find which annoying tab/activity/app is playing!</p>\u000a\u000a<p>I toyed a little bit with the doppler effect, but found it to be\u000aterrible for music. Because in the moving case, each track moves with\u000aits own velocity relative to the viewer, frequency shifts are\u000anon-uniform, leading to a cacophany of out-of-tune instruments. For\u000aspoken word, it worked quite well, though. The caveat to all this is that the\u000acurrent <a href="http://crbug.com/439644">doppler API is deprecated</a>, so I\u000adidn't delve too deeply into doppler until we have a new implementation.</p>\u000a\u000a<h2>Pitfalls and workarounds</h2>\u000a\u000a<p><strong>Set your listener's up vector properly.</strong> Something you should beware\u000aof is to always set the up vector correctly in the\u000a<code>listener.setOrientation(...)</code> call. Initially, I was only setting the\u000adirection vector, keeping up fixed at <code>(0, 1, 0)</code>, but this yielded\u000aunpredictable results and took a long time to track down.</p>\u000a\u000a<p><strong>Streaming is broken in mobile implementations.</strong> A couple of issues\u000arelated to loading audio bit me as I was developing, proving to be\u000anearly show stoppers (please star if you feel strongly):</p>\u000a\u000a<ul>\u000a<li>Streaming audio doesn't work on Android (or iOS). This means that\u000aevery track we play needs to be first loaded, and then decoded:\u000a<a href="http://crbug.com/419446">http://crbug.com/419446</a></li>\u000a<li>Decoding mp3 on Android takes a very very long time (same in Firefox):\u000a<a href="http://crbug.com/232973">http://crbug.com/232973</a></li>\u000a<li>Though it doesn't directly affect my spatial sound experiments, the\u000ainability to bring in remote WebRTC audio streams into the audio graph\u000ais blocking other ideas: <a href="https://crbug.com/121673">https://crbug.com/121673</a></li>\u000a</ul>\u000a\u000a<p>I tried to work around the streaming issue by doing my own chunking\u000alocally and then writing a <code>ChunkedAudioPlayer</code>, but this is harder than\u000ait seems, especially when you want to synchronize multiple chunked\u000atracks.</p>\u000a\u000a<p><strong>Beware of implementation differences.</strong> It's also worth noting that\u000adifferent browsers have slightly different behaviors when it comes to\u000aPannerNodes. In particular, Firefox spatialization can appear to sound\u000abetter, but this is simply because it's louder (the same effect can be\u000areplicated in Chrome by just increasing gain). Also, on iOS, it seems\u000athat the spatialization effect is weaker -- potentially because they are\u000ausing a different HRTF, or maybe they are just panning.</p>\u000a\u000a<p><strong>Spatialization effect can be subtle.</strong> I found that there wasn't\u000aenough oomph to the effect provided by WebAudio's HRTF. Perhaps it is\u000aacoustically correct, but it just wasn't obvious or compelling enough as\u000ais. I had to fudge the situation slightly, and implement a sound cone\u000afor the observer, so that sources that are within the field of view got\u000aa slight gain boost.</p>\u000a\u000a<h2>Parting words and links</h2>\u000a\u000a<p><a href="http://www.ee.usyd.edu.au/people/philip.leong/UserFiles/File/papers/errors_hr97.pdf">The nature and distribution of errors in sound localization</a>\u000ais a seminal paper from 1997, giving a thorough psychoacoustic analysis\u000aon our hearing limits. In this web audio context, however, it is unclear\u000ahow much of this perceptual accuracy is lost due to variations in\u000aheadphone style and quality, and software implementation details.  To\u000atruly bring my Austrian cathedral experience to the web, we would\u000aprobably need a personalized HRTF, and also a more sophisticated room\u000amodel that could simulate reflections from the walls of the building.\u000aThis is concievable on the web in the near future, especially with the\u000aprospect of the <a href="https://plus.sandbox.google.com/+ChrisWilson/posts/QapzKucPp6Y">highly anticipated</a> <a href="http://webaudio.github.io/web-audio-api/#audio-worker-examples">AudioWorker</a>.</p>\u000a\u000a<p>Let me conclude by linking you to a couple more spatial audio demos:</p>\u000a\u000a<ul>\u000a<li>Ilmari wrote an <a href="http://www.html5rocks.com/en/tutorials/webaudio/positional_audio/">html5rocks post</a> a while back about three.js\u000aand the Web Audio API.</li>\u000a<li>Mozilla built the <a href="https://hacks.mozilla.org/2013/10/songs-of-diridum-pushing-the-web-audio-api-to-its-limits/">Songs of Diridum demo</a>, showing a cute\u000aspatialized jazz band.</li>\u000a<li>Arturo recently emailed me, showing me a <a href="http://inspirit.unboring.net/">pretty compelling WebVR +\u000aAudio project</a>, in the spirit of WebVR.</li>\u000a<li>Not web-based, but a painstakingly recorded and tweaked <a href="https://www.youtube.com/watch?v=IUDTlvagjJA">binaural\u000ahaircut simulation</a> just to illustrate the potential.</li>\u000a</ul>\u000a
p2456
tp2457
Rp2458
sg13
V/spatial-audio-web-vr
p2459
sg15
Nsg16
I01
sg17
VSpatial audio and web VR
p2460
sg20
VLast summer I visited Austria, the capital of classical music.
p2461
sg6
V<p>Last summer I visited Austria, the capital of classical music. I had the\u000apleasure of hearing the <a href="http://en.wikipedia.org/wiki/Vespro_della_Beata_Vergine">Vespers of 1610</a> in the great\u000a<a href="http://goo.gl/e3WBB2">Salzburger Dom (photosphere)</a>. The most memorable part of\u000athe piece was that the soloists movedbetween movements, so their voices\u000aand instruments emanated from surprising parts of the great hall.\u000aInspired, I returned to the west coast and eventually came around to\u000abuilding a spatial audio prototypes like this one:</p>\u000a\u000a<p><a href="http://borismus.github.io/moving-music"><img src="collage_small.jpg" alt="Screenshot of a demo" /></a></p>\u000a\u000a<p>Spatial audio is an important part of any good VR experience, since the\u000amore senses we simulate, the more compelling it feels to our sense\u000afusing mind. WebVR, WebGL, and WebAudio all act as complementary specs\u000ato enable this necessary experience. As you would expect, because it\u000auses the <a href="https://github.com/borismus/webvr-boilerplate">WebVR boilerplate</a>, this demo can be viewed on\u000amobile, desktop, in Cardboard or an Oculus Rift. In all cases, you will\u000aneed headphones :)</p>\u000a\u000a
p2462
sg25
g169
sg33
g2459
sg170
(dp2463
g172
S'Mar'
p2464
sg174
S'March 19, 2015'
p2465
sg176
I3
sg177
S'2015-03-19T09:00:00-00:00'
p2466
sg179
I1426780800
sg180
I2015
sg181
I19
ssg65
g182
sg31
S'spatial-audio-web-vr'
p2467
sS'posted'
p2468
g188
(S'\x07\xdf\x03\x13'
p2469
tp2470
Rp2471
ssg34
S'content/posts/2015/spatial-audio-web-vr/index.md'
p2472
sg36
F1443390754.0
sa(dp2473
g2
(dp2474
g4
V I recently returned from ISWC 2015, where I presented the  Cardboard Magnet  paper. In addition to seeing old friends, meeting new ones, and being inspired by some interesting research, it was an excellent excuse to visit Osaka, Japan! This year, ISWC was co-located with UbiComp, and the combined conference had four tracks. This post is by no means exhaustive, just some of the more interesting work I got a chance to see.   
p2475
sg28
g7
(g8
g9
V<p>I recently returned from ISWC 2015, where I presented the <a href="/magnetic-input-mobile-vr/">Cardboard\u000aMagnet</a> paper. In addition to seeing old friends,\u000ameeting new ones, and being inspired by some interesting research, it was an\u000aexcellent excuse to visit Osaka, Japan! This year, ISWC was co-located with\u000aUbiComp, and the combined conference had four tracks. This post is by no means\u000aexhaustive, just some of the more interesting work I got a chance to see.</p>\u000a\u000a<!--more-->\u000a\u000a<p><strong>Opening keynote: Visualizing and Manipulating Brain Dynamics</strong>. <a href="http://www.cns.atr.jp/~kawato/">Mitsuo\u000aKawato</a> showed some impressive <a href="https://ieeetv.ieee.org/conference-highlights/cb-exploring-neuroscience-withhumanoid-research-platform?">self-balancing\u000arobots</a>.\u000aIt seems that we've <a href="http://tumblr.forgifs.com/post/111425301004/robot-soccer-kick-fail">come a long\u000away</a>. Most\u000aof what he showed was around deep brain stimulation, artificial cochleas and\u000aretinas, old work but mostly new to me. This is a recent, very impressive and\u000asomewhat terrifying paper on reconstructing low-resolution grayscale <a href="http://neurosurgery.washington.edu/Lectures/science.1234330.full.pdf">imagery\u000afrom\u000adreams</a>.</p>\u000a\u000a<h2>Novel input technology</h2>\u000a\u000a<p><strong>SoQr: Sonically Quantifying the Content Level inside Containers</strong> is a\u000aconvoluted way of determining if you're out of milk. Inspired by acoustically\u000achecking ripeness of watermellons, the idea is to use <a href="https://goo.gl/photos/BVhjSZXyn7MJV2sv7">contact speaker and mic\u000apair</a> to determine how full a container\u000ais. The method's efficacy depends a lot on the placement of the sensor,\u000aproperties of the container, and other environmental factors, like whether or\u000anot any other items are touching the container. Seems overly complex to me, you\u000acould use another approach to reach higher fidelity (eg. a scale). That said,\u000amaybe this can be done very inexpensively?</p>\u000a\u000a<p><strong>MagnifiSense: Inferring Device Interaction Using Wrist-Worn Passive\u000aMagneto-Inductive Sensors</strong> is about determining which electronic device is\u000abeing used. The idea is to use an inductor coil to detect nearby electromagnetic\u000aradiation. They built their own hardware for the purpose which samples at a very\u000ahigh frequency (44.1 KHz). They detect <a href="https://goo.gl/photos/RQXQ8fBJpJU4f8wn6">unique EM radiation\u000apatterns</a> for each type of device.\u000aSupposedly they can do the same using a regular smartphone magnetometer, but I'm\u000avery skeptical. They also claim to be able to determine who is using the device,\u000abut that part wasn't very clear from the talk.</p>\u000a\u000a<p><strong>DoppleSleep: A Contactless Unobtrusive Sleep Sensing System Using Short-Range\u000aDoppler Radar</strong> uses a 24 GHz doppler radar typically mounted near the bedside\u000ato detect sleep patterns. The benefits are huge: you don't have to wear anything\u000aor instrument the bed. <a href="https://en.wikipedia.org/wiki/Polysomnography">Medical sleep\u000atrackers</a> require\u000a7 electrodes, and <a href="https://en.wikipedia.org/wiki/Actigraphy">consumer ones</a>\u000adon't work well.  They also had a demo where you just sit at your desk and the\u000adoppler tracks your heart rate, breathing rate, as well as more macro\u000amovements. It didn't work as well as I had hoped, but being a research demo, I\u000aremain hopeful!</p>\u000a\u000a<p><strong>Activity tracking and indoor positioning with a wearable magnet</strong> was a poster\u000ashowing a very cheap way of tracking just by placing magnetometers in strategic\u000alocations and giving the user a magnet. More details <a href="https://goo.gl/photos/2un4nc5nrE7evC4D8">on the\u000aposter</a>.</p>\u000a\u000a<p><strong>IDyLL: Indoor Localization using Inertial and Light Sensors on Smartphones</strong>\u000aattempts to solve indoor localization using the light sensor, and lamps as\u000afeatures for tracking. They extract features from the lights using peak finding,\u000anot absolute intensity. Then they wrote a kalman filter to fuse the IMU and\u000alight-derived features. There's a lot of problems, like needing to have\u000astructured light (eg. in a hallway with a low ceiling), and identify ambiguity\u000a(ie. you're under a light, but which one?).</p>\u000a\u000a<p><strong>Monitoring Building Door Events using Barometer Sensor in Smartphones</strong> used\u000athe ubiquitous smartphone barometer, which is currently used to get faster GPS\u000alock and assist in weather forecasting, to determine if a door opens in a\u000abuilding. This only works in buildings with HVAC systems, but it was pretty\u000aclever, and they found that it can work reliably, even for multiple doors. Basic\u000aidea <a href="https://goo.gl/photos/yKPbBYPv2RayMrcf6">described in this slide</a>.</p>\u000a\u000a<p><strong>ProximityHat - A Head-Worn System for Subtle Sensory Augmentation with Tactile\u000aStimulation</strong> reminded me of various <a href="http://www.cc.gatech.edu/~acosgun3/papers/cosgun2014guidance.pdf">vibro-tactile belt\u000aprojects</a>, and\u000aserved a similar purpose: to exploit the sense of touch to give the wearer\u000aanother sense. This has many benefits like not blocking other senses. Anyway,\u000athey built a hat and gave it ultrasonic sensors all around and inward-facing\u000alinear actuators, not vibrator motors.  They studied sensitivity around the head\u000aand found high variation around users, and that the forehead was generally less\u000asensitive. Main application appears to be navigation, and they did some blind\u000auser studies.</p>\u000a\u000a<p><strong>Controlling Stiffness with Jamming for Wearable Haptics</strong> makes it easier and\u000aharder to move sliders with the help of a pneumatic bladder, and layered\u000amaterial. As the bladder inflates, the additional force on the layered material\u000acauses increased friction. Previous layer jamming had low fidelity (binary), so\u000athis is a big improvement. They are currently using sandpaper, so it's unclear\u000ahow robust the effect would be over time.</p>\u000a\u000a<p><strong>PneuHaptic: Delivering Haptic Cues with a Pneumatic Armband</strong> used a wearable\u000a<a href="https://goo.gl/photos/XkFSKrbRmpiZgzCu9">pneumatic band with 2 pumps and 3\u000avalves</a> to give haptic feedback. This\u000ais a nice alternative to vibrating motors and linear actuators, but not sure how\u000aminiaturizable in practice.</p>\u000a\u000a<p><strong>Fast Blur Removal for Wearable QR Code Scanners</strong> is an image processing paper\u000afor improving QR code detection on wearable devices. The proposed method uses\u000aun-blurring techniques which involve predicting the blur direction and applying\u000ade-convolutions. They also use an IMU to better guess the direction of movement.\u000aHowever <a href="http://picturesofpeoplescanningqrcodes.tumblr.com/">QR codes are dead to\u000ame</a>.</p>\u000a\u000a<h2>Gadgets and fads</h2>\u000a\u000a<p><strong>Why we use and abandon smart devices</strong> tried to answer the question of why\u000apeople abandon their various health and tracking devices so quickly. Basically,\u000apeople are motivated by curiosity and novelty, and these health trackers are too\u000agimmicky. Studies of Fitbit trackers saw majority of them abandoned (65%\u000aabandoned in 2 weeks). Design implications are that encouraging routines\u000a(changing behavior) and minimizing maintenance (charging) are the critical\u000athings.This study had participants come up with a goal, and $1K to buy devices,\u000aso quite contrived given that people didn't even choose to use the devices on\u000atheir own, but motivated by a study.</p>\u000a\u000a<p>In a less contrived study about the same thing, <strong>No Longer Wearing:\u000aInvestigating the Abandonment of Personal Health-Tracking Technologies on\u000aCraigslist</strong> scraped Craigslist for this data. They found that only 25% of\u000apeople sell their devices just for abandonment reasons. In many other cases,\u000athey upgrade to something else, or reach their goals. That said, it's very\u000abiased sample, since these people are selling (many just abandon, and don't sell\u000aon CL).</p>\u000a\u000a<h2>Machine learning</h2>\u000a\u000a<p><strong>DeepEar: Robust Smartphone Audio Sensing in Unconstrained Acoustic\u000aEnvironments Using Deep Learning</strong> used RNNs to learn whatever sound the user is\u000ainterested in. They did an interesting comparison to similar specialized systems\u000a(eg. those that do speaker identification, stress detection, emotion, etc) and\u000aclaim to do better. Also, their RNN runs in hardware on a chip, which I thought\u000awas super impressive.</p>\u000a\u000a<p>In <strong>Sensor-based Stroke Detection and Stroke Type Classification in Table\u000aTennis</strong>, the authors instrumented paddles with IMUs and got people to perform\u000avarious strokes (in a somewhat controlled environment). They performed stroke\u000adetection through peak recognition and thresholding, and then had a classifier\u000afor stroke type determination. 97% detection and classification rates!\u000aImpressive, but contrived. Wondering how it would do for a full game?</p>\u000a\u000a<p><strong>Recognizing New Activities with Limited Training Data</strong> was an interesting\u000apaper about recognizing new activities based on small amounts of labeled data.\u000aTheir idea was to leverage "semantic attributes" from core activities to learn\u000aa new activity.  Example: biking is like sitting (body is not changing angle),\u000arunning (legs move up and down) and driving (hands are steering). They proposed\u000aan <a href="https://goo.gl/photos/Rn2BbvQjhU3Lhv1K7">Activity-Attribute matrix</a>, and a\u000acascaded classifier. Problem is that multiple activities can share the same\u000aattributes. So they combine this with a traditional approach.</p>\u000a\u000a<p><strong>When Attention is not Scarce - Detecting Boredom from Mobile Phone Usage</strong>\u000apredicted boredom with higher accuracy than I predicted. They collected a ground truth\u000aof boredom data by polling users multiple times a day, asking if they were\u000abored, and collected activity traces (semantic location, demographics, network\u000ausage, recent number of notifications, sensor data). They managed to detect\u000aboredom with 73% accuracy. They then built an app which sent buzzfeed articles\u000awhen bored and compared engagement and click ratio to the random condition.\u000aCTR was 8% for random, 20% when bored, and people were much more engaged.</p>\u000a\u000a<h2>Virtual and augmented reality</h2>\u000a\u000a<p><strong>Wearing Another Personality: A Human-Surrogate System with a Telepresence\u000aFace</strong> was probably the most bizarre paper at the conference. This work\u000abasically proposes to use a human surrogate instead of a telepresence robot. The\u000asurrogate wears an HMD with pass-through camera feed and a tablet on their face.\u000aThe tablet shows the face of the director. The director gets audio and video\u000afeed from the surrogate, and the surrogate gets audio instructions from the\u000adirector. They did creepy user studies like going to a city office to get a\u000apublic document (friend as surrogate), or meeting your grandmother (mother as\u000asurrogate). Surprisingly, many participants liked being surrogates. The big\u000atechnical problem is camera pass through latency. If you go through the whole\u000aJava stack it's something crazy like 300ms. Here's a <a href="https://goo.gl/photos/tZPoR4wvQiDekzg86">video from the\u000aconference</a> to give you a better sense.</p>\u000a\u000a<p><strong>Comparing Order Picking Assisted by Head-Up Display versus Pick-by-Light with\u000aExplicit Pick Confirmation</strong> compared two order picking methods in warehouses.\u000aThe current method is via digital labels on each tray that count how many items\u000ayou're supposed to take from that tray. The new method is to show <a href="https://www.youtube.com/watch?v=yUZFaCP6rP4">which trays\u000ato pick from using augmented\u000areality</a>. The benefit is that you\u000adon't need an instrumented warehouse, so it's much cheaper. This was interesting\u000abecause it was a specific, potentially useful application for a Google\u000aGlass-type device. At the same time, it may be an obsolete problem since aren't\u000arobots supposed to automate that sort of thing pretty soon?</p>\u000a\u000a<p><strong>ConductAR: An AR based tool for iterative design of conductive ink circuits</strong>\u000ais a project that validates hand drawn circuits using augmented reality. You\u000asketch your circuit with a conductive pen, and then the tool takes a picture and\u000agives you the right voltage drops etc. The presenter showed resistance\u000acalculation (the thicker the line, the more resistive), using <a href="https://goo.gl/photos/wLaMQZqoqf66ap477">a FEM\u000amethod</a>. But I wasn't convinced that\u000athis is worthwhile. Sketching circuits should be exploratory and does not need\u000ato be precise, that's sort of the point.</p>\u000a\u000a<p><strong>An Approach to User Identification for Head-Mounted Displays</strong> uses blink and\u000ahead movements to identify users. They play a particular video and track your\u000apatterns using Google Glass. They extract blinks using IR peaks, and head track\u000ausing the IMU. It takes about 30s to verify uniqueness, but not sure how large\u000atheir user base is. Results are good: 94% balanced accuracy, and blink features\u000aare most important.</p>\u000a\u000a<p><strong>Glass-Physics: Using Google Glass for Physics Experiments</strong> compared using\u000aGoogle Glass to just a tablet for assisting students doing physics experiments.\u000aThe idea is to remove drudgery from data collection. The experiment was to\u000adetermine <a href="https://goo.gl/photos/kcxvxouYtDs6CfQv7">effect of fill level in a water glass on frequency of\u000asound</a> when the vessel was hit with a\u000afork. A Google Glass app did automatic collection of frequency and of water\u000alevel. People liked the wearable version more, but the tablet app involved\u000amanual input. My theory is that a tablet app with AR features to auto-measure\u000afill level would do as well as an HMD.</p>\u000a\u000a<p><strong>WISEglass: Multi-purpose Context-aware Smart Eyeglasses</strong> was like Google\u000aGlass, except without the display. The main contribution was a light sensor on\u000athe bridge of the nose, which could reliably determine when you are at the\u000acomputer (from the screen update frequency). Other than that, seems pretty much\u000athe same as wearing an IMU anywhere else (eg. smartwatch).</p>\u000a\u000a<h2>E-textiles are impressive</h2>\u000a\u000a<p>I saw some nice demos of <a href="https://goo.gl/photos/cui8ucmGYjdXq2tj9">stretch-sensitive fabric\u000a(video)</a>, and <a href="https://goo.gl/photos/HbXp9xK2GtBgcoEo8">pressure/capacitative\u000afabric (video)</a>. The real question is\u000awhere to embed the controller, and what to do about battery life (their stats\u000awere pretty bad). E-textiles are interesting because everybody wears clothing,\u000awhich is not true for glasses or watches.</p>\u000a\u000a<p><strong>Closing keynote: Behind the scenes</strong> delivered by <a href="http://www.daito.ws/en/">Daito\u000aManabe</a> was sequentially translated, which was\u000ainitially jarring, but the talk was so visually stimulating, it didn't really\u000amatter. Daito walked through a lot of his data arts work, mind-blowingly\u000aimpressive art pieces involving drones, 3D graphics, depth cameras, etc. A nice,\u000aif somewhat non-sequitur ending to the conference.</p>\u000a\u000a<p>Signing off. Arigatou gozaimasu!</p>\u000a
p2476
tp2477
Rp2478
sg13
V/ubicomp-iswc-2015
p2479
sg15
Nsg16
I01
sg17
VUbiComp and ISWC 2015
p2480
sg20
VI recently returned from ISWC 2015, where I presented the [Cardboard\u000aMagnet](/magnetic-input-mobile-vr/) paper.
p2481
sg6
V<p>I recently returned from ISWC 2015, where I presented the <a href="/magnetic-input-mobile-vr/">Cardboard\u000aMagnet</a> paper. In addition to seeing old friends,\u000ameeting new ones, and being inspired by some interesting research, it was an\u000aexcellent excuse to visit Osaka, Japan! This year, ISWC was co-located with\u000aUbiComp, and the combined conference had four tracks. This post is by no means\u000aexhaustive, just some of the more interesting work I got a chance to see.</p>\u000a\u000a
p2482
sg25
g169
sg33
g2479
sg170
(dp2483
g172
S'Sep'
p2484
sg174
S'September 16, 2015'
p2485
sg176
I9
sg177
S'2015-09-16T09:00:00-00:00'
p2486
sg179
I1442419200
sg180
I2015
sg181
I16
ssg65
g182
sg31
S'ubicomp-iswc-2015'
p2487
sS'posted'
p2488
g188
(S'\x07\xdf\t\x10'
p2489
tp2490
Rp2491
ssg34
S'content/posts/2015/ubicomp-iswc-2015/index.md'
p2492
sg36
F1442447839.0
sa(dp2493
g2
(dp2494
g4
V The web platform is uniquely great for networked copresence. To demonstrate, I built a multi-user chat prototype that uses peer-to-peer audio and data connections to establish a virtual audio experience.  Voices are spatialized based on the position and orientation of each participant (using Web Audio). Also, you can shrink and grow, which, in addition to changing your avatar's size, pitch shifts your voice. Large avatars have deep, god-like voices, while smaller ones start to sound very mousey!        Check out the  demo for yourself . It works on desktop (mouse look and spacebar triggers movement), on mobile (magic window) and in VR (through the  WebVR API , via  the polyfill ).   
p2495
sg28
g7
(g8
g9
V<p>The web platform is uniquely great for networked copresence. To demonstrate, I\u000abuilt a multi-user chat prototype that uses peer-to-peer audio and data\u000aconnections to establish a virtual audio experience.  Voices are spatialized\u000abased on the position and orientation of each participant (using Web Audio).\u000aAlso, you can shrink and grow, which, in addition to changing your avatar's\u000asize, pitch shifts your voice. Large avatars have deep, god-like voices, while\u000asmaller ones start to sound very mousey!</p>\u000a\u000a<iframe width="560" height="395" src="https://www.youtube.com/embed/FPJDNQJt2DQ" frameborder="0" allowfullscreen></iframe>\u000a\u000a<p>Check out the <a href="https://borismus.github.io/copresence-vr/">demo for yourself</a>. It works on desktop (mouse look and\u000aspacebar triggers movement), on mobile (magic window) and in VR (through the\u000a<a href="https://webvr.info/">WebVR API</a>, via <a href="https://github.com/borismus/webvr-polyfill/">the polyfill</a>).</p>\u000a\u000a<!--more-->\u000a\u000a<h1>Better together: copresence is compelling</h1>\u000a\u000a<p>The best things in life are enjoyed in good company. Virtual experiences are no\u000aexception. My fondest gaming memories were from two decades ago with close\u000afriends huddled around a CRT, whether it was Morris the Moose and Blombo the\u000aElephant <a href="https://3drealms.com/catalog/wacky-wheels_16/">racing around</a> the track, or co-strategizing in <a href="https://www.youtube.com/watch?v=hBrYtNTOTyE">Civ</a>. It\u000awasn't so much about the games, more about the people, and the experience of\u000abeing there together.</p>\u000a\u000a<p>Putting a computer on your face greatly increases your odds of having an\u000aisolating experience.  One of the biggest downsides of VR is that social\u000aexperiences are much harder to produce. While physically copresent VR is\u000apossible, it presents logistical challenges. And since you are fully immersed in\u000aa virtual world, the physical presence of your friends is nearly irrelevant.\u000aGiven the constraints, perhaps the best remedy to loneliness is to provide\u000anetworked friends. This can be fun too! <a href="https://en.wikipedia.org/wiki/Warcraft:_Orcs_%26_Humans">Orcs and Humans</a> over PBX, anyone?</p>\u000a\u000a<h1>WebAudio + WebRTC + WebVR = \u2764</h1>\u000a\u000a<p>The web is the ideal platform for building copresent VR experiences. VR\u000acopresence requires low latency connections between peers. It also requires a\u000areal time audio channel, with a much smaller emphasis on remote video, since the\u000auser is wearing a headset and their face is obscured. The powerful Web Audio\u000aAPI has long been available on all modern browsers, and is well equipped for\u000aprocessing audio of all sorts: spatialization, effects. WebRTC is widely\u000aavailable too, with <a href="http://www.apple.com/safari/">one unfortunate exception</a>. And with the exception\u000aof Service Workers and company, if you're on the web, you have connectivity. </p>\u000a\u000a<p>Thanks to some <a href="http://crbug.com/121673">excellent bug squashing</a>, it's now possible to pipe\u000aremote WebRTC streams into a Web Audio context. This enables devs to spatialize\u000aand otherwise manipulate the remote stream to their heart's content.\u000aSpecifically, the prototype I'm launching today has a few fun audio features:</p>\u000a\u000a<ul>\u000a<li><p>Each remote stream is spatialized based on the pose of the peer using the\u000a<code>PannerNode</code> (see <a href="/spatial-audio-web-vr/">my previous post</a> about this for more details).</p></li>\u000a<li><p>Remote streams are analyse for voice activity, using an <code>AnalyserNode</code> to\u000ainspect the frequency content between 300 Hz and 3400 Hz (the typical human\u000avocal range), and doing a simple thresholding. This is then used to animate\u000athe Southpark-style avatar's mouth.</p></li>\u000a<li><p>Changing the size of your avatar also changes how you hear your peer's voice.\u000aI'm using the <a href="https://github.com/mmckegg/soundbank-pitch-shift">soundbank-pitch-shift</a> library to achieve this, courtesy of\u000a<a href="https://twitter.com/cwilso">Chris Wilson</a> and <a href="http://twitter.com/MattMcKegg">Matt McKegg</a>.</p></li>\u000a</ul>\u000a\u000a<h1>Technical details: in the weeds with WebRTC</h1>\u000a\u000a<p>Hoping to avoid learning the intricacies of WebRTC, which is a fairly low level\u000aand intimidating API, I started exploring higher level abstractions around it.\u000aThe most popular wrapper I found was <a href="http://peerjs.com/">peer.js</a>, but unfortunately the\u000aproject doesn't seem to be actively maintained, and relies on a special Node.js\u000aWebSocket server which, in my experience, often drops clients.</p>\u000a\u000a<p>So I moved to Firebase which, in my implementation, performs the duty of\u000asignaling server, and also maintains a roster of all connected users and their\u000acurrent state. For each connected user, we store their display name (which\u000aclients can set), and the room ID (if the user is currently in a room).</p>\u000a\u000a<pre><code>{\u000a  username: 'Your Name',\u000a  roomId: 'A Random Room Identifier'\u000a}\u000a</code></pre>\u000a\u000a<h2>Bird's eye view of WebRTC</h2>\u000a\u000a<p>Having moved away from peer.js, I could no longer afford to let the intricacies\u000aof WebRTC be handled by some third party, and had to get into the weeds. It was\u000aespecially important to understand how to handle multiple <code>RTCPeerConnections</code>\u000anecessary for the case with more than peer-to-peer. Although I found the docs to\u000abe quite obtuse, the core of the WebRTC API is fairly straight forward:</p>\u000a\u000a<ol>\u000a<li><p>The caller (A) gets its local stream and uses the signal server to send an\u000a"offer" message to the callee (B), which includes information about A's local\u000astream.</p></li>\u000a<li><p>The callee (B) gets A's "offer" and registers A's local stream as its remote\u000astream. It then gets its own local stream, and responds A's offer via the\u000asignal server, sending an "answer" message to the caller (A), which contains\u000aits own local information.</p></li>\u000a<li><p>The caller (A) gets B's "answer" and registers B's local stream as its remote\u000astream. At this point, both A and B have basic information about one\u000aanother's local and remote streams.</p></li>\u000a<li><p>At this point, A and B exchange ICE (Interactive Connectivity Establishment)\u000aCandidates to work out the details of how to establish a peer-to-peer stream.\u000aEventually, when both sides are satisfied, we have contact.</p></li>\u000a</ol>\u000a\u000a<p>Hopefully the above serves as a useful summary. It certainly will be for me, as\u000aI found the existing WebRTC documentation confusing. Many of the samples connect\u000ato themselves, which does not give a great sense of what the protocol between\u000aclients should actually be.</p>\u000a\u000a<p>At the ICE stage, invoke more acronyms! STUN and TURN come into play in trickier\u000anetwork topologies (ie. those involving NAT servers).  Google already provides a\u000aSTUN server by default, and I ended up using a <a href="http://xirsys.com/">free service</a> for TURN\u000aserver support. Each <code>RTCPeerConnection</code> is initialized with the specific STUN\u000aand TURN servers that we use.</p>\u000a\u000a<h1>Copresence is essential for VR</h1>\u000a\u000a<p>Given the inherent isolation of virtual reality, copresence becomes an even more\u000acompelling ingredient than ever before. Copresence is essential for VR, and the\u000aweb is a great place to make it happen. <a href="https://borismus.github.io/copresence-vr/">Try it out</a> with a friend, or\u000ausing two of your own devices. Oh, and if you find bugs, please let me know via\u000a<a href="https://github.com/borismus/copresence-vr">github</a>.</p>\u000a
p2496
tp2497
Rp2498
sg13
V/copresence-webvr
p2499
sg15
Nsg16
I01
sg17
VCopresence in WebVR
p2500
sg20
VThe web platform is uniquely great for networked copresence.
p2501
sg6
V<p>The web platform is uniquely great for networked copresence. To demonstrate, I\u000abuilt a multi-user chat prototype that uses peer-to-peer audio and data\u000aconnections to establish a virtual audio experience.  Voices are spatialized\u000abased on the position and orientation of each participant (using Web Audio).\u000aAlso, you can shrink and grow, which, in addition to changing your avatar's\u000asize, pitch shifts your voice. Large avatars have deep, god-like voices, while\u000asmaller ones start to sound very mousey!</p>\u000a\u000a<iframe width="560" height="395" src="https://www.youtube.com/embed/FPJDNQJt2DQ" frameborder="0" allowfullscreen></iframe>\u000a\u000a<p>Check out the <a href="https://borismus.github.io/copresence-vr/">demo for yourself</a>. It works on desktop (mouse look and\u000aspacebar triggers movement), on mobile (magic window) and in VR (through the\u000a<a href="https://webvr.info/">WebVR API</a>, via <a href="https://github.com/borismus/webvr-polyfill/">the polyfill</a>).</p>\u000a\u000a
p2502
sg25
g169
sg33
g2499
sg170
(dp2503
g172
S'Aug'
p2504
sg174
S'August 8, 2016'
p2505
sg176
I8
sg177
S'2016-08-08T09:00:00-00:00'
p2506
sg179
I1470672000
sg180
I2016
sg181
I8
ssg65
g182
sg31
S'copresence-webvr'
p2507
sS'posted'
p2508
g188
(S'\x07\xe0\x08\x08'
p2509
tp2510
Rp2511
ssg34
S'content/posts/2016/copresence-webvr/index.md'
p2512
sg36
F1470859518.0
sa(dp2513
g2
(dp2514
g4
V Back in June, I gave Trump a coldhearted 55% chance of winning the 2016 US election. You don't have to believe me, since I recorded it in my  Predictions.md  file, and never on  gjopen.com , where it belongs. My assessment was mostly based on anecdotal observations that recent, related polls have been terribly wrong. Brexit and then Trump's surprise Republican nomination both came as a complete surprise to experts from all sides.    But, despite my dire predictions, it somehow didn't  feel  that I could be right on the eve of the election. When the final result was revealed, I was just as disturbed as everyone else. In retrospect, I attribute my "successful" prediction mostly to luck combined with my apparently contrarian tendencies, rather than to skill. Nearly a week after the announcement of President Trump, I'm still processing the verdict. Two big questions loom: 1) Why did he win?, and 2) Why didn't we see it coming?   
p2515
sg28
g7
(g8
g9
V<p>Back in June, I gave Trump a coldhearted 55% chance of winning the 2016 US\u000aelection. You don't have to believe me, since I recorded it in my\u000a<code>Predictions.md</code> file, and never on <a href="http://gjopen.com">gjopen.com</a>, where it\u000abelongs. My assessment was mostly based on anecdotal observations that recent,\u000arelated polls have been terribly wrong. Brexit and then Trump's surprise\u000aRepublican nomination both came as a complete surprise to experts from all\u000asides.</p>\u000a\u000a<p>But, despite my dire predictions, it somehow didn't <em>feel</em> that I could be right\u000aon the eve of the election. When the final result was revealed, I was just as\u000adisturbed as everyone else. In retrospect, I attribute my "successful"\u000aprediction mostly to luck combined with my apparently contrarian tendencies,\u000arather than to skill. Nearly a week after the announcement of President Trump,\u000aI'm still processing the verdict. Two big questions loom: 1) Why did he win?,\u000aand 2) Why didn't we see it coming?</p>\u000a\u000a<!--more-->\u000a\u000a<h1>Who voted for him?</h1>\u000a\u000a<p>Trump's electoral base was quite diverse. Working class whites were only a small\u000apart of his base. I was surprised to learn that 37% of 18-29 year olds, and 29%\u000aof hispanics voted for him, and that the average Trump voter had an annual\u000aincome of $71K.</p>\u000a\u000a<p><strong>Single issue voters</strong>. Many people value only one issue, or weigh a single issue\u000aso heavily that others pale in comparison. Fundamentalist Christians may really\u000awant a pro-life president. If they care enough about that issue, they would be\u000awilling to deal with a Buffoon, and forgive all the hate, racism and bigotry in\u000athe world.</p>\u000a\u000a<p><strong>People with nothing to lose</strong>. I suppose that the economic situation many people\u000aare facing are worse than priveledged people like me can fully appreciate. If\u000ayou are low on Maslow's heirarchy, liberal values tend to fall by the wayside.\u000aWhen offered a chance to burn it down and start fresh, people with nothing are\u000awilling to oblige.</p>\u000a\u000a<p><strong>Tired of walking on eggshells</strong>. The left won the culture war. In many\u000aenvironments: at work, in universities, etc, expressing a dissenting opinion\u000aputs you in dire straits. Loud and self-righteous activists have a trained ear,\u000aand will be incredibly quick to label you a racist or a bigot for merely\u000aentertaining certain notions, or bringing up controversial questions despite no\u000aintended harm.</p>\u000a\u000a<p>This atmosphere leads to reduced viewport diversity. Without the ability to have\u000aan honest, civil conversation about difficult topics, people have fewer\u000aopportunities to change their minds, and become deeper and deeper entrenched in\u000atheir current beliefs.</p>\u000a\u000a<p><strong>Genuine xenophobes</strong>. Certainly some of Trump's electoral base are actually\u000aracist immigrant haters. It's very hard to imagine a racist voting for Hilary.\u000aBut I continue to believe that the visibility of this small group is magnified\u000aby media bias. Turns out it's really interesting to read about crazed people on\u000athe fringe.</p>\u000a\u000a<p>But as a side note, a group's support of a candidate doesn't imply that all or\u000amost of the candidate's supporters are members of the that group. And this\u000astatement holds even for deplorables: when the candidate is Trump and the group\u000ais the KKK.</p>\u000a\u000a<h1>Why did he win?</h1>\u000a\u000a<p><strong>Attention economy</strong>. Goebbels supposedly said "If you repeat a lie often enough,\u000apeople will believe it, and you will even come to believe it yourself." Despite\u000ahis purported hatred of the media, I think Trump was really helped by it. The\u000aamount of free publicity Trump's shenanigans received even from the most liberal\u000apublications like NYTimes is staggering (and any publicity is good publicity).</p>\u000a\u000a<p><strong>Terrible alternatives</strong>. It's hard to get excited about a candidate because they\u000awon't burn the country down. Many people simply could not in good conscience\u000asupport Clinton because of legimiate grievances.  Yet Trump is clearly a\u000acomplete Buffoon. And principled voters found themselves stuck between a rock\u000aand a hard place, hence the low turnout.</p>\u000a\u000a<p><strong>Complacency due to expected outcomes</strong>. In the weeks leading up to the election,\u000amany papers prominently featured polls leaning heavily in favor of Clinton. The\u000aNYT ran an election forecast on their front page which depicted the race being\u000aclosest in July, giving Clinton a 30% lead over Trump. The day before the\u000aelection, Clinton had a whopping 70% lead. Why would you go out and vote if you\u000aknow that it's going to be Clinton anyway?</p>\u000a\u000a<h1>Why didn't we see it coming?</h1>\u000a\u000a<p>Experts and laymen love to tell you what will happen in the the next five years.\u000aYet with Trump and Brexit, we have collectively been unable to predict what will\u000ahappen the next day. Given this sad observation, whatever model we are using is\u000aclearly broken. We must look inward, critically questioning many deeply held\u000aassumptions about the world, or be prepared for a lot more surprises.</p>\u000a\u000a<p><strong>Polling is broken</strong>. A lot of predictions rely on polls. 538 was based mainly on\u000aaggregating existing poll data. Nate Silver's theory was that by including\u000aenough polling companies, inacuraccies in each poll would be ironed out.\u000aUnfortunately, if all of the polls are systematically skewed, this approach is\u000ascrewed. And I think it is! Most polling is done by phone, which is quite\u000adifferent from a secret ballot. Imagine you are a disenfranchised voter and a\u000apollster from Gallup calls you, you would naturally tell them to go fuck\u000athemselves.</p>\u000a\u000a<p><strong>Filter bubbles distort reality</strong>. On election day, my twitter feed proudly\u000aannounced that "I'm With Her". The day after, when Trump won, people mourned,\u000aobserved that it was the anniversary of Kristallnacht, ushered in the antichrist\u000aand took to the streets yelling "Not My President!". Yet of half the voting\u000apopulation (not my twitter feed though) celebrated a 'uuuuuge victory.</p>\u000a\u000a<h1>Parting thoughts</h1>\u000a\u000a<p>The world is shrinking, and this is not always a good thing. Long ago, you would\u000abe born into a village, hang out with the butcher and the baker and be forced,\u000aby virtue of your birth, to listen to the candlestick maker's racist ramblings.\u000aAffordable air travel allowed us to self-organize according to professions,\u000abeliefs, and lifestyles. Social networks are hypersonic airplanes for the mind.\u000aDid someone tweet something you didn't quite like? Relief is just one unfollow\u000aaway. And so we end up in an increasingly polarized world, a bimodal\u000adistribution with increasing peak separation.</p>\u000a\u000a<p>I am deeply concerned about the political future of the US. Now is the time for\u000asupposedly open minded liberals like me to prove it. This means not running away\u000ato Canada, but accepting the democratically elected president. It means turning\u000ainward and trying to understand why the world behaves so differently from our\u000ainternal model, and starting by fixing the model. I recommend <a href="https://www.amazon.com/Righteous-Mind-Divided-Politics-Religion/dp/0307455777">Righteous Mind by Jon\u000aHaidt</a>\u000aas a relevant starting point.</p>\u000a\u000a<p>Without conversation, there is no hope.</p>\u000a
p2516
tp2517
Rp2518
sg13
V/election-2016
p2519
sg15
Nsg16
I01
sg17
VElection 2016
p2520
sg20
VBack in June, I gave Trump a coldhearted 55% chance of winning the 2016 US\u000aelection.
p2521
sg6
V<p>Back in June, I gave Trump a coldhearted 55% chance of winning the 2016 US\u000aelection. You don't have to believe me, since I recorded it in my\u000a<code>Predictions.md</code> file, and never on <a href="http://gjopen.com">gjopen.com</a>, where it\u000abelongs. My assessment was mostly based on anecdotal observations that recent,\u000arelated polls have been terribly wrong. Brexit and then Trump's surprise\u000aRepublican nomination both came as a complete surprise to experts from all\u000asides.</p>\u000a\u000a<p>But, despite my dire predictions, it somehow didn't <em>feel</em> that I could be right\u000aon the eve of the election. When the final result was revealed, I was just as\u000adisturbed as everyone else. In retrospect, I attribute my "successful"\u000aprediction mostly to luck combined with my apparently contrarian tendencies,\u000arather than to skill. Nearly a week after the announcement of President Trump,\u000aI'm still processing the verdict. Two big questions loom: 1) Why did he win?,\u000aand 2) Why didn't we see it coming?</p>\u000a\u000a
p2522
sg25
g169
sg33
g2519
sg170
(dp2523
g172
S'Nov'
p2524
sg174
S'November 13, 2016'
p2525
sg176
I11
sg177
S'2016-11-13T09:00:00-00:00'
p2526
sg179
I1479056400
sg180
I2016
sg181
I13
ssg65
g182
sg31
S'election-2016'
p2527
sS'posted'
p2528
g188
(S'\x07\xe0\x0b\r'
p2529
tp2530
Rp2531
ssg34
S'content/posts/2016/election-2016/index.md'
p2532
sg36
F1479108691.0
sa(dp2533
g2
(dp2534
g4
V According to the  American Press Institute ,        News is that part of communication that keeps us informed of the changing   events, issues, and characters in the world outside.      There are many ways for news to be uninformative or even outright misleading. Two trends in particular have received a lot of attention recently. The first is social recommendation systems and selective unfollowing, which creates a reality-distorting echo chamber. The second is fake news, which sure is in vogue these days, and is obviously a problem that we should tackle.    This post is about a different trend:  real news presented with misleading frequency . The issue at stake is the media's ability to inform its readers and serve the public interest.   
p2535
sg28
g7
(g8
g9
V<p>According to the <a href="https://www.americanpressinstitute.org/journalism-essentials/what-is-journalism/purpose-journalism/">American Press Institute</a>,</p>\u000a\u000a<blockquote>\u000a  <p>News is that part of communication that keeps us informed of the changing\u000a  events, issues, and characters in the world outside.</p>\u000a</blockquote>\u000a\u000a<p>There are many ways for news to be uninformative or even outright misleading.\u000aTwo trends in particular have received a lot of attention recently. The first is\u000asocial recommendation systems and selective unfollowing, which creates a\u000areality-distorting echo chamber. The second is fake news, which sure is in vogue\u000athese days, and is obviously a problem that we should tackle.</p>\u000a\u000a<p>This post is about a different trend: <a href="https://twitter.com/hamandcheese/status/801893793540796416">real news presented with misleading\u000afrequency</a>. The issue at stake is the media's ability to inform its readers\u000aand serve the public interest.</p>\u000a\u000a<!--more-->\u000a\u000a<h1>Real news, dubious frequency</h1>\u000a\u000a<p>If you are a New York Times reader, you may have noticed a certain individual\u000aprominently mentioned in the newspaper over the last several months. I wanted to\u000aknow just how much, and started daily screenshots on August 3rd, 2016. To my\u000agreat surprise, this was exactly one hundred days before the announcement of the\u000a45th president-elect. My analysis was very simple.</p>\u000a\u000a<ol>\u000a<li>Does the lead story (that is, top-left corner article) include "Trump" in the\u000atitle? <strong>(50 / 100 days)</strong></li>\u000a<li>Does the "Trump" appear in any headline above the fold? <strong>(86 / 100 days)</strong>.</li>\u000a</ol>\u000a\u000a<p>I should note some limitations of this approach. This process itself was more\u000amanual than I would have liked. My script took screenshots only, so I did not\u000ahave access to the markup of the page. I tried auto-reconstructing the images using\u000a<code>tesseract</code>, but the results were not perfect, <code>pdfgrep</code> had only limited use,\u000aand I had to visually inspect the page to get my data analyzed. The data are <a href="https://docs.google.com/spreadsheets/d/e/2PACX-1vT-7Cz1s9YpvlXDz9ejuJa_0JP9pV6coeAMA2j_R0KxEZZpnZ4daMsNOdI86qWgDIwUyZhy8rUAs-2Y/pubhtml">in\u000athis spreadsheet</a>, and I've made the <a href="https://drive.google.com/open?id=0B4Nj-yDXjBs_S3kwZXVqanZNdEU">raw screenshots</a>\u000aavailable too.  There are non-technical problems too. The web version of the New\u000aYork Times is updated on a more frequent than daily basis, so my screenshots\u000amissed some versions. Also, notions of "lead story" and "above the fold" don't\u000areally make sense online. I arbitrarily defined "the fold" to be 1280 px, as in\u000athis example:</p>\u000a\u000a<p><img src="nytimes.png" alt="Example NYT screenshot from September 16th" /></p>\u000a\u000a<p>As for the results? Half of the NYT's headlines had "Trump" in the name, and the\u000afrequency matched my intuition. It sure did feel like this dude was getting a\u000awhole lot of coverage. In fact, he got far more coverage than my method reveals,\u000asince many lead articles without explicit mention of his name in the title were\u000astill mostly about him.</p>\u000a\u000a<p>I was amused to see long runs of adjacent days of headlines about the\u000apresident-elect-to-be followed by many days of respite. For example, from August\u000a9th to 17th, "Trump" was featured consecutively, only to be broken by the\u000anews of Zika having spread to Florida.  The longest respite took place between\u000aSeptember 19th and 27th, when the nation's focus switched to the Manhattan bomb\u000ascare. Things heated back up on October 8th, and stayed hot until the 18th, when\u000ahis boasts about groping women were released to the public.</p>\u000a\u000a<p>Just for fun, I looked through some <a href="https://en.wikipedia.org/wiki/Portal:Current_events">other news</a> that lost the\u000acontest for most important story of the day. Here are three randomly chosen\u000aexamples:</p>\u000a\u000a<ul>\u000a<li><p>On August 14th, the NYT focused on G.O.P. politics rather than the Russian and\u000aSyrian jets which conducted 26 airstrikes across the Idlib province, killing\u000a122 civilians.</p></li>\u000a<li><p>On September 8th, the NYT decided that the future potential president's vows\u000ato bolster military capacity and raise spending were more important than Wells\u000aFargo's agreement to pay $190 million to settle a case involving deceptive\u000asales that pushed customers into fee-generating accounts they never requested.</p></li>\u000a<li><p>On October 21st, the NYT featured a story about a presidential candidate\u000athreatening to reject the election result rather than the Watts Bar Nuclear\u000aPlant, which was the first U.S. nuclear reactor to enter commercial operation\u000ain 20 years.</p></li>\u000a</ul>\u000a\u000a<p>I claim that the distribution of coverage in the days running up to the election\u000adid a bad job of keeping us informed of significant events, and went against the\u000apublic interest.</p>\u000a\u000a<h3>On not being well informed</h3>\u000a\u000a<p>Only 15 reporters were present when Harry Truman announced the use of nuclear\u000aweapons against Japan. <a href="https://www.amazon.com/Six-OClock-Presidency-Presidential-Television/dp/0275935981">By 1990</a>, nearly 2000 reporters held\u000apasses to the White House pressroom. The presidency has become an increasingly\u000aimportant focal point for the media. But as I write here in 2016, the coverage\u000aof a mere presidential candidate has come to eclipse absolutely everything else.\u000aReasons why this is bad:</p>\u000a\u000a<ul>\u000a<li><p><strong>Opportunity cost</strong>: People have limited attention. By devoting half of the\u000aheadlines to one issue, we are inevitably less well informed about other\u000athings.</p></li>\u000a<li><p><strong>Reporting on outrage</strong>: The majority of the headlines were about his\u000aRump's outrageous statements: "twitter barage taunts Ryan as weak and ineffective",\u000a"clung to birther lie for years, and still isn't apologetic", "assails his\u000aaccusers as liars and unattractive", "failing efforts to tame his tongue". Is\u000athat truly newsworthy?</p></li>\u000a</ul>\u000a\u000a<h3>On not serving the public interest</h3>\u000a\u000a<p>In one interpretation, "public interest" taken literally means providing the\u000apublic with what they are interested in. And if the object of interest is a\u000aparticular individual, so be it! That's how YouTube works: you watch a lot of\u000acat videos, YouTube learns you are into cat videos and gives you more cat\u000avideos. This is problematic <a href="http://www.timewellspent.io/">in and of itself</a>. But capital-J journalism\u000athat the New York Times is associated with is held to a different standard.</p>\u000a\u000a<p>The literal definition leaves a lot to be desired. My view of public interest is\u000amore paternalistic and centers on that which is beneficial for a <a href="http://reutersinstitute.politics.ox.ac.uk/publication/journalism-democracy-and-public-interest">well\u000afunctioning democracy</a>. In that view, here are a few reasons why focusing\u000aso heavily on Cheeto Jesus is counterproductive:</p>\u000a\u000a<ul>\u000a<li><p><strong>General fairness</strong>: No individual is important enough to have half of\u000aall stories in a newspaper be about them.</p></li>\u000a<li><p><strong>Playing into his little hands</strong>: The NYT gave a person widely regarded as\u000a"remarkably narcissistic" fifty percent of all headlines. Thanks guys!</p></li>\u000a<li><p><strong>Conditioning effects</strong>: By reporting so heavily on a single topic,\u000apeople become morbidly obsessed with it. People wonder, "What's going to\u000ahappen next?" instead of moving on to more important stories.</p></li>\u000a</ul>\u000a\u000a<p>Events have a timeline, but if the object of obsession is an individual, it's\u000athe gift that keeps on giving.</p>\u000a\u000a<h1>Engagement, clicks, and the bottom line</h1>\u000a\u000a<p>Why did the New York Times cover that despicable man to the extent that they\u000adid? I venture a couple of guesses:</p>\u000a\u000a<ol>\u000a<li>Genuine fear of him being elected, and thus a desire to warn the people of\u000ahis evil ways.</li>\u000a<li>A desire to increase views, catering in part to the morbid fascination\u000aof their readers.</li>\u000a</ol>\u000a\u000a<p>All things considered, I lean more towards the second possibility. While I can't\u000arule out the first guess completely, it seems to be contradicted by the\u000a<a href="http://www.nytimes.com/interactive/2016/upshot/presidential-polls-forecast.html">explicit confidence</a> the NYT had in a Democratic win. Back in March,\u000aVox <a href="http://www.vox.com/2016/3/3/11148296/donald-trump-media">asked the same question</a> and answered it simply:</p>\u000a\u000a<blockquote>\u000a  <p>The media covers him a lot because his campaign is fascinating and people are\u000a  interested in it.</p>\u000a</blockquote>\u000a\u000a<p>Notably, at the time, 13 percent of Vox stories were about the short fingered\u000avulgarian, but generated 26 percent of their readership. With statistics like\u000athat, guess what topic gets covered more? As for public interest, the author of\u000athe piece doubted that his attention-getting tactics would continue to work in\u000athe candidate's favor. Carte blanche justifying continued coverage? How\u000aconvenient!</p>\u000a\u000a<p>The internet has changed the media in a fundamental way. Even great publications\u000alike the New York Times haven't quite figured out how to balance their\u000a(understandable) corporate need for profit with their journalistic\u000aresponsibility to the public's interests. Our culture of not paying for news\u000acontent leads to media companies to seek other sources of income, mostly in the\u000aform of ads. But you get what you pay for! Rather than covering the world in a\u000abalanced way, the world is covered in a way that more people will want to read.</p>\u000a\u000a<p>The result is a killer combination of consumer driven demand (oh my god, what is\u000ahe going to say next?), and a desire for publications to maximize ad revenue.\u000aThe term is "engagement", and <a href="https://medium.com/@edelwax/is-anything-worth-maximizing-d11e648eb56f#.bt1ua0z6g">Joe Edelman</a> does a great job of explaining\u000awhy this is a dangerous thing to maximize.</p>\u000a\u000a<p>If newspapers did not need to maximize engagement to be profitable, there would\u000abe room to make decision that are aligned with actual public interest.</p>\u000a\u000a<h1>Problems of novelty</h1>\u000a\u000a<p>What makes a story newsworthy? Two ingredients at the very least,</p>\u000a\u000a<ol>\u000a<li>It must have happened recently, and</li>\u000a<li>it must be of sufficient interest to the public.</li>\u000a</ol>\u000a\u000a<p>The thing is, it takes time and effort to decide whether something is of\u000asufficient interest to the public. So there is an inverse tradeoff between the\u000aability to deliver both. The faster you deliver news, the more shortcuts must be\u000ataken to measure public interestingness.</p>\u000a\u000a<p>The general trend in the media is one of spacetime compression. Before the\u000atelegraph, it would take months for transatlantic news to travel. The fastest\u000away to get information across was a slow moving frigate. Before Gutenberg, it\u000awould take weeks more for the newly arrived information to spread. Over the last\u000athree centuries, the news cycle shrunk down to a daily basis. Today, since\u000aeveryone has a mini-printing press in their pocket, the news period is\u000aarbitrarily small. According to <a href="http://www.tristanharris.com/2016/05/how-technology-hijacks-peoples-minds%E2%80%8A-%E2%80%8Afrom-a-magician-and-googles-design-ethicist/">Tristan Harris</a>, the average\u000aperson checks their phone 150 times a day.</p>\u000a\u000a<p>With people checking the news every 10 minutes, there is a lot of pressure for\u000ajournalists to produce more content, more often. And indeed, there's a\u000alot of content online. But more does not mean better. Increasingly, news is\u000are-aggregated and re-published. Commentary is cheap and can be attached to a\u000abrand or personality, which means more clicks. Investigative journalism takes\u000atime. Unfortunately, by the time your investigation is finished... oh look a\u000asquirrel!</p>\u000a\u000a<p>It's tempting to blame the media for this, but the media is a reflection of our\u000acollective psyche magnified by modern technology. We need to value novelty less\u000aand learn how to delay gratification. Take a lesson from wine tasting and\u000ameditation. Accept that news happens and let it breathe undisturbed for a period\u000aof time. After all, hindsight is 20/20.</p>\u000a\u000a<h1>Any publicity is good publicity</h1>\u000a\u000a<p>Systematic flaws with today's media (see above) make it easy for cynical\u000aoperators to exploit. Fake news happens when the media-savvy operator runs a\u000anews website. Stories are completely fictional, but negatively resonate with the\u000apublic, creating pure click bait. <em>Oh my god, <a href="https://www.buzzfeed.com/craigsilverman/viral-fake-election-news-outperformed-real-news-on-facebook?utm_term=.rwlPqdb6Po#.ds5Yo0mwYE">Hillary sold weapons to\u000aISIS</a>!</em></p>\u000a\u000a<p>In another, even more cynical and dangerous variant, the media-savvy operator\u000amostly says and does outrageous things that may or not be true, purely for\u000aattention. <em>Oh my god, he said he would date his daughter</em>! Minor celebrities\u000alike <a href="https://swagbymilo.com/">M</a> have perfected this technique, but the Orange One is a true\u000amaster. Sometime in July, his wife gave a speech which was <a href="http://www.politico.com/magazine/story/2016/07/donald-trump-2016-convention-melania-trump-speech-dark-art-of-pr-214083">blatantly\u000aplagiarized</a>. His response to the controversy?</p>\u000a\u000a<blockquote>\u000a  <p>\u201cGood news is Melania\u2019s speech got more publicity than any in the history of\u000a  politics,\u201d he said, \u201cespecially if you believe that all press is good press!\u201d</p>\u000a</blockquote>\u000a\u000a<p>Straight from the horse's mouth. On the internet, we have an expression: <a href="http://rationalwiki.org/wiki/Don't_feed_the_Troll">"don't\u000afeed the troll"</a>. I guess the New York Times didn't get the memo?\u000aWhatever the case may be, the news media just helped feed the United States of\u000aAmerica to the biggest troll ever.</p>\u000a\u000a<h1>This is bad, what do we do?</h1>\u000a\u000a<p>Here are some possible things to try:</p>\u000a\u000a<ol>\u000a<li><p>Switch to a weekly news digest. At least this way events that happened early\u000ain the week will have had time to settle. The Guardian has a <a href="https://www.theguardian.com/weekly">weekly\u000aversion</a>, but I struggled to find a subscribable <a href="http://www.nytimes.com/newsletters">weekly news\u000adigest for the New York Times</a>.</p></li>\u000a<li><p>Read a more balanced news source that is not driven by engagement. My new\u000afavorite source for daily news is <a href="https://en.wikipedia.org/wiki/Portal:Current_events">Wikipedia's Current Events</a>. News\u000avia Wikipedia has a nice side benefit: context for the story is readily\u000aavailable in the form of other Wikipedia articles!</p></li>\u000a<li><p>Focus on more international content. The US is a special snowflake, but it's\u000anot <em>that</em> special. The world is increasingly global, but papers like the NYT\u000atend to weigh US politics very heavily. <a href="http://www.bbc.co.uk/worldserviceradio">BBC World Service</a> may be a\u000aremedy.</p></li>\u000a<li><p>Support your favorite news source through a subscription. This will reduce\u000atheir dependency on ads, which hopefully means less click-bait.</p></li>\u000a<li><p>Don't give Demagogues a platform. In this post, I have taken the Voldemort\u000atactic: avoid mentioning their name.</p></li>\u000a</ol>\u000a
p2536
tp2537
Rp2538
sg13
V/front-page-blues
p2539
sg15
Nsg16
I01
sg17
VFront page blues
p2540
sg20
VAccording to the [American Press Institute][api],\u000a\u000a> News is that part of communication that keeps us informed of the changing\u000a> events, issues, and characters in the world outside.
p2541
sg6
V<p>According to the <a href="https://www.americanpressinstitute.org/journalism-essentials/what-is-journalism/purpose-journalism/">American Press Institute</a>,</p>\u000a\u000a<blockquote>\u000a  <p>News is that part of communication that keeps us informed of the changing\u000a  events, issues, and characters in the world outside.</p>\u000a</blockquote>\u000a\u000a<p>There are many ways for news to be uninformative or even outright misleading.\u000aTwo trends in particular have received a lot of attention recently. The first is\u000asocial recommendation systems and selective unfollowing, which creates a\u000areality-distorting echo chamber. The second is fake news, which sure is in vogue\u000athese days, and is obviously a problem that we should tackle.</p>\u000a\u000a<p>This post is about a different trend: <a href="https://twitter.com/hamandcheese/status/801893793540796416">real news presented with misleading\u000afrequency</a>. The issue at stake is the media's ability to inform its readers\u000aand serve the public interest.</p>\u000a\u000a
p2542
sg25
g169
sg33
g2539
sg170
(dp2543
g172
S'Dec'
p2544
sg174
S'December 7, 2016'
p2545
sg176
I12
sg177
S'2016-12-07T09:00:00-00:00'
p2546
sg179
I1481130000
sg180
I2016
sg181
I7
ssg65
g182
sg31
S'front-page-blues'
p2547
sS'posted'
p2548
g188
(S'\x07\xe0\x0c\x07'
p2549
tp2550
Rp2551
ssg34
S'content/posts/2016/front-page-blues/index.md'
p2552
sg36
F1481176623.0
sa(dp2553
g2
(dp2554
g4
V My site has a little section called  Clippings . It's meant as a visual record of some of the things I've found inspiring on the web. How do I add new items to this visual record? Well, I'm glad you asked!    About a year ago, I cobbled together a Chrome extension for exactly this purpose: screen grabs from any webpage. Releasing it on the webstore has been on my backburner ever since. Over the last few weeks, I've spent a bit of time improving it and today, I'm ready to release it for broader testing. I call it  Inspirata . Inspirata can be downloaded from the  Web Store , and it works like this:      Click the Inspirata icon button.   Select part of the page to save.   Enter an optional caption, et voila!          
p2555
sg28
g7
(g8
g9
V<p>My site has a little section called <a href="http://smus.com/inspiration/">Clippings</a>. It's meant as a visual\u000arecord of some of the things I've found inspiring on the web. How do I add\u000anew items to this visual record? Well, I'm glad you asked!</p>\u000a\u000a<p>About a year ago, I cobbled together a Chrome extension for exactly this\u000apurpose: screen grabs from any webpage. Releasing it on the webstore has been on\u000amy backburner ever since. Over the last few weeks, I've spent a bit of time\u000aimproving it and today, I'm ready to release it for broader testing. I call it\u000a<a href="https://inspirata.xyz">Inspirata</a>. Inspirata can be downloaded from the <a href="https://chrome.google.com/webstore/detail/oaddmiclfpjkcehhbhhojmphflhlompo">Web Store</a>, and it\u000aworks like this:</p>\u000a\u000a<ol>\u000a<li>Click the Inspirata icon button.</li>\u000a<li>Select part of the page to save.</li>\u000a<li>Enter an optional caption, et voila!</li>\u000a</ol>\u000a\u000a<p><img src="inspirata.gif" alt="How Inspirata works video" /></p>\u000a\u000a<!--more-->\u000a\u000a<h1>Bookmarks and breadcrumbs</h1>\u000a\u000a<p>Bookmarks are like breadcrumbs. Hansel and Gretel left a trail of them to follow\u000ahome. But GPS made this application of breadcrumbs obsolete! In a similar\u000afashion, search engines killed bookmarking. Rather than browsing your curated\u000abookmarks to find your way to content, you can just search for it.</p>\u000a\u000a<p>Bookmarks, like breadcrumbs, go stale quickly. When a website goes down, the\u000abookmark becomes useless, just like Hansel's breadcrumbs which got eaten by\u000abirds. When a page does dark, your bookmark leaves no record of what used to be\u000athere. And when your bookmarking service gets turned down, say bye bye to your\u000acarefully curated archive!</p>\u000a\u000a<p>Bookmarks, like breadcrumbs may be <a href="http://del.icio.us/">delicious</a> but aren't very appealing. A\u000aURL has no appeal in itself, only the content at that URL does. And the bookmark\u000adoes not capture anything about that content: neither the content itself, nor\u000athe presentation, nor a deep link into which part of that content spoke to you.\u000aA pile of breadcrumbs, like a pile of bookmarks, is pretty nondescript.</p>\u000a\u000a<h1>Why?</h1>\u000a\u000a<p>I'm an avid user of little paper notebooks that I carry around in my pocket,\u000aalong with a trusty black pen. Sometimes while strolling down Valencia St on an\u000aerrand, I'll have an idea, stop and jot down it down using a wall or street\u000alight for support. This way I don't get sucked into my phone, and capture\u000awhatever's on my mind. Even if I don't revisit my note, the act of writing\u000aitself has served a purpose. This notion is found explicitly in the <a href="https://fieldnotesbrand.com/">Field\u000aNotes</a> tagline: "I'm not writing it down to remember it later, I'm\u000awriting it down to remember it now". </p>\u000a\u000a<p>Inspirata serves a similar purpose for content on the web. If something inspires\u000ame, I want to capture it, not for the purpose of revisiting later or sharing\u000asocially, but for the act itself. Perhaps a utilitarian argument can be made as\u000awell: being on the lookout for inspiration helps to maintain a sharp and active\u000aeye.</p>\u000a\u000a<p>I am interested in being more creative in my consumption. Over the last year, I\u000ahave developed a habit of writing a summary after finishing each book, as if I\u000awas going to share it with others. This forces me to gather my thoughts on the\u000asubject, sometimes even taking notes while reading or listening to make my\u000asummary more complete. I'm hoping this will increase retention and engagement.\u000aInspirata can perhaps serve a similar purpose in my web browsing endeavors.</p>\u000a\u000a<h1>Ease of use is (always) key</h1>\u000a\u000a<p>I continue using notebooks because of their amazing usability. The battery is\u000anever dead, it's quick to turn on (open up, uncap pen, good to go), and writing\u000ais a pleasant experience overall (perfect pen tip tracking, zero latency).\u000aTechnology at its finest!</p>\u000a\u000a<p>I tried to make Inspirata as minimal and convenient as possible. One click on\u000athe browser action button, select the area of interest, leave an optional\u000acomment, and you're set.</p>\u000a\u000a<p>There are a few entry points into capturing content:</p>\u000a\u000a<ul>\u000a<li>By clicking the Inspirata browser action (extension button), and clipping part\u000aof the webpage you're currently on.</li>\u000a<li>By right clicking an image and saving the inspiration.</li>\u000a<li>By selecting text and right clicking it as above, or by clicking the extension\u000abutton.</li>\u000a</ul>\u000a\u000a<p>In all cases, Inspirata ultimately saves an image, even if the content in\u000aquestion is text. An image of just the text can give interesting additional\u000acontext, such as layout and typography which is missing from the content\u000aitself.</p>\u000a\u000a<h1>Public by default and presentation agnostic</h1>\u000a\u000a<p>Images captured with Inspirata are added to a public Firebase and hosted in the\u000aFirebase bucket storage, which is part of the new <a href="https://firebase.google.com/docs/">Firebase 3.0\u000aplatform</a>. As an aside, the new Firebase platform is pretty amazing\u000aonce you work out some migration kinks. In terms of security, only you can\u000apublish new Inspirata for yourself, but your Inspirata are publically available. </p>\u000a\u000a<p>The Inspirata website <a href="https://inspirata.xyz">https://inspirata.xyz</a> provides a default public gallery\u000aview. To give you a sense of what this looks like, here is <a href="https://inspirata.xyz/?uid=US3UvWWOBhhi21AZgKkyUK0QTHL2">my public\u000agallery</a>. Each users' feed is <a href="https://project-4121485576010625868.firebaseio.com/users/US3UvWWOBhhi21AZgKkyUK0QTHL2.json">served as JSON</a>, which is how its\u000astored in Firebase. This means your list of inspirata can be presented in any\u000away you like on any embedding website (as I have done <a href="http://smus.com/inspiration/">on mine</a>). This, as\u000afar as I know, is not possible in other visual bookmarking services.</p>\u000a
p2556
tp2557
Rp2558
sg13
V/inspirata
p2559
sg15
Nsg16
I01
sg17
VInspirata: for what inspires you
p2560
sg20
VMy site has a little section called [Clippings][clip].
p2561
sg6
V<p>My site has a little section called <a href="http://smus.com/inspiration/">Clippings</a>. It's meant as a visual\u000arecord of some of the things I've found inspiring on the web. How do I add\u000anew items to this visual record? Well, I'm glad you asked!</p>\u000a\u000a<p>About a year ago, I cobbled together a Chrome extension for exactly this\u000apurpose: screen grabs from any webpage. Releasing it on the webstore has been on\u000amy backburner ever since. Over the last few weeks, I've spent a bit of time\u000aimproving it and today, I'm ready to release it for broader testing. I call it\u000a<a href="https://inspirata.xyz">Inspirata</a>. Inspirata can be downloaded from the <a href="https://chrome.google.com/webstore/detail/oaddmiclfpjkcehhbhhojmphflhlompo">Web Store</a>, and it\u000aworks like this:</p>\u000a\u000a<ol>\u000a<li>Click the Inspirata icon button.</li>\u000a<li>Select part of the page to save.</li>\u000a<li>Enter an optional caption, et voila!</li>\u000a</ol>\u000a\u000a<p><img src="inspirata.gif" alt="How Inspirata works video" /></p>\u000a\u000a
p2562
sg25
g169
sg33
g2559
sg170
(dp2563
g172
S'May'
p2564
sg174
S'May 31, 2016'
p2565
sg176
I5
sg177
S'2016-05-31T09:00:00-00:00'
p2566
sg179
I1464710400
sg180
I2016
sg181
I31
ssg65
g182
sg31
S'inspirata'
p2567
sS'posted'
p2568
g188
(S'\x07\xe0\x05\x1f'
p2569
tp2570
Rp2571
ssg34
S'content/posts/2016/inspirata/index.md'
p2572
sg36
F1464641517.0
sa(dp2573
g2
(dp2574
g4
V What would the web look like if there were no scrollbars, no mouse cursors, and no clickable links? That's what VR is like today. On one hand, this is great! Developers are completely free to build however they want, leading to a lot of interesting experiments. On the other hand, it takes a lot of engineering effort to just get basic interactions up and running. Furthermore, it lacks consistency. The alluring promise of being able to navigate from world to world may be diluted by the frustration of having to rediscover new interaction paradigms every time.    While sane interaction defaults are badly needed, baking them into the platform violates principles of the  Extensible Web . With that in mind, I implemented a basic Ray-based interaction library called  RayInput , which provides reasonable defaults for interacting with 3D objects in and outside of VR. Here's what the interaction looks like on various platforms:       
p2575
sg28
g7
(g8
g9
V<p>What would the web look like if there were no scrollbars, no mouse cursors, and\u000ano clickable links? That's what VR is like today. On one hand, this is great!\u000aDevelopers are completely free to build however they want, leading to a lot of\u000ainteresting experiments. On the other hand, it takes a lot of engineering effort\u000ato just get basic interactions up and running. Furthermore, it lacks\u000aconsistency. The alluring promise of being able to navigate from world to world\u000amay be diluted by the frustration of having to rediscover new interaction\u000aparadigms every time.</p>\u000a\u000a<p>While sane interaction defaults are badly needed, baking them into the platform\u000aviolates principles of the <a href="https://github.com/extensibleweb/manifesto">Extensible Web</a>. With that in mind, I\u000aimplemented a basic Ray-based interaction library called <a href="https://github.com/borismus/ray-input">RayInput</a>, which\u000aprovides reasonable defaults for interacting with 3D objects in and outside of\u000aVR. Here's what the interaction looks like on various platforms:</p>\u000a\u000a<iframe width="600" height="340" src="https://www.youtube.com/embed/gjj2XQYC998" frameborder="0" allowfullscreen></iframe>\u000a\u000a<!--more-->\u000a\u000a<h2>What does Ray Input actually do?</h2>\u000a\u000a<p>Ray Input aims to provide reasonable interaction defaults, relying on the\u000ahardware available for each platform:</p>\u000a\u000a<ul>\u000a<li>On desktop, look around by dragging, interact by clicking.</li>\u000a<li>On mobile, look around via magic window or touch pan, interact by tapping.</li>\u000a<li>In VR, interaction depends on a reticle or on a ray.\u000a<ul>\u000a<li>If there is no controller (eg. Cardboard), use a gaze based reticle to\u000ainteract with objects.</li>\u000a<li>If there is a 3DOF controller (eg. Daydream), apply an arm model and\u000ainteract with objects using a ray emanating from the controller.</li>\u000a<li>If there is a 6DOF controller, interact with objects using the ray.</li>\u000a</ul></li>\u000a</ul>\u000a\u000a<p>Of course, you may want to customize your interactions on a per-platform basis.\u000aFor example, if you are developing an application primarily for the Vive, you\u000amay want to take advantage of the specific richness that a Vive controller\u000aprovides. Ray Input is not meant to be prescriptive, merely to provide\u000areasonable defaults.</p>\u000a\u000a<h2>API</h2>\u000a\u000a<p>The library's API is documented on the <a href="https://github.com/borismus/ray-input">github page</a>, and I also provide a\u000a<a href="https://borismus.github.io/ray-input">simple example that uses Ray Input</a> to pick items from a 2D menu.</p>\u000a\u000a<h2>Arm models for orientation-only controllers</h2>\u000a\u000a<p>If a VR controller is present, Ray Input defaults to using a ray-based input\u000amethod, which behaves much like a laser pointer.</p>\u000a\u000a<p>The Daydream View controller is not position tracked. The only pose information\u000ait provides is the orientation, which is in the same coordinate system as the\u000ahead. Where should we position such orientation-only (3DOF) controllers? In\u000aparticular, where should the ray come from? Having it emanate from the stomach\u000aor head, like the arm of an exotic god, would be very unnatural. So we need to\u000abe slightly more clever.</p>\u000a\u000a<p>Enter the arm model, which, given a controller orientation, spits out a\u000aplausible controller position. Obviously the position it provides is only a\u000areasonable guess, and may not correspond to the controller's actual position.\u000aBut it sure is a lot better than nothing. This sort of problem is common in\u000agraphics and robotics, and can be solved with inverse kinematics.</p>\u000a\u000a<p>In this case, we follow a simpler approach. Most of the <a href="https://github.com/borismus/ray-input/blob/master/src/orientation-arm-model.js">code to do\u000athis</a> is lifted from a native implementation of Daydream arm model.\u000aTo debug it, I built a very rough simulator, which lets you specify the\u000aorientation of a virtual head and hand, run it through the model, and visualize\u000athe resulting pose of the controller:</p>\u000a\u000a<p><a href="https://borismus.github.io/ray-input/daydream-simulator.html"><img src="arm-model.png" alt="Daydream arm model simulator" /></a></p>\u000a\u000a<p>As always, very open to feedback, bug reports, and pull requests via\u000a<a href="https://github.com/borismus/ray-input">github</a>.</p>\u000a
p2576
tp2577
Rp2578
sg13
V/ray-input-webvr-interaction-patterns
p2579
sg15
Nsg16
I01
sg17
VRay Input: WebVR interaction patterns
p2580
sg20
VWhat would the web look like if there were no scrollbars, no mouse cursors, and\u000ano clickable links? That's what VR is like today.
p2581
sg6
V<p>What would the web look like if there were no scrollbars, no mouse cursors, and\u000ano clickable links? That's what VR is like today. On one hand, this is great!\u000aDevelopers are completely free to build however they want, leading to a lot of\u000ainteresting experiments. On the other hand, it takes a lot of engineering effort\u000ato just get basic interactions up and running. Furthermore, it lacks\u000aconsistency. The alluring promise of being able to navigate from world to world\u000amay be diluted by the frustration of having to rediscover new interaction\u000aparadigms every time.</p>\u000a\u000a<p>While sane interaction defaults are badly needed, baking them into the platform\u000aviolates principles of the <a href="https://github.com/extensibleweb/manifesto">Extensible Web</a>. With that in mind, I\u000aimplemented a basic Ray-based interaction library called <a href="https://github.com/borismus/ray-input">RayInput</a>, which\u000aprovides reasonable defaults for interacting with 3D objects in and outside of\u000aVR. Here's what the interaction looks like on various platforms:</p>\u000a\u000a<iframe width="600" height="340" src="https://www.youtube.com/embed/gjj2XQYC998" frameborder="0" allowfullscreen></iframe>\u000a\u000a
p2582
sg25
g169
sg33
g2579
sg170
(dp2583
g172
S'Oct'
p2584
sg174
S'October 11, 2016'
p2585
sg176
I10
sg177
S'2016-10-11T09:00:00-00:00'
p2586
sg179
I1476201600
sg180
I2016
sg181
I11
ssg65
g182
sg31
S'ray-input-webvr-interaction-patterns'
p2587
sS'posted'
p2588
g188
(S'\x07\xe0\n\x0b'
p2589
tp2590
Rp2591
ssg34
S'content/posts/2016/ray-input-webvr-interaction-patterns/index.md'
p2592
sg36
F1476812820.0
sa(dp2593
g2
(dp2594
g4
V Economic inequality is rising in the US. A viral video from several years ago made this abundantly clear:           The gap between desire, expectation and reality is truly shocking, and inspired me to learn more. In particular, whether or not inequality is actually a big problem, and then to better understand issues that the video above did not address:      How did the US become so economically unequal?   How can this inequality be reduced?      My answers come in the form of simple simulations. For example, the following simulation has two agents with different salaries, but the same spending habits. You can  play with it  yourself!              In the first part of this post, I try to provide some background on economic inequality: how to measure it, various forms of it, and whether or not it's a problem.  In the last part, I try to explain how we got to the status quo, and how inequality can potentially be reduced. Rather than just making claims, I use simulations like the one above to defend my claims. This way, you can see more clearly where I'm coming from, and if you disagree, you can  make your own simulation  with better assumptions.   
p2595
sg28
g7
(g8
g9
V<p>Economic inequality is rising in the US. A viral video from several years ago\u000amade this abundantly clear:</p>\u000a\u000a<p><a href="https://www.youtube.com/watch?v=QPKKQnijnsM"><img src="video_small.jpg" alt="Wealth inequality in U.S." /></a></p>\u000a\u000a<p>The gap between desire, expectation and reality is truly shocking, and inspired\u000ame to learn more. In particular, whether or not inequality is actually a big\u000aproblem, and then to better understand issues that the video above did not\u000aaddress:</p>\u000a\u000a<ol>\u000a<li>How did the US become so economically unequal?</li>\u000a<li>How can this inequality be reduced?</li>\u000a</ol>\u000a\u000a<p>My answers come in the form of simple simulations. For example, the following\u000asimulation has two agents with different salaries, but the same spending habits.\u000aYou can <a href="http://borismus.github.io/inequality-simulator/?model=1-world-income-ineq-doesnt-lead-to-wealth-ineq.js">play with it</a> yourself!</p>\u000a\u000a<p><a href="http://borismus.github.io/inequality-simulator/?model=1-world-income-ineq-doesnt-lead-to-wealth-ineq.js">\u000a<video src="simulator.mp4" autoplay loop style="width: 100%"></video>\u000a</a></p>\u000a\u000a<p>In the first part of this post, I try to provide some background on economic\u000ainequality: how to measure it, various forms of it, and whether or not it's a\u000aproblem.  In the last part, I try to explain how we got to the status quo, and\u000ahow inequality can potentially be reduced. Rather than just making claims, I use\u000asimulations like the one above to defend my claims. This way, you can see more\u000aclearly where I'm coming from, and if you disagree, you can <a href="http://github.com/borismus/inequality-simulator">make your own\u000asimulation</a> with better assumptions.</p>\u000a\u000a<!--more-->\u000a\u000a<h2>What is economic inequality?</h2>\u000a\u000a<p>There are <a href="http://www.economist.com/blogs/freeexchange/2014/07/measuring-inequality">three main ways</a> to measure economic inequality: income,\u000aconsumption, and wealth. Income inequality in particular has become a huge\u000anational issue, with <a href="https://berniesanders.com/issues/income-and-wealth-inequality/">some presidential candidates</a> focusing large\u000aamounts of their time addressing it directly, with policies such as the $15\u000afederal minimum wage.</p>\u000a\u000a<p>Inequality can be measured using the Gini coefficient. The greater the Gini, the\u000amore unequal a society is. A Gini of 0 means perfect equality: everybody has the\u000asame. A Gini of 1 means perfect tyranny: the winner takes it all. This metric\u000acan be applied to any distribution: wealth, income, or consumption.</p>\u000a\u000a<p>All forms of inequality are unequal, but some are more unequal than others! In\u000ageneral, wealth is the most unequally distributed of the three indicators,\u000aconsumption the least. But which measure of inequality is most important to\u000aconsider?</p>\u000a\u000a<h3>Types of economic inequality</h3>\u000a\u000a<p>Income inequality is difficult to measure. What constitutes income? Obviously a\u000asalary is included, but how about investment income? Unsold stocks? Options?\u000aThe <a href="https://en.wikipedia.org/wiki/One-dollar_salary">list of $1 salary CEOs</a> is famously long, but what is their effective\u000aincome? Pew Research provides <a href="http://www.pewresearch.org/fact-tank/2015/09/22/the-many-ways-to-measure-economic-inequality/">many more reasons</a> why income is hard to\u000ameasure, and may not be a meaningful indicator:</p>\u000a\u000a<blockquote>\u000a  <p>Some economists say income data have too many flaws to be the primary measure\u000a  of inequality. For one thing, many income inequality measures use income\u000a  before accounting for the impact of taxes and transfer payments. [...]\u000a  In addition, critics of the income-based approach note that an individual\u2019s\u000a  (or household\u2019s) income can vary considerably over time, and may not reflect\u000a  all available economic resources.</p>\u000a</blockquote>\u000a\u000a<p>Income also isn't a great indicator for quality of life. Indeed, many economists\u000aagree that <a href="http://www.aei.org/wp-content/uploads/2012/06/-a-new-measure-of-consumption-inequality_142931647663.pdf">consumption inequality</a> is a better proxy for that.\u000a<a href="http://www.economist.com/blogs/freeexchange/2014/07/measuring-inequality">The Economist</a> writes:</p>\u000a\u000a<blockquote>\u000a  <p>For the purpose of measuring how inequality affects a community [income\u000a  inequality] is also probably the least interesting yardstick of the three.\u000a  Consumption inequality, though harder to measure, provides a better proxy of\u000a  social welfare. This is because people\u2019s living standards depend on the amount\u000a  of goods and services they consume, rather than the number of dollars in their\u000a  wage packet.</p>\u000a</blockquote>\u000a\u000a<p>But consumption inequality has its limitations too. For one, it is difficult to\u000ameasure directly. More importantly, consumption is an indication of the current\u000astate, but does not reflect ones ability to deal with the future. Wealth and\u000aconsumption are tightly linked. When times get tough, only the wealthy can\u000amaintain their lifestyle by dipping into their savings. Having this reserve of\u000a"potential energy" is especially important in inevitable periods of instability.</p>\u000a\u000a<p>This leaves us with wealth inequality, which I will focus on from now on.</p>\u000a\u000a<h3>Adverse effects of extreme wealth inequality</h3>\u000a\u000a<p>There are plenty of arguments to be made for dangers of high wealth inequality.\u000aThe common sense reason is the diminishing marginal utility of wealth. For an\u000aunemployed person, suddenly having a job that pays $40K is a game changer. But\u000afor a top-1% income earner already making $500K, the additional $40K makes no\u000apractical difference.</p>\u000a\u000a<p>Another economic argument goes something like this. Low wealth causes reduced\u000apurchasing power, which ultimately means less money going to corporations, fewer\u000ajobs, and a slower economy. More people should have spending power, which will\u000akeep our economy running smoothly. Robert Reich makes this point well in his\u000amoving <a href="http://www.pbs.org/newshour/making-sense/why-robert-reich-cares-so-passionately-about-economic-inequality/">Inequality for All</a>. However, the link between low wealth and\u000areduced spending is somewhat tenuous, given the much less extreme consumption\u000ainequality distribution.</p>\u000a\u000a<p>Wealth as potential energy also has a psychological dimension. Wealth gives some\u000apeace of mind that you have a buffer against unforseeable problems, increasing\u000awell being. This is especially important in countries with weak social programs\u000aand relatively small safety nets. In such scenarios, more people feel the need\u000ato accumulate wealth as a personal buffer.</p>\u000a\u000a<p>There is also something philosophically unfair about wealth concentration. What\u000amakes a society fair is a matter of opinion, but the <a href="https://en.wikipedia.org/wiki/Original_position">Original Position</a>, a\u000athought experiment proposed by John Rawls provides an interesting starting\u000apoint. In the Original Position, you and your hypothetical countrymen select\u000aprinciples that will determine the basic structure of the society you will live\u000ain. This choice is made from behind what he calls a veil of ignorance, which\u000aprevents you from knowing your economic status.</p>\u000a\u000a<p>Being born into a society with high wealth inequality, you are subject to a\u000a"lottery of birth". Quoth <a href="http://www.economist.com/blogs/freeexchange/2014/07/measuring-inequality">The Economist</a>:</p>\u000a\u000a<blockquote>\u000a  <p>Wealth is also an important metric since it can be inherited, unlike income.\u000a  When wealth inequality increases, the lottery of birth becomes an increasingly\u000a  important determinant of living standards. Consequently, a society which wants\u000a  to ensure an equal level of opportunity, in which outcomes are not closely\u000a  linked to surnames, will endeavour to keep wealth inequality at tolerably low\u000a  levels.</p>\u000a</blockquote>\u000a\u000a<h3>Some wealth inequality is good</h3>\u000a\u000a<p>Yet clearly we don't want complete economic equality. It's important that people\u000awork and create value. The best way to do this is to incentivize them by\u000arewarding high performing individuals. History has shown socialist societies\u000alike the Soviet Union fail in part because there was no incentive to work. In Soviet\u000aRussia, wealth inequality was low: everybody except the ruling class had the\u000asame amount of the sad little pie. A small piece of a much larger (eg. American)\u000apie is better than an average slice of a small (eg. Soviet) one. This <a href="https://www.khanacademy.org/economics-finance-domain/macroeconomics/gdp-topic/piketty-capital/v/inequality-good-or-bad">Khan\u000aAcademy video</a> makes this point well.</p>\u000a\u000a<p>Wealth inequality alone is not a great indicator of prosperity either. Many\u000aScandinavian countries have very high wealth inequality, potentially because\u000alife is already so good. According to <a href="http://www.businessinsider.com/why-socialist-scandinavia-has-some-of-the-highest-inequality-in-europe-2014-10">Credit Suisse</a>,</p>\u000a\u000a<blockquote>\u000a  <p>Strong social security programs, good public pensions, free higher education\u000a  or generous student loans, unemployment and health insurance can greatly\u000a  reduce the need for personal financial assets.</p>\u000a</blockquote>\u000a\u000a<p>Wealth acts as a personal safety net. In countries with significant public\u000asafety nets for ailing citizens, accumulating wealth is less important. Compare\u000aa society with high inequality but a solid public safety net, with one with\u000aequality but no public safety net. From the perspective of Rawls' Original\u000aPosition, the solid safety net is preferred, since even if you are the poorest\u000ain such a society, you are still guaranteed a standard of living.</p>\u000a\u000a<h2>Modeling inequality</h2>\u000a\u000a<p>Inspired by <a href="http://worrydream.com/">Bret Victor</a>, <a href="http://ncase.me/">Nicky Case</a>, and chats with <a href="http://mikejohnstn.com/">Mike\u000aJohnston</a>, I've built a visualizer to give an <a href="http://worrydream.com/ExplorableExplanations/">explorable explanation</a>\u000aof how wealth inequality arises, and how policy changes can reduce it.</p>\u000a\u000a<p>The simulation itself is very simple, consisting of a set of rules which can be\u000adefined in JSON, and then households that have property bags. Every year, each\u000arule is applied to each household in order. The result of each rule is some\u000achange in the net worth of the household. Each simulation is defined by a\u000acollection of rules and actors. In the GUI, you can inspect rules and actors by\u000aclicking on them. The visualization itself is implemented in <a href="http://threejs.org/">three.js</a>.\u000aFor more information, check out the <a href="http://github.com/borismus/inequality-simulator">github</a> page.</p>\u000a\u000a<p>Using these models, let's jump in and explore some factors contributing to our\u000acurrent state of wealth inequality. Then, some policies that can change the\u000astatus quo.</p>\u000a\u000a<h3>Cause 1: Income inequality</h3>\u000a\u000a<p>In this first simulation, we consider two households: one with low income and\u000aone with high income. They have the same spending habits, but the high income\u000ahousehold has twice the income.</p>\u000a\u000a<p><a href="http://borismus.github.io/inequality-simulator/?model=1-world-income-ineq-doesnt-lead-to-wealth-ineq.js"><img src="screenshots/1-world-income-ineq-doesnt-lead-to-wealth-ineq.png" alt="Income inequality not sole cause wealth inequality." /></a></p>\u000a\u000a<p>As you can see above (or if you click the image and <a href="http://borismus.github.io/inequality-simulator/?model=1-world-income-ineq-doesnt-lead-to-wealth-ineq.js">run the simulation\u000ayourself</a>), such a scenario does not yield huge differences in wealth.\u000aEven a hundred years later, wealth remains proportional to income, so we look to\u000aother factors to explain the wealth inequality we see today.</p>\u000a\u000a<h3>Cause 2: Investors win over the long term</h3>\u000a\u000a<p>In addition to salaries, households can also invest money. For simplicity,\u000aassume that the net worth of each household is subject to some investment\u000areturn. Most Americans (52%) <a href="http://www.gallup.com/poll/182816/little-change-percentage-americans-invested-market.aspx">avoid the stock market</a> entirely, which\u000acuts them out from any investment income.</p>\u000a\u000a<p>In the next simulation, one household does not invest at all, and another\u000ahousehold invests its whole net worth. We assume that the yield is the average\u000areturn of the market, which is about <a href="http://www.marketwatch.com/story/8-lessons-from-80-years-of-market-history-2014-11-19">10% from 1930 to 2013</a>.</p>\u000a\u000a<p><a href="http://borismus.github.io/inequality-simulator/?model=2-investing-ability.js"><img src="screenshots/2-investing-ability.png" alt="Simulation of investors vs. non-investors" /></a></p>\u000a\u000a<p>This is effectively a demonstration of compound interest. Given the <a href="http://www.bloombergview.com/articles/2015-09-25/the-rich-are-different-they-re-better-investors-">correlation\u000abetween wealth and investment ability</a>, the impact of investing is huge on\u000awealth inequality.</p>\u000a\u000a<h3>Cause 3: Entrepreneurship can have huge payoff</h3>\u000a\u000a<p>Many of the wealthiest people in the world became so by creating new companies.\u000aMost enterprises fail, but it only takes one incredible success to generate\u000amassive amounts of wealth.</p>\u000a\u000a<p>I found that modeling this accurately is very difficult, but for the purposes of\u000aillustration, this next simulation includes three households: a\u000anon-entrepreneur, a regular entrepreneur, and a lucky entrepreneur. An\u000aenterprise failure (10% yearly chance) is modeled as a 5% reduction in wealth,\u000awhile a success (1% yearly chance, 2% if lucky) is modeled as a 50% increase in\u000awealth.</p>\u000a\u000a<p>The expected wealth of the regular entrepreneur is the same as the\u000anon-entrepreneur, but the lucky entrepreneur has a 2% chance of success, and\u000athus a higher expected wealth. Entrepreneurship introduces volatility and can\u000alead to more wealth inequality.</p>\u000a\u000a<p><a href="http://borismus.github.io/inequality-simulator/?model=3-with-entrepreneurship.js"><img src="screenshots/3-with-entrepreneurship.png" alt="Simulation of entrepreneurs vs. non-entrepreneurs" /></a></p>\u000a\u000a<h3>All together: income, investments, and entrepreneurship</h3>\u000a\u000a<p>Consider all of these factors together: varying salaries, investment abilities\u000aand entrepreneurial inclinations/luck. Here we have 8 agents with varying\u000aparameters along these dimensions.</p>\u000a\u000a<p><a href="http://borismus.github.io/inequality-simulator/?model=4-income-invest-entrepreneur.js"><img src="screenshots/4-income-invest-entrepreneur.png" alt="Simulation of entrepreneurs vs. non-entrepreneurs" /></a></p>\u000a\u000a<p>We can see that after 50 years, we have a Gini of 0.44. In the real world, the\u000aspread of incomes is much greater than here, the most successful entrepreneurs\u000amake orders of magnitudes more than regular employees, and the best investors\u000aare wildly successful. The <a href="https://docs.google.com/spreadsheets/d/1HZBF47q9DYgFj49Q2ZLBb_3gpsoayAuGF_kUdYIM9nQ/pubhtml?gid=0&amp;single=true">real world wealth Gini</a> of the US is\u000a0.8.</p>\u000a\u000a<h2>Reducing inequality</h2>\u000a\u000a<p>Working within the system, inequality can be reduced through progressive\u000ataxation of the wealthy. However, it's key to avoid becoming a <a href="https://wiki.lesswrong.com/wiki/Paperclip_maximizer">paperclip\u000amaximizer</a> when it comes to the Gini coefficient. Making the\u000awealthiest slightly less wealthy will certainly reduce the Gini, but will do\u000alittle to improve life for actual poor people.</p>\u000a\u000a<p>Through additional taxation, the wealthy end up being less wealthy, with the\u000adifference going to the government. Implied is a hope that the government is\u000acapable and sufficiently efficient to use this extra money for good. By\u000ainvesting in public works, creating relevant jobs, and establishing a more solid\u000asafety net, there is potential to improve lives of those that are less\u000afortunate.</p>\u000a\u000a<h3>Solution 1: Tax capital gains like income</h3>\u000a\u000a<p>Compound interest is a powerful force. Once an individual's wealth is large\u000aenough, returns on investing their wealth will exceed even their salary.\u000aHowever, the US currently imposes a very low capital gains tax, a long-term\u000acapital gain tax rate of 15% for most normal annual incomes.</p>\u000a\u000a<p>An easy solution is to increase capital gains taxes, or simply to treat capital\u000agains like regular income. This would effectively reduce the return rate on\u000ainvestment and reduce inequality. The following simulation shows what happens\u000awhen investment income is taxed at a flat 40%. This is a crude estimate, since\u000ait would actually be subject to a variable tax rate like the income tax, but\u000agets the point across.</p>\u000a\u000a<p><a href="http://borismus.github.io/inequality-simulator/?model=5-higher-capital-gains-tax.js"><img src="screenshots/5-higher-capital-gains-tax.png" alt="Simulation of higher capital gains." /></a></p>\u000a\u000a<p>As you can see, the Gini at 100 years is much smaller than before.</p>\u000a\u000a<h3>Solution 2: Estate taxes</h3>\u000a\u000a<p>Estate tax is intended as an effective tool for preventing the concentration of\u000awealth in the hands of a relatively few powerful families. It also encourages\u000acharitable giving, since the money that is to be bequeathed is subject to the\u000atax.</p>\u000a\u000a<p>Estate tax is collected when the deceased transfers their wealth to the\u000arecipient of their inheritance. The deceased's net worth exceeding a certain\u000athreshold is subject to the estate tax rate. Both the threshold and the tax rate\u000ahave varied a surprising <a href="https://docs.google.com/spreadsheets/d/1lWzzz6RlMmxWTGYoU9kxxXqL9Q8Pto4cbzZRozVc8wU/pubhtml">amount over time</a>:</p>\u000a\u000a<p><img src="estate-tax-history.png" alt="Historical estate taxes" /></p>\u000a\u000a<p>In a previous simulation, we saw what would have happened with no estate tax (as\u000awas the case in 2010, a good year to die). The following simulation shows the\u000aaverage tax rate since 2000, which is 41%, with a threshold of 100 units.</p>\u000a\u000a<p><a href="http://borismus.github.io/inequality-simulator/?model=6-estate-tax.js"><img src="screenshots/6-estate-tax.png" alt="Simulation of entrepreneurs vs. non-entrepreneurs" /></a></p>\u000a\u000a<h3>Solution 3: Wealth taxes</h3>\u000a\u000a<p><a href="http://www.cnbc.com/2015/03/10/why-we-need-a-global-wealth-tax-piketty.html">Piketty's solution</a> to inequality is a global wealth tax. The idea is\u000athat individuals with over a certain amount of wealth (here, 100 units) be taxed\u000aat some rate for just maintaining that level of wealth. This seems difficult to\u000aenforce, especially since in a global economy, a single neutral country\u000a(say, Switzerland?) that does not impose a wealth tax will end up being a\u000anatural safe haven for the rich.</p>\u000a\u000a<p><a href="http://borismus.github.io/inequality-simulator/?model=7-wealth-tax.js"><img src="screenshots/7-wealth-tax.png" alt="Simulation of wealth taxes" /></a></p>\u000a\u000a<h2>Conclusion</h2>\u000a\u000a<p>Geez, you're still here?</p>\u000a\u000a<h3>Practical limitations</h3>\u000a\u000a<p>Theoretically, inequality is not an insurmountable issue by any stretch. As\u000ashown, by introducing policies like increased capital gains tax, estate tax and\u000awealth tax, inequality can be reduced. The real question is how much inequality\u000ais actually desirable, and how effective the above policies are in practice.</p>\u000a\u000a<p><a href="http://borismus.github.io/inequality-simulator/?model=8-solution.js"><img src="screenshots/8-solution.png" alt="Estate taxes, wealth taxes and " /></a></p>\u000a\u000a<p>In practice, estate tax is often avoided or minimized, <a href="http://www.calculator.net/estate-tax-calculator.html">according to the\u000aUrban-Brookings Tax Policy Center</a>,</p>\u000a\u000a<blockquote>\u000a  <p>Among the 3,780 estates that owe any tax, the "effective" tax rate \u2014 that is,\u000a  the percentage of the estate's value that is paid in taxes \u2014 is 16.6 percent,\u000a  on average. </p>\u000a</blockquote>\u000a\u000a<p>A wealth tax is even harder to enforce, since you can simply move your wealth to\u000aa country that does not have a wealth tax.</p>\u000a\u000a<h2>Conclusion</h2>\u000a\u000a<p><strong>Roll your own</strong>. The good news is that the models above show how inequality\u000acan arise and how inequality can be effectively reduced! The bad news is that I\u000ajust made these models up with only a minimal understanding of how the world\u000aworks. However, more good news! If you are wise in the ways of economics and/or\u000ahave a suggestion for a more accurate, or perhaps more provocative way of\u000amodeling wealth inequality, get in touch! Or if you just want to DIY, simulation\u000aand visualizer are <a href="http://github.com/borismus/inequality-simulator">on the githubs</a>.</p>\u000a\u000a<p><strong>Eyes on the prize</strong>. <a href="https://docs.google.com/spreadsheets/d/1HZBF47q9DYgFj49Q2ZLBb_3gpsoayAuGF_kUdYIM9nQ/pubhtml?gid=0&amp;single=true">Zimbabwe and Denmark</a> both have high\u000awealth ginis (over 0.8), while <a href="https://docs.google.com/spreadsheets/d/1HZBF47q9DYgFj49Q2ZLBb_3gpsoayAuGF_kUdYIM9nQ/pubhtml?gid=0&amp;single=true">Yemen and Japan</a> both have low\u000awealth ginis (under 0.6), yet these pairs of countries couldn't be more\u000adifferent. Wealth inequality in itself is not really the problem, just an\u000aindicator. The Rawlsian goal is not to reduce it arbitrarily, but to make life\u000aactually better for everybody.</p>\u000a\u000a<p><strong>Micro to macro</strong>. The simple two household simulations I started with feel\u000alike microeconomics. The more complex simulations we ended with started feeling\u000amore like something from macroeconomics. Put another way, each household starts\u000awith just a couple of bars of wealth, but as the simulation proceeds, the canvas\u000abegins to resemble a bar chart. I found this quantity-to-quality transition\u000afascinating.</p>\u000a\u000a<p><strong>On simulations</strong>. I'm intrigued by simulations as way of explaining\u000acomplicated things to non-experts. However, any simulation is inherently\u000ainaccurate, as it approximates the real world in order to have explanatory\u000apower. In other words, there is some continuum between accuracy and\u000ainsight. The simulations in this post are more simple than they are realistic,\u000ahowever, I hope they are at least somewhat informative and interesting.</p>\u000a
p2596
tp2597
Rp2598
sg13
V/simulating-wealth-inequality
p2599
sg15
Nsg16
I01
sg17
VSimulating wealth inequality
p2600
sg20
VEconomic inequality is rising in the US.
p2601
sg6
V<p>Economic inequality is rising in the US. A viral video from several years ago\u000amade this abundantly clear:</p>\u000a\u000a<p><a href="https://www.youtube.com/watch?v=QPKKQnijnsM"><img src="video_small.jpg" alt="Wealth inequality in U.S." /></a></p>\u000a\u000a<p>The gap between desire, expectation and reality is truly shocking, and inspired\u000ame to learn more. In particular, whether or not inequality is actually a big\u000aproblem, and then to better understand issues that the video above did not\u000aaddress:</p>\u000a\u000a<ol>\u000a<li>How did the US become so economically unequal?</li>\u000a<li>How can this inequality be reduced?</li>\u000a</ol>\u000a\u000a<p>My answers come in the form of simple simulations. For example, the following\u000asimulation has two agents with different salaries, but the same spending habits.\u000aYou can <a href="http://borismus.github.io/inequality-simulator/?model=1-world-income-ineq-doesnt-lead-to-wealth-ineq.js">play with it</a> yourself!</p>\u000a\u000a<p><a href="http://borismus.github.io/inequality-simulator/?model=1-world-income-ineq-doesnt-lead-to-wealth-ineq.js">\u000a<video src="simulator.mp4" autoplay loop style="width: 100%"></video>\u000a</a></p>\u000a\u000a<p>In the first part of this post, I try to provide some background on economic\u000ainequality: how to measure it, various forms of it, and whether or not it's a\u000aproblem.  In the last part, I try to explain how we got to the status quo, and\u000ahow inequality can potentially be reduced. Rather than just making claims, I use\u000asimulations like the one above to defend my claims. This way, you can see more\u000aclearly where I'm coming from, and if you disagree, you can <a href="http://github.com/borismus/inequality-simulator">make your own\u000asimulation</a> with better assumptions.</p>\u000a\u000a
p2602
sg25
g169
sg33
g2599
sg170
(dp2603
g172
S'Jan'
p2604
sg174
S'January 19, 2016'
p2605
sg176
I1
sg177
S'2016-01-19T09:00:00-00:00'
p2606
sg179
I1453222800
sg180
I2016
sg181
I19
ssg65
g182
sg31
S'simulating-wealth-inequality'
p2607
sS'posted'
p2608
g188
(S'\x07\xe0\x01\x13'
p2609
tp2610
Rp2611
ssg34
S'content/posts/2016/simulating-wealth-inequality/index.md'
p2612
sg36
F1453228372.0
sa(dp2613
g2
(dp2614
g4
V Immersion requires a large field of view. This could be achieved by putting a large curved spherical display on your face, but alas such technology is prohibitively expensive. A more affordable solution to increasing the field of view is to look at small ubiquitous rectangular displays through lenses:         Lenses placed close to your eyes greatly increase your field of view, but there is a cost: the image becomes spherically distorted. The larger the field of view, the more distorted the image. This post is a quick summary of three different approaches to undistorting the image, all of which have been implemented in JavaScript for various WebVR-related projects.   
p2615
sg28
g7
(g8
g9
V<p>Immersion requires a large field of view. This could be achieved by putting a\u000alarge curved spherical display on your face, but alas such technology is\u000aprohibitively expensive. A more affordable solution to increasing the field of\u000aview is to look at small ubiquitous rectangular displays through lenses:</p>\u000a\u000a<p><img src="how-lenses-increase-fov.png" alt="Why VR needs lenses" /></p>\u000a\u000a<p>Lenses placed close to your eyes greatly increase your field of view, but there\u000ais a cost: the image becomes spherically distorted. The larger the field of\u000aview, the more distorted the image. This post is a quick summary of three\u000adifferent approaches to undistorting the image, all of which have been\u000aimplemented in JavaScript for various WebVR-related projects.</p>\u000a\u000a<!--more-->\u000a\u000a<p>Here is a closer look at the lens distortion of a typical head mounted display.\u000aThe lenses cause a pincushion effect:</p>\u000a\u000a<p><img src="pincushion-distortion.png" alt="Pincushion distortion due to lenses" /></p>\u000a\u000a<p>The solution is to apply barrel distortion to the image. When we look at it through\u000athe distorting lenses, the image looks neutral:</p>\u000a\u000a<p><img src="barrel-predistortion.png" alt="Barrel pre-distortion" /></p>\u000a\u000a<p>Lens distortion is well understood mathematically, governed by equations <a href="https://en.wikipedia.org/wiki/Distortion_(optics)#Software_correction">like\u000athese</a>, with distortion coefficients corresponding to the particular lens.\u000aTo undo the distortion properly, we also need to calculate the centers of the\u000aeyes, which requires knowing a bit about the geometry of the display and the\u000aenclosure itself. This can all be done, even on the web! I summarize a few\u000aimplementation options below.</p>\u000a\u000a<h1>1. Fragment based solution (bad)</h1>\u000a\u000a<p>The simplest way to using two pass rendering. First, we render the left and\u000aright eyes onto a texture, and then process that texture with a fragment (pixel)\u000ashader, moving each pixel inward in relation to the centroid of the eye:</p>\u000a\u000a<p><img src="dense.png" alt="Per-pixel based distortion" /></p>\u000a\u000a<p>This is the first and simplest method, which is also the least efficient, since\u000aeach pixel is processed separately. The <a href="https://github.com/borismus/webvr-boilerplate/blob/d91cc2866bd54e65d59022800f62c7e160dc9fee/src/cardboard-distorter.js">first version</a> of the\u000aWebVR Boilerplate implemented this method.</p>\u000a\u000a<h1>2. Mesh based solution (better)</h1>\u000a\u000a<p>Rather than processing each pixel separately, we distort the vertices of a\u000arelatively sparse mesh (40x20 works well). </p>\u000a\u000a<p><img src="sparse.png" alt="Mesh based distortion" /></p>\u000a\u000a<p>This can save some direct computation and let the GPU do a fair amount of\u000ainterpolation. Rather than having to apply to every single pixel (<code>1920 * 1080 ~=\u000a2e6</code>), we do the calculation for every vertex in the mesh (<code>40 * 20 = 800</code>). The\u000aresult is a significant reduction (3 magnitudes or so) of computation, and a\u000anice boost in performance. The <a href="https://github.com/borismus/webvr-polyfill/blob/master/src/cardboard-distorter.js">WebVR Polyfill</a> currently implements\u000athis approach.</p>\u000a\u000a<p>Applying distortion isn't the only expensive part in this rendering method. A\u000alot of time is wasted copying the whole scene to an intermediate texture.</p>\u000a\u000a<h1>3. Vertex displacement based solution (best)</h1>\u000a\u000a<p>This brings us to the most efficient method of the three, which eliminates the\u000aneed to render to an intermediate texture in the first place. In this approach,\u000athe geometry itself is distorted using a custom vertex shader. The idea is that\u000aknowing the position of the camera, we can displace vertices in such a way that\u000athe resulting 2D render is already barrel distorted. In this case, no shader\u000apass is needed, and we save the expensive step of copying the rendering into a\u000atexture. </p>\u000a\u000a<p>This method does require a certain vertex density on every mesh that is being\u000adeformed. Imagine the simple case of a large, 4-vertex rectangle being\u000arendered very close to the camera. Distorting these vertices would still yield a\u000a4-vertex flat rectangle, and clearly there's no barreling effect. Because of\u000athis, this is method does not generalize without extra work on the\u000adeveloper's part.</p>\u000a\u000a<p><img src="cdl.png" alt="Cardboard Design Lab screenshot" /></p>\u000a\u000a<p>This approach is used in the <a href="https://github.com/googlesamples/cardboard-unity/tree/master/Samples/CardboardDesignLab">Cardboard Design Lab</a> and in the open sourced\u000a<a href="https://github.com/google/vrview/blob/master/src/vertex-distorter.js">VR View project</a>. Geometry-based distortion can also result in sharper\u000alooking renderings, since the two pass approach can cause aliasing, especially\u000aif the intermediate texture is small. You can read more about this distortion\u000amethod in <a href="http://www.gamasutra.com/blogs/BrianKehrer/20160125/264161/VR_Distortion_Correction_using_Vertex_Displacement.php">this helpful explainer</a>.</p>\u000a
p2616
tp2617
Rp2618
sg13
V/vr-lens-distortion
p2619
sg15
Nsg16
I01
sg17
VThree approaches to VR lens distortion
p2620
sg20
VImmersion requires a large field of view.
p2621
sg6
V<p>Immersion requires a large field of view. This could be achieved by putting a\u000alarge curved spherical display on your face, but alas such technology is\u000aprohibitively expensive. A more affordable solution to increasing the field of\u000aview is to look at small ubiquitous rectangular displays through lenses:</p>\u000a\u000a<p><img src="how-lenses-increase-fov.png" alt="Why VR needs lenses" /></p>\u000a\u000a<p>Lenses placed close to your eyes greatly increase your field of view, but there\u000ais a cost: the image becomes spherically distorted. The larger the field of\u000aview, the more distorted the image. This post is a quick summary of three\u000adifferent approaches to undistorting the image, all of which have been\u000aimplemented in JavaScript for various WebVR-related projects.</p>\u000a\u000a
p2622
sg25
g169
sg33
g2619
sg170
(dp2623
g172
S'Apr'
p2624
sg174
S'April 20, 2016'
p2625
sg176
I4
sg177
S'2016-04-20T09:00:00-00:00'
p2626
sg179
I1461168000
sg180
I2016
sg181
I20
ssg65
g182
sg31
S'vr-lens-distortion'
p2627
sS'posted'
p2628
g188
(S'\x07\xe0\x04\x14'
p2629
tp2630
Rp2631
ssg34
S'content/posts/2016/vr-lens-distortion/index.md'
p2632
sg36
F1461259040.0
sa(dp2633
g2
(dp2634
g4
V  VR View  was just updated to version 2! This release includes some nice new features, the main one of which is a JavaScript API. This allows VR Views to be much more interactive. You can now load new content dynamically, play and pause videos, and add hotspots that link from one piece of 360 imagery to another. Here's a simple auto-advancing 360 slideshow showing some of my recent escapes around Seattle...   
p2635
sg28
g7
(g8
g9
V<p><a href="https://github.com/googlevr/vrview">VR View</a> was just updated to version 2! This release includes some nice\u000anew features, the main one of which is a JavaScript API. This allows VR Views to\u000abe much more interactive. You can now load new content dynamically, play and\u000apause videos, and add hotspots that link from one piece of 360 imagery to\u000aanother. Here's a simple auto-advancing 360 slideshow showing some of my recent\u000aescapes around Seattle...</p>\u000a\u000a<!--more-->\u000a\u000a<div id="vrview"></div>\u000a\u000a<script src="//storage.googleapis.com/vrview/2.0/build/vrview.min.js"></script>\u000a\u000a<script src="index.js"></script>\u000a\u000a<p>The <a href="https://storage.googleapis.com/vrview/2.0/examples/index.html">official samples</a> show more complex and interesting examples.\u000aThe <a href="https://developers.google.com/vr/concepts/vrview-web">docs</a> are also updated to reflect VR View's new capabilities.</p>\u000a\u000a<h1>Other new things</h1>\u000a\u000a<p>Also added some other features:</p>\u000a\u000a<ul>\u000a<li>WebVR 1.1 support for compatibility with <a href="https://webvr.info/">Chrome WebVR</a> builds.</li>\u000a<li>Programmatic playback controls and volume setting.</li>\u000a<li>Support for handling clicks, taps, VR button presses.</li>\u000a<li>Automatic panning mode for desktop.</li>\u000a</ul>\u000a\u000a<h1>Cardboard camera compatibility</h1>\u000a\u000a<p>I captured the photos with the very handy <a href="https://itunes.apple.com/us/app/cardboard-camera/id1095487294?mt=8">Cardboard Camera</a>. But before\u000aI could embed them into the VR View above, I had to do a conversion step.</p>\u000a\u000a<p>VR View expects stereo images to be in ODS format, which is a square JPEG with\u000athe left eye sphere stacked on top of the right eye sphere. Both spheres are\u000aprojected onto 2:1 rectangles using equirectangular projection.</p>\u000a\u000a<p>The native Cardboard Camera format is different. Cardboard Camera produces an\u000aimage of the left eye only. The right JPEG is base64 encoded and embedded in an\u000aXMP header, alongside other <a href="https://developers.google.com/streetview/spherical-metadata">Photo Sphere XMP metadata</a>. The images don't\u000aneed to be full photospheres, and may be cropped. Stopping a pano capture\u000amid-way, for example, will create a half-sphere. Also, the north and south poles\u000aof the sphere are never captured, since the sweep is horizontal.</p>\u000a\u000a<p>Now that you know more than you wanted about photosphere file formats, you can\u000aforget it all. I've streamlined the conversion process through a web-based\u000a<a href="https://storage.googleapis.com/cardboard-camera-converter/index.html">Cardboard Camera to ODS convertor</a>.</p>\u000a\u000a<h1>Future work</h1>\u000a\u000a<p>I was initially overjoyed by Safari 10's support for inline video texture\u000aplayback, which lets us finally play spherical video without <a href="http://stackoverflow.com/questions/29621199/three-js-video-textures-in-ios-play-back-in-a-separately-launched-player-ideas">gross\u000ahacks</a>. Unfortunately, their current video texture rendering performance\u000ais pretty abysmal. I'm getting about 10 FPS on a 2K (2048 x 2048) spherical\u000avideo in Cardboard mode, while Chrome, even on older Android hardware performs\u000asubstantially better.</p>\u000a\u000a<p>Many thanks to <a href="https://twitter.com/lbayliss">Leon Bayliss</a> for writing the <a href="https://storage.googleapis.com/vrview/2.0/examples/index.html">official samples</a>\u000aand test the API, and to <a href="https://twitter.com/aerotwist">Paul Lewis</a> for implementing tree shaking to\u000asubstantially reduce the size of the library.</p>\u000a
p2636
tp2637
Rp2638
sg13
V/vr-view-2.0
p2639
sg15
Nsg16
I01
sg17
VVR View 2.0: JavaScript API
p2640
sg20
V[VR View][vrview] was just updated to version 2! This release includes some nice\u000anew features, the main one of which is a JavaScript API.
p2641
sg6
V<p><a href="https://github.com/googlevr/vrview">VR View</a> was just updated to version 2! This release includes some nice\u000anew features, the main one of which is a JavaScript API. This allows VR Views to\u000abe much more interactive. You can now load new content dynamically, play and\u000apause videos, and add hotspots that link from one piece of 360 imagery to\u000aanother. Here's a simple auto-advancing 360 slideshow showing some of my recent\u000aescapes around Seattle...</p>\u000a\u000a
p2642
sg25
g169
sg33
g2639
sg170
(dp2643
g172
S'Nov'
p2644
sg174
S'November 14, 2016'
p2645
sg176
I11
sg177
S'2016-11-14T09:00:00-00:00'
p2646
sg179
I1479142800
sg180
I2016
sg181
I14
ssg65
g182
sg31
S'vr-view-2.0'
p2647
sS'posted'
p2648
g188
(S'\x07\xe0\x0b\x0e'
p2649
tp2650
Rp2651
ssg34
S'content/posts/2016/vr-view-2.0/index.md'
p2652
sg36
F1481237158.0
sa(dp2653
g2
(dp2654
g4
V During a two week trip to India, I took over 1000 shots, including photos, videos and a few photospheres. A picture is worth one thousand words, but how many pictures is a photosphere worth? We may never know, but I digress. My favorite photosphere was of friends posing inside one of the turrets of the Jaigarh Fort:         function DeviceMotionSender(){if(!this.isIOS_()){return}window.addEventListener("devicemotion",this.onDeviceMotion_.bind(this),false);this.iframes=document.querySelectorAll("iframe.vrview")}DeviceMotionSender.prototype.onDeviceMotion_=function(e){var message={type:"DeviceMotion",deviceMotionEvent:this.cloneDeviceMotionEvent_(e)};for(var i=0;i<this.iframes.length;i++){var iframe=this.iframes[i];var iframeWindow=iframe.contentWindow;if(this.isCrossDomainIframe_(iframe)){iframeWindow.postMessage(message,"*")}}};DeviceMotionSender.prototype.cloneDeviceMotionEvent_=function(e){return{acceleration:{x:e.acceleration.x,y:e.acceleration.y,z:e.acceleration.z},accelerationIncludingGravity:{x:e.accelerationIncludingGravity.x,y:e.accelerationIncludingGravity.y,z:e.accelerationIncludingGravity.z},rotationRate:{alpha:e.rotationRate.alpha,beta:e.rotationRate.beta,gamma:e.rotationRate.gamma},interval:e.interval}};DeviceMotionSender.prototype.isIOS_=function(){return/iPad|iPhone|iPod/.test(navigator.userAgent)&&!window.MSStream};DeviceMotionSender.prototype.isCrossDomainIframe_=function(iframe){var html=null;try{var doc=iframe.contentDocument||iframe.contentWindow.document;html=doc.body.innerHTML}catch(err){}return html===null};var dms=new DeviceMotionSender;     I captured this using the photosphere camera which ships with Android. It's embedded into my blog using  VR View , which  launched today . The embed above lets you include an interactive photosphere right in your website, which is especially fun on mobile, where the image reacts directly to your phone's movements. You can view it in full screen mode, and even in Cardboard mode (only on mobile).    But you know what's cooler than a photosphere? A stereo photosphere! And luckily, you can capture stereo photospheres using  Cardboard Camera , and then use a VR View to  embed them too . You can even embed mono or  stereo videos . Check out  the docs  for more info. Eager to hear what you think!  
p2655
sg28
g7
(g8
g9
V<p>During a two week trip to India, I took over 1000 shots, including photos,\u000avideos and a few photospheres. A picture is worth one thousand words, but how\u000amany pictures is a photosphere worth? We may never know, but I digress. My\u000afavorite photosphere was of friends posing inside one of the turrets of the\u000aJaigarh Fort:</p>\u000a\u000a<iframe class="vrview" width="100%" height="300px" allowfullscreen frameborder="0" src="//storage.googleapis.com/vrview/index.html?image=//smus.com/vr-views/india-photosphere-4096.jpg&preview=//smus.com/vr-views/india-photosphere-1024.jpg&is_stereo=false"></iframe>\u000a\u000a<script>\u000afunction DeviceMotionSender(){if(!this.isIOS_()){return}window.addEventListener("devicemotion",this.onDeviceMotion_.bind(this),false);this.iframes=document.querySelectorAll("iframe.vrview")}DeviceMotionSender.prototype.onDeviceMotion_=function(e){var message={type:"DeviceMotion",deviceMotionEvent:this.cloneDeviceMotionEvent_(e)};for(var i=0;i<this.iframes.length;i++){var iframe=this.iframes[i];var iframeWindow=iframe.contentWindow;if(this.isCrossDomainIframe_(iframe)){iframeWindow.postMessage(message,"*")}}};DeviceMotionSender.prototype.cloneDeviceMotionEvent_=function(e){return{acceleration:{x:e.acceleration.x,y:e.acceleration.y,z:e.acceleration.z},accelerationIncludingGravity:{x:e.accelerationIncludingGravity.x,y:e.accelerationIncludingGravity.y,z:e.accelerationIncludingGravity.z},rotationRate:{alpha:e.rotationRate.alpha,beta:e.rotationRate.beta,gamma:e.rotationRate.gamma},interval:e.interval}};DeviceMotionSender.prototype.isIOS_=function(){return/iPad|iPhone|iPod/.test(navigator.userAgent)&&!window.MSStream};DeviceMotionSender.prototype.isCrossDomainIframe_=function(iframe){var html=null;try{var doc=iframe.contentDocument||iframe.contentWindow.document;html=doc.body.innerHTML}catch(err){}return html===null};var dms=new DeviceMotionSender;\u000a</script>\u000a\u000a<p>I captured this using the photosphere camera which ships with Android. It's\u000aembedded into my blog using <a href="https://developers.google.com/cardboard/vrview">VR View</a>, which <a href="https://developers.googleblog.com/2016/03/introducing-vr-view-embed-immersive.html">launched\u000atoday</a>. The embed above lets you include an interactive photosphere\u000aright in your website, which is especially fun on mobile, where the image reacts\u000adirectly to your phone's movements. You can view it in full screen mode, and\u000aeven in Cardboard mode (only on mobile).</p>\u000a\u000a<p>But you know what's cooler than a photosphere? A stereo photosphere! And\u000aluckily, you can capture stereo photospheres using <a href="https://play.google.com/store/apps/details?id=com.google.vr.cyclops&amp;hl=en">Cardboard Camera</a>, and\u000athen use a VR View to <a href="https://storage.googleapis.com/vrview/examples/pano/index.html">embed them too</a>. You can even embed mono or\u000a<a href="https://storage.googleapis.com/vrview/examples/video/index.html">stereo videos</a>. Check out <a href="https://developers.google.com/cardboard/vrview">the docs</a> for more info. Eager\u000ato hear what you think!</p>\u000a
p2656
tp2657
Rp2658
sg13
V/vr-views
p2659
sg15
Nsg16
I00
sg17
VEmbedding VR content on the web
p2660
sg20
VDuring a two week trip to India, I took over 1000 shots, including photos,\u000avideos and a few photospheres.
p2661
sg6
g2658
sg25
g169
sg33
g2659
sg170
(dp2662
g172
S'Mar'
p2663
sg174
S'March 31, 2016'
p2664
sg176
I3
sg177
S'2016-03-31T09:00:00-00:00'
p2665
sg179
I1459440000
sg180
I2016
sg181
I31
ssg65
g182
sg31
S'vr-views'
p2666
sS'posted'
p2667
g188
(S'\x07\xe0\x03\x1f'
p2668
tp2669
Rp2670
ssg34
S'content/posts/2016/vr-views/index.md'
p2671
sg36
F1459367189.0
sa(dp2672
g2
(dp2673
g4
V WebVR provides a solid technical foundation on which to build compelling VR experiences. But it does not answer a critical question, which is the topic of this post:              What could the web become in a Virtual Reality environment?          Gear VR provides a simple and straightforward answer: same same. The fundamental unit is still a page, but you use the immersion of VR to increase your effective screen size. The input constraints result in a worse experience for the user. Scrolling with your finger on your temple is tiring and head-based typing is a massive pain. Given the input constraints, we need to beef up the output and make it better matched to what VR excels at. A responsive design inspired solution would involve deconstructing the page to better suit the nature of the immersive environment.    Another approach is to make a clean break from legacy web content. What if certain web pages had parallel content tailored for virtual reality? In this post, I'll explore this idea with an example focused on Wikipedia.          
p2674
sg28
g7
(g8
g9
V<p>WebVR provides a solid technical foundation on which to build compelling VR\u000aexperiences. But it does not answer a critical question, which is the topic of\u000athis post:</p>\u000a\u000a<blockquote>\u000a  <blockquote>\u000a    <p>What could the web become in a Virtual Reality environment?</p>\u000a  </blockquote>\u000a</blockquote>\u000a\u000a<p>Gear VR provides a simple and straightforward answer: same same. The fundamental\u000aunit is still a page, but you use the immersion of VR to increase your effective\u000ascreen size. The input constraints result in a worse experience for the user.\u000aScrolling with your finger on your temple is tiring and head-based typing is a\u000amassive pain. Given the input constraints, we need to beef up the output and\u000amake it better matched to what VR excels at. A responsive design inspired\u000asolution would involve deconstructing the page to better suit the nature of the\u000aimmersive environment.</p>\u000a\u000a<p>Another approach is to make a clean break from legacy web content. What if\u000acertain web pages had parallel content tailored for virtual reality? In this\u000apost, I'll explore this idea with an example focused on Wikipedia.</p>\u000a\u000a<p><a href="https://youtu.be/HcSvBAEXcWA"><img src="vr-forest.jpg" alt="Video of VR Wikipedia" /></a></p>\u000a\u000a<!--more-->\u000a\u000a<h1>Navigating the VR Forest</h1>\u000a\u000a<p>The web in general and Wikipedia specifically covers a vast amount of\u000ainformation &mdash; nearly everything. Everything is a daunting place to start,\u000aso we will begin with something specific: a forest.</p>\u000a\u000a<p>To begin, navigate to a (fake) <a href="http://borismus.github.io/wikipedia-vr/pages/moose">wikipedia article about a moose</a>, and hit\u000athe VR button. This takes you to a forest meadow, with a life sized moose in front of\u000ayou. You are free to move inside the forest (focus on the grass and click) and\u000ainteract with other animals in it. Looking at an animal gives you some basic\u000aWikipedia-inspired information about it.  Clicking it focuses you in on\u000ait and presents options. If you leave VR when focused on an animal, you end up\u000aon the associated Wikipedia article. This closes the navigation loop: you can\u000astart from one webpage, enter VR mode, navigate to another entity inside VR,\u000aleave VR and end up on another webpage.</p>\u000a\u000a<p>Try out this <a href="http://borismus.github.io/wikipedia-vr/pages/moose">Wikipedia VR sample</a> on your mobile phone in Cardboard. It\u000aalso works on desktop using the spacebar to simulate the Cardboard click.</p>\u000a\u000a<p><img src="navigation.mp4.gif" alt="Video of navigating between pages" /></p>\u000a\u000a<p>VR and education are naturally matched. As <a href="https://youtu.be/UuceLtGjDWY?t=1m40s">Ben explains eloquently in an I/O\u000atalk</a>, "VR is a chance to scale experiential learning". Remembering what\u000ayou learned in class is much harder than remembering your favorite vacation. In\u000athis Wikipedia example, you immediately get a sense of the animal's grandeur,\u000awhich is hard to convey in words and images. You can get a feeling for quickly\u000ait runs, and what it sounds like. </p>\u000a\u000a<h1>Closing thoughts</h1>\u000a\u000a<p><strong>Tip of the iceberg</strong>. The entities in this Wikipedia demo (in green) represent\u000aa tiny subgraph of Wikipedia:</p>\u000a\u000a<p><img src="knowledge-graph.png" alt="Picture of the subgraph we implement" /></p>\u000a\u000a<p>All of the above are positioned in a much bigger subgraph of Wikipedia which\u000amight can be represented in VR. Of course, many Wikipedia pages are really\u000adifficult to imagine in VR. Could <a href="https://en.wikipedia.org/wiki/Philosophy">Wikipedia's Philosophy</a> article have a\u000acompelling VR version?</p>\u000a\u000a<p><strong>Changing scale</strong>. The ability to change scale would make it possible to place\u000aevery entity from the above graph into VR Wikipedia. Imagine diving into the\u000ahide of the moose, learning about symbiotic insects and hair folicles, then\u000agoing deeper to learn about the structure of hair on a molecular level. Or vice\u000aversa, zooming out to look at planet Earth to see where moose live, or going\u000ainto an abstract view to explore Family Cervidae. It's easy to lose an hour or\u000atwo in Wikipedia's hyperlink maze. One day, it may be even easier to do this in\u000aVR.</p>\u000a\u000a<p><img src="alice-shrinking.jpg" alt="Alice in wonderland shrinking" /></p>\u000a\u000a<p><strong>Content is king</strong>. The big open question is how to generate this content. Even a\u000ascoped down project to VR-ify categories of Wikipedia pages (say, only forest\u000aanimals), is incredibly ambitious. Where do you get all of the models? How do\u000ayou animiate them to run, jump, stand around, sleep, play, eat and be eaten? How\u000ado you place them in a the forest in a meaningful way? Doing this automatically\u000aseems, at a glance, AI-hard.</p>\u000a\u000a<p><strong>Limited knowledge graphs</strong>. Even if you imagine that we have a series of animated\u000amodels, how do we compose them together? Do moose and canaries live in the same\u000aenvironment? Can you find fire ants in the bark of a Sequoia? How many? How big\u000aare hyenas, and how quickly do they run? This information is missing from even\u000athe best known knowledge graph.</p>\u000a\u000a<p><strong>3D modeling is difficult</strong>. Wikipedia's giant corpus of quality content exists\u000abecause it's easy for many people to collaborate. Wikipedians need to be good\u000awriters, well versed in their topic, and motivated to contribute. There is a\u000atechnical barrier - learning Wikipedia markup - but it is not incredibly\u000adifficult. For a Wikipedia in VR, the technical barriers are much higher. Even\u000awith a good collaborative editor, it seems inevitable that contributors would\u000aneed to have some sense of 3D modelling, and a far more specialized skillset.</p>\u000a\u000a<p><strong>Artistic considerations</strong>. One of the challenges for a large community project\u000alike Wikipedia is establishing a consistent style. Imagine if every Wikipedia\u000aimage was hand drawn. Artistic abilities vary wildly, and you can imagine a\u000afunny and chaotic result. Aaron Koblin's now classic <a href="http://www.thesheepmarket.com/">Sheep Market</a>\u000aexperiment comes to mind:</p>\u000a\u000a<p><img src="sheep-market.png" alt="Sheep market screenshot" /></p>\u000a
p2675
tp2676
Rp2677
sg13
V/wikipedia-vr
p2678
sg15
Nsg16
I01
sg17
VBrowsing Wikipedia in VR
p2679
sg20
VWebVR provides a solid technical foundation on which to build compelling VR\u000aexperiences.
p2680
sg6
V<p>WebVR provides a solid technical foundation on which to build compelling VR\u000aexperiences. But it does not answer a critical question, which is the topic of\u000athis post:</p>\u000a\u000a<blockquote>\u000a  <blockquote>\u000a    <p>What could the web become in a Virtual Reality environment?</p>\u000a  </blockquote>\u000a</blockquote>\u000a\u000a<p>Gear VR provides a simple and straightforward answer: same same. The fundamental\u000aunit is still a page, but you use the immersion of VR to increase your effective\u000ascreen size. The input constraints result in a worse experience for the user.\u000aScrolling with your finger on your temple is tiring and head-based typing is a\u000amassive pain. Given the input constraints, we need to beef up the output and\u000amake it better matched to what VR excels at. A responsive design inspired\u000asolution would involve deconstructing the page to better suit the nature of the\u000aimmersive environment.</p>\u000a\u000a<p>Another approach is to make a clean break from legacy web content. What if\u000acertain web pages had parallel content tailored for virtual reality? In this\u000apost, I'll explore this idea with an example focused on Wikipedia.</p>\u000a\u000a<p><a href="https://youtu.be/HcSvBAEXcWA"><img src="vr-forest.jpg" alt="Video of VR Wikipedia" /></a></p>\u000a\u000a
p2681
sg25
g169
sg33
g2678
sg170
(dp2682
g172
S'May'
p2683
sg174
S'May 26, 2016'
p2684
sg176
I5
sg177
S'2016-05-26T09:00:00-00:00'
p2685
sg179
I1464278400
sg180
I2016
sg181
I26
ssg65
g182
sg31
S'wikipedia-vr'
p2686
sS'posted'
p2687
g188
(S'\x07\xe0\x05\x1a'
p2688
tp2689
Rp2690
ssg34
S'content/posts/2016/wikipedia-vr/index.md'
p2691
sg36
F1464286046.0
sa(dp2692
g2
(dp2693
g4
V     We have a choice. We have two options as human beings. We have a choice   between conversation and violence. That's it.         \u2013 Sam Harris      As technological progress plows forward, human nature is unchanged. We each look at the world through our own lens. In a previous post, I found that  translating a query between English and Russian greatly determines search results . In the same way that language matters, so do religious views, culture, political leanings, and much more. Here's a recent example highlighting a news source-based lens on the same topic (Nancy Pelosi and Russia):         Humanity has always been divided, and in hindsight, the unifying promise of the internet was a techno-utopian dream. By shrinking the world into a "global village" (famously coined by communication theorist  Marshall McLuhan ) we have balkanized into increasingly specialized sub-cultures and increased cross-cultural conflicts. More recently, personalized search results, curated social network feeds only serve to deepen the divide.     Debaters  is a new side project which aims to bring you and someone with an opposing view into a private, friendly, anonymous conversation. It's still in development, but I want to share it with you both as a milestone and to get early feedback.   
p2694
sg28
g7
(g8
g9
V<blockquote>\u000a  <p>We have a choice. We have two options as human beings. We have a choice\u000a  between conversation and violence. That's it. </p>\u000a  \u000a  <p>\u2013 Sam Harris</p>\u000a</blockquote>\u000a\u000a<p>As technological progress plows forward, human nature is unchanged. We each look\u000aat the world through our own lens. In a previous post, I found that <a href="/hot-bread-delicious-deadly/">translating\u000aa query between English and Russian greatly determines search results</a>.\u000aIn the same way that language matters, so do religious views, culture, political\u000aleanings, and much more. Here's a recent example highlighting a news\u000asource-based lens on the same topic (Nancy Pelosi and Russia):</p>\u000a\u000a<p><img src="pelosi.png" alt="Nancy pelosi russia on nytimes vs. breitbart" /></p>\u000a\u000a<p>Humanity has always been divided, and in hindsight, the unifying promise of the\u000ainternet was a techno-utopian dream. By shrinking the world into a "global\u000avillage" (famously coined by communication theorist <a href="http://www.marshallmcluhan.com/biography/">Marshall McLuhan</a>)\u000awe have balkanized into increasingly specialized sub-cultures and increased\u000across-cultural conflicts. More recently, personalized search results, curated\u000asocial network feeds only serve to deepen the divide.</p>\u000a\u000a<p><a href="https://debate.rs/">Debaters</a> is a new side project which aims to bring you and someone\u000awith an opposing view into a private, friendly, anonymous conversation. It's\u000astill in development, but I want to share it with you both as a milestone and\u000ato get early feedback.</p>\u000a\u000a<!--more-->\u000a\u000a<h2>The problem</h2>\u000a\u000a<p>We are social animals. Rather than starting with a blank slate and using our\u000abrilliant brains to arrive at independent conclusions, we prefer to jump to\u000aour conclusions first through social means, and then rationalize why we are\u000aright. Once we <em>know</em> the answer, it's unlikely that we will change our minds.\u000aBecause of my-side bias (aka confirmation bias), arguments in favor will stick,\u000awhile arguments against will be easily swatted. Entrenched in our socially\u000adefined beliefs, our social circles and personalized information sources quell\u000apotential for dissent, while strengthening our worldview. On a macro scale, this\u000aleads to a polarized society. We can tolerate anything <a href="https://slatestarcodex.com/2014/09/30/i-can-tolerate-anything-except-the-outgroup/">but the outgroup</a>.</p>\u000a\u000a<p>In light of the above, we are unlikely change our minds. But if you are one of\u000athose rare people that are open to changing their mind, you may have read\u000aarticles like <a href="https://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind">this one</a>. However, it's far easier to be understand\u000athe theory of mind changing than it is to actually change your mind on a\u000aspecific issue. Many public intellectuals are actively involved in conversations\u000athat test their limits, but normal folks like you and me don't often get the\u000achance.</p>\u000a\u000a<p>I've only attended one <a href="https://www.meetup.com/Bay-Area-Conservatives/">conservative meetup</a>. I chose not to reveal my\u000aidentity as a crooked centrist, feeling that this would impede further\u000aconversation. I think a similar thing happens in many cities: there must be\u000aTrump supporters (dozens of them?) among us, but they seem to keep a low\u000aprofile. Sad!</p>\u000a\u000a<p>The way we get better at anything is through practice, which in this case means\u000ato actively test ourselves on new ideas and with new people. Projects like\u000a<a href="http://www.livingroomconversations.org/">Living Room Conversations (LRC)</a> in the real world, or <a href="https://www.reddit.com/r/changemyview/">Change My View\u000a(CMV)</a> online try to create an environment that enables conversations where\u000awe can practice actual open mindedness.</p>\u000a\u000a<h2>Some problems with existing mind changing tools</h2>\u000a\u000a<p><a href="http://www.livingroomconversations.org/">LRC</a> requires getting a group of people together physically, and have a\u000astructured conversation about a controversial topic. This is difficult to do\u000asince you must find a group of friendly but disagreeing people in-person. I'd\u000alove to try it, but haven't been able to find a more right-leaning\u000aco-facilitator yet. It is also a social risk, since you are likely pulling in\u000apeople from your social circle. Presumably you have briefed them on the plan and\u000athey have consented, but conversations may still escalate and feelings can\u000aeasily be hurt. In addition, the prospect of a serious, structured conversation\u000awith close friends sounds quite awkward to me.</p>\u000a\u000a<p>Online, <a href="https://www.reddit.com/r/changemyview/">CMV</a> is great but has its own problems, despite the efforts of\u000awell meaning and intelligent moderators. Some users that start threads seem to\u000ause CMV as a way of pressure testing their own view. They get all of the counter\u000aarguments, learn how to counter them, and get even better at rationalizing away\u000aany future doubts. Some respondents may make arguments whether or not they\u000aactually think that way just for the sake of deltas. As a subreddit, CMV\u000ausers tend to fall into Reddit's skewed demographics. This means less potential\u000afor viewpoint diversity. Lastly, CMV is public and not truly anonymous. This\u000aencourages people to be clever rather than honest, although there is no shortage\u000aof <a href="https://www.reddit.com/r/The_Donald/">subreddits</a> whose members prefer honesty over cleverness.</p>\u000a\u000a<h2>What is Debaters?</h2>\u000a\u000a<p>So, to address some of the shortcomings of existing approaches like CMV and LRC,\u000aI've been working on a side project provisionally called <a href="https://debate.rs/">Debaters</a>.</p>\u000a\u000a<blockquote>\u000a  <p>Debaters enables one-on-one conversations on a controversial topic, with\u000a  someone of the opposing view. You may not be convinced by their arguments,\u000a  but your conversation may lead to a better understanding on both sides.\u000a  Interaction with someone with a different viewpoint will lead to reduced\u000a  animosity toward their whole group.</p>\u000a</blockquote>\u000a\u000a<p><img src="debaters.png" alt="Screenshots of Debaters" /></p>\u000a\u000a<p>I took a bit of time off recently and got carried away on the implementation,\u000awith <a href="https://twitter.com/abmcosta">Antonio's help</a> on the UX front, and now it's live on\u000a<a href="https://debate.rs">https://debate.rs</a>. If you visit, you will be presented with a list of issues\u000aand asked to opine on each. After you provide your opinion you are matched to\u000asomeone with the opposite view, then you engage in a conversation about the\u000aissue.</p>\u000a\u000a<p>Let me address one common question up-front: trolling. I do not think trolling\u000ais much of an issue for Debaters. Trolls want to make an impact. In other words,\u000athey want to either reach a lot of people, or affect some people in a\u000asignificant way. In a 1:1 conversation, their reach is limited. In an anonymous\u000acontext, the amount of personal harm a troll can inflict is limited too.</p>\u000a\u000a<h3>Crowdsourced beta testing</h3>\u000a\u000a<p>To work out bugs and test out the platform, I took to Mechanical Turk. That's\u000aright, I paid people to have an argument in the spirit of Monty Python's\u000aArgument Clinic:</p>\u000a\u000a<iframe width="600" height="337" src="https://www.youtube.com/embed/kQFKtI6gn9Y" frameborder="0" allowfullscreen></iframe>\u000a\u000a<p>Kidding aside, Turkers effectively became poorly paid ($0.25 per session) QA\u000atesters. I asked them to try out Debaters and answer a question or two.\u000aMeanwhile, I would assume the position of devil's advocate (as needed) and we\u000awould have 5-10 minute long conversations. This helped iron out the bugs and\u000aprioritize features.</p>\u000a\u000a<p>Getting people to use the service without bribes was hard, mainly because\u000aDebaters is a marketplace. Two people are required to answer the same question\u000adifferently to get matched. So inevitably, the first respondent needs to wait\u000awhile a match is found. Attention spans are short, and Debaters users are few.\u000aDebaters attempts to address this by taking advantage of <a href="https://developers.google.com/web/fundamentals/engage-and-retain/push-notifications/">web\u000anotifications</a>. Once a match is made, you are notified through a\u000abrowser notification. By this point though, you may be less likely to be up for\u000aa conversation.</p>\u000a\u000a<p>One of my milestones for the first version of Debaters was to facilitate a\u000aconversation between two people I didn't know. I managed to do this by actively\u000apromoting it on Twitter while also paying users on Mechanical Turk, creating a\u000acritical mass so that people would get matched without too much waiting. This\u000aworked out, and finally I had a half organic conversation. This one was about\u000aa federal minimum wage. Dustin answered "Not sure", Lawrence answered "No". In\u000acase you are wondering, Debaters assigns names and avatars randomly.</p>\u000a\u000a<pre><code>Dustin Collier:    hi Lawrence\u000aLawrence Castillo: hi dustin\u000a                   guessing these names are not real\u000aDustin Collier:    hehe. mine isn't, dunno about yours :)\u000aLawrence Castillo: i was scared for a second and thought they were real but\u000a                   thats good\u000aDustin Collier:    is yours really lawrence!!!!!\u000a                   or did you forget your name for a sec\u000aLawrence Castillo: no no\u000a                   i saw your name and was like "oh shit people can\u000a                   see names"\u000a                   glad they're fake\u000aDustin Collier:    ah yea\u000a                   anonymous.\u000a                   u dont like minimum wage?\u000aLawrence Castillo: i think federal minimum wage, at least how we've been\u000a                   talking about it is pretty flawed\u000aDustin Collier:    how so?\u000aLawrence Castillo: like, the minimum wage in nebraska should be very\u000a                   different from the minimum wage in nyc\u000a                   if we want a minimum wage it needs to be a percent of\u000a                   cost of living\u000aDustin Collier:    ah yeah, cost of living adjusted\u000aLawrence Castillo: the idea of 15 dollars is kind of crazy\u000a                   people in ny are still poor, and business can't pay it\u000a                   in rural areas\u000a                   i feel that way about most federal laws though\u000aDustin Collier:    yea i agree, but that's not even on the table\u000a                   bernie was all like "$15"\u000aLawrence Castillo: yeah i loved the energy but...\u000a</code></pre>\u000a\u000a<p>The next milestone is to have a fully organic conversation, where both sides\u000aarrive at Debaters without monetary incentives, but out of legimiate interest.</p>\u000a\u000a<h2>Problems with Debaters</h2>\u000a\u000a<p>Now that the first version of Debaters is released, the technical problems have\u000abeen addressed, and the UX is in an early but usable state. The fundamental\u000aproblem is <strong>how to attract users</strong>. </p>\u000a\u000a<p>I think that the name "Debaters" connotes exactly the wrong thing. Debates are\u000asomething you win, and invoke a high school debate club. The name is also\u000asuggestive of conflict, which people generally tend to avoid. Unfortunately I\u000awas unable to come up with a catchy alternative.</p>\u000a\u000a<p>That said, the name is not the limiting factor on user acquisition; there are\u000amore fundamental forces at play. In today's political climate, people want to be\u000aupset and angry. We are constantly outraged, and <a href="https://www.nytimes.com/2017/02/27/opinion/the-uses-of-outrage.html">some view it as a good\u000athing</a> that builds social cohesion. We don't want to change our minds,\u000athat would be like colluding with the enemy. After all, <a href="https://wiki.lesswrong.com/wiki/Arguments_as_soldiers">arguments are\u000asoldiers</a>. I disagree.</p>\u000a\u000a<p>Conversations with people that hold different views is like getting kids to eat\u000atheir vegetables. It's good for them, but they aren't necessarily going to like\u000ait.</p>\u000a\u000a<h2>Tricking people into friendly debate</h2>\u000a\u000a<p>A common tactic for getting kids to each their vegetables is to disguise them as\u000asomething else. Could a similar approach be taken with Debaters?</p>\u000a\u000a<p>One avenue might be to target people that want to proselytize their ideas.\u000aThey might come to Debaters to sway others about one issue they are passionate\u000aabout, and then become engaged in another conversation on another issue, where\u000athey are more likely to listen. This is pure theory. Maybe proselytizers are\u000acertain about everything.</p>\u000a\u000a<p>Another avenue might be to target neurotic people. This has sort of been tried\u000ain the form of <a href="http://asteroidsclub.org/">The Asteroids Club</a>. This project is framed as a\u000a"non-debate on America's biggest problems, which are hurtling toward us through\u000aspace and time at an alarming rate of speed". Unfortunately it hasn't taken off\u000ayet.</p>\u000a\u000a<p>People are inherently curious. Projects like <a href="http://wolfmanproductions.com/haider-hamza/">Talk to an Iraqi</a> and <a href="https://www.theswedishnumber.com/">The\u000aSwedish Number</a> have been effective at attracting an audience. Haidar\u000aHamza's public booth seems to have also been effective at bringing up difficult\u000apolitical issues. Could we take advantage of this curiosity by surfacing\u000asomething unusual about your future interlocutor?</p>\u000a\u000a<p>And yet resorting to trickery may not work. Even a more oblique form of it,\u000a<a href="https://en.wikipedia.org/wiki/Nudge_theory">nudging</a>, has <a href="https://www.theguardian.com/commentisfree/2014/apr/24/nudge-backlash-free-society-dignity-coercion">had significant opposition</a>. But, as Sam\u000aHarris starkly puts it, the only tools we have for changing minds are\u000aconversation and violence. My opinion? I'd like to avoid the latter, so intend\u000ato continue thinking about and building in this difficult but incredibly\u000aimportant problem space.</p>\u000a
p2695
tp2696
Rp2697
sg13
V/debaters-friendly-disagreement
p2698
sg15
Nsg16
I01
sg17
VDebaters: friendly disagreement
p2699
sg20
V> We have a choice.
p2700
sg6
V<blockquote>\u000a  <p>We have a choice. We have two options as human beings. We have a choice\u000a  between conversation and violence. That's it. </p>\u000a  \u000a  <p>\u2013 Sam Harris</p>\u000a</blockquote>\u000a\u000a<p>As technological progress plows forward, human nature is unchanged. We each look\u000aat the world through our own lens. In a previous post, I found that <a href="/hot-bread-delicious-deadly/">translating\u000aa query between English and Russian greatly determines search results</a>.\u000aIn the same way that language matters, so do religious views, culture, political\u000aleanings, and much more. Here's a recent example highlighting a news\u000asource-based lens on the same topic (Nancy Pelosi and Russia):</p>\u000a\u000a<p><img src="pelosi.png" alt="Nancy pelosi russia on nytimes vs. breitbart" /></p>\u000a\u000a<p>Humanity has always been divided, and in hindsight, the unifying promise of the\u000ainternet was a techno-utopian dream. By shrinking the world into a "global\u000avillage" (famously coined by communication theorist <a href="http://www.marshallmcluhan.com/biography/">Marshall McLuhan</a>)\u000awe have balkanized into increasingly specialized sub-cultures and increased\u000across-cultural conflicts. More recently, personalized search results, curated\u000asocial network feeds only serve to deepen the divide.</p>\u000a\u000a<p><a href="https://debate.rs/">Debaters</a> is a new side project which aims to bring you and someone\u000awith an opposing view into a private, friendly, anonymous conversation. It's\u000astill in development, but I want to share it with you both as a milestone and\u000ato get early feedback.</p>\u000a\u000a
p2701
sg25
g169
sg33
g2698
sg170
(dp2702
g172
S'Mar'
p2703
sg174
S'March 5, 2017'
p2704
sg176
I3
sg177
S'2017-03-05T09:00:00-00:00'
p2705
sg179
I1488733200
sg180
I2017
sg181
I5
ssg65
g182
sg31
S'debaters-friendly-disagreement'
p2706
sS'posted'
p2707
g188
(S'\x07\xe1\x03\x05'
p2708
tp2709
Rp2710
ssg34
S'content/posts/2017/debaters-friendly-disagreement/index.md'
p2711
sg36
F1488779771.0
sa(dp2712
g2
(dp2713
g4
V News reporting suffers from two major issues I'd like to tackle. The first is a bias towards negative, emotionally laden events. The second is the difficulty of capturing information about gradual changes.    These two deficiencies distort our perception. They make it easy for demagogues to claim that the world has gone to shit. The data tells a different story, as the late  Hans Rosling  was fond of reminding us. My hypothesis is that if base rates were provided in a compelling way alongside news stories (or even headlines), the public would be better informed. The challenges are many: first, getting and analyzing the data, but even more important, presenting it in a reasonable way.    In this post, let's explore what that would entail, from data collection, to analysis, to visualization. We'll go through a couple of examples.   
p2714
sg28
g7
(g8
g9
V<p>News reporting suffers from two major issues I'd like to tackle. The first is a\u000abias towards negative, emotionally laden events. The second is the difficulty of\u000acapturing information about gradual changes.</p>\u000a\u000a<p>These two deficiencies distort our perception. They make it easy for demagogues\u000ato claim that the world has gone to shit. The data tells a different story, as\u000athe late <a href="https://www.ted.com/talks/hans_rosling_shows_the_best_stats_you_ve_ever_seen">Hans Rosling</a> was fond of reminding us. My hypothesis is that\u000aif base rates were provided in a compelling way alongside news stories (or even\u000aheadlines), the public would be better informed. The challenges are many: first,\u000agetting and analyzing the data, but even more important, presenting it in a\u000areasonable way.</p>\u000a\u000a<p>In this post, let's explore what that would entail, from data collection, to\u000aanalysis, to visualization. We'll go through a couple of examples.</p>\u000a\u000a<!--more-->\u000a\u000a<h2>The problems with news</h2>\u000a\u000a<p>I've already complained about the news in a <a href="/front-page-blues">previous blog post</a>, but\u000athis time around, I'd like to hone in on two specific issues: negativity and\u000agradual changes:</p>\u000a\u000a<ul>\u000a<li><p>News is generally biased toward negative, emotionally laden events. A\u000aterrorist rampage that claims five victims is practically guaranteed to make the\u000afront page, while a cure that saves five hundred certainly wouldn't.</p></li>\u000a<li><p>News does not inform about gradual changes. Many important\u000aprocesses, such as climate change, are gradual. Like boiling a frog, there are\u000ano specific events to report on, so they get no coverage in the news (until the\u000afrog dies).</p></li>\u000a</ul>\u000a\u000a<p>The goal here is for perception to approach reality. I will assume that you\u000aagree with me that this is a worthy goal to pursue. Otherwise, we now return you\u000ato your regularly <a href="https://www.socialistalternative.org/">scheduled</a>\u000a<a href="http://www.breitbart.com/">program</a>.</p>\u000a\u000a<h2>Headlines invite questions</h2>\u000a\u000a<p>I went through some recent news stories (on <a href="https://en.wikipedia.org/wiki/Portal:Current_events">Wikipedia</a>), asking some simple\u000aquestions. For example:</p>\u000a\u000a<p><style>\u000atable#headline-question {\u000a  font-size: 70%;\u000a}\u000atable#headline-question td {\u000a  padding: 1em;\u000a  text-align: left;\u000a}\u000a</style></p>\u000a\u000a<table id="headline-question">\u000a<tr>\u000a<th>Headline</th><th>Questions</th>\u000a</tr>\u000a<tr>\u000a<td>The death toll from the Rigopiano avalanche rises to 29.</td><td>How frequent are avalanche deaths? What about just in\u000aItaly? What are some big recent avalanches?</td>\u000a</tr>\u000a<tr>\u000a<td>Ken Wyatt is sworn in as the first Indigenous Australian to serve in\u000aAustralia's cabinet.</td><td>What is the population of Indigenous Australians?\u000aWhat is the racial breakdown in Australia's cabinet? What about other countries?\u000aWhat about historically?</td>\u000a</tr>\u000a<tr>\u000a<td>The Kremlin arrests four people, one from Kaspersky Lab and three from the\u000aFederal Security Service, reportedly on treason charges for passing information\u000ato America's CIA.</td><td>How many arrests does the Kremlin typically make? How\u000amany for treason? How about the US government?</td>\u000a</tr>\u000a</table>\u000a\u000a<p>Firstly, to even ask the question requires a skeptical mindset. Secondly,\u000afinding the data requires time and research. Lastly, presenting the data in a\u000acompelling way takes some thought and creativity. Keeping in mind that I make no\u000aclaims to any of the above, let's give it a shot.</p>\u000a\u000a<h2>Why are base rates important?</h2>\u000a\u000a<p>The questions above attempt to get at the <a href="https://en.wikipedia.org/wiki/Base_rate">base rates</a> relevant to the\u000anews stories, which is important context to get a better understanding:</p>\u000a\u000a<blockquote>\u000a  <p>It may at first seem impressive that 1000 people beat their winter cold while\u000a  using 'Treatment X', until we look at the entire 'Treatment X' population and\u000a  find that the base rate of success is actually only 1/100.</p>\u000a</blockquote>\u000a\u000a<p>It is also well known from a large number of psych studies that people are\u000a<a href="https://en.wikipedia.org/wiki/Base_rate_fallacy">really bad at integrating base rates</a> into their thinking. Maybe\u000athis is why they are so rarely featured in the news? My hope is that by\u000apairing each headline with a bit of base rate information, we can become better\u000ainformed and address both negativity and get a better sense for trends over time.</p>\u000a\u000a<h2>Exhibit A: avalanche deaths (time series data)</h2>\u000a\u000a<p>Let's start with a simple quantitative (if morbid) example: Avalanche deaths.\u000aWe can better understand just how extreme the Rigopiano avalanche was if we put\u000ait into context. But what sort of context makes sense? If we consider geography,\u000awe can imagine concentric circles around Rigopiano.</p>\u000a\u000a<p><img src="rigopiano.png" alt="Possible geographic context for the Rigopiano avalanche" /></p>\u000a\u000a<p>On one extreme, we could consider other avalanches at Rigopiano specifically.\u000aBut for most people, especially outside of Italy, this is too specific.\u000aExpanding our search, we could consider all of the Apennines (the mountain range\u000acontaining Rigopiano), but I found that getting data for avalanche fatalities in\u000athis region was challenging. The outermost circle of the map above represents\u000athe European Alps, which does not include the Apennines. But it is the\u000ageographically closest region with readily available data.</p>\u000a\u000a<p><a href="https://docs.google.com/spreadsheets/d/1PyX0vav_NPziiaL9LWKhPOhTQLY-mcMYqYBl_VjUSmg/edit#gid=1197783313">This spreadsheet</a> contains data that I extracted from <a href="http://www.geogr-helv.net/71/147/2016/gh-71-147-2016.pdf">Avalanche\u000afatalities in the European Alps: long-term trends and statistics</a>,\u000awhich includes contiguous coverage from 1970 to 2015. Naturally, the paper\u000adidn't link to a data set, so I had to create the spreadsheet by visually\u000ainspecting the graph (ouch).</p>\u000a\u000a<p>The paper contains some interesting findings. For example, the number of\u000aavalanche deaths in controlled terrain (eg. ski resorts, where ski patrol\u000aengages in <a href="https://en.wikipedia.org/wiki/Avalanche_control">avalanche control tactics</a>) has decreased\u000asignificantly, but that the number of avalanche deaths in uncontrolled terrain\u000aremains significant (in the Alps, 100 yearly) and stable. Note that the numbers are\u000anot adjusted for the increasing global population, or for the increasing numbers\u000aof back country tourists.</p>\u000a\u000a<p><img src="alps-graph.png" alt="Avalanche deaths in the European Alps between 1970 and 2015" /></p>\u000a\u000a<p>One of the things that becomes clear is the important distinction between\u000acontrolled and uncontrolled accidents. We now have context for better\u000aunderstanding the tragedy at Rigopiano: it was a controlled accident that will\u000asend the statistics for 2017 through the roof. Let's see it in the context of\u000aother significant avalanches (controlled and not) over the years. The following\u000aclaimed more than 20 people since 1970, according to Wikipedia:</p>\u000a\u000a<p><img src="avalanches-since-1970.png" alt="Significant global avalanches since 1970" /></p>\u000a\u000a<p>Now we are armed to the teeth with data, but how do we present inline in the\u000anews? There are <a href="http://flowingdata.com/2017/01/24/one-dataset-visualized-25-ways/">tons of ways</a> of visualizing data in a compelling\u000away, but in this case we want it to appear in-situ in a digital newspaper. Why\u000anot start with <a href="https://www.edwardtufte.com/bboard/q-and-a-fetch-msg?msg_id=0001OR">Tufte</a>-inspired sparklines, since they are compact and\u000acan be placed adjacent to a headline.</p>\u000a\u000a<p><style>\u000aiframe#avalanche {\u000a  border: 0;\u000a  height: 200px;\u000a}\u000a</style></p>\u000a\u000a<iframe id="avalanche" src="avalanche-example.html"></iframe>\u000a\u000a<p>From the first graph, we can immediately see that significant avalanches are\u000arare, so this event is definitely newsworthy, but didn't claim as many lives as\u000asome of the most fatal ones, even recently. The second sparkline shows that\u000aavalanches on controlled terrain (in the Alps) claim fewer lives, which makes\u000aRigopiano even more significant. Then, to satisfy our curiosity, the third\u000asparkline shows annual avalanche fatalities on uncontrolled terrain is\u000apersistently high (c. 100 yearly). We now have some context to better understand\u000athis story.</p>\u000a\u000a<p>A quick note on technology. The above is a slightly modified version of <a href="https://github.com/phuu/sparksvg.git">Spark\u000aSVG</a>. I added a few things to the basic <code>bar.svg</code>:</p>\u000a\u000a<ul>\u000a<li>Set y-axis scale for fair comparisons across different graphs.</li>\u000a<li>Labels (x, y) values on hover.</li>\u000a<li>Ability to transpose the graph.</li>\u000a</ul>\u000a\u000a<p>As an aside, I was amused to discover the <a href="http://caaml.org/">Canadian Avalanche Association Markup\u000aLanguage (CAAML)</a>, which is a "standard for the electronic representation\u000aof information pertinent to avalanche safety operations". I had naively hoped to\u000aone day escape XML by becoming a ski bum. Not so fast!</p>\u000a\u000a<h2>Exhibit B: cabinet composition</h2>\u000a\u000a<p>Let us now turn our attention to Ken Wyatt, the newly appointed member of the\u000aAustralian cabinet. How ethnically diverse is the Australian cabinet? At\u000aminimum, we can look at base rates for ethnicity in the Australian cabinet. In\u000a1997, the cabinet was 100% white, but now with Wyatt's joining, the cabinet is\u000a96% white (he is the only non-white member). Not much to visualize yet, so let's\u000aexpand our scope.</p>\u000a\u000a<p>Consider three metrics: % female, % non-white and % non-christian for each\u000acabinet, and compare them across three cabinets: US, Australian and Canadian,\u000abetween two years: 1997 and 2017. I've collected this data <a href="https://docs.google.com/spreadsheets/d/1r6e92Xf4h8e7T83lrex-BghQilswGh_Hj-xmOaZCasc/edit?usp=sharing">in a\u000aspreadsheet</a>. It was a fair amount of work to skim Wikipedia\u000apages for six sets of cabinet members to try to gleam gender (easy), ethnicity\u000a(tricky) and religion (hard). While there are surely mistakes in the\u000aspreadsheet (please <a href="/about">email me</a> if you find one), it should be good\u000aenough for broad strokes. My own position would favor a qualified cabinet over a\u000adiverse one, but all things being equal, a cabinet that is representative of the\u000ageneral population is a good thing. Here's the headline with data alongside:</p>\u000a\u000a<p><style>\u000aiframe#cabinet {\u000a  border: 0;\u000a  height: 200px;\u000a}\u000a</style></p>\u000a\u000a<iframe id="cabinet" src="cabinet-example.html"></iframe>\u000a\u000a<p>To summarize, <a href="cabinet-sign.jpg">I've seen stronger cabinets at IKEA</a>!</p>\u000a\u000a<p>One thing that is clear from the above is that indeed, Australia has a very\u000awhite cabinet. Of course, in the spirit of representation, we should be\u000acomparing ethnicity numbers to the general population, but I'll leave that out\u000afor now (for reference, 3% is indigenous, and ~10% is non-white).</p>\u000a\u000a<p>Another thing the chart above shows are trends over the 20 year period.\u000aAustralia's cabinet is becoming more female, while staying roughly as white and\u000aas Christian. Canada's cabinet has become vastly more representative in gender,\u000aethnicity and religion. In stark contrast to Canada, the US has actually\u000aregressed in diversity on all fronts. Over the last 20 years, its has become\u000amore male dominated, more Christian, and more white. The last is especially\u000adisappointing since the US is far more ethnically diverse than Canada and\u000aAustralia put together (at "just" 72% white).</p>\u000a\u000a<p>I should mention a couple caveats. First, I really fudged the % Christian\u000acalculations, since it was so difficult to accurately determine religion for\u000amany cabinet members. Also, this analysis would greatly benefit from more data\u000apoints. For example, Clinton's cabinet in 1997 was quite diverse but probably\u000abecame even more diverse under Obama, but that data point is missing. Getting\u000aadditional data points for Canadian and Australian cabinets is more challenging,\u000asince there are no term limits, and cabinet members flow more freely in and out,\u000achange roles inside them, and sometimes even hold multiple offices. Lastly\u000athanks to the <a href="https://twitter.com/borismus/status/831641415604064256">good people on Twitter</a>, who sent me many constructive\u000asuggestions for improvement. I still think it's a bit too information dense,\u000abut it has come a long way.</p>\u000a\u000a<h2>Summing up</h2>\u000a\u000a<p>We looked at two headlines: one clearly well suited for contextualizing through\u000adata visualization (longitudinal time series), and another somewhat less so,\u000aregarding the composition of the Australian cabinet. In both cases, my\u000aunderstanding of the world has been enriched by the context that data\u000asurrounding it provided.</p>\u000a\u000a<p>Of course, there are many ways to <a href="https://en.wikipedia.org/wiki/Misleading_graph">mislead with graphs</a>, and sparklines\u000acan succumb to some of them. The axes are unlabeled, so the time scale is\u000aunknown unless specified. Nor is it clear whether or not the y-axis has been\u000adeliberately truncated. As a result, it can also be unclear whether or not\u000agraphs can be cross-compared. In the cabinet example, I had to explicitly\u000aspecify that all of the cabinet sparklines have a maximum value of 50% to\u000afacilitate this visual comparison, and exposed raw data on mouse hover.</p>\u000a\u000a<p>Imagine headlines from <a href="https://en.wikipedia.org/wiki/Portal:Current_events">your favorite news source</a> enhanced with a bit of\u000alongitudinal base rate for context. This would bring more clarity to the news,\u000agiving readers a better sense for general trends, as well as putting the event\u000ain a broader context. In many cases, the broader context is actually pretty\u000apositive: avalanche deaths in controlled areas have gone down drastically,\u000acabinets in many developed nations are becoming more representative.</p>\u000a\u000a<p>Some headlines may not fit the mold I'm proposing. Many of them are\u000aanecdotal in nature, like gossip stories, where You Won't Believe What Happened,\u000abecause it's such a unique situation. A certain president doing certain crazy\u000ashit comes to mind.  For other stories it can be very challenging to acquire the\u000adata required, like the Kremlin FSB arrest story. (I may or may not be privy to\u000athat sort of information. If I told you, I'd have to kill you.)</p>\u000a\u000a<p>One downside to this whole thing is that it requires a journalist to do more\u000awork: data sleuthing, careful thought about presentation, possibly even\u000aimplementing a new visualization. This work has intrinsic value, since it forces\u000athe author to broaden their understanding of the subject, and then whittle it\u000adown to the substantive kernel for public consumption. But ultimately, just like\u000aI really enjoy <a href="http://www.pewsocialtrends.org/interactives/what-do-police-think/">Pew Research's</a> approach to visualizing polls, headlines\u000awith visualizations of relevant base rates would make for a much more\u000ainformative and interesting read, and ultimately make us better informed\u000acitizens. What do you think?</p>\u000a
p2715
tp2716
Rp2717
sg13
V/headlines-meet-sparklines-news-in-context
p2718
sg15
Nsg16
I01
sg17
VHeadlines, meet sparklines: news in context
p2719
sg20
VNews reporting suffers from two major issues I'd like to tackle.
p2720
sg6
V<p>News reporting suffers from two major issues I'd like to tackle. The first is a\u000abias towards negative, emotionally laden events. The second is the difficulty of\u000acapturing information about gradual changes.</p>\u000a\u000a<p>These two deficiencies distort our perception. They make it easy for demagogues\u000ato claim that the world has gone to shit. The data tells a different story, as\u000athe late <a href="https://www.ted.com/talks/hans_rosling_shows_the_best_stats_you_ve_ever_seen">Hans Rosling</a> was fond of reminding us. My hypothesis is that\u000aif base rates were provided in a compelling way alongside news stories (or even\u000aheadlines), the public would be better informed. The challenges are many: first,\u000agetting and analyzing the data, but even more important, presenting it in a\u000areasonable way.</p>\u000a\u000a<p>In this post, let's explore what that would entail, from data collection, to\u000aanalysis, to visualization. We'll go through a couple of examples.</p>\u000a\u000a
p2721
sg25
g169
sg33
g2718
sg170
(dp2722
g172
S'Feb'
p2723
sg174
S'February 17, 2017'
p2724
sg176
I2
sg177
S'2017-02-17T09:00:00-00:00'
p2725
sg179
I1487350800
sg180
I2017
sg181
I17
ssg65
g182
sg31
S'headlines-meet-sparklines-news-in-context'
p2726
sS'posted'
p2727
g188
(S'\x07\xe1\x02\x11'
p2728
tp2729
Rp2730
ssg34
S'content/posts/2017/headlines-meet-sparklines-news-in-context/index.md'
p2731
sg36
F1487356068.0
sa(dp2732
g2
(dp2733
g4
V In a famous letter dating back to 1772, Benjamin Franklin described how he made decisions to a friend who was facing a dilemma. Franklin's method involved enumerating pros and cons of an argument, and then attempting to weigh one against the other to ultimately decide which of the two possibilities to pursue. Franklin wrote:        My way is to divide half a sheet of paper by a line into two columns; writing   over the one Pro, and over the other Con. Then, during three or four days   consideration, I put down under the different heads short hints of the different   motives, ... I endeavor to estimate their respective weights.      This post attempts to modernize Franklin's method to attempt to overcome some of its shortcomings. Once we have gathered our thoughts in one place using this spreadsheet format, we can, with the help of others or using (aspirational) AI, assist the decision maker to help them combat common mistakes.   
p2734
sg28
g7
(g8
g9
V<p>In a famous letter dating back to 1772, Benjamin Franklin described how he made\u000adecisions to a friend who was facing a dilemma. Franklin's method involved\u000aenumerating pros and cons of an argument, and then attempting to weigh one\u000aagainst the other to ultimately decide which of the two possibilities to pursue.\u000aFranklin wrote:</p>\u000a\u000a<blockquote>\u000a  <p>My way is to divide half a sheet of paper by a line into two columns; writing\u000a  over the one Pro, and over the other Con. Then, during three or four days\u000a  consideration, I put down under the different heads short hints of the different\u000a  motives, ... I endeavor to estimate their respective weights.</p>\u000a</blockquote>\u000a\u000a<p>This post attempts to modernize Franklin's method to attempt to overcome some of\u000aits shortcomings. Once we have gathered our thoughts in one place using this\u000aspreadsheet format, we can, with the help of others or using (aspirational) AI,\u000aassist the decision maker to help them combat common mistakes.</p>\u000a\u000a<!--more-->\u000a\u000a<h2>Modern tools for decision making</h2>\u000a\u000a<p>Franklin's method is explicitly qualitative: "...the weight of the reasons\u000acannot be taken with the precision of algebraic quantities". Of course, this has\u000anot stopped many scientists and engineers from attempting to create quantitative\u000atools that assist in decision making, called <a href="https://en.wikipedia.org/wiki/Decision_support_system">decision support systems</a>.\u000aHowever these are mostly targeted at companies and not individuals. I\u000a<a href="https://1000minds.com">tried</a> <a href="https://meenymo.com/">a couple</a> and failed to find one that was simple enough\u000afor my purposes.</p>\u000a\u000a<p>As a result, it seems that the state of the art for individuals hasn't advanced\u000amuch beyond Franklin's method. Product comparisons are one notable exception:</p>\u000a\u000a<p><img src="product-comparison.png" alt="Product comparison chart example" /></p>\u000a\u000a<p>What if we could take product comparison charts, but make them a bit more\u000aquantitative, and then apply the technique to decision making?</p>\u000a\u000a<h2>Decision support spreadsheets</h2>\u000a\u000a<p>Simply stated, a topic is controversial (or a decision is difficult) if:</p>\u000a\u000a<blockquote>\u000a  <p>...there are good arguments on all sides. Good thinking involves balancing these\u000a  arguments in a quantitative way, taking into account their relative strengths\u000a  and weaknesses.</p>\u000a</blockquote>\u000a\u000a<p>Inspired by this and other parts of Jon Baron's <a href="https://www.amazon.com/Thinking-Deciding-4th-Jonathan-Baron/dp/0521680433">Thinking and\u000aDeciding</a>, I made <a href="https://docs.google.com/spreadsheets/d/1HBjUBa1NaD2jGr4QzN1prIbQgCBFqcjtHnRkXuenIHs/edit#gid=0">a spreadsheet</a> attempting to codify what he\u000adescribes as the "search-inference" process. Here's an example of an imaginary\u000arocket scientist deciding between two job offers based on two goals, resulting\u000ain a 2x2 sheet:</p>\u000a\u000a<p><img src="decision-spreadsheet.png" alt="Screenshot of Google Sheets decision spreadsheet" /></p>\u000a\u000a<p>Structurally, it works like this:</p>\u000a\u000a<ul>\u000a<li>Columns are possible courses of action (eg. NASA vs SpaceX).</li>\u000a<li>Rows are goals that you are trying to achieve (eg. improve the world, work\u000awith great people).</li>\u000a<li>Cells contain evidence pertaining to the associated possibility (row) and goal\u000a(column). In this case, the NASA job would improve the world by enabling much\u000afaster space travel.</li>\u000a</ul>\u000a\u000a<p>There are also numbers involved:</p>\u000a\u000a<ul>\u000a<li>Each goal (row) has a number between 1 and 5 under it, corresponding to how\u000aimportant the goal is to you. The higher the number, the more important.</li>\u000a<li>Each piece of evidence (sub-cell) has a weight to the right between -2 and 2.\u000aPositive weights are pros, negative ones are cons.</li>\u000a</ul>\u000a\u000a<p>A <a href="https://docs.google.com/spreadsheets/d/1HBjUBa1NaD2jGr4QzN1prIbQgCBFqcjtHnRkXuenIHs/edit#gid=374695355">second sheet</a> does all of the calculations. Each cell reduces to a weight in a\u000adecision matrix. Ultimately, each possibility (column) is given a score between\u000a0 and 1. The recommended course of action is the possibility with the highest\u000ascore. So the above 2x2 spreadsheet is converted into this decision matrix.</p>\u000a\u000a<p><img src="decision-spreadsheet-calculations.png" alt="Second sheet of Google Sheets decision\u000aspreadsheet" /></p>\u000a\u000a<p>Then, given these weights, we do a simple calculation for each possibility\u000a(column): a normalized, weighted sum. So for the first possibility, we calculate:</p>\u000a\u000a<pre><code>(0.6 * 0.9 + 0.8 * 0.63) / (0.6 + 0.8) = 0.75\u000a</code></pre>\u000a\u000a<p>We do the same for each possibility, and the one with the highest resulting\u000ascore is the "best" course of action.</p>\u000a\u000a<h3>Advantages of this method</h3>\u000a\u000a<p>Even geniuses like Franklin have a limited capacity for holding multiple\u000athoughts in their heads at once. After laying out all of the arguments, Franklin\u000awrote, "the whole lies before me, I think I can judge better". With all of the\u000apossibilities, goals and evidence in one place, you too can be like Franklin.</p>\u000a\u000a<p>As for the specifics of my spreadsheet above, I can't claim that this method is\u000aoptimal or even particularily good (though feedback on this would be\u000aappreciated). I created it as a placeholder, loosely inspired by Baron,\u000aFranklin, and other less rigorous approaches I've tried in the past. As it\u000aturns out, this method is essentially an example of <a href="https://en.wikipedia.org/wiki/Analytic_hierarchy_process">Analytic Hierarchy Process\u000a(AHP)</a>.</p>\u000a\u000a<p>Rather than sticking to Franklin's two column split, this spreadsheet is\u000asomewhat more complex, but there are some advantages:</p>\u000a\u000a<ol>\u000a<li>Most decisions <a href="http://lesswrong.com/lw/hu/the_third_alternative/">aren't actually binary</a>, and this is captured by\u000ahaving multiple columns.</li>\u000a<li>The method makes the notion of your goals and their relative importance\u000aexplicit.</li>\u000a<li>Rather than pros and cons, we collect evidence that helps you decide about a\u000agoal and a possibility, which can then be graded numerically.</li>\u000a</ol>\u000a\u000a<p>Despite the mechanistic appearance of this approach, Baron emphasizes the\u000anonlinearity of the thinking process. As you collect evidence, you may uncover\u000anew possibilities and goals. With all of the evidence laid out, you can begin\u000aasking better questions, attempting to fight known failure modes in human\u000athinking.</p>\u000a\u000a<h3>Reducing and increasing complexity</h3>\u000a\u000a<p>One significant challenge with the above approach is that of assigning weights.\u000aAt the moment, my method involves coming up with two kinds of weights: goal\u000aweights (eg. how important is it for you improve the world, really?), and\u000aevidence weights (eg. is space travel really such a world improving thing?).\u000aThis method is flexible enough to be easily simplified. For example:</p>\u000a\u000a<ol>\u000a<li>Evidence weights can be simplified by scoring pros as +1, and cons as -1.</li>\u000a<li>Goal weights can be simplified by binary ranking (eg. 1 is critical, 0 is\u000anice-to-have).</li>\u000a</ol>\u000a\u000a<p>A potentially better approach is known as <a href="https://en.wikipedia.org/wiki/Potentially_all_pairwise_rankings_of_all_possible_alternatives">PAPRIKA</a>, which establishes\u000aweights based on a bunch of pair-wise comparisons. This might work well, and\u000acould actually be useful for capturing additional points of evidence. To get a\u000afeeling for it, there's a consumer-oriented decision support system <a href="https://meenymo.com/">called\u000aMeenyMo</a> that does this. The process is quite tedious though, involving\u000atens of comparisons like this:</p>\u000a\u000a<p><img src="meenymo.png" alt="MeenyMo's PAPRIKA style comparisons" /></p>\u000a\u000a<p>The other downside of PAPRIKA is that it requires discrete categories (eg. cost\u000aof living: cheap, moderate, expensive).</p>\u000a\u000a<h2>Thinkos: inevitable irrationality</h2>\u000a\u000a<p>People aren't perfect, and neither is our thinking. Biases are sort of like\u000athinking bugs that make our thoughts less rational. Irrational thinking leads to\u000aconclusions that are further from the actual objective truth. This is, as I\u000ahope you'll agree, undesirable.</p>\u000a\u000a<p>Now that we're on the same page, Baron suggests that certain tactics that can\u000ahelp us make better decisions by improve thinking and reducing bias. These he\u000abroadly describes as "active open mindedness":</p>\u000a\u000a<ul>\u000a<li>Seek alternative possibilities. Anchoring bias tends to favor the first\u000apossibilities you generate, but it is entirely possible that you haven't\u000asearched enough.</li>\u000a<li>Formulate goals better. What are you actually trying to achieve? (eg. "protect\u000awalls from child's scribbling" vs. "prevent child from scribbling on walls").</li>\u000a<li>Look for counterevidence (eg. if there are strong pros, see if there are some\u000acons too).</li>\u000a<li>Avoid belief overkill, which happens when there is a strong correlation\u000abetween different goals (eg. most people are against capital punishment because\u000ait is both ineffective and immoral, whereas those <em>for</em> capital punishment are in\u000afavor because it's effective and moral. But why do both go together? They\u000ashould be unrelated.)</li>\u000a<li>Allocate time that is proportionate to the importance of the decision.\u000aFranklin's method suggests to take "three or four days consideration" to\u000acapture evidence, and then "a day or two of further consideration" to let it\u000aall settle.</li>\u000a</ul>\u000a\u000a<p>There are <a href="https://en.wikipedia.org/wiki/List_of_cognitive_biases">many other biases</a> that can lead to bad decisions. The above\u000aserves as an example of some thinkos that can be reduced with external help:\u000aother people or software.</p>\u000a\u000a<h2>Summing up...</h2>\u000a\u000a<p>In some facets of life, it is impossible to apply this level of rigor.\u000aQuantifying your love for a person, for example, feels cold hearted and\u000acalculating, and I try to avoid it. Ironically, one of the most famous uses of\u000aFranklin's method was used by Charles Darwin in deciding whether or not to marry\u000aEmma Wedgwood. For what it's worth, the method appears to have worked, with\u000aDarwin emphatically scribbling "Marry, Marry, Marry, QED" after his\u000acalculations.</p>\u000a\u000a<p>It is hard to fully discount the role of feeling. The quality of the\u000arational decision making process depends heavily on your ability to formulate\u000ayour true goals and possibilities, and collect all of the evidence and score it\u000acorrectly. Gut feeling, (or as Kahneman says, System 1 thinking), can actually\u000aincorporate many arguments that one might not even be able to formulate, and yet\u000athose intangibles may end up being incredibly important.</p>\u000a\u000a<p>And lastly, there is the question of practicality. Life is dynamic and\u000acircumstances can change quickly. For the spreadsheet-powered decision maker,\u000athis means constant revision, which can be complicated and time consuming. I\u000aexperienced this first hand, attempting to use this method to help make a career\u000amove. Just when I thought I had established the teams that would have me,\u000aanother one emerged, and I had to re-enter additional evidence, remove options\u000athat seemed appealing, but in retrospect were duds, and re-calibrate weights.</p>\u000a\u000a<p>Thinking cannot be reduced to a spreadsheet, but when used in moderation, I hope\u000athat this method can be useful for some.</p>\u000a
p2735
tp2736
Rp2737
sg13
V/making-better-decisions
p2738
sg15
Nsg16
I01
sg17
VTools for making better decisions
p2739
sg20
VIn a famous letter dating back to 1772, Benjamin Franklin described how he made\u000adecisions to a friend who was facing a dilemma.
p2740
sg6
V<p>In a famous letter dating back to 1772, Benjamin Franklin described how he made\u000adecisions to a friend who was facing a dilemma. Franklin's method involved\u000aenumerating pros and cons of an argument, and then attempting to weigh one\u000aagainst the other to ultimately decide which of the two possibilities to pursue.\u000aFranklin wrote:</p>\u000a\u000a<blockquote>\u000a  <p>My way is to divide half a sheet of paper by a line into two columns; writing\u000a  over the one Pro, and over the other Con. Then, during three or four days\u000a  consideration, I put down under the different heads short hints of the different\u000a  motives, ... I endeavor to estimate their respective weights.</p>\u000a</blockquote>\u000a\u000a<p>This post attempts to modernize Franklin's method to attempt to overcome some of\u000aits shortcomings. Once we have gathered our thoughts in one place using this\u000aspreadsheet format, we can, with the help of others or using (aspirational) AI,\u000aassist the decision maker to help them combat common mistakes.</p>\u000a\u000a
p2741
sg25
g169
sg33
g2738
sg170
(dp2742
g172
S'Feb'
p2743
sg174
S'February 1, 2017'
p2744
sg176
I2
sg177
S'2017-02-01T09:00:00-00:00'
p2745
sg179
I1485968400
sg180
I2017
sg181
I1
ssg65
g182
sg31
S'making-better-decisions'
p2746
sS'posted'
p2747
g188
(S'\x07\xe1\x02\x01'
p2748
tp2749
Rp2750
ssg34
S'content/posts/2017/making-better-decisions/index.md'
p2751
sg36
F1486003277.0
sa.