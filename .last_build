(lp0
(dp1
S'info'
p2
(dp3
S'snip'
p4
ccopy_reg
_reconstructor
p5
(cmarkdown2
UnicodeWithAttrs
p6
c__builtin__
unicode
p7
V<p>Welcome</p>\u000a
p8
tp9
Rp10
sS'permalink'
p11
V/
p12
sS'description'
p13
NsS'is_long'
p14
I00
sS'title'
p15
VWelcome
p16
sS'list'
p17
NsS'summary'
p18
V
p19
sS'filter'
p20
(lp21
S'post'
p22
asS'verb'
p23
S'Published'
p24
sS'limit'
p25
I10
sS'content'
p26
g10
sS'type'
p27
S'index'
p28
sS'slug'
p29
S'main'
p30
sS'computed_link'
p31
g12
ssS'path'
p32
S'content/main.md'
p33
sS'modified'
p34
F1433949532.0
sa(dp35
g2
(dp36
g4
g5
(g6
g7
V<p></p>\u000a
p37
tp38
Rp39
sg11
V/atom
p40
sg13
Nsg14
I00
sg15
VBoris Smus
p41
sS'list'
p42
Nsg18
g19
sS'filter'
p43
(lp44
S'post'
p45
aS'link'
p46
aS'talk'
p47
asg23
g24
sS'limit'
p48
I50
sg26
g39
sS'type'
p49
S'feed'
p50
sg29
S'atom'
p51
sg31
g40
ssg32
S'content/feeds/atom.md'
p52
sg34
F1433976468.0
sa(dp53
g2
(dp54
g26
g5
(g6
g7
V<p><img id='image-me' src='/static/images/stipple.png'/></p>\u000a\u000a<p>Hello from the west coast!</p>\u000a\u000a<p>My passion is to invent and build enjoyable, useful, and novel user\u000ainterfaces at the cusp of hardware and software. I am a proficient\u000afront-end and mobile engineer with experience leading teams. In my\u000aprojects, I tend to gravitate to web technologies because of the\u000aflexibility of the web platform. My professional interests include\u000aeducation, audio and music, sensors and interaction design.</p>\u000a\u000a<p>My job at Google is a mix of software engineering and user experience\u000aprototyping. I have had the good fortune to help launch <a href="http://g.co/cardboard">Google\u000aCardboard</a>, <a href="http://g.co/tone">Google Tone</a>, and build many other exciting\u000aproducts with a small, fast moving team.</p>\u000a\u000a<p>At my previous job as an engineer on the Chrome developer relations team\u000awe pushed the limits of the modern web platform and educated web\u000adevelopers. I helped launch <a href="https://play.google.com/store/apps/details?id=com.android.chrome&amp;hl=en">Chrome for Android</a>, <a href="http://www.html5rocks.com/">wrote\u000aarticles</a>, <a href="/talks">gave talks</a> and released open source libraries\u000aand samples. I also authored a short <a href="http://www.amazon.com/Web-Audio-API-Boris-Smus/dp/1449332684">book on the Web Audio API</a>.</p>\u000a\u000a<p>Before Google, I was a graduate student at <a href="http://www.hcii.cmu.edu/academics/mhci">Carnegie Mellon</a>, where\u000aI focused on social and <a href="https://scholar.google.com/citations?user=bIgFmUwAAAAJ">wearable computing research</a> and\u000abuilt <a href="http://crowdforge.com/">CrowdForge</a>, a system for crowdsourcing complex\u000atasks. Prior to that, I worked at Apple for two years, contributing to\u000aiWeb, iWork and helping start <a href="https://www.apple.com/iwork-for-icloud/">iWork.com</a>. For the curious, my\u000a<a href="/resume">resume is online</a>.</p>\u000a\u000a<p>I am Canadian. I studied Mathematics and Computer Science at the\u000a<a href="https://www.cs.ubc.ca/">University of British Columbia</a>, and spent my formative years in\u000aVancouver.</p>\u000a\u000a<ul>\u000a<li>Email: <a href="mailto:boris@smus.com">boris@smus.com</a></li>\u000a<li>Twitter: <a href="http://twitter.com/borismus">@borismus</a></li>\u000a<li>GitHub: <a href="http://github.com/borismus">@borismus</a></li>\u000a</ul>\u000a
p55
tp56
Rp57
sg11
V/about
p58
sg13
Nsg14
I00
sg15
VAbout the author
p59
sg18
V<img id='image-me' src='/static/images/stipple.
p60
sg4
g57
sg23
g24
sS'type'
p61
S'page'
p62
sg29
S'about'
p63
sg31
g58
ssg32
S'content/pages/about.md'
p64
sg34
F1433895699.0
sa(dp65
g2
(dp66
g4
g5
(g6
g7
V<p>Everything on this blog.</p>\u000a
p67
tp68
Rp69
sg11
V/blog
p70
sg13
Nsg14
I00
sg15
VBlog
p71
sS'list'
p72
Nsg18
VEverything on this blog.
p73
sS'filter'
p74
S'post'
p75
sg23
g24
sS'limit'
p76
I1000
sg26
g69
sS'type'
p77
S'archive'
p78
sg29
S'blog'
p79
sg31
g70
ssg32
S'content/pages/blog.md'
p80
sg34
F1433904595.0
sa(dp81
g2
(dp82
g26
g5
(g6
g7
V<p>Interesting, inspirational and informative objects that I've collected\u000afrom the the digital and physical worlds.</p>\u000a
p83
tp84
Rp85
sg11
V/inspiration
p86
sg13
Nsg14
I00
sg15
VBlog
p87
sg18
VInteresting, inspirational and informative objects that I've collected\u000afrom the the digital and physical worlds.
p88
sg4
g85
sg23
g24
sS'type'
p89
S'gallery'
p90
sg29
S'inspiration'
p91
sg31
g86
ssg32
S'content/pages/inspiration.md'
p92
sg34
F1433979496.0
sa(dp93
g2
(dp94
g26
g5
(g6
g7
V<h2>Experience</h2>\u000a\u000a<p><strong>2012 - Current: Software Engineer, <a href="http://www.google.com">Google</a></strong></p>\u000a\u000a<ul>\u000a<li>Prototyping emerging user interfaces at Google Research.</li>\u000a</ul>\u000a\u000a<p><strong>2010 - 2012: Chrome Developer Programs Engineer, <a href="http://www.google.com">Google</a></strong></p>\u000a\u000a<ul>\u000a<li>Released and maintained widely used libraries to help mobile web\u000adevelopers</li>\u000a<li>Wrote significant open source sample applications (see <a href="https://github.com/borismus/">github</a>)</li>\u000a<li>Wrote technical articles on <a href="http://html5rocks.com">http://html5rocks.com</a> and\u000a<a href="http://smus.com">http://smus.com</a> totalling over 500K page views</li>\u000a<li>Presented over 20 public talks, reaching over 100K people (including\u000avideo views)</li>\u000a<li>Software engineer on the Google +1 chrome extension</li>\u000a</ul>\u000a\u000a<p><strong>Summer 2010: Software Engineer Intern, <a href="http://www.google.com">Google</a></strong></p>\u000a\u000a<ul>\u000a<li>Wrote product requirements and designed UI concepts for a major\u000aGoogle TV feature</li>\u000a<li>Implemented a working prototype in Java/Android</li>\u000a<li>Designed and executed a lab usability study</li>\u000a</ul>\u000a\u000a<p><strong>2006 \u2013 2009: Software Engineer, <a href="http://www.apple.com">Apple</a></strong></p>\u000a\u000a<ul>\u000a<li>Developed software architecture for significant portions of\u000a<a href="http://www.iwork.com/">iWork.com</a></li>\u000a<li>Implemented new features for iWeb and the iWork suite in Objective-C</li>\u000a<li>Created a text and object selection engine in JavaScript</li>\u000a<li>Built a cross-browser graphics library using SVG, Canvas and VML</li>\u000a<li>Prototyped new product ideas using JavaScript and Ruby on Rails</li>\u000a<li>Worked closely with visual designers on new products</li>\u000a<li>Wrote parts of the Microsoft Office document viewer for iPhone Mail</li>\u000a</ul>\u000a\u000a<p><strong>Summer 2005: Software Engineer Intern, <a href="http://www.rim.com">Research in Motion</a></strong></p>\u000a\u000a<ul>\u000a<li>Developed features for a 2D graphics engine in J2ME, part of\u000aBlackBerry OS</li>\u000a<li>Created a python program to automate SVG content generation to test\u000athis engine</li>\u000a<li>Proposed a test suite to verify rendering output against a image</li>\u000a</ul>\u000a\u000a<p><strong>Summer 2004: Software Engineer Intern, <a href="http://www.customhouse.ca/">Custom House</a></strong></p>\u000a\u000a<ul>\u000a<li>Created a GUI to facilitate user friendly mapping from excel into a\u000aMSSQL database</li>\u000a<li>Contributed improvements to a large currency exchange system written\u000ain C#</li>\u000a<li>Followed Agile software development principles: unit testing, scrums</li>\u000a</ul>\u000a\u000a<p><strong>Summer 2003: Software Engineer Intern, SchemaSoft (acquired by Apple in 2005)</strong></p>\u000a\u000a<ul>\u000a<li>Developed bindings for a subset of the SVG DOM in C++</li>\u000a<li>Implemented in C# an editor for easily editing and navigating XML\u000adocuments</li>\u000a<li>Maintained a network of 100 computers running Linux, Mac, and\u000aWindows</li>\u000a<li>Provided technical support to a large international conference: SVG\u000aOpen 2003</li>\u000a</ul>\u000a\u000a<h2>Education</h2>\u000a\u000a<p><strong>2009 \u2013 2010: Graduate Student, <a href="http://www.cmu.edu">Carnegie Mellon University</a></strong></p>\u000a\u000a<ul>\u000a<li>Masters in Human-Computer Interaction.</li>\u000a<li>Emphasis on physical computing.</li>\u000a</ul>\u000a\u000a<p><strong>2002 \u2013 2007: Undergraduate Student, <a href="http://www.ubc.edu">University of British Columbia</a></strong></p>\u000a\u000a<ul>\u000a<li>BSc (with honors) in Mathematics and Computer Science (with Co-op)</li>\u000a<li>Emphasis on computer graphics and discrete math.</li>\u000a</ul>\u000a\u000a<h2>Other</h2>\u000a\u000a<p><strong>Publications</strong></p>\u000a\u000a<ul>\u000a<li><p>Kittur, N., Smus, B., and Kraut, R.E., Khamkar, S. CrowdForge:\u000aCrowdsourcing Complex Work. <em>Proceedings of UIST 2011, Santa Barbara,\u000aCA.</em></p></li>\u000a<li><p>Rogstadius, J., Kittur, N., Kostakos, V., Smus, B., Laredo, J., Vukovic, M.\u000aAn Assessment of Intrinsic and Extrinsic Motivation on Task Performance in\u000aCrowdsourcing Markets. <em>Conference on Weblogs and Social Media 2011,\u000aBarcelona, Spain</em></p></li>\u000a<li><p>Kittur, N., Smus, B., and Kraut, R.E. CrowdForge: Crowdsourcing Complex\u000aWork. <em>Proceedings of the extended abstracts of CHI 2011, Vancouver,\u000aCanada.</em></p></li>\u000a<li><p>Smus, B., Kostakos, V. Running gestures: hands-free interaction\u000aduring physical activity.  <em>Adjunct proceedings of Ubicomp 2010,\u000aCopehnagen, Denmark.</em></p></li>\u000a<li><p>Smus, B., Gross, M. Ubiquitous drums: a tangible, wearable musical\u000ainterface. <em>Proceedings of the extended abstracts of CHI 2010, Atlanta,\u000aGeorgia.</em></p></li>\u000a</ul>\u000a\u000a<p><strong>Patents</strong></p>\u000a\u000a<ul>\u000a<li><p>10+ pending at Google.</p></li>\u000a<li><p>Smus, B. Automatically switching between input modes for a user\u000ainterface <a href="">US 9026939</a></p></li>\u000a<li><p>Smus, B. Automatic device login based on wearable sensor fusion,\u000aGoogle Inc <a href="">US 8928587</a></p></li>\u000a<li><p>Smus, B. Keyboard event detection and network event generation, Google\u000aInc <a href="">US 8803808</a></p></li>\u000a<li><p>Bultrowicz, M., Smus, B. Shared Comments for Online Document\u000aCollaboration, Apple Inc <a href="">US 8803808</a></p></li>\u000a</ul>\u000a
p95
tp96
Rp97
sg11
V/resume
p98
sg13
Nsg14
I00
sg15
VResume
p99
sg18
V## Experience\u000a\u000a**2012 - Current: Software Engineer, [Google][]**\u000a\u000a- Prototyping emerging user interfaces at Google Research.
p100
sg4
g97
sg23
g24
sg61
g62
sg29
S'resume'
p101
sg31
g98
ssg32
S'content/pages/resume.md'
p102
sg34
F1433908931.0
sa(dp103
g2
(dp104
g26
g5
(g6
g7
V<p>This site is statically generated with <a href="https://github.com/borismus/lightning">Lightning</a>, a\u000ablogging engine I wrote. The basic idea is that I write all of my\u000acontent like this:</p>\u000a\u000a<pre><code>About this site\u000a===============\u000a\u000aThis site is statically generated with [Lightning][lightning]\u000a</code></pre>\u000a\u000a<p>Then a script converts it into the HTML/CSS that you are reading now.</p>\u000a\u000a<p>There are many such projects, (eg. <a href="http://ringce.com/hyde">Hyde</a>,\u000a<a href="http://octopress.org/">Octopress</a>). This approach provides clear advantages over a\u000atraditional database-based blog:</p>\u000a\u000a<ul>\u000a<li>Use any editor you like.</li>\u000a<li>Use Markdown for structured text.</li>\u000a<li>No worries about database management and security.</li>\u000a</ul>\u000a\u000a<h2>Lightning</h2>\u000a\u000a<p>I wrote Lightning to scratch some specific itches (which may or may not\u000ahave been addressed in other similar projects by the time you read\u000athis):</p>\u000a\u000a<ul>\u000a<li>Incremental builds (only rebuild what changed).</li>\u000a<li>Quick deployment.</li>\u000a<li>Logical placement of content-specific assets (in same dir as\u000acontent).</li>\u000a<li>Decouple content structure from output structure.</li>\u000a<li>Minimalist metadata header.</li>\u000a<li>Output that doesn't require htaccess, rewrite rules and other HTTPD\u000aspecific setup.</li>\u000a</ul>\u000a\u000a<p>An early version of <a href="https://github.com/borismus/lightning">Lightning is available on github</a>.</p>\u000a
p105
tp106
Rp107
sg11
V/site
p108
sg13
Nsg14
I00
sg15
VAbout this site
p109
sg18
VThis site is statically generated with [Lightning][lightning], a\u000ablogging engine I wrote.
p110
sg4
g107
sg23
g24
sg61
g62
sg29
S'site'
p111
sg31
g108
ssg32
S'content/pages/site.md'
p112
sg34
F1433965388.0
sa(dp113
g2
(dp114
g26
g5
(g6
g7
V<p>I'm thankful for many opportunities to co-present with and learn from\u000asome amazing people. Here are some talks I gave recently.</p>\u000a\u000a<ul>\u000a<li><p>Co-presented <a href="g.co/cardboard">Cardboard</a> with <a href="https://twitter.com/dav_cz">David\u000aCoz</a> and <a href="http://plagemann.net/">Christian\u000aPlagemann</a> at <a href="https://www.google.com/events/io">Google I/O\u000a2014</a>. The <a href="https://www.youtube.com/watch?v=DFog2gMnm44">talk was\u000arecorded</a>.</p></li>\u000a<li><p>Co-presented Point, Click, Tap, Touch with <a href="https://twitter.com/rickbyers">Rick\u000aByers</a> at <a href="https://developers.google.com/events/io/">Google I/O\u000a2013</a>. The talk was\u000a<a href="http://www.youtube.com/watch?v=DujfpXOKUp8">recorded</a> and the <a href="https://docs.google.com/a/google.com/presentation/d/1-n1qyzewpagREbzW2zm0wOalq33UhbtbSkWf9mEdly8/edit">slides</a> are posted.</p></li>\u000a<li><p>Presented Building fast UIs for the cross-device web at <a href="https://developers.google.com/events/io/2012/">Google I/O\u000a2012</a> and at the <a href="http://www.sfhtml5.org/">SF\u000aHTML5 meetup</a>. The\u000a<a href="http://www.youtube.com/watch?v=ie4I7B-umbA">video</a> and\u000a<a href="http://smustalks.appspot.com/io-12">slides</a> are available.</p></li>\u000a<li><p>Co-presented Audio on the web with <a href="http://gonze.com/">Lucas Gonze</a> at\u000athe <a href="http://www.sfmusictech.com/">SF Music Tech conference in 2012</a>.\u000aThe <a href="http://smustalks.appspot.com/sfmt-12/">slides are available</a>.</p></li>\u000a<li><p>Presented <a href="http://crowdforge.com">CrowdForge</a> at <a href="/uist-2011">UIST\u000a2011</a>. The <a href="http://smustalks.appspot.com/crowdforge-11/">slides are available</a>.</p></li>\u000a<li><p>Traveled to India and presented the keynote at both the Bangalore and\u000aHyderabad DevFests in 2011. The <a href="http://smustalks.appspot.com/keynote-india-devfest-11">slides are\u000aavailable</a>.</p></li>\u000a<li><p>Co-organized and presented at the HTML5 in-app payments hackathon in\u000aGoogle Kirkland with <a href="http://petelepage.com/">Pete LePage</a> and <a href="https://twitter.com/pengying">Peng\u000aYing</a>. <a href="http://html5hack.appspot.com/preso/crhack.html">Slides</a> are posted.</p></li>\u000a<li><p>Presented the Chrome Developer Tools at <a href="http://beta.theexpressiveweb.com/">Adobe's HTML5\u000aCamp</a> and later at\u000a<a href="http://www.meetup.com/vancouver-javascript-developers/">VanJS</a>. Slides\u000a<a href="http://smustalks.appspot.com/devtools-adobe-11">are available</a>.</p></li>\u000a<li><p>Presented on mobile web touch events at the <a href="http://html5summerdevfest.eventbrite.com/">HTML5 Summer Dev\u000aFest</a> in Vancouver,\u000aorganized by <a href="https://twitter.com/bmann">Boris Mann</a>. The <a href="http://smustalks.appspot.com/touch-11/">slides are available</a>.</p></li>\u000a<li><p>In 2011, I traveled to Sao Paulo, Brazil and gave a general\u000apresentation on HTML5 and Chrome to a variety of Brazilian media\u000acompanies. The <a href="http://smustalks.appspot.com/brazil-11/">slides are\u000aavailable</a>.</p></li>\u000a<li><p>Co-presented a workshop on the Chrome Developer Tools with <a href="http://paulirish.com/">Paul\u000aIrish</a> at Google I/O 2011 Bootcamp in San\u000aFrancisco, CA. The materials from the workshop are <a href="https://github.com/borismus/DevTools-Lab">available on\u000agithub</a>.</p></li>\u000a<li><p>My first public talk was about freakin' HTML5, co-presented with Paul\u000aIrish at SXSW 2011. Our <a href="http://smustalks.appspot.com/sxsw-11/">slides are\u000aposted</a>.</p></li>\u000a</ul>\u000a
p115
tp116
Rp117
sg11
V/talks
p118
sg13
Nsg14
I00
sg15
VTalks I've given
p119
sg18
VI'm thankful for many opportunities to co-present with and learn from\u000asome amazing people.
p120
sg4
g117
sg23
g24
sg61
g62
sg29
S'talks'
p121
sg31
g118
ssg32
S'content/pages/talks.md'
p122
sg34
F1433867556.0
sa(dp123
g2
(dp124
g26
g5
(g6
g7
V<script>\u000aGALLERY_URL = '/projects/all.json'\u000a</script>\u000a\u000a<p>Recent projects I've worked on.</p>\u000a
p125
tp126
Rp127
sg11
V/projects
p128
sg13
Nsg14
I00
sg15
VBlog
p129
sg18
V<script>\u000aGALLERY_URL = '/projects/all.
p130
sg4
g127
sg23
g24
sS'type'
p131
S'gallery'
p132
sg29
S'projects'
p133
sg31
g128
ssg32
S'content/pages/projects/index.md'
p134
sg34
F1434226119.0
sa(dp135
g2
(dp136
g26
g5
(g6
g7
V<p>I wanted a tool that would analyze a complete web site, and report what\u000aCSS selectors and IDs are dead. By dead, I mean one of two things --\u000aeither the ID or selector is referenced from the HTML and undefined in\u000athe CSS or it is defined in the CSS but never referenced in the HTML.</p>\u000a\u000a<p>A search for some CSS finding utilities proved somewhat fruitful. I\u000afound a brief <a href="http://www.aggiorno.com/blogs/aggiornings/post/Detecting-unused-CSS-selectors-.aspx">survey of related utilities</a> for this finding or\u000acleaning dead CSS, the most promising of which was a Firefox plug-in\u000acalled <a href="http://www.sitepoint.com/dustmeselectors/">Dust-Me Selectors</a>. But I wanted something that could be\u000aintegrated into an automatic build process, easily invokable from the\u000acommand line without requiring a browser, so I started thinking about a\u000acustom solution.</p>\u000a\u000a<p>The problem can be solved as follows:</p>\u000a\u000a<ol>\u000a<li>Find all referenced IDs and classes, called R</li>\u000a<li>Find all defined IDs and classes, called D</li>\u000a<li>Take a difference between the sets so that the list of undefined IDs\u000aand classes is (R - D), and the list of unreferenced IDs and classes\u000ais (D - R)</li>\u000a</ol>\u000a\u000a<p>I've already had the pleasure of using <a href="http://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a> python library\u000ato parse all sorts of HTML documents, and I quickly found <a href="http://cthedot.de/cssutils/">cssutils</a>\u000ato be a very handy CSS parser. In a matter of hours I was able to whip\u000aup a basic dead CSS finder in 100 lines of code using these great tools.\u000aI named it 7sense, after <a href="http://www.imdb.com/title/tt0167404/">the movie</a>. To run it, you just need to\u000ainvoke <code>./7sense.py &lt;list of files and directories&gt;</code> and the specified\u000afiles will be parsed as if they were all part of the same webpage.\u000aDirectories will be walked recursively, with all encountered CSS and\u000aHTML files assumed to be part of the web page.</p>\u000a\u000a<p>But things are not as simple as I had hoped, and my program has several\u000anotable limitations.</p>\u000a\u000a<ol>\u000a<li>Due to lack of time, 7sense does not look at what stylesheets are\u000areferenced by an HTML page. Instead, you have to tell it explicitly\u000awhat stylesheets are used by passing them as arguments.</li>\u000a<li>7sense does not properly decipher heirarchical CSS selectors like\u000a'#myContainer .aboutBox'. Instead, it splits the selector into\u000awhitespace separated tokens, ignoring their structure. I skimped on\u000athis feature also due to lack of time.</li>\u000a<li>More fundamentally, 7sense is not aware of any Javascript\u000amodifications to the DOM. This could be remedied on a case-by-case\u000abasis. For example, one could write a parser to seek jQuery.setClass\u000ainvocations, and extract additional classes from there.</li>\u000a</ol>\u000a\u000a<p>At any rate, here's <a href="http://www.borismus.com/wp-content/uploads/2008/12/7sense.py">7sense so far</a>. Though flawed, it's a useful\u000astart. Ideas and code improvements, especially addressing the above\u000alimitations are very much appreciated!</p>\u000a
p137
tp138
Rp139
sg11
V/i-see-dead-css
p140
sg13
Nsg14
I01
sg15
VI see dead CSS
p141
sg18
V\u000a\u000aI wanted a tool that would analyze a complete web site, and report what\u000aCSS selectors and IDs are dead.
p142
sS'snip'
p143
g5
(g6
g7
V<p>A rudimentary attempt to write a tool that detects unused CSS styles in a stylesheet.</p>\u000a
p144
tp145
Rp146
sg23
S'Published'
p147
sg31
g140
sS'posted_info'
p148
(dp149
S'month_name'
p150
S'Dec'
p151
sS'formatted'
p152
S'December 11, 2008'
p153
sS'month'
p154
I12
sS'rfc'
p155
S'2008-12-11T09:00:00-00:00'
p156
sS'unix'
p157
I1229014800
sS'year'
p158
I2008
sS'day'
p159
I11
ssg61
S'post'
p160
sg29
S'i-see-dead-css'
p161
sS'categories'
p162
(lp163
S'web'
p164
asS'posted'
p165
cdatetime
date
p166
(S'\x07\xd8\x0c\x0b'
p167
tp168
Rp169
ssg32
S'content/posts/2008/i-see-dead-css/index.md'
p170
sg34
F1433825937.0
sa(dp171
g2
(dp172
g26
g5
(g6
g7
V<p>When I want to learn a new song on guitar, I often search for chords\u000aonline. There are many sites that provide chords and tabs, and Google\u000aindexes them nicely. But the quality of chords is often poor, and\u000athere's no way to submit corrections. When I ran a MoinMoin wiki, I kept\u000amy fixed versions of songs there. Even making modifications to existing\u000achords was painful though, since it involved hand-editing a plain text\u000afile and ensuring that the chords were properly aligned with the lyrics.\u000aMy preferred solution to this problem is to write a web application to\u000afacilitate easy collaborative editing of simple folk/rock/pop songs. </p>\u000a\u000a<p>I started working on a proof-of-concept prototype. To begin with, I wanted\u000aan easy way of finding song lyrics, which provide the skeleton for most\u000aguitar pieces that I'm interested in. Later, I planned to work on\u000aannotating those lyrics. As I prepared to whip up a light django\u000aapplication to scrape lyrics sites, I realized that there may be a\u000asimpler way: if I found a friendly lyrics API, there would be no need to\u000awrite any server side code. Could the entire service be written in\u000aJavascript? </p>\u000a\u000a<p>LyricWiki.org had exactly what I need: a simple way to\u000aaccess <a href="http://lyricwiki.org/api.php?artist=Slipknot&amp;song=Before_I_Forget&amp;fmt=json">lyrics in JSON</a> format. Of course, you can't just do an\u000aXmlHttpRequest to lyricswiki.org because of XSS security restrictions.\u000aInstead, you <em>can</em> work around this cross-domain scripting issue by\u000awriting out <script\u005c&gt; tags, and specifying the script src dynamically.\u000aLuckily, the LyricWiki JSON is wrapped (padded) in a variable named\u000a'song'. As I later discovered, this practice of wrapping JSON in a\u000avariable is well established, and called <a href="http://en.wikipedia.org/wiki/JSONP">JSONP</a>. This technique makes\u000ait easy to simply evaluate the script tag, and wait for the song\u000avariable to change. Now If only there was a safe way of doing this sort\u000aof cross-domain scripting without introducing a <a href="http://personalized20.blogspot.com/2006/02/jsonp-service-and-security.html">host</a> of <a href="http://unclehulka.com/ryan/blog/archives/2005/12/12/jsonpyoure-joking-right/">XSS</a>\u000a<a href="http://www.west-wind.com/Weblog/posts/107136.aspx">vulnerabilities</a>, writing mashups would be a walk in the park! </p>\u000a\u000a<p>After only two hours of hacking around, including learning jQuery, I came up\u000awith a <a href="lyricwiki">little demo application</a>, written in pure Javascript. Hooray for\u000ajQuery, humanmsg, and the LyricsWiki API!</p>\u000a
p173
tp174
Rp175
sg11
V/mashups-in-pure-js
p176
sg13
Nsg14
I01
sg15
VMusical mashups in pure JavaScript
p177
sg18
V\u000a\u000aWhen I want to learn a new song on guitar, I often search for chords\u000aonline.
p178
sS'snip'
p179
g5
(g6
g7
V<p>A musical hack about using JSONP to fetch lyrics cross domain from LyricWiki.org.</p>\u000a
p180
tp181
Rp182
sg23
g147
sg31
g176
sg148
(dp183
g150
S'Dec'
p184
sg152
S'December 4, 2008'
p185
sg154
I12
sg155
S'2008-12-04T09:00:00-00:00'
p186
sg157
I1228410000
sg158
I2008
sg159
I4
ssg61
g160
sg29
S'mashups-in-pure-js'
p187
sS'categories'
p188
(lp189
S'web'
p190
aS'music'
p191
asS'posted'
p192
g166
(S'\x07\xd8\x0c\x04'
p193
tp194
Rp195
ssg32
S'content/posts/2008/mashups-in-pure-js/index.md'
p196
sg34
F1433825943.0
sa(dp197
g2
(dp198
g26
g5
(g6
g7
V<p>Fear (of <a href="http://www.shaw.ca/en-ca">Shaw</a> downtime) is the mind killer. So as of today, this\u000ablog is hosted on <a href="http://www.webfaction.com/">WebFaction</a>.</p>\u000a
p199
tp200
Rp201
sg11
V/moving-day
p202
sg13
Nsg14
I00
sg15
VMoving day
p203
sg18
V\u000a\u000aFear (of [Shaw][] downtime) is the mind killer.
p204
sg4
g201
sg23
g147
sg31
g202
sg148
(dp205
g150
S'Nov'
p206
sg152
S'November 24, 2008'
p207
sg154
I11
sg155
S'2008-11-24T09:00:00-00:00'
p208
sg157
I1227546000
sg158
I2008
sg159
I24
ssg61
g160
sg29
S'moving-day'
p209
sS'categories'
p210
(lp211
S'personal'
p212
asS'posted'
p213
g166
(S'\x07\xd8\x0b\x18'
p214
tp215
Rp216
ssg32
S'content/posts/2008/moving-day/index.md'
p217
sg34
F1433825946.0
sa(dp218
g2
(dp219
g26
g5
(g6
g7
V<p>After several weeks of casual spare-time research and implementation,\u000aI've finally built a fully working piano playback robot. The usage is\u000asimple: someone plays or sings an arbitrary monophonic melody, and the\u000arobot, parked on a piano bench, will play it back.</p>\u000a\u000a<iframe class="youtube-16x9" title="YouTube video player"\u000a  src="http://www.youtube.com/embed/Bo0eCSkjy-0" frameborder="0"\u000a  allowfullscreen></iframe>\u000a\u000a<p>The physical construction of the robot is very simple: it consists of a car\u000awith a crane-like arm mounted on it. The arm is used to push down and release a\u000asingle piano key. On either end of the car, there are sensors which detect if\u000athe robot has come too close to the edges of the piano bench. The simplicity of\u000athe robot comes at the price of significant limitations, such as only being\u000aable to play back melodies on the white keys. </p>\u000a\u000a<p>Software is the challenging part of the project. The Mindstorms sound sensor is\u000atoo primitive to use for pitch analysis. Without hacking it, you can only\u000aextract the amplitude of the sound signal, not any frequency details.  Instead\u000aof the NXT sound sensor, I use a macbook pro and it's built-in microphone. A\u000acomputer separate from the NXT is involved, so an additional set of\u000acommunication problems arose. </p>\u000a\u000a<p>Here's a rough outline of happens to make the playback work, from capturing the\u000amelody line to playing the melody back.</p>\u000a\u000a<ol>\u000a<li>On the mac, using a <a href="http://appscript.sourceforge.net/">Python AppleScript bridge</a>, the QuickTime\u000aPlayer is invoked and starts capturing audio.</li>\u000a<li>Once the audio is captured into an AIFF file, a very useful \u000a<a href="http://aubio.org/">pitch detection library</a> called aubio processes the audio file and\u000aextracts raw frequency-to-time data, sampled at some tick rate.\u000aCompiling this library on OS X was quite a feat!</li>\u000a<li>Next, the raw data is processed by throwing out extraneous values\u000aand extracting a melody</li>\u000a<li>Once we have the melody line, we inject it into <a href="http://bricxcc.sourceforge.net/nbc/">an NXC program</a>,\u000aand compile it with the nbc compiler</li>\u000a<li>This program is then sent via bluetooth to the robot via nxtcom</li>\u000a<li>Using <a href="http://home.comcast.net/~dplau/nxt_python/">NXT_Python</a> and <a href="http://www.cs.wlu.edu/~levy/software/nxt_lightblue_glue/">Lightblue Glue</a>, the robot is told to\u000aexecute the program.</li>\u000a</ol>\u000a\u000a<p>Please let me know if you have any questions or suggestions!</p>\u000a
p220
tp221
Rp222
sg11
V/robotic-piano-playback
p223
sg13
Nsg14
I01
sg15
VRobotic piano playback
p224
sg18
V\u000a\u000aAfter several weeks of casual spare-time research and implementation,\u000aI've finally built a fully working piano playback robot.
p225
sS'snip'
p226
g5
(g6
g7
V<p>About a LEGO Mindstorms robot that listens to a melody and then plays it back on the piano.</p>\u000a
p227
tp228
Rp229
sg23
g147
sg31
g223
sg148
(dp230
g150
S'Nov'
p231
sg152
S'November 15, 2008'
p232
sg154
I11
sg155
S'2008-11-15T09:00:00-00:00'
p233
sg157
I1226768400
sg158
I2008
sg159
I15
ssg61
g160
sg29
S'robotic-piano-playback'
p234
sS'categories'
p235
(lp236
S'physical'
p237
asS'posted'
p238
g166
(S'\x07\xd8\x0b\x0f'
p239
tp240
Rp241
ssg32
S'content/posts/2008/robotic-piano-playback/index.md'
p242
sg34
F1433825950.0
sa(dp243
g2
(dp244
g26
g5
(g6
g7
V<p>Web design used to be a black art. Ten years ago, browser differences used to\u000abe so dramatic that the only viable solution for an HTML designer was to fall\u000aback to the least common denominator for page layout, which was HTML tables. In\u000atoday's web design community, using table layouts is considered to be a heinous\u000acrime, since most popular modern rendering engines (IE, Gecko and WebKit) are\u000aconverging to some shared interpretation of web standards. Unlike layout\u000aengines on the web, though, rich mail interpreters have remained stagnant, and\u000ain some cases have regressed. <strike>Without pointing any fingers...</strike></p>\u000a\u000a<p>Much of the blame for this difference lies in Microsoft's decision to use the\u000asame engine for composing and viewing email in Outlook 2007. They wanted to\u000amake the life of email designers easier, or so the story goes. But since\u000aInternet Explorer doesn't have editing functionality, and Front Page is too\u000aheavy to embed, the remaining choice was Word '07. Unfortunately, Word's\u000arendering engine is extremely limited, in the following notable ways:</p>\u000a\u000a<ol>\u000a<li>No positioning or floating elements, so CSS-based layouts are out</li>\u000a<li>No CSS backgrounds, combined with (1) means that there's no way to\u000ahave an image background at all.</li>\u000a</ol>\u000a\u000a<p>So, if you care a significant population of email readers (7% use Outlook '07),\u000aand want to deliver a rich media email, you are condemned to designing with\u000atable layouts, or to skip HTML altogether, and simply use images. </p>\u000a\u000a<p>For a long time, there was no good way of determining a\u000abreakdown of email client usage. Recently, the good people at\u000afingerprint have come up with a solution, and now use a large sample of\u000apeople to determine global <a href="http://fingerprintapp.com/email-client-stats">email client usage statistics</a>. According\u000ato them, Hotmail, Yahoo Mail and Gmail add up to roughly 50% of all\u000ausage. </p>\u000a\u000a<p>Since the UI of a web-based email client is written in HTML, so it is critical\u000afor webmail developers to ensure that the CSS and HTML found in the email does\u000anot interfere with the global look and feel. The canonical solutions are to\u000adecorate all ids and classes in the email with some kind of prefix to ensure\u000athat there are no name collisions, and use a white list approach to CSS styles.\u000aThese white lists are usually quite long, but lack some important and oft-used\u000aproperties. For example, CSS image backgrounds are disallowed, except on Yahoo.\u000aAnd of course, Microsoft had to leave its bizarre mark too: Hotmail strips the\u000amargin-top property, but not the margin-bottom property. </p>\u000a\u000a<p>Historically,\u000awebmail clients used to ignore anything outside of the <code>&lt;body\u005c&gt;</code> tag,\u000awhich meant that all CSS had to be written inline, leading to\u000aunmaintainable layouts. In recent tests, however, popular webmail\u000aclients no longer ignore <code>&lt;style\u005c&gt;</code> elements in the <code>&lt;head\u005c&gt;</code>, and instead,\u000aapply the CSS sparingly inline. Among modern native mail clients, there\u000ais a positive trend as well. Microsoft Live Mail uses the IE7 rendering\u000aengine, Thunderbird uses Gecko and Mail.app uses WebKit. So it looks\u000alike there is light at the end of the tunnel for downtrodden HTML email\u000adesigners. In the meantime though, I send them my deepest condolences.</p>\u000a
p245
tp246
Rp247
sg11
V/the-sorry-state-of-html-mail
p248
sg13
Nsg14
I01
sg15
VThe sorry state of HTML mail
p249
sg18
V\u000a\u000aWeb design used to be a black art.
p250
sS'snip'
p251
g5
(g6
g7
V<p>Complaining about how terrible it is to create HTML email that renders properly in multiple mail clients.</p>\u000a
p252
tp253
Rp254
sg23
g147
sg31
g248
sg148
(dp255
g150
S'Oct'
p256
sg152
S'October 31, 2008'
p257
sg154
I10
sg155
S'2008-10-31T09:00:00-00:00'
p258
sg157
I1225468800
sg158
I2008
sg159
I31
ssg61
g160
sg29
S'the-sorry-state-of-html-mail'
p259
sS'categories'
p260
(lp261
S'web'
p262
asS'posted'
p263
g166
(S'\x07\xd8\n\x1f'
p264
tp265
Rp266
ssg32
S'content/posts/2008/the-sorry-state-of-html-mail/index.md'
p267
sg34
F1433825958.0
sa(dp268
g2
(dp269
g26
g5
(g6
g7
V<p>Yesterday I got back from the Web 2.0 Expo in New York. There were many\u000ainteresting sessions, and I, for the most part, took notes! Here is a\u000ashort list of my five favorite speakers, in no particular order:</p>\u000a\u000a<ol>\u000a<li>Cal Henderson's whirlwind introduction to video on the web. Cal\u000areally likes speak really fast, to make typos in his slides and to\u000acuss, all of which makes for a very informative and entertaining\u000ahour.</li>\u000a<li>John Resig's talk about Processing.js, his port of\u000a<a href="http://processing.org/">Processing</a> to canvas. I'm constantly dealing with graphics on\u000athe web, and it's awesome to have such a powerful library available.\u000aPretty demos, too!</li>\u000a<li>Jason Fried's short but sweet keynote talk, focusing on minimalism\u000ain product management, and proper (read: very narrow) scoping of\u000afeatures. The philosophical question of "what would your software be\u000alike if it was physical?" struck me as a very useful thing to think\u000aabout.</li>\u000a<li>A browser panel including Chris Wilson from MS, Brendan Eich\u000afrom Mozilla and an unnamed developer from Chrome responded to a\u000anice set of questions about future directions of browsers. Poor\u000aChris got a beating IE7's flaws, lack of canvas/SVG support,\u000abarriers to plugin development for IE.</li>\u000a<li>Geir Magnusson Jr delivered an excellent introduction to scaling\u000adata in the cloud.</li>\u000a</ol>\u000a\u000a<p>I also had the chance to talk to and bounce some ideas off of Pete\u000aKoomen, a Google App Engine product manager. He told me that there are\u000aplans both for process scheduling, and for support of django-1.0 down\u000athe road, but of course gave no time frame for either. Overall, the\u000aconference was interesting - there were other good sessions and keynotes\u000awhich I simply haven't bothered to write up. The crowd wasn't very\u000atechnical though, comprising in large part designers, marketers and\u000amanagers.</p>\u000a
p270
tp271
Rp272
sg11
V/top-5-sessions-of-web-2-expo-ny
p273
sg13
Nsg14
I01
sg15
VTop 5 sessions of Web 2.0 Expo NY
p274
sg18
V\u000a\u000aYesterday I got back from the Web 2.
p275
sS'snip'
p276
g5
(g6
g7
V<p>A recap of my favorite sessions at my first web conference.</p>\u000a
p277
tp278
Rp279
sg23
g147
sg31
g273
sg148
(dp280
g150
S'Sep'
p281
sg152
S'September 23, 2008'
p282
sg154
I9
sg155
S'2008-09-23T09:00:00-00:00'
p283
sg157
I1222185600
sg158
I2008
sg159
I23
ssg61
g160
sg29
S'top-5-sessions-of-web-2-expo-ny'
p284
sS'categories'
p285
(lp286
S'conference'
p287
aS'web'
p288
asS'posted'
p289
g166
(S'\x07\xd8\t\x17'
p290
tp291
Rp292
ssg32
S'content/posts/2008/top-5-sessions-of-web-2-expo-ny/index.md'
p293
sg34
F1433826442.0
sa(dp294
g2
(dp295
g26
g5
(g6
g7
V<p>My favorite class at CMU is probably <a href="http://mtifall09.wordpress.com/">Making Things Interactive</a>. For me\u000ait is an opportunity to take my thus far casual electronics hacking to to\u000athe next level. In this article, I'll briefly outline my submission for the\u000a"Motion" assignment. I used a servo motor to control a Nerf gun. I built it\u000aand installed it in my MHCI lab, which has a handful of Nerf pistols\u000afloating around. The idea was to have the gun automatically fire at\u000aunsuspecting visitors as they entered the room. </p>\u000a\u000a<p>To arm it, one manually cocks the gun, loads a dart and resets the program\u000aby pushing the button on the Arduino board. The program then allows 10\u000aseconds for the door to be open before it arms the system. When the system\u000ais armed, the servo activates and shoots the gun as soon as the door is\u000aopened.</p>\u000a\u000a<iframe title="YouTube video player" width="600" height="480"\u000a  src="http://www.youtube.com/embed/-XkLRZ2OBRo" frameborder="0"\u000a  allowfullscreen></iframe>\u000a\u000a<p>As you can see, I generously used rubber bands and binder clips in this\u000aproject. I used them to fasten the servo motor to the Nerf gun. I also used\u000athem to harness a telephone cable by wrapping a rubber band around the RJ11\u000aconnector, carefully inserting jumpers, and applying additional pressure (to\u000aensure contact) with binder clips. This hacked telephone cable stretched\u000afrom the gun to the door sensor. </p>\u000a\u000a<p>The circuit was dead simple.  The door acted as a switch for the pull down\u000aresistor circuit. </p>\u000a
p296
tp297
Rp298
sg11
V/arduino-nerf
p299
sg13
Nsg14
I01
sg15
VArduino-nerf mashup
p300
sg18
V\u000a\u000a\u000aMy favorite class at CMU is probably [Making Things Interactive][].
p301
sS'snip'
p302
g5
(g6
g7
V<p>Using an Arduino and a servo motor to retrofit a nerf gun with an auto-fire mechanism.</p>\u000a
p303
tp304
Rp305
sg23
g147
sg31
g299
sg148
(dp306
g150
S'Oct'
p307
sg152
S'October 20, 2009'
p308
sg154
I10
sg155
S'2009-10-20T09:00:00-00:00'
p309
sg157
I1256054400
sg158
I2009
sg159
I20
ssg61
g160
sg29
S'arduino-nerf'
p310
sS'categories'
p311
(lp312
S'physical'
p313
asS'posted'
p314
g166
(S'\x07\xd9\n\x14'
p315
tp316
Rp317
ssg32
S'content/posts/2009/arduino-nerf/index.md'
p318
sg34
F1433825860.0
sa(dp319
g2
(dp320
g26
g5
(g6
g7
V<p>At the core of the traditional HTML/CSS developer's toolkit is a set of nested\u000aboxes describing offset, margin, border and padding, known as the box model.\u000aVariations on the box theme are sufficient to describe most page layouts, but\u000ain some complex applications, it's necessary to render something more\u000ainteresting, like diagonal lines, or polygons. There are currently two\u000arelatively well-supported web graphics technologies: SVG and Canvas. There are\u000asignificant performance differences, however, which I would like to discuss in\u000athis article. </p>\u000a\u000a<p><a href="http://en.wikipedia.org/wiki/Svg">Scalable Vector Graphics</a> (SVG) is by far the oldest of the two. It is a\u000adeclarative, graphical language used to describe geometrical primitives via DOM\u000aelements. SVG was drafted in the late 90s, and the latest version of \u000a<a href="http://www.w3.org/TR/SVG11/">SVG, version 1.1</a> was finalized in 2003. It took 3 more years for it to be\u000aincorporated into shipping versions of Mozilla Firefox and Safari.\u000aUnfortunately, the length of the SVG adoption process caused the web\u000adevelopment community to seek other options. The <a href="http://en.wikipedia.org/wiki/Canvas_(HTML_element)">HTML5 Canvas</a> element was\u000aintroduced as a much simpler alternative to graphics on the web. It provides an\u000aimage-like graphics context which can be accessed via a set of javascript\u000acalls, similar to a 2D subset of OpenGL. It was originally introduced by Apple\u000ain WebKit builds, but is now supported in Mozilla Firefox as well. </p>\u000a\u000a<p>I produced some metrics to compare the two technologies in terms of performance\u000aby writing a Javascript program for <a href="canvas-svg-benchmark/">collecting performance data</a>. This\u000aprogram draws rows of circles onto a fixed-size drawing area in SVG and in\u000aCanvas, and then compares how long various operations take. It also records the\u000aduration to create the initial drawing context, to render the scene and to\u000aclear the scene. A test runner invokes the benchmark with the following\u000avariables: number of circles, drawing area dimensions and circle size. Each of\u000athese variables are varied independantly and automatically resulting in the\u000afollowing observations: </p>\u000a\u000a<p><img src="varying-number-of-objects.png" alt="Varying the number of objects" /> </p>\u000a\u000a<p>Here are the results of the first fruitful experiment, which clearly shows that\u000aSVG performance degrades quickly (exponentially on Safari?) in the number of\u000aobjects, but Canvas performance remains at a near-constant low. This makes\u000asense, since Canvas is just a bitmap buffer, while SVG has to maintain\u000aadditional references to each object that it renders. Also, though not\u000apictured, note that performance in clearing an SVG element also decreases in\u000athe number of drawn objects. </p>\u000a\u000a<p><img src="varying-drawing-area-height.png" alt="Varying drawing area height" /> </p>\u000a\u000a<p>When varying the size of the drawing area, canvas performance degrades\u000asignificantly, while SVG performance is completely unaffected.  Canvas\u000arendering performance seems to degrade linearly in the number of pixels in the\u000acanvas area. Not pictured on the graph is clear performance for large canvases,\u000awhich also suffers linearly in pixel count.</p>\u000a\u000a<p>I did not include the graph resulting from varying circle size as it had no\u000asignificant impact on render time. Another interesting observation is that\u000acreating the canvas element takes a mysterious 10 ms on Firefox, but not on\u000aSafari. This is not significant unless you are dealing with large numbers of\u000acanvas elements.</p>\u000a\u000a<p>Whether or not you use Canvas or SVG mostly depends on your specific\u000aapplication. A graphics-intensive game, where many objects are redrawn all the\u000atime is probably best implemented in Canvas. On the other hand, applications\u000alike map viewers may involve large rendering areas and might lend themselves\u000abetter to SVG.</p>\u000a\u000a<p>As always, please comment if something is unclear, inconsistent, boring or\u000aomitted. Thanks!</p>\u000a
p321
tp322
Rp323
sg11
V/canvas-vs-svg-performance
p324
sg13
Nsg14
I01
sg15
VPerformance of canvas versus SVG
p325
sg18
V\u000a\u000aAt the core of the traditional HTML/CSS developer's toolkit is a set of nested\u000aboxes describing offset, margin, border and padding, known as the box model.
p326
sS'snip'
p327
g5
(g6
g7
V<p>A benchmark for comparing Canvas to SVG performance as a function of render element size and number of objects drawn.</p>\u000a
p328
tp329
Rp330
sg23
g147
sg31
g324
sg148
(dp331
g150
S'Jan'
p332
sg152
S'January 19, 2009'
p333
sg154
I1
sg155
S'2009-01-19T09:00:00-00:00'
p334
sg157
I1232384400
sg158
I2009
sg159
I19
ssg61
g160
sg29
S'canvas-vs-svg-performance'
p335
sS'categories'
p336
(lp337
S'web'
p338
asS'posted'
p339
g166
(S'\x07\xd9\x01\x13'
p340
tp341
Rp342
ssg32
S'content/posts/2009/canvas-vs-svg-performance/index.md'
p343
sg34
F1433825868.0
sa(dp344
g2
(dp345
g26
g5
(g6
g7
V<p>In late March, I was completely thrilled to hear that the [Carnegie\u000aMellon University][] <a href="http://www.hcii.cmu.edu/">Human Computer Interaction Institute</a> accepted\u000ame into their <a href="http://www.hcii.cmu.edu/masters-program">Master</a>'s program! In addition to admission, they\u000aoffered a very juicy scholarship to spend part of the time on the\u000abeautiful island of <a href="http://en.wikipedia.org/wiki/Madeira">Madeira</a>. CMU was my most far fetched reach\u000aschool, so I attribute this wonderful fortune to a clerical error made\u000aby the admission committee. </p>\u000a\u000a<p>In addition to studying at one of the best\u000auniversities in the world, traveling to Europe, and relaxing on the\u000abeach, here are a couple of reasons why my inner geek can't contain his\u000aexcitement, try as he might. These incredible courses are offered at CMU\u000ain September 2009:</p>\u000a\u000a<h3>Making Things Interactive:</h3>\u000a\u000a<p>In this hands-on design-build class you will learn the skills to embed\u000asensors and actuators (light, sound, touch, motion, etc.) into everyday\u000athings (and places etc.) and to program their interactive behavior using\u000aa microcontroller. Through weekly exercises and a term project the class\u000awill introduce basic analog electronics and microcontroller programming,\u000aas well as exploration into using kinetics and materials to make the\u000athings you design perform. Emphasis will be on creating innovative\u000aexperiences using simple robotic technologies. The graduate edition of\u000athis course will require additional work including a paper that can be\u000asubmitted to a peer-reviewed interaction design conference such as CHI,\u000aUIST, or TEI.Students from all disciplines are welcome: but please note\u000athat the class demands that you master technical material. Experience in\u000aat least one of: programming, electronics, or physical fabrication is\u000astrongly recommended.</p>\u000a\u000a<h3>Principles of Human-Robot Interaction:</h3>\u000a\u000a<p>This course focuses on the emerging field of human-robot interaction,\u000abringing together research and application of methodology from robotics,\u000ahuman factors, human-computer interaction, interaction design, cognitive\u000apsychology, education and other fields to enable robots to have more\u000anatural and more rewarding interactions with humans throughout their\u000aspheres of functioning. This course is a combination of state-of-art\u000areading and discussions, focused team exercises and problem-solving\u000asessions in human-robot interaction, and a special team project\u000aresulting in the implementation of a human-robot interaction system.</p>\u000a
p346
tp347
Rp348
sg11
V/carnegie-mellon-university
p349
sg13
Nsg14
I00
sg15
VCarnegie Mellon University!?
p350
sg18
V\u000a\u000aIn late March, I was completely thrilled to hear that the [Carnegie\u000aMellon University][] [Human Computer Interaction Institute][] accepted\u000ame into their [Master][]'s program! In addition to admission, they\u000aoffered a very juicy scholarship to spend part of the time on the\u000abeautiful island of [Madeira][].
p351
sg4
g348
sg23
g147
sg31
g349
sg148
(dp352
g150
S'Apr'
p353
sg152
S'April 18, 2009'
p354
sg154
I4
sg155
S'2009-04-18T09:00:00-00:00'
p355
sg157
I1240070400
sg158
I2009
sg159
I18
ssg61
g160
sg29
S'carnegie-mellon-university'
p356
sS'categories'
p357
(lp358
S'personal'
p359
asS'posted'
p360
g166
(S'\x07\xd9\x04\x12'
p361
tp362
Rp363
ssg32
S'content/posts/2009/carnegie-mellon-university/index.md'
p364
sg34
F1332684374.0
sa(dp365
g2
(dp366
g26
g5
(g6
g7
V<p>Somehow I often find myself arguing in defense of the web browser as a\u000aviable platform for developing rich applications. In many such\u000adiscussions, the issue of interoperability with the desktop arises.\u000aSomeone will astutely observe that they <strong>can't even drag and drop</strong>\u000afrom their OS file manager into their browser, and all hell will break\u000aloose. </p>\u000a\u000a<p>Happily, this is changing! Since version 3, Safari on Mac OS X\u000ahas had support for <a href="http://www.jakeri.net/2008/04/drag-and-drop-into-file-upload-in-safari/">dragging and dropping files</a> from the finder into\u000afile input boxes. In various kludgy ways, <a href="https://addons.mozilla.org/en-US/firefox/addon/2190">Firefox</a> and <a href="http://www.download.com/HTTP-File-Upload-ActiveX-Control/3000-2206_4-10451672.html">IE</a> are now\u000afollowing suite. </p>\u000a\u000a<p>Unfortunately, even in Safari, the default look of the\u000a<code>&lt;input type="file"&gt;</code> box is quite ugly and the element itself is\u000a<a href="http://www.quirksmode.org/dom/inputfile.html">difficult to style</a>. In addition, clicking anywhere in the file input\u000aelement causes the default open file dialog to appear. I wanted to\u000aprovide drag-and-drop uploading without ugly boxes or browser dialogs.\u000aThe solution I came up with involves hiding the file upload box entirely\u000aby setting its opacity to 0, and then preventing the default action on\u000aclick via <code>event.preventDefault()</code>. Here's a sample of what I mean, with\u000athe entire browser window <a href="drag-drop-upload.html">converted into a drag area</a>. </p>\u000a\u000a<p>Note that the drag area must be the first DOM element to receive the drop event\u000afor this approach to work. Unfortunately I ran into a bug where the file dialog\u000arefuses to bubble click events to other elements below it. This is baffling to\u000ame, since <code>event.preventDefault()</code> should not stop event propagation, but only\u000aprevent the default browser handler from being called. You can see what I mean\u000aby trying to click the link in the <a href="drag-drop-upload.html">sample HTML</a>\u000afile. If this is not a bug, and someone has an answer, I would really\u000aappreciate it.</p>\u000a\u000a<p>Note also that there are <a href="http://www.radinks.com/upload/">java applet</a>-based drag and drop solutions, but\u000athey are reserved for developers who have nothing but disdain for their users.</p>\u000a
p367
tp368
Rp369
sg11
V/clean-drag-and-drop-upload-in-safari
p370
sg13
Nsg14
I01
sg15
VClean drag and drop upload in Safari
p371
sg18
V\u000a\u000aSomehow I often find myself arguing in defense of the web browser as a\u000aviable platform for developing rich applications.
p372
sS'snip'
p373
g5
(g6
g7
V<p>A how-to about implementing file drag and drop from the desktop into Safari, and presenting the user with a nice UI.</p>\u000a
p374
tp375
Rp376
sg23
g147
sg31
g370
sg148
(dp377
g150
S'Feb'
p378
sg152
S'February 15, 2009'
p379
sg154
I2
sg155
S'2009-02-15T09:00:00-00:00'
p380
sg157
I1234717200
sg158
I2009
sg159
I15
ssg61
g160
sg29
S'clean-drag-and-drop-upload-in-safari'
p381
sS'categories'
p382
(lp383
S'web'
p384
aS'design'
p385
asS'posted'
p386
g166
(S'\x07\xd9\x02\x0f'
p387
tp388
Rp389
ssg32
S'content/posts/2009/clean-drag-and-drop-upload-in-safari/index.md'
p390
sg34
F1332684374.0
sa(dp391
g2
(dp392
g26
g5
(g6
g7
V<p>For the last few days, I've had the chance to get my feet wet in the\u000a<a href="https://services.google.com/fb/forms/wavesignupfordev/">developer sandbox</a> of Google Wave. My first impressions are very\u000apositive. I am as awed now by the scope and potential impact of Wave as\u000aI was after watching the hour long <a href="http://wave.google.com/">video from Google I/O</a>. </p>\u000a\u000a<p>As consolation to those itching to <em>try it already</em>, bear in mind that the\u000asandbox is in a very raw state right now. Performance issues aside, informative\u000atext such as:</p>\u000a\u000a<blockquote>\u000a  <p>This wave is experiencing some slight turbulence, and may explode. If\u000a  you don't wanna explode, please re-open the wave. Some recent changes\u000a  may not be saved.</p>\u000a</blockquote>\u000a\u000a<p>and</p>\u000a\u000a<blockquote>\u000a  <p>"Everything's shiny, Cap'n. Not to fret!" Unfortunately, you'll need\u000a  to <em>refresh</em>. Wanna tell Dr. Wave what happened?</p>\u000a</blockquote>\u000a\u000a<p>appear quite often. Of course, this is totally expected for pre-alpha\u000asoftware -- I'm merely pointing out the humorous messages!</p>\u000a\u000a<p>For a more intimate look into Wave, especially some of the deeper, more social\u000aissues, here is part of an ongoing ha-ha-only-serious joke from the internal\u000awave-discuss group about extending existing <a href="http://mashable.com/2009/05/28/google-wave-guide/">Wave terminology</a>. The list is\u000areally telling, since indeed, I have read many ridiculous Drips and most of my\u000afresh waves Surge only to get Borked by rickrolley.</p>\u000a\u000a<ul>\u000a<li><strong>Wavejack</strong> -- To hijack the contents of a wave to where it no longer resembles the original idea</li>\u000a<li><strong>Drip</strong> -- Stupid question no one answers</li>\u000a<li><strong>Bork</strong> -- To add useless, noisy or destructive bots to a wave. (aka The Swedish Chef or rickrolley or the borkforceone bot)</li>\u000a<li><strong>Surge</strong> -- The effect of a fresh wave that elicits the chaos of mass editing</li>\u000a<li><strong>Drown</strong> -- To have so many waves to follow that one can't keep up with them </li>\u000a<li><strong>Sea Sick</strong> -- A state of dizziness induced by an overly active wave </li>\u000a<li><strong>Tuna/Noise/Herring</strong> -- Wave speak for Spam</li>\u000a<li><strong>Fishnet</strong> -- Spam filter</li>\u000a</ul>\u000a\u000a<p>Despite instabilities and widespread antisocial behavior, the wave sandbox is\u000aalive with a flurry of activity. People are busy creating annoying bots,\u000aforming role playing communities and writing collaborative books. So Google has\u000aa lot of issues to work out, not in the least how to stop us all from drowning\u000a:) </p>\u000a\u000a<p>So far, I haven't had a chance to write any interesting wave extensions, but\u000ait's in my things to do!</p>\u000a
p393
tp394
Rp395
sg11
V/extending-google-wave-terminology
p396
sg13
Nsg14
I01
sg15
VExtending Google Wave terminology
p397
sg18
V\u000a\u000aFor the last few days, I've had the chance to get my feet wet in the\u000a[developer sandbox][] of Google Wave.
p398
sS'snip'
p399
g5
(g6
g7
V<p>A chronicle of some emergent lingo from the Google Wave service.</p>\u000a
p400
tp401
Rp402
sg23
g147
sg31
g396
sg148
(dp403
g150
S'Jul'
p404
sg152
S'July 22, 2009'
p405
sg154
I7
sg155
S'2009-07-22T09:00:00-00:00'
p406
sg157
I1248278400
sg158
I2009
sg159
I22
ssg61
g160
sg29
S'extending-google-wave-terminology'
p407
sS'categories'
p408
(lp409
S'social'
p410
asS'posted'
p411
g166
(S'\x07\xd9\x07\x16'
p412
tp413
Rp414
ssg32
S'content/posts/2009/extending-google-wave-terminology/index.md'
p415
sg34
F1433825879.0
sa(dp416
g2
(dp417
g26
g5
(g6
g7
V<p>One day I wanted to add a feature to <a href="http://www.guitarunleashed.com">Guitar Unleashed</a> which exists\u000ain some of the better guitar tab sites. When a user hovers over a chord,\u000athey are shown a diagram representing the guitar fret with overlaid\u000afinger positions required to produce this chord. Many of the most\u000apopular sites do this by showing a crude, plain-text representation of\u000athe chord.</p>\u000a\u000a<p>For example, a C chord is shown as follows:</p>\u000a\u000a<pre><code>e ---|---|---|---|---|\u000aB -x-|---|---|---|---|\u000aG ---|---|---|---|---|\u000aD ---|-x-|---|---|---|\u000aA ---|---|-x-|---|---|\u000aE ---|---|-o-|---|---|\u000a</code></pre>\u000a\u000a<p>Typeset guitar chord representation looks very different. Two most\u000apopular variants of the C chord would appear above the staff with small\u000asymbols that look something like this: </p>\u000a\u000a<p><img src="C.png" alt="C" title="C" />\u000a<img src="C_1.png" alt="C_1" title="C_1" /> </p>\u000a\u000a<p>Such images have many benefits as compared to the plain text version:</p>\u000a\u000a<ul>\u000a<li>Convention: many guitar players are used to learning from sheet\u000amusic, so this chord notation is familiar to them.</li>\u000a<li>Readability: the image representation is more compact and more\u000apleasant to read.</li>\u000a<li>Completeness: the image version has an easy way to display barre\u000achords and specify fingerings (not pictured)</li>\u000a</ul>\u000a\u000a<p>True to Guitar Unleashed's mission of being a truly usable guitar chords\u000asite, I decided to display typeset fret diagrams. For the\u000aimplementation, a program would pre-generate images on the server based\u000aon the encoded chord shape and then serve these images dynamically via\u000aJavaScript. Unfortunately, I found no software to generate such images\u000afor all popular chords, so I ended up developing my own. </p>\u000a\u000a<p>From some brief\u000aresearch, I decided that <a href="http://lilypond.org/doc/v2.9/Documentation/user/lilypond/Fret-diagrams">lilypond fret diagrams</a> are the most elegant\u000away of creating such images. The alternative was to use <a href="http://www.aei.mpg.de/~peekas/gchords/">GCHORDS</a>\u000awhose output I liked less, and which required depending on TeX.\u000aTypically lilypond typesets an entire piece of sheet music on a staff,\u000awith clefs, key signatures, etc. After discovering a really great\u000a<a href="http://netcetera.org/cgi-bin/tmbundles.cgi#Lilypond">lilypond bundle for textmate</a>, I managed to get rid of these\u000aunnecessary features and display just a fret diagram. The following\u000apython-parametrized lilypond template does the trick:</p>\u000a\u000a<pre><code>\u005cinclude "lilypond-book-preamble.ly"\u000a\u005cversion "2.10.0"\u000a\u005cmarkup\u000a\u005cfret-diagram-terse #"%(chord_markup)s"\u000a</code></pre>\u000a\u000a<p>Then, using lilypond's fret diagram encoding, a python script replaces\u000a<code>%(chord_markup)s</code> with the desired markup. Lilypond's markup is very\u000aterse but mostly manageable. Sample markup for a C chord looks like\u000athis: <code>x;3;2;o;1;o;</code>, while the barre version looks a little bit more\u000abizarre, like this: <code>3-(;3;5;5;5;3-);</code>. </p>\u000a\u000a<p>My python program contains a list of chord shapes for the most popular chords,\u000aencoded in the manner above, then outputs images for each chord and variant\u000ainto a directory.  I've made it <a href="chord-image-generator.zip">available for download</a>, in case your next\u000aproject needs generated fret diagrams with a highly customizable look. You can\u000aalso see the diagrams live at <a href="http://www.guitarunleashed.com">guitarunleashed.com</a>.</p>\u000a
p418
tp419
Rp420
sg11
V/generating-guitar-chord-diagrams
p421
sg13
Nsg14
I01
sg15
VGenerating guitar chord diagrams
p422
sg18
V\u000a\u000aOne day I wanted to add a feature to [Guitar Unleashed][] which exists\u000ain some of the better guitar tab sites.
p423
sS'snip'
p424
g5
(g6
g7
V<p>A python script that generates all possible ways of playing the specified guitar chord. I used this for Guitar Unleashed.</p>\u000a
p425
tp426
Rp427
sg23
g147
sg31
g421
sg148
(dp428
g150
S'Jun'
p429
sg152
S'June 14, 2009'
p430
sg154
I6
sg155
S'2009-06-14T09:00:00-00:00'
p431
sg157
I1244995200
sg158
I2009
sg159
I14
ssg61
g160
sg29
S'generating-guitar-chord-diagrams'
p432
sS'categories'
p433
(lp434
S'music'
p435
asS'posted'
p436
g166
(S'\x07\xd9\x06\x0e'
p437
tp438
Rp439
ssg32
S'content/posts/2009/generating-guitar-chord-diagrams/index.md'
p440
sg34
F1433825885.0
sa(dp441
g2
(dp442
g26
g5
(g6
g7
V<p>Since moving to the states, I have had nothing but grief from the .ca at the\u000aend of my all-purpose email address. Even in Canada, people would constantly\u000aconfuse z3.ca for z3.com, resulting in email bounces. To resolve this problem\u000aonce and for all, I decided to switch to Gmail like all the cool kids. I\u000aregistered boris.smus long ago out but never used it. The first thing I did was\u000atry to link my z3.ca to the Gmail. When I couldn't figure out how to do that, I\u000adeleted my Gmail account in order to re-create a new pre-linked account with\u000athe same name. Sounds innocent enough, right? </p>\u000a\u000a<p><em>Wrong</em>! Google has an uncharacteristically evil account deletion policy which\u000ais not at all clearly communicated. The deletion page simply says:</p>\u000a\u000a<blockquote>\u000a  <p>You're trying to delete your Google Account that provides access to\u000a  the Google products listed below. Please select each checkbox to\u000a  confirm you fully understand that you'll no longer be able to use any\u000a  of these products and all information associated with them, and that\u000a  your account will be lost.</p>\u000a</blockquote>\u000a\u000a<p>Meanwhile, the <a href="http://www.google.com/support/accounts/bin/answer.py?hl=en&amp;answer=32046">F.A.Q.</a> reads:</p>\u000a\u000a<blockquote>\u000a  <p>If you use Gmail with your account, you'll no longer be able to access\u000a  that email. You'll also be unable to reuse your Gmail username.</p>\u000a</blockquote>\u000a\u000a<p>Long story short, I tried everything in my power to recover the old username. I\u000afound handfuls of frustrated users in the same position as me; some having\u000adeleted their account by accident, others victims of pranks and identity theft.\u000aI asked on official and unofficial Gmail forums, and even consulted with my\u000aGoogler friends, all to no avail.  After mourning the loss of\u000aboris.smus@gmail.com, it was time to take a critical look at Google's email\u000aofferings. </p>\u000a\u000a<p>I enjoy Gmail's webmail client very much. It's a fast, intuitive, search and\u000atag based model with virtually unlimited mailbox storage. In terms of\u000ausability, however, I much prefer Mail on Mac. On the iPhone, the native Mail\u000aclient is far superior to the mobile web Gmail client. Fortunately, Google\u000aprovides SMTP and IMAP services to fill this need. Sadly, both of these\u000aservices are plagued with issues. </p>\u000a\u000a<p>I recently sent an email which had roughly 50 bcc recipients. Google's SMTP\u000aserver thought I was a spammer and banned me, <em>despite my having authenticated\u000avia SSL</em>.  Perhaps sending email to 50 people is slightly unusual. Still, I\u000awould expect my mail gateway to be capable of performing such a 'feat'. </p>\u000a\u000a<p>The way the Google IMAP maps directories is fundamentally incompatible with\u000aMail.app expectations. Further, the IMAP server has a limit of 10 simultaneous\u000aconnections, which often causes the connection threshold to be reached with\u000a<a href="http://mail.google.com/support/bin/answer.py?hl=en&amp;answer=97150">just two connected clients</a>. Additionally, the IMAP server is often down; I\u000awish I had some data to support that, but I don't. </p>\u000a\u000a<p>When I finally settled on boris@borismus.com, I had a decision to make: do I\u000ause Google Apps or Webfaction for email? I quite like Webfaction, and, perhaps\u000airrationally, trust them more than Google with my private data. With the above\u000alimitations of Gmail in mind, I did not hesitate to choose Webfaction. I still\u000amiss having a first.last@gmail.com though. What's with the oddly draconian\u000aaccount deletion rules?</p>\u000a
p443
tp444
Rp445
sg11
V/gmail-rant
p446
sg13
Nsg14
I01
sg15
VNever delete your gmail account
p447
sg18
V\u000a\u000aSince moving to the states, I have had nothing but grief from the .
p448
sS'snip'
p449
g5
(g6
g7
V<p>Bitching and moaning about the perils of deleting your GMail account.</p>\u000a
p450
tp451
Rp452
sg23
g147
sg31
g446
sg148
(dp453
g150
S'Aug'
p454
sg152
S'August 28, 2009'
p455
sg154
I8
sg155
S'2009-08-28T09:00:00-00:00'
p456
sg157
I1251475200
sg158
I2009
sg159
I28
ssg61
g160
sg29
S'gmail-rant'
p457
sS'categories'
p458
(lp459
S'misc'
p460
asS'posted'
p461
g166
(S'\x07\xd9\x08\x1c'
p462
tp463
Rp464
ssg32
S'content/posts/2009/gmail-rant/index.md'
p465
sg34
F1433825891.0
sa(dp466
g2
(dp467
g26
g5
(g6
g7
V<p>I've been collaborating with my dad on an experimental web-based guitar chord\u000aediting service. It's still a work in progress, but we are ready to launch a\u000abeta version. Please visit <a href="http://www.guitarunleashed.com">http://www.guitarunleashed.com/</a> to check it out\u000aand provide feedback. </p>\u000a\u000a<p>We hope that the major features of the service are easily discoverable, but in\u000acase they are not, here is a list:</p>\u000a\u000a<ul>\u000a<li>Add chords to your lyrics by clicking on any character</li>\u000a<li>Move chords via drag and drop</li>\u000a<li>Search for songs created by other users or from external sources</li>\u000a<li>Save your work and share it with others</li>\u000a<li>Add songs that you like to your favorites list</li>\u000a<li>Already have lyrics with chords? Create a new song, paste lyrics\u000awith chords in and see the chords come to life.</li>\u000a</ul>\u000a\u000a<p>Please note that the application relies heavily on JavaScript, so it needs to\u000abe enabled in your browser.</p>\u000a\u000a<h2>Known issues</h2>\u000a\u000a<ul>\u000a<li>Some lyrics may display incorrectly. This is an issue with\u000a<a href="http://lyricwiki.org/Main_Page">LyricWiki.org</a></li>\u000a<li>Long chords placed at the beginning or the end of lyrics line may\u000aextend outside of the editing area</li>\u000a<li>Long lines in lyrics are not handled elegantly</li>\u000a</ul>\u000a\u000a<h2>Future features</h2>\u000a\u000a<ul>\u000a<li>Editing lyric text</li>\u000a<li>Commenting on songs</li>\u000a<li>Searching sources of lyrics and chords other than LyricWiki</li>\u000a<li>Song books</li>\u000a</ul>\u000a\u000a<p>Please give us feedback! Use the feedback button on guitarunleashed.com or\u000acontact us directly at <a href="mailto:feedback@guitarunleashed.com">feedback@guitarunleashed.com</a></p>\u000a
p468
tp469
Rp470
sg11
V/guitar-unleashed
p471
sg13
Nsg14
I01
sg15
VGuitar Unleashed
p472
sg18
V\u000a\u000aI've been collaborating with my dad on an experimental web-based guitar chord\u000aediting service.
p473
sS'snip'
p474
g5
(g6
g7
V<p>A web application that lets users collaboratively edit guitar chords. It aims to globally improve the quality of guitar chords.</p>\u000a
p475
tp476
Rp477
sg23
g147
sg31
g471
sg148
(dp478
g150
S'Mar'
p479
sg152
S'March 23, 2009'
p480
sg154
I3
sg155
S'2009-03-23T09:00:00-00:00'
p481
sg157
I1237824000
sg158
I2009
sg159
I23
ssg61
g160
sg29
S'guitar-unleashed'
p482
sS'categories'
p483
(lp484
S'web'
p485
aS'music'
p486
asS'posted'
p487
g166
(S'\x07\xd9\x03\x17'
p488
tp489
Rp490
ssg32
S'content/posts/2009/guitar-unleashed/index.md'
p491
sg34
F1433825897.0
sa(dp492
g2
(dp493
g26
g5
(g6
g7
V<p>After spending a fair bit of time <a href="http://search.twitter.com/search?q=iwork.com">monitoring twitter feeds</a>, I was\u000apleasantly surprised by the <a href="http://smokingapples.com/opinion/iworkcom-understanding-apples-online-office-extension/">world</a>'s <a href="http://www.appleinsider.com/articles/09/01/07/an_extensive_look_at_apples_new_iwork_com_service.html">response</a> to the recently\u000aannounced <a href="http://www.apple.com/iwork/iwork-dot-com/">iwork.com</a>. There were some <a href="http://www.engadget.com/2009/01/06/apple-announces-iwork-com-beta/">premature</a> <a href="http://www.gizmodo.com.au/2009/01/apple_sends_iwork_to_the_clouds_introduces_iworkcom.html">flames</a>, but\u000amostly as a result of a fundamental misunderstanding of iwork.com's\u000apurpose. Someone even gave a <a href="http://ajaxian.com/archives/technical-details-behind-iworkcom/">technical breakdown</a> of the application,\u000awhich is especially endearing to me as an insider.</p>\u000a
p494
tp495
Rp496
sg11
V/iworkcom-feedback
p497
sg13
Nsg14
I00
sg15
ViWork.com feedback
p498
sg18
V\u000a\u000aAfter spending a fair bit of time [monitoring twitter feeds][], I was\u000apleasantly surprised by the [world][]'s [response][] to the recently\u000aannounced [iwork.
p499
sg4
g496
sg23
g147
sg31
g497
sg148
(dp500
g150
S'Jan'
p501
sg152
S'January 7, 2009'
p502
sg154
I1
sg155
S'2009-01-07T09:00:00-00:00'
p503
sg157
I1231347600
sg158
I2009
sg159
I7
ssg61
g160
sg29
S'iworkcom-feedback'
p504
sS'categories'
p505
(lp506
S'personal'
p507
asS'posted'
p508
g166
(S'\x07\xd9\x01\x07'
p509
tp510
Rp511
ssg32
S'content/posts/2009/iworkcom-feedback/index.md'
p512
sg34
F1433825904.0
sa(dp513
g2
(dp514
g26
g5
(g6
g7
V<p>Since starting this site, I've been using a lightly tweaked version of the\u000a<a href="http://wp-themes.com/clockworkair/">ClockWorkAir</a> theme. The old design featured a prominent quote in prime\u000ascreen real estate and a tag cloud in the upper sidebar. A blue title bar,\u000areminiscent of the default wordpress theme, graced the blog header. The main\u000atext was small and there was hardly enough room to place images in the left\u000amargin. Here was my old blog design:</p>\u000a\u000a<p><img src="old-design.png" alt="old" /></p>\u000a\u000a<p>As I had hoped, the design class I'm taking at CMU gave me some good ideas.\u000aHere's my new blog design. I'm much happier with it than with the former look,\u000abut still have a backlog of things to tweak. My design was inspired by several\u000a<a href="http://limi.net/">people's</a> <a href="http://www.joehewitt.com/">web logs</a>, which bring together form and function in a way\u000awhich appeals to me. Please give me constructive feedback on the new design if\u000ayou have time. I will release the wordpress theme if anyone wants it \u2014 I just\u000ahaven't had time to bundle it yet. Here is my new blog design:</p>\u000a\u000a<p><img src="new-design.png" alt="new" /></p>\u000a\u000a<p>As of this month it's been a year since my first blog post. Surprisingly, I've\u000abeen writing one to two blog entries per month and hope to keep my musings\u000aflowing at approximately the same rate in the future. It's very rewarding as a\u000awriter to have a steady flow of visitors, some recurring readership, and even\u000athe odd discussion here or there. Thank you all.</p>\u000a
p515
tp516
Rp517
sg11
V/new-design
p518
sg13
Nsg14
I01
sg15
VNew design
p519
sg18
V\u000a\u000aSince starting this site, I've been using a lightly tweaked version of the\u000a[ClockWorkAir][] theme.
p520
sS'snip'
p521
g5
(g6
g7
V<p>A much needed blog redesign.</p>\u000a
p522
tp523
Rp524
sg23
g147
sg31
g518
sg148
(dp525
g150
S'Sep'
p526
sg152
S'September 28, 2009'
p527
sg154
I9
sg155
S'2009-09-28T09:00:00-00:00'
p528
sg157
I1254153600
sg158
I2009
sg159
I28
ssg61
g160
sg29
S'new-design'
p529
sS'categories'
p530
(lp531
S'design'
p532
aS'web'
p533
asS'posted'
p534
g166
(S'\x07\xd9\t\x1c'
p535
tp536
Rp537
ssg32
S'content/posts/2009/new-design/index.md'
p538
sg34
F1433825907.0
sa(dp539
g2
(dp540
g26
g5
(g6
g7
V<p>In anticipation of 48-739: Making Things Interactive, I've been itching to\u000abuild something interesting. I decided to create a printing plotter out of my\u000abrother's Mindstorms set. There are already many <a href="http://www.norgesgade14.dk/plotter.php">excellent plotter designs</a>\u000afloating around in the Mindstorms community, so I decided to try something new.\u000aPlotters typically draw straight, edge-aligned lines, since they have a caret\u000amotor which travels along the x-axis and a feed motor which aligns along the\u000ay-axis. There are many variations on this theme. </p>\u000a\u000a<p>What if, instead of using the Cartesian coordinate system, a plotter was built\u000aagainst radial coordinates. That is, one motor would control the rotation of an\u000aarm (theta in radian coordinates), and one would drive a caret along the arm,\u000acontrolling the distance from the origin (r in radian coordinates). I began\u000abuilding, and despite the ardent help of my young brother, we failed to create\u000aa reasonable construction. The problem we ran into was that as r increased to\u000athe maximum length of the arm, the engine driving theta would not have enough\u000astrength to rotate due to the increased torque. I'm ashamed to admit that we\u000agave up. </p>\u000a\u000a<p>A month later, I picked up the project again, and due to limited time, decided\u000ato build a Cartesian plotter after all.  I did not consult state of the art of\u000aNXT plotters, and ended up with a plotter of unconventional design. Instead of\u000aa paper feed, a caret travels along the y-axis, and another caret travels along\u000athe first caret on the x-axis. I borrowed wheels from the RCX set, but the rest\u000ais stock NXT. </p>\u000a\u000a<iframe title="YouTube video player" width="600" height="368"\u000a  src="http://www.youtube.com/embed/tZhqjrHSIfE" frameborder="0"\u000a  allowfullscreen></iframe>\u000a\u000a<p>I named the robot "Malevich 2" in honor of <a href="http://en.wikipedia.org/wiki/Kazimir_Malevich">Kazimir Malevich</a>, an avant-guard\u000aRussian painter, famous for pioneering geometric abstract art, especially a\u000aseries of <a href="http://en.wikipedia.org/wiki/File:Malevich.black-square.jpg">paintings of squares</a>. I prefer Malevich 2's rendition of the\u000asquare, but maybe that's just me.</p>\u000a
p541
tp542
Rp543
sg11
V/nxt-plotter
p544
sg13
Nsg14
I01
sg15
VPlotting something radial
p545
sg18
V\u000a\u000aIn anticipation of 48-739: Making Things Interactive, I've been itching to\u000abuild something interesting.
p546
sS'snip'
p547
g5
(g6
g7
V<p>Attempting to build a plotter based on the radial coordinate system using LEGO Mindstorms. Eventually I gave up and built a cartesian plotter.</p>\u000a
p548
tp549
Rp550
sg23
g147
sg31
g544
sg148
(dp551
g150
S'Aug'
p552
sg152
S'August 7, 2009'
p553
sg154
I8
sg155
S'2009-08-07T09:00:00-00:00'
p554
sg157
I1249660800
sg158
I2009
sg159
I7
ssg61
g160
sg29
S'nxt-plotter'
p555
sS'categories'
p556
(lp557
S'physical'
p558
asS'posted'
p559
g166
(S'\x07\xd9\x08\x07'
p560
tp561
Rp562
ssg32
S'content/posts/2009/nxt-plotter/index.md'
p563
sg34
F1433825913.0
sa(dp564
g2
(dp565
g26
g5
(g6
g7
V<p>In the midst of my graduate studies, I somehow found the time to write a\u000asimple prototype for a mobile Guitar Unleashed client. It's more of a\u000aproof-of-concept for some cool new technologies that I've been meaning\u000ato play with. Two things led me down this path:</p>\u000a\u000a<ol>\u000a<li>Since I'm no longer bound by corporate affiliation, I feel compelled\u000ato finally develop an interesting application for iPhone.</li>\u000a<li>I'm very bad at remembering guitar chords and lyrics, but never\u000abother making a cheat sheet to take to the campfire. I nearly always\u000ahave my phone in my pocket, though.</li>\u000a</ol>\u000a\u000a<p>The GU mobile prototype is very simple. Users can manage a list of\u000afavorite songs, and view their lyrics with chords. At first, I set out\u000ato create a native iPhone application to do this, but several\u000aconsiderations made me change my mind:</p>\u000a\u000a<ol>\u000a<li>I don't want to deal with a potential App Store rejection, keeping\u000ain mind the dubious legality of lyrics</li>\u000a<li>The simplicity and data-centric nature of the app lends itself well\u000ato a web implementation.</li>\u000a</ol>\u000a\u000a<p><img src="phone.png" class="left"></p>\u000a\u000a<p>There are a few interesting technical challenges to overcome in implementing\u000athis application. Firstly, the app needs to blend visually with the iPhone look\u000aand feel. I chose the <a href="http://code.google.com/p/iui/">iUI</a> web framework to develop my application. In\u000aretrospect, <a href="http://www.jqtouch.com/">jQTouch</a> would have probably been a better choice, since I ended\u000aup using jQuery. In addition, iUI is very much not bug free, and I don't like\u000aiUI's anchor-based navigation. Apple provides several iPhone-specific meta tags\u000ato enhance the web experience. For example, <code>&lt;link rel="apple-touch-icon"\u000ahref="logo-touch-icon.png" /&gt;</code> allows to specify the icon that will appear in\u000athe iPhone springboard. Setting <code>&lt;meta name="apple-touch-fullscreen"\u000acontent="YES" /&gt;</code> will remove the lower hud of MobileSafari. The full list of\u000aoptions is available <a href="http://developer.apple.com/safari/library/documentation/AppleApplications/Reference/SafariHTMLRef/Articles/MetaTags.html">from Apple</a>.</p>\u000a\u000a<p>The second broad challenge for mine and iPhone web applications in general is\u000athat they need to remain functional without an internet connection. For my\u000amobile client, data would be provided by two calls returning JSON:\u000a<code>/song/search?query=myQuery</code> returning a list of songs and <code>/song/get/myID</code>\u000areturning specific song info. My first instinct was to implement local\u000apersistence via cookies, as is customary in web development. There's a 4K limit\u000aon the size of each cookie, so storing all songs in one cookie was out of the\u000aquestion. A simple alternative was to store a cookie with an array of song IDs,\u000aand a cookie for each song. This strategy worked quite well on WebKit and in\u000athe iPhone simulator. Unfortunately, cookie persistence works differently on\u000athe actual iPhone, likely for security reasons. Every time the phone reboots,\u000aMobileSafari's cookie jar is emptied. </p>\u000a\u000a<p>The alternative to this is a much more modern approach: <a href="http://webkit.org/blog/126/webkit-does-html5-client-side-database-storage/">HTML5 databases</a>.\u000aWebKit now allows you to store structured data locally in an SQLite database. I\u000afound this approach to work very well on iPhone. The database backend persists\u000athrough reboots, as expected. I implemented both storage schemes in separate\u000afiles, available for your scrutiny: the class SongJar for cookies in <a href="offline-mobile/jar.js">jar.js</a>\u000aand SongDatabase for HTML5 storage <a href="offline-mobile/database.js">database.js</a>.</p>\u000a\u000a<p>The last piece of the puzzle is how to force the web application's source files\u000ato get cached on the iPhone, so that the app remains accessible even when the\u000aphone is offline. This is done with a <a href="http://www.w3.org/TR/2009/WD-html5-20090212/offline.html">cache manifest</a>, also new in HTML5. I\u000aran into several problems trying to set this up, and would have benefited from\u000athese tips:</p>\u000a\u000a<ol>\u000a<li>Reference the manifest from the HTML with\u000a<code>&lt;html manifest="cache-manifest"&gt;</code></li>\u000a<li>Serve the manifest with the <code>text/cache-manifest</code> mime type</li>\u000a<li>Ensure that all paths in the manifest are accessible</li>\u000a</ol>\u000a\u000a<p>Though my app is a mere prototype, I'm quite happy with the result.\u000aAfter adding the application to the springboard, it <em>almost</em> feels like\u000aa first class iPhone application. You can try it out at\u000a<a href="http://www.guitarunleashed.com/m/">http://www.guitarunleashed.com/m/</a>, and please bear in mind that it's\u000aa proof of concept.</p>\u000a
p566
tp567
Rp568
sg11
V/offline-web-iphone
p569
sg13
Nsg14
I01
sg15
VOffline web apps on the iPhone
p570
sg18
V\u000a\u000aIn the midst of my graduate studies, I somehow found the time to write a\u000asimple prototype for a mobile Guitar Unleashed client.
p571
sS'snip'
p572
g5
(g6
g7
V<p>A how-to about building offline-capable mobile web applications using AppCache. Also, a rough version of the Guitar Unleashed mobile app.</p>\u000a
p573
tp574
Rp575
sg23
g147
sg31
g569
sg148
(dp576
g150
S'Sep'
p577
sg152
S'September 6, 2009'
p578
sg154
I9
sg155
S'2009-09-06T09:00:00-00:00'
p579
sg157
I1252252800
sg158
I2009
sg159
I6
ssg61
g160
sg29
S'offline-web-iphone'
p580
sS'categories'
p581
(lp582
S'web'
p583
asS'posted'
p584
g166
(S'\x07\xd9\t\x06'
p585
tp586
Rp587
ssg32
S'content/posts/2009/offline-web-iphone/index.md'
p588
sg34
F1433825918.0
sa(dp589
g2
(dp590
g26
g5
(g6
g7
V<p>In late January 2009, <a href="http://www.youtube.com/">YouTube</a> decided to change the default look of their\u000aembedded videos. They silently added an informative header which includes the\u000avideo title and rating. Some time before then, a default search bar appeared at\u000athe top of the video. Thanks to these changes, most haphazardly embedded\u000aYouTube videos on the internet sport a repulsive new look. </p>\u000a\u000a<p>What, you might ask, can be done about this excessive ugliness? Well, you could\u000aswitch to <a href="http://www.vimeo.com">Vimeo</a>, which has a much nicer set of defaults, but that would\u000amean bidding the YouTube community farewell -- and what a tragedy that would\u000abe! A better alternative is to learn the <a href="http://code.google.com/apis/youtube/player_parameters.html">YouTube Embedding API</a>, but there\u000aare still problems:</p>\u000a\u000a<ol>\u000a<li>Embedding code for YouTube videos is ugly.</li>\u000a<li>The YouTube embedding scheme often silently changes.</li>\u000a<li>If you embed multiple videos, there is no way to specify a default\u000aset of embedding preferences.</li>\u000a</ol>\u000a\u000a<p>If you are a wordpress user, all of these issues are resolved by the most\u000aexcellent <a href="http://www.prelovac.com/vladimir/wordpress-plugins/smart-youtube">Smart YouTube</a> plugin. With it you can embed videos by simply\u000ainserting the URL to any YouTube video into your wordpress page or post and\u000areplacing http with http<strong>v</strong>. Through a settings page, you can modify the way\u000aall of your embedded videos look with one fell swoop.</p>\u000a
p591
tp592
Rp593
sg11
V/prettifying-embedded-youtube
p594
sg13
Nsg14
I01
sg15
VPrettifying embedded YouTube
p595
sg18
V\u000a\u000aIn late January 2009, [YouTube][] decided to change the default look of their\u000aembedded videos.
p596
sS'snip'
p597
g5
(g6
g7
V<p>Raving about the Smart Youtube wordpress plugin.</p>\u000a
p598
tp599
Rp600
sg23
g147
sg31
g594
sg148
(dp601
g150
S'Feb'
p602
sg152
S'February 27, 2009'
p603
sg154
I2
sg155
S'2009-02-27T09:00:00-00:00'
p604
sg157
I1235754000
sg158
I2009
sg159
I27
ssg61
g160
sg29
S'prettifying-embedded-youtube'
p605
sS'categories'
p606
(lp607
S'web'
p608
asS'posted'
p609
g166
(S'\x07\xd9\x02\x1b'
p610
tp611
Rp612
ssg32
S'content/posts/2009/prettifying-embedded-youtube/index.md'
p613
sg34
F1433825923.0
sa(dp614
g2
(dp615
g26
g5
(g6
g7
V<p>Shortly after buying my first iPhone over a year ago, I found the right,\u000alow-budget solution to my in-car music needs. The ingredients involved\u000aare the following commonly found household items:</p>\u000a\u000a<ul>\u000a<li>One Compact Disk</li>\u000a<li>Some Duct Tape</li>\u000a</ul>\u000a\u000a<p>The idea is to take the iPhone dock and stick it onto the CD with duct\u000atape. Next, jam the CD into a tight slot in the dash of your car (easy\u000ato find for a Del Sol owner). Finally, attach power cables and/or AUX\u000aaudio jacks to the dock. The result is a flexible, shock-absorbent mount\u000afor your everyone's favorite phone. I've used this hack for well over a\u000ayear now with no problems. Here's how it looks in the interior of my\u000acar:</p>\u000a\u000a<iframe title="YouTube video player" width="600" height="368"\u000a  src="http://www.youtube.com/embed/Yg3VrsDXIpI" frameborder="0"\u000a  allowfullscreen></iframe>\u000a\u000a<p>In a word: awesome.</p>\u000a
p616
tp617
Rp618
sg11
V/the-best-iphone-car-kit
p619
sg13
Nsg14
I01
sg15
ViPhone car kit: roll your own
p620
sg18
V\u000a\u000aShortly after buying my first iPhone over a year ago, I found the right,\u000alow-budget solution to my in-car music needs.
p621
sS'snip'
p622
g5
(g6
g7
V<p>A joke-hack involving a CD, duct tape, an iPhone dock and your car.</p>\u000a
p623
tp624
Rp625
sg23
g147
sg31
g619
sg148
(dp626
g150
S'Feb'
p627
sg152
S'February 5, 2009'
p628
sg154
I2
sg155
S'2009-02-05T09:00:00-00:00'
p629
sg157
I1233853200
sg158
I2009
sg159
I5
ssg61
g160
sg29
S'the-best-iphone-car-kit'
p630
sS'categories'
p631
(lp632
S'physical'
p633
asS'posted'
p634
g166
(S'\x07\xd9\x02\x05'
p635
tp636
Rp637
ssg32
S'content/posts/2009/the-best-iphone-car-kit/index.md'
p638
sg34
F1332684374.0
sa(dp639
g2
(dp640
g26
g5
(g6
g7
V<p>Look at the riders of any city bus. Many of them are plugged into their music\u000aplayers, tapping away to the beat. I propose to augment our natural love of\u000arhythm into a ubiquitous wearable drum system. The target user of this system\u000aisn\u2019t only the typical rhythm loving bus rider, but also an amateur drummer.\u000aDrum kits are heavy and unwieldy, making them difficult to transport to a jam\u000asession. The proposed system can also act as a stand-in for a full drum-kit for\u000aquick, impromptu jamming.</p>\u000a\u000a<p>I took a pair of jeans and imbued them with two <a href="http://www.sparkfun.com/commerce/product_info.php?products_id=9376">force-sensitive resistors</a>,\u000aone on each knee. The left pocket houses a sparkfun box containing an arduino\u000aand a breadboard. Wires run through the pant legs to connect the pads to the\u000abox. Wiring the pants was surprisingly easy, since as I discovered, electric\u000atape easily adheres to denim.</p>\u000a\u000a<iframe title="YouTube video player" width="600" height="368"\u000a  src="http://www.youtube.com/embed/HjWx9fp-8oU" frameborder="0"\u000a  allowfullscreen></iframe>\u000a\u000a<p>The two FSRs are hooked into pull-down switches which connect to analog ports\u000aof the Arduino. Every time a pad is hit, Arduino sends the pad\u000aID and the force of the impact through the serial port. A python program running on my machine\u000alistens on the serial port and synthesizes sounds corresponding to the data\u000ausing <a href="http://pyserial.sourceforge.net/">pyserial</a> and <a href="http://www.pygame.org/news.html">pygame</a> respectively.</p>\u000a\u000a<pre><code>import pygame, sys, serial\u000a\u000a# initialize the pygame mixer\u000apygame.mixer.init(frequency=22050, size=-16, channels=2, buffer=256)\u000a\u000a# map positions to sounds\u000asamples = {\u000a    'lknee': pygame.mixer.Sound('media/hihat-open.aif'),    \u000a    #...\u000a}\u000a\u000a# initialize serial port\u000as = serial.Serial('/dev/tty.usbserial-A6008ea9', 9600)\u000a\u000awhile 1:\u000a    # read from the serial port\u000a    line = s.readline().strip()\u000a    # ...\u000a    sample = samples[pad_id]\u000a    sample.play()\u000a\u000aif __name__ == '__main__':\u000a    pass\u000a</code></pre>\u000a\u000a<p>This first prototype of Drum Pants is intentionally crude. Aside from\u000aincreasing this system\u2019s production value, there are a number of limitations\u000athat should be addressed. The current prototype requires a computer to\u000asynthesize sounds, which greatly hinders portability. By retrofitting the\u000aArduino with a <a href="http://asynclabs.com/wiki/index.php?title=WiShield_1.0">wifi shield</a>, the system could communicate with any\u000awifi-capable synthesizer, such as an Android phone.</p>\u000a\u000a<p>Another issue with this system is that it\u2019s built entirely into a pair of\u000apants. This makes putting drum pads into other items of clothing impossible. To\u000aaddress this problem, the pads could wirelessly communicate to the Arduino\u000adevice. In this case, the pads would be self-contained transmitters that could\u000abe placed anywhere. This opens up a wide variety of applications, such as\u000aplacing the pad onto a pair of shoes to simulate a kick or hi-hat pedal. Do you\u000alike this idea? Please give me feedback below!</p>\u000a\u000a<p><strong>Update:</strong> Do you want to build your own drum pants? Check out this\u000a<a href="http://www.instructables.com/id/Drum-Wear-drums-in-your-clothing/">instructable</a>!</p>\u000a\u000a<p><strong>Update 2:</strong> Accepted as a CHI 2010 WIP. Many thanks to <a href="http://code.arc.cmu.edu/~mdg/">Mark Gross</a>!</p>\u000a
p641
tp642
Rp643
sg11
V/ubiquitous-drums
p644
sg13
Nsg14
I01
sg15
VUbiquitous drums
p645
sg18
V\u000a\u000a\u000aLook at the riders of any city bus.
p646
sS'snip'
p647
g5
(g6
g7
V<p>Is that a portable drum kit in your pants or are you just happy to see me?</p>\u000a
p648
tp649
Rp650
sg23
g147
sg31
g644
sg148
(dp651
g150
S'Nov'
p652
sg152
S'November 18, 2009'
p653
sg154
I11
sg155
S'2009-11-18T09:00:00-00:00'
p654
sg157
I1258563600
sg158
I2009
sg159
I18
ssg61
g160
sg29
S'ubiquitous-drums'
p655
sS'categories'
p656
(lp657
S'physical'
p658
asS'posted'
p659
g166
(S'\x07\xd9\x0b\x12'
p660
tp661
Rp662
ssg32
S'content/posts/2009/ubiquitous-drums/index.md'
p663
sg34
F1433825930.0
sa(dp664
g2
(dp665
g26
g5
(g6
g7
V<p><a href="http://phonegap.com/">PhoneGap</a> is a very useful cross-platform web application wrapper\u000athat lets developers package web applications into native apps. It also\u000alets web apps access functionality traditionally only available in\u000anative apps. This how-to is about exposing additional Android\u000afunctionality to the web with PhoneGap plugins. <a href="http://developer.android.com/reference/android/content/Intent.html">Intents</a> are a\u000afundamental part of the Android ecosystem, allowing a sort of\u000amessage-passing mechanism between applications, but they are not exposed\u000ato web applications. The sample Android plugin I wrote is called\u000aWebIntent, which lets you create a first class Android applications in\u000aJavaScript.</p>\u000a\u000a<h2>WebIntent</h2>\u000a\u000a<p>Firstly, the WebIntent plugin is a means of creating Android activities\u000avia Intents. With just four parameters, <code>action, url, type, extras</code>,\u000ait's possible to get a lot of mileage from the Android OS. For example,\u000ayou can send email: </p>\u000a\u000a<pre><code>Android.sendEmail = function(subject, body) { \u000a  var extras = {};\u000a  extras[WebIntent.EXTRA_SUBJECT] = subject;\u000a  extras[WebIntent.EXTRA_TEXT] = body;\u000a  window.plugins.webintent.startActivity({ \u000a      action: WebIntent.ACTION_SEND,\u000a      type: 'text/plain', \u000a      extras: extras \u000a    }, \u000a    function() {}, \u000a    function() {\u000a      alert('Failed to send email via Android Intent');\u000a    }\u000a  ); \u000a};\u000a</code></pre>\u000a\u000a<p>Or, you can load Google Maps: </p>\u000a\u000a<pre><code>Android.showMap = function (address) {\u000a  window.plugins.webintent.startActivity({\u000a    action: WebIntent.ACTION_VIEW,\u000a    url: 'geo:0,0?q=' + address,\u000a  }, function () {}, function () {\u000a    alert('Failed to open URL via Android Intent');\u000a  });\u000a};\u000a</code></pre>\u000a\u000a<p>Secondly, the plugin lets you react when your PhoneGap\u000aapplication gets invoked with certain intents. To do this, you need to\u000asetup correct intent-filters in the AndroidManifest.xml. For example, to\u000arespond to ACTION_SEND, something like the following should appear in\u000athe manifest: </p>\u000a\u000a<pre><code>&lt;intent-filter&gt; \u000a  &lt;action android:name="android.intent.action.SEND" /&gt;\u000a  &lt;category android:name="android.intent.category.DEFAULT" /&gt;\u000a  &lt;data android:mimeType="text/plain" /&gt;\u000a&lt;/intent-filter&gt;\u000a</code></pre>\u000a\u000a<p>Then, in the JavaScript, you can check what extras were specified during\u000ainvocation:</p>\u000a\u000a<pre><code>// deviceready is PhoneGap's init event\u000adocument.addEventListener('deviceready', function () {\u000a  window.plugins.webintent.getExtra(WebIntent.EXTRA\u005c_TEXT, function (url) {\u000a    // url is the value of EXTRA_TEXT \u000a  }, function() {\u000a    // There was no extra supplied.\u000a  });\u000a});\u000a</code></pre>\u000a\u000a<p>This lets you respond to Intents without writing native code. The plugin\u000acode is available for your use and/or perusal on <a href="https://github.com/phonegap/phonegap-plugins/tree/master/Android/WebIntent/">my github</a>. To install\u000athe plugin, move <a href="https://github.com/phonegap/phonegap-plugins/blob/master/Android/WebIntent/webintent.js">webintent.js</a> to your project's www folder and include a\u000areference to it in your html files. Create a folder called "borismus" within\u000ayour project's src/com/ folder and move <a href="https://github.com/phonegap/phonegap-plugins/blob/master/Android/WebIntent/WebIntent.java">WebIntent.java</a> into it.  </p>\u000a\u000a<h2>Writing Android PhoneGap Plugins</h2>\u000a\u000a<p>The PhoneGap project provides a <a href="http://blogs.nitobi.com/joe/2009/12/17/introducing-ponygap-phonegap-plugins-for-android/">plugin architecture</a>, which isn't very\u000awell documented. Perhaps as a result, there are very few Android plugins\u000a(see <a href="https://github.com/phonegap/phonegap-plugins/tree/master/Android/">the github</a>). There are two parts of an Android plugin: a native\u000aJava class that extends <code>com.phonegap.api.Plugin</code>, and a JavaScript wrapper\u000afor that Java class. The JavaScript wrapper registers the plugin with the\u000aPhoneGap plugin manager. A plugin API is defined in JavaScript, and exposed\u000avia <code>window.plugins.myplugin</code>. Calling the API is done as follows: </p>\u000a\u000a<pre><code>window.plugins.webintent.foo({ arg1: 'val1', arg2: 'val2', // etc });\u000a</code></pre>\u000a\u000a<p>The plugin API is defined in the <code>MyPlugin</code> JavaScript class:</p>\u000a\u000a<pre><code>var MyPlugin = function () {};\u000aMyPlugin.prototype.foo = function (params, success, fail) {\u000a  return PhoneGap.exec(success, fail, 'MyPlugin', 'startActivity', [params]);\u000a};\u000a</code></pre>\u000a\u000a<p>The plugin needs to be registered with PhoneGap before it can be\u000aused. </p>\u000a\u000a<pre><code>PhoneGap.addConstructor(function () {\u000a  // Creates window.plugins.myplugin, an instance of MyPlugin\u000a  PhoneGap.addPlugin('myplugin', new MyPlugin()); \u000a  // Binds MyPlugin to the Java class com.example.MyPlugin \u000a  PluginManager.addService('MyPlugin', 'com.example.MyPlugin');\u000a});\u000a</code></pre>\u000a\u000a<p>Finally, there needs to be a Java class that actually implements the desired\u000abehavior. The execute method takes an action parameter, which is the name of\u000athe function (foo in this example), and an array of arguments: </p>\u000a\u000a<pre><code>public class MyPlugin extends Plugin {\u000a  public PluginResult execute(String\u000a  action, JSONArray args, String callbackId) {\u000a    if (action.equals("foo")) {\u000a      // Implementation \u000a    }\u000a  }\u000a}\u000a</code></pre>\u000a\u000a<p>This architecture is the key to unlock any Android functionality to your\u000aPhoneGap-wrapped mobile web application. One of huge benefits of PhoneGap\u000aand mobile web is that it's a cross-platform solution. I admit that writing\u000aplatform-specific plugins seems counter-productive to this end. However\u000auntil the mobile web gets the love it deserves (via extra sweet mobile\u000abrowsers), there will be a place for platform-specific PhoneGap plugins to\u000amake web applications fit better into native mobile platforms.</p>\u000a
p666
tp667
Rp668
sg11
V/android-phonegap-plugins
p669
sg13
Nsg14
I01
sg15
VWebIntent, an Android PhoneGap plugin
p670
sg18
V\u000a\u000a[PhoneGap][] is a very useful cross-platform web application wrapper\u000athat lets developers package web applications into native apps.
p671
sS'snip'
p672
g5
(g6
g7
V<p>An article on how to write your own PhoneGap plugin for Android. I also write about the WebIntent plugin which lets you create Android intents from the web.</p>\u000a
p673
tp674
Rp675
sg23
g147
sg31
g669
sg148
(dp676
g150
S'Nov'
p677
sg152
S'November 25, 2010'
p678
sg154
I11
sg155
S'2010-11-25T09:00:00-00:00'
p679
sg157
I1290704400
sg158
I2010
sg159
I25
ssg61
g160
sg29
S'android-phonegap-plugins'
p680
sS'categories'
p681
(lp682
S'web'
p683
aS'android'
p684
asS'posted'
p685
g166
(S'\x07\xda\x0b\x19'
p686
tp687
Rp688
ssg32
S'content/posts/2010/android-phonegap-plugins/index.md'
p689
sg34
F1433825738.0
sa(dp690
g2
(dp691
g26
g5
(g6
g7
V<p>A few projects around the internet use an Android phone to control the\u000aLEGO Mindstorms NXT brick. Most involve an ugly hack in which the phone\u000acommunicates with a computer over WiFi, and the computer (paired to the\u000aNXT through bluetooth) submits the command to the brick. These projects\u000atypically use Android as a remote control for the NXT robot, and not as\u000apart of the robot itself. Here is a missed opportunity: the NXT could be\u000aaugmented by an <a href="http://developer.android.com/reference/android/hardware/Sensor.html">impressive list</a> of sensors, GPS and Internet access\u000aprovided by an Android phone. </p>\u000a\u000a<p>This project does just that, while eliminating the need for a computer in\u000athe loop, so that the Android directly communicates to the NXT. This allows\u000afor more powerful Android-powered NXT robots. As an example, I made a fully\u000aautonomous twitter-controlled robot. The NXT uses two motors to spin in\u000aplace or move forward, and a third motor to control the tilt of a Android\u000aphone cradle. The Android phone keeps track of its orientation (compass\u000aheading and tilt), polls <a href="http://search.twitter.com/">twitter search</a> for new commands and sends\u000acommands to the NXT brick. After each command completes, the Android phone\u000atakes a picture and sends it to twitter. Any twitter user can look at the\u000alast few photos, decide which command makes sense to perform next, and issue\u000ait. This approach can be summarized succinctly as "<a href="http://en.wikipedia.org/wiki/Crowdsourcing">crowdsourced</a>\u000a<a href="http://en.wikipedia.org/wiki/Teleoperation">teleoperation</a>". </p>\u000a\u000a<p>Here's a demonstration video of the robot in action:</p>\u000a\u000a<iframe title="YouTube video player" width="600" height="368"\u000a  src="http://www.youtube.com/embed/ATQ_0tySttM" frameborder="0"\u000a  allowfullscreen></iframe>\u000a\u000a<p>I think that this marriage of Android and NXT can fuel a very interesting\u000aset of robots impossible to build with the NXT alone. <a href="http://twitter.com/mindstorms">@mindstorms</a> is\u000aoffline for now to save some battery life, but code is available at my new\u000a<a href="http://github.com/borismus/android-nxt">github</a>. If you make use of my code or have some feedback, please reply\u000abelow!</p>\u000a
p692
tp693
Rp694
sg11
V/android-powered-mindstorms
p695
sg13
Nsg14
I01
sg15
VAndroid-powered mindstorms
p696
sg18
V\u000a\u000aA few projects around the internet use an Android phone to control the\u000aLEGO Mindstorms NXT brick.
p697
sS'snip'
p698
g5
(g6
g7
V<p>This LEGO Mindstorms robot is controlled using twitter. When the robot is online, it responds to at-replied command to @mindstorms.</p>\u000a
p699
tp700
Rp701
sg23
g147
sg31
g695
sg148
(dp702
g150
S'Jun'
p703
sg152
S'June 27, 2010'
p704
sg154
I6
sg155
S'2010-06-27T09:00:00-00:00'
p705
sg157
I1277654400
sg158
I2010
sg159
I27
ssg61
g160
sg29
S'android-powered-mindstorms'
p706
sS'categories'
p707
(lp708
S'android'
p709
aS'web'
p710
asS'posted'
p711
g166
(S'\x07\xda\x06\x1b'
p712
tp713
Rp714
ssg32
S'content/posts/2010/android-powered-mindstorms/index.md'
p715
sg34
F1433825741.0
sa(dp716
g2
(dp717
g26
g5
(g6
g7
V<p>There are hundreds of question answering sites on the Internet. Most of\u000athem focus on specific topics (<a href="http://stackoverflow.com">Stack Overflow</a> for software\u000aengineering, <a href="http://news.ycombinator.com">Hacker News</a> for startups, etc). However, a number of\u000aQ&amp;A sites exist for answering any kind of question, including the\u000acontentious <a href="http://www.somethingawful.com/flash/shmorky/babby.swf">"How is Babby formed?"</a>. Open domain Q&amp;A sites have a\u000adistinct advantage over closed domain sites, since there is no need to\u000aknow which closed domain site is most appropriate for the question at\u000ahand. Still, the asker needs to decide which open domain Q&amp;A site to ask\u000aon! What follows is a self-referential approach to ranking popular open\u000adomain Q&amp;A sites.</p>\u000a\u000a<h2>Who questions the Q&amp;A sites?</h2>\u000a\u000a<p>I compiled a list of the top 7 most popular Q&amp;A sites. Since\u000a<a href="http://alexa.com">alexa.com</a> doesn't track subdomains (ex. <a href="http://answers.yahoo.com">answers.yahoo.com</a>, and\u000a<a href="http://askville.amazon.com">askville.amazon.com</a>), I was forced to use <a href="http://compete.com">compete.com</a>. I then\u000aasked each popular site a "recursive" question:</p>\u000a\u000a<blockquote>\u000a  <p>What is the best question answering site (besides $SITENAME), and why?</p>\u000a</blockquote>\u000a\u000a<p>Eventually, most Q&amp;A sites responded with one or two answers, which I\u000aharvested and analyzed. Clearly this sample is too small to draw any\u000ascientifically valid conclusions, so my analysis is anecdotal at best.\u000aStill it paints an interesting picture of the open domain Q&amp;A space. In\u000aaddition to the responses themselves, I looked at other data, such as\u000aresponse time and response length. The following diagram summarizes the\u000aresults. </p>\u000a\u000a<p><img src="qa-analysis.png" alt="image" /> </p>\u000a\u000a<p>The arrows in the above diagram represent\u000arecommendations from one community to another. The surveyed Q&amp;A sites\u000aare grouped based on popularity into three buckets. The most popular\u000abucket includes Yahoo! Answers and Answers.com, with 44 million and 31\u000amillion monthly uniques respectively. Despite having been around for the\u000alongest (Answers.com launched in 1996), these are the most useless Q&amp;A\u000awebsites. It took 4 days to get the answer "Another good answering site\u000ais answers.com" from a Y! Answers member, and no response ever came from\u000aAnswers.com in 3 weeks.</p>\u000a\u000a<p>The moderately popular bucket of Q&amp;A sites\u000aincludes Answerbag, Mahalo Answers and AskVille. These sites get about 4\u000amillion uniques a month, and generally gave reasonable answers. Mahalo\u000aanswers yielded the longest response, a 120 word paragraph recommending\u000aAardvark. The least visited sites and relative newcomers to the scene\u000aare the most promising and active communities. Aardvark's response speed\u000ais unparalleled, the first answer taking just 17 minutes, probably due\u000ato GTalk integration. While Quora didn't actually yield a Q&amp;A site\u000arecommendation, the respondent produced an reasonable retort to my\u000arather questionable question.</p>\u000a\u000a<blockquote>\u000a  <p>A Q&amp;A "uber-site" that will do a good job being all things to all\u000a  people is simply not going to happen - it is like "one size fits all"\u000a  underwear, which never really fits anyone.</p>\u000a</blockquote>\u000a\u000a<h2>The Ranking</h2>\u000a\u000a<p>Overall, here's my anecdotal list of top 7 open domain Q&amp;A sites (and\u000alinks to the questions I posted):</p>\u000a\u000a<ol>\u000a<li><a href="http://vark.com/t/Tljh9Q">Aardvark</a> \u2013 Quick answers to your questions. Welcome to Aardvark!</li>\u000a<li><a href="https://www.quora.com/What-is-the-best-question-answering-site-besides-Quora-and-why">Quora</a> - Your question is flawed. Allow me to explain why.</li>\u000a<li><a href="http://www.mahalo.com/answers/what-is-the-best-question-answering-site-besides-mahalo-and-why">Mahalo</a> \u2013 I'll answer your question with quite a bit of detail,\u000abut it'll take a while.</li>\u000a<li><a href="http://www.answerbag.com/a_view/9737054">Answerbag</a> - heres a decent answer but forget punctuation</li>\u000a<li><a href="http://askville.amazon.com/question-answering-site-askville/AnswerViewer.do?requestId=74225856&amp;startIndex=1&amp;filter=ActivityOnQuestionsOnly">AskVille</a> - Takes forever to give you a really bizarre answer.</li>\u000a<li><a href="http://answers.yahoo.com/question/index;_ylt=AlzEGEI5OLG69C1Ev0RgR4Pty6IX;_ylv=3?qid=20101007131829AA8cjLR">Yahoo! Answers</a> \u2013 This community is brain damaged. Don't bother.</li>\u000a<li><a href="http://wiki.answers.com/Q/What_is_the_best_question_answering_site_besides_Answers_dot_com_and_why">Answers.com</a> \u2013 This community is inactive. Don't bother.</li>\u000a</ol>\u000a\u000a<p>With a lot more answers from the Q&amp;A communities, and some notion of\u000aweight, you could imagine doing a PageRank-style calculation to\u000adetermine a more meaningful ordering of the top Q&amp;A sites. We leave the\u000adetails of that approach to the reader :) PS. Thanks Sean, Jenn and Pat!</p>\u000a
p718
tp719
Rp720
sg11
V/best-question-answering-sites
p721
sg13
Nsg14
I01
sg15
VThe best question answering sites
p722
sg18
V\u000a\u000aThere are hundreds of question answering sites on the Internet.
p723
sS'snip'
p724
g5
(g6
g7
V<p>A fun self-referential experiment in which I asked (via question answering sites) what the best question answering site is.</p>\u000a
p725
tp726
Rp727
sg23
g147
sg31
g721
sg148
(dp728
g150
S'Oct'
p729
sg152
S'October 25, 2010'
p730
sg154
I10
sg155
S'2010-10-25T09:00:00-00:00'
p731
sg157
I1288022400
sg158
I2010
sg159
I25
ssg61
g160
sg29
S'best-question-answering-sites'
p732
sS'categories'
p733
(lp734
S'social'
p735
asS'posted'
p736
g166
(S'\x07\xda\n\x19'
p737
tp738
Rp739
ssg32
S'content/posts/2010/best-question-answering-sites/index.md'
p740
sg34
F1433825748.0
sa(dp741
g2
(dp742
g26
g5
(g6
g7
V<p>I'm confirmed to go to <a href="http://chi2010.org/">CHI2010</a> in Atlanta, so I spent some time\u000amaking a poster for the <a href="/ubiquitous-drums">Ubiquitous Drums</a> project that was\u000amiraculously accepted as a <a href="ubiquitous-drums-paper.pdf">WIP</a>. It's nice to pretend to be a visual\u000adesigner sometimes. Thanks to Mark, Jenn and Rebeca for your input.</p>\u000a\u000a<p><img src="ubiquitous-drums-poster.jpg" alt="image" /></p>\u000a\u000a<p>I would really appreciate additional suggestions on how to improve the\u000a<a href="ubiquitous-drums-poster.pdf">poster</a>, or your thoughts on academic posters in general. Why are\u000athey usually so ugly?</p>\u000a
p743
tp744
Rp745
sg11
V/chi-2010-poster
p746
sg13
Nsg14
I00
sg15
VCHI 2010 poster
p747
sg18
V\u000a\u000aI'm confirmed to go to [CHI2010][] in Atlanta, so I spent some time\u000amaking a poster for the [Ubiquitous Drums][] project that was\u000amiraculously accepted as a [WIP][].
p748
sg4
g745
sg23
g147
sg31
g746
sg148
(dp749
g150
S'Mar'
p750
sg152
S'March 21, 2010'
p751
sg154
I3
sg155
S'2010-03-21T09:00:00-00:00'
p752
sg157
I1269187200
sg158
I2010
sg159
I21
ssg61
g160
sg29
S'chi-2010-poster'
p753
sS'categories'
p754
(lp755
S'design'
p756
asS'posted'
p757
g166
(S'\x07\xda\x03\x15'
p758
tp759
Rp760
ssg32
S'content/posts/2010/chi-2010-poster/index.md'
p761
sg34
F1433825753.0
sa(dp762
g2
(dp763
g26
g5
(g6
g7
V<p>It's a little late to join the Chrome Extension writing party, and\u000aunfortunately <a href="http://www.readwriteweb.com/archives/chrome_web_store_delayed_until_december.php">still too early</a> for the Chrome Web Store launch.\u000aTiming issues aside, <a href="https://chrome.google.com/extensions/detail/hghoinoackbjefgfkbgnkjknmneajoof">here's an extension</a> that presents\u000athesixtyone.com in a smaller, simpler interface, effectively saving a\u000atab in Chrome and removing all of the (useless to me) social game\u000amechanics from thesixtyone. This post walks through some technical\u000adetails of the implementation, message passing between pages of an\u000aextension, JavaScript injection and DOM event generation. </p>\u000a\u000a<p>Wrapping a web application in an extension may sound deceptively easy:\u000ajust load the website in an iframe in the background (via \u000a<a href="https://code.google.com/chrome/extensions/background_pages.html">background pages</a>), and then control the website from the background page\u000avia commands from the popup page that appears when you click the extension icon\u000ain the toolbar. Not so fast! The key issue with this approach is the control\u000apart. For good reason, browser security disallows executing JavaScript in an\u000aembedded iframe if the src is another domain. The solution to this problem is\u000atwo fold:</p>\u000a\u000a<ol>\u000a<li>Chrome extensions can <a href="https://code.google.com/chrome/extensions/content_scripts.html">inject custom javascript</a> into web pages.</li>\u000a<li>It's possible to <a href="https://code.google.com/chrome/extensions/messaging.html">pass messages</a> between multiple pages running in\u000aChrome. This is an implementation of <a href="http://www.whatwg.org/specs/web-apps/current-work/multipage/comms.html">HTML5 postMessage</a>.</li>\u000a</ol>\u000a\u000a<p>With these two tools, we can inject javascript that implements a message\u000alistener into a target page. This lets you define a JavaScript API\u000aaround the target page. In my case, the API around thesixtyone.com was\u000athe following set of simple commands: next, previous, play, pause and\u000agetSongInfo. Once we have injected JavaScript running in\u000athesixtyone.com, however, it's impossible to call the site's native\u000aJavaScript for security reasons \u000a(from <a href="http://code.google.com/chrome/extensions/content_scripts.html">Chrome Extension Developer Guide</a>):</p>\u000a\u000a<blockquote>\u000a  <p>Content scripts execute in a special environment called an isolated\u000a  world. They have access to the DOM of the page they are injected into,\u000a  but not to any JavaScript variables or functions created by the page.</p>\u000a</blockquote>\u000a\u000a<p>Thus, I had to resort to some unfortunate hackery: generating fake mouse\u000aclicks. One of the benefits of Chrome Extension writing is that there's\u000ano cross-browser issues to deal with, which makes this sort of trick\u000amuch more reliable:</p>\u000a\u000a<pre><code>function simulateClick(elementId) {\u000a  var clickEvent = document.createEvent('MouseEvents');\u000a  clickEvent.initMouseEvent('click', true, false,  document,\u000a      0, 0, 0, 0, 0, false, false, false, false, 0, null);\u000a  document.getElementById(elementId).dispatchEvent(clickEvent);\u000a}\u000a</code></pre>\u000a\u000a<p>Overall, the interaction between the popup, background and injected code\u000ais rather complex and looks something like this: </p>\u000a\u000a<p><img src="chrome-extension-diagram.png" alt="image" /></p>\u000a\u000a<p>One Chrome issue came up in the course of development: when you change\u000aan iframe's src attribute such that the only difference compared to the\u000aold one is the hash part of the URL, the iframe src page does not\u000arefresh. I haven't had the chance to test test this on other browsers\u000ayet.</p>\u000a\u000a<p>Strictly speaking, what I described isn't really a mashup, since I only\u000ause one source. However, this approach is scalable to multiple sources,\u000aand could easily be used to mash multiple web applications up inside an\u000aextension. I hope this post is useful for people trying to wrap one or\u000amany cross-domain websites in their extension. </p>\u000a\u000a<p><img src="radio-61-screenshot.png" alt="image" /> </p>\u000a\u000a<p>I added a sprinkle of design and out came a Chrome Extension called\u000aRadio 61! Let me know what you think if you try it out. Source code is\u000aavailable on <a href="https://github.com/borismus/Radio-61">my github</a>.</p>\u000a
p764
tp765
Rp766
sg11
V/chrome-extension-mashups
p767
sg13
Nsg14
I01
sg15
VChrome extension for thesixtyone
p768
sg18
V\u000a\u000aIt's a little late to join the Chrome Extension writing party, and\u000aunfortunately [still too early][] for the Chrome Web Store launch.
p769
sS'snip'
p770
g5
(g6
g7
V<p>My first Chrome extension that encapsulates thesixtyone music player in a background page.</p>\u000a
p771
tp772
Rp773
sg23
g147
sg31
g767
sg148
(dp774
g150
S'Oct'
p775
sg152
S'October 31, 2010'
p776
sg154
I10
sg155
S'2010-10-31T09:00:00-00:00'
p777
sg157
I1288540800
sg158
I2010
sg159
I31
ssg61
g160
sg29
S'chrome-extension-mashups'
p778
sS'categories'
p779
(lp780
S'web'
p781
aS'chrome'
p782
asS'posted'
p783
g166
(S'\x07\xda\n\x1f'
p784
tp785
Rp786
ssg32
S'content/posts/2010/chrome-extension-mashups/index.md'
p787
sg34
F1433825757.0
sa(dp788
g2
(dp789
g26
g5
(g6
g7
V<p>Last semester at CMU, I was involved in a research project involving\u000a<a href="http://www.mturk.com/">Mechanical Turk</a>. The goal was to get Mechanical Turk users (turkers)\u000ato collaborate on creating online wikipedia-style articles. Prior to my\u000ateam's involvement, an undergraduate created a mediawiki-based platform\u000ato allow turkers to collaborate on articles. Despite a high\u000acompensation, few turkers completed the task. My team tackled the\u000aproblem and came up with some interesting videos on the way. </p>\u000a\u000a<p>We began by conducting contextual interviews with turkers living in\u000aPittsburgh, all of whom rather unexpectedly, were female. The general\u000atakeaway was clear: turkers are used to very short and repetitive tasks,\u000abut article creation requires a prolonged period of concentration. Our\u000asolution was to significantly tweak the task, making it seem less\u000aarduous. In addition to simplifying the HIT's flow, we switched from\u000amediawiki to <a href="http://www.etherpad.com/">etherpad</a> as the article editing and collaboration\u000aplatform. As a result of these changes, we were able to churn out\u000aturker-created articles on a given topic for under ten dollars. Here's a\u000avideo of turkers collaborating on an article about Halloween:</p>\u000a\u000a<iframe title="YouTube video player" width="600" height="480"\u000a  src="http://www.youtube.com/embed/AmUq_Uovqek" frameborder="0"\u000a  allowfullscreen></iframe>\u000a\u000a<p>We started out by creating an etherpad instance with a simple paragraph\u000aabout the topic, as well as some article quality guidelines. Next, we\u000acreated a series of Mechanical Turk HITs referencing the etherpad\u000ainstance's URL. We paid our turkers a quarter up front for accepting the\u000atask, and provided a nickel (up to one dollar) every time they returned\u000ato edit the pad. We had no good way to verify the bonus mechanism, so we\u000agenerally gave out the maximum bonus to every active collaborator.\u000aHere's the evolution of an article on Windows 7: </p>\u000a\u000a<iframe title="YouTube video player" width="600" height="480"\u000a  src="http://www.youtube.com/embed/C7pV9fXIo0M" frameborder="0"\u000a  allowfullscreen></iframe>\u000a\u000a<p>Watching the etherpad explode in color as multiple turkers\u000asimultaneously edit the same article is still mesmerizing. Though the\u000aquality of the articles was quite low, it generally increased with each\u000aturker's successive pass. Also it's worth noting that errors that we\u000adeliberately inserted in the starting paragraph as well as in real time\u000awere swiftly edited out. Not much quantitative analysis of this\u000acollaboration data has been done yet, though there are plans to conduct\u000amore scientific experiments in the future. </p>\u000a\u000a<p>Several other researchers have been conducting interesting studies on\u000amturk. Greg Little's work at MIT generated an interesting project called\u000a<a href="http://groups.csail.mit.edu/uid/turkit/">TurKit</a>, intended to simplify setting up experiments such as the one\u000aoutlined above. Panos Ipeirotis at NYU runs a variety of turk\u000aexperiments as well as an <a href="http://hyperion.stern.nyu.edu/mturk/">mturk statistics monitor</a>, which\u000acontinually scrapes Mechanical Turk and generates summary data. Most\u000arecently, Jennifer Boriss surveyed the Turk community about their\u000abrowser preferences <a href="http://jboriss.wordpress.com/2010/01/13/mechanical-turk-studies-show-ie-users-discontent-a-growing-interest-in-chrome/">projecting a growing interest in Chrome</a>. </p>\u000a\u000a<p>These varied Mechanical Turk projects represent only a small fraction of\u000athe potential of crowd-sourced marketplaces. It's especially interesting\u000ato take complex tasks, break them down into turk-sized morsels, and\u000arecombine them again. To improve the article collaboration scenario\u000adiscussed here, one could provide an outline of an article and task\u000aturkers to elaborate on each point. This seems to be exactly what Greg's\u000agroup is doing in an <a href="http://groups.csail.mit.edu/uid/deneme/?p=603">collaborative essay writing experiment</a>.  Such\u000aan approach may also be effectively applicable to crowd-sourced software\u000adevelopment, which I hope to explore in the near future. Do you know\u000aother interesting projects and resources related to Mechanical Turk? If\u000aso, please respond below!</p>\u000a
p790
tp791
Rp792
sg11
V/crowdsourcing-articles-with-mechanical-turk
p793
sg13
Nsg14
I01
sg15
VCrowdsourcing articles with mechanical turk
p794
sg18
V\u000a\u000aLast semester at CMU, I was involved in a research project involving\u000a[Mechanical Turk][].
p795
sS'snip'
p796
g5
(g6
g7
V<p>At CMU we conducted user research with real life Mechanical Turk users. We also tried to get them to collaboratively write articles using etherpad.</p>\u000a
p797
tp798
Rp799
sg23
g147
sg31
g793
sg148
(dp800
g150
S'Jan'
p801
sg152
S'January 14, 2010'
p802
sg154
I1
sg155
S'2010-01-14T09:00:00-00:00'
p803
sg157
I1263488400
sg158
I2010
sg159
I14
ssg61
g160
sg29
S'crowdsourcing-articles-with-mechanical-turk'
p804
sS'categories'
p805
(lp806
S'social'
p807
asS'posted'
p808
g166
(S'\x07\xda\x01\x0e'
p809
tp810
Rp811
ssg32
S'content/posts/2010/crowdsourcing-articles-with-mechanical-turk/index.md'
p812
sg34
F1433825763.0
sa(dp813
g2
(dp814
g26
g5
(g6
g7
V<p>As a follow up to my <a href="/crowdsourcing-articles-with-mechanical-turk/">last post</a>, I posted a HIT on Mechanical Turk\u000aasking 20 turkers if they know Java. I paid them 5 cents to answer the\u000aquestion. Surprisingly, 9 of 20 claimed to know. I was amazed at how\u000astrong selection bias was in this case, since surely not 50% of turkers\u000aknow how to program! I then asked those turkers who know Java to\u000acomplete the following trivial Java method. If they wrote it correctly,\u000aI paid them a 45 cent bonus.</p>\u000a\u000a<pre><code>public static String reverse(String source) {\u000a  // your code here \u000a}\u000a</code></pre>\u000a\u000a<p>Here are the results:</p>\u000a\u000a<ul>\u000a<li>4 turkers used <code>StringBuffer.reverse</code></li>\u000a<li>3 turkers created a new string by iterating through the original\u000astring in reverse</li>\u000a<li>1 used recursion</li>\u000a<li>1 used <code>Collections.sort(l)</code>. I'm not sure what was intended</li>\u000a</ul>\u000a\u000a<p>I was hoping that people would fill in the empty reverse method with\u000atheir code, but many of them implemented their own methods and helpers.\u000aOne person implemented their own class with extensive comments. This\u000adata as a nice existence proof, indicating that turkers can be harnessed\u000afor programming-related crowdsourcing. </p>\u000a\u000a<p>I'd like to turn Mechanical Turkers into Mechanical Coders. Given a set\u000aof unit tests and a method to implement, their work could be\u000aautomatically verified based on passing the unit tests. Furthermore,\u000asome turkers could be tasked to write additional unit tests for this\u000amethod. Through this technique, it's conceivable to harness the power of\u000aThe Turk to implement whole classes. Code quality aside, what sort of\u000asoftware quality could be achieved by following this approach?</p>\u000a
p815
tp816
Rp817
sg11
V/crowdsourcing-code
p818
sg13
Nsg14
I01
sg15
VCrowdsourcing code
p819
sg18
V\u000a\u000aAs a follow up to my [last post][], I posted a HIT on Mechanical Turk\u000aasking 20 turkers if they know Java.
p820
sS'snip'
p821
g5
(g6
g7
V<p>A brief Mechanical Turk survey to determine whether or not turkers might be willing to write code.</p>\u000a
p822
tp823
Rp824
sg23
g147
sg31
g818
sg148
(dp825
g150
S'Jan'
p826
sg152
S'January 16, 2010'
p827
sg154
I1
sg155
S'2010-01-16T09:00:00-00:00'
p828
sg157
I1263661200
sg158
I2010
sg159
I16
ssg61
g160
sg29
S'crowdsourcing-code'
p829
sS'categories'
p830
(lp831
S'social'
p832
asS'posted'
p833
g166
(S'\x07\xda\x01\x10'
p834
tp835
Rp836
ssg32
S'content/posts/2010/crowdsourcing-code/index.md'
p837
sg34
F1433825767.0
sa(dp838
g2
(dp839
g26
g5
(g6
g7
V<p>Google recently unveiled Instant, a search enhancement which show\u000aresults as you type. The real technical challenge here is scaling the\u000abackend, which now needs to handle a lot more load. The frontend\u000aimplementation, however, is quite simple. Yearning for some web\u000adevelopment, I decided to get my hands dirty. Here's a minimal\u000a<a href="instant-search.html">implementation</a> in under 60 lines of jQuery code.</p>\u000a\u000a<p>Instant Search relies on two separate data sources: a suggestions API,\u000aand a web search API. Roughly speaking, it works as follows: as you type\u000ain the search box, AJAX requests are made to the suggestion API. The top\u000asuggestion is then used as the query to search. In this case, I used the\u000afollowing two services, both of which support JSONP:</p>\u000a\u000a<ul>\u000a<li>Suggest: <a href="http://suggestqueries.google.com/complete/search">http://suggestqueries.google.com/complete/search</a></li>\u000a<li>Search: <a href="http://ajax.googleapis.com/ajax/services/search/web">http://ajax.googleapis.com/ajax/services/search/web</a></li>\u000a</ul>\u000a\u000a<p>To make your own search instant, all you need are your own suggest and\u000asearch feeds, and some tweaks to my code. Just make sure your server can\u000ahandle the load!</p>\u000a
p840
tp841
Rp842
sg11
V/instant-search
p843
sg13
Nsg14
I01
sg15
VInstant search in 60 lines
p844
sg18
V\u000a\u000aGoogle recently unveiled Instant, a search enhancement which show\u000aresults as you type.
p845
sS'snip'
p846
g5
(g6
g7
V<p>A short snippet of jQuery code that approximately implements the instant search experience.</p>\u000a
p847
tp848
Rp849
sg23
g147
sg31
g843
sg148
(dp850
g150
S'Oct'
p851
sg152
S'October 9, 2010'
p852
sg154
I10
sg155
S'2010-10-09T09:00:00-00:00'
p853
sg157
I1286640000
sg158
I2010
sg159
I9
ssg61
g160
sg29
S'instant-search'
p854
sS'categories'
p855
(lp856
S'web'
p857
asS'posted'
p858
g166
(S'\x07\xda\n\t'
p859
tp860
Rp861
ssg32
S'content/posts/2010/instant-search/index.md'
p862
sg34
F1433825770.0
sa(dp863
g2
(dp864
g26
g5
(g6
g7
V<p>Ever wanted to join a band? I bet you have! Why? Because collaborative\u000amusic making is an incredibly enjoyable and rewarding experience. But\u000athe barriers to entry are high: not only do you need to have baseline\u000amusical skills, you also need considerable managerial talent to find and\u000abring together disorganized musicians. To find partners to jam with,\u000apeople use craigslist and band matching sites to try to establish\u000arelationships with randoms. Why not leverage our social networks for\u000athis purpose?</p>\u000a\u000a<p>Okay, now that you're fully convinced that there's a huge opportunity to\u000atap into this friend-jam space, let me introduce <a href="http://www.jamhunt.com/">Jam Hunt</a>. The idea\u000abehind Jam Hunt is to allow you to manage your musical profile by\u000aspecifying a list of instruments you are skilled at and a list of songs\u000ayou know how to play. If your friends also maintain such profiles, Jam\u000aHunt can look across the social graph and discover friends to try\u000ajamming with. Thus, the application enables spontaneous <em>flash bands</em>\u000a(in the spirit of <a href="http://en.wikipedia.org/wiki/Flash_mob">flash mobs</a>) to form brief, friendly jam sessions. </p>\u000a\u000a<p>I developed a Jam Hunt prototype for <a href="http://www.hcii.cmu.edu/courses/software-architecture-user-interfaces-0">SAUI class</a> while pleasantly\u000astranded in Pittsburgh as a result of <a href="http://www.nytimes.com/2010/04/25/weekinreview/25kimmelman.html">Eyjafjallajkull's eruption</a>.\u000aThe assignment stipulated that I implement a Facebook application, which\u000awas initially distressing to me, due to the <a href="http://www.codinghorror.com/blog/2007/06/avoiding-walled-gardens-on-the-internet.html">walled-garden nature</a> of\u000athe platform. I was slightly mollified when I discovered three things:</p>\u000a\u000a<ol>\u000a<li>that there is a <a href="http://wiki.developers.facebook.com/index.php/User:PyFacebook_Tutorial">way to use django</a> to develop Facebook apps.</li>\u000a<li>that the <em>average</em> Facebook user has a whopping 130 friends.</li>\u000a<li>that there is a potentially <a href="http://developers.facebook.com/docs/opengraph">bright future</a> ahead for Facebook</li>\u000a</ol>\u000a\u000a<p>My prospects for having fun while developing something useful, and\u000apotentially viral, and not entirely evil, were on the rise. </p>\u000a\u000a<p>As it turns out, developing a django application for Facebook is no\u000acakewalk.  Firstly, python is not an officially supported language for\u000aFacebook development. As a result, there are a number of\u000a<a href="http://code.google.com/p/simplefacebook/">semi-abandoned</a> <a href="http://github.com/sciyoshi/pyfacebook/tree/master">open source projects</a> to <a href="http://code.google.com/p/minifb/">bridge that gap</a>.\u000aCoupled with Facebook's outright disregard for API stability, calling\u000aNotifications.send and Stream.write were next to impossible from python.\u000aBut surely writing PHP applications must be a breeze, right? Well,\u000aduring the week that I was developing Jam Hunt,\u000a<a href="http://forum.developers.facebook.com/">forum.developers.facebook.com</a>, one of the most indexed resources on\u000aFacebook API questions, was consistently down. The sorry state of their\u000ahybrid <a href="http://wiki.developers.facebook.com/index.php/New_Design_Platform_Changes">documentation-wiki</a> system was just icing on the cake. </p>\u000a\u000a<p>Anyway, enough bitching! If you have some spare cycles and a Facebook\u000aaccount, please try <a href="http://www.jamhunt.com/">Jam Hunt</a>. Whether you find it interesting,\u000aappealing, pointless, ugly, or just outright broken, let me know.</p>\u000a
p865
tp866
Rp867
sg11
V/jam-hunt
p868
sg13
Nsg14
I01
sg15
VJam Hunt: friendly jam sessions
p869
sg18
V\u000a\u000aEver wanted to join a band? I bet you have! Why? Because collaborative\u000amusic making is an incredibly enjoyable and rewarding experience.
p870
sS'snip'
p871
g5
(g6
g7
V<p>A Facebook application for finding impromptu jam partners.</p>\u000a
p872
tp873
Rp874
sg23
g147
sg31
g868
sg148
(dp875
g150
S'Apr'
p876
sg152
S'April 24, 2010'
p877
sg154
I4
sg155
S'2010-04-24T09:00:00-00:00'
p878
sg157
I1272124800
sg158
I2010
sg159
I24
ssg61
g160
sg29
S'jam-hunt'
p879
sS'categories'
p880
(lp881
S'web'
p882
aS'music'
p883
asS'posted'
p884
g166
(S'\x07\xda\x04\x18'
p885
tp886
Rp887
ssg32
S'content/posts/2010/jam-hunt/index.md'
p888
sg34
F1433825775.0
sa(dp889
g2
(dp890
g26
g5
(g6
g7
V<p>I'm about to embark on a 3-week heads-down coding spree to finish off\u000athe final project of my masters. I convinced my team and our clients\u000athat it's a good idea to use jQuery Mobile instead of native Android to\u000aimplement our data-centric application, so I've been playing with it\u000aquite a bit over the last week. jQM is still in Alpha 2, so to get my\u000afeet wet, I decided to write a demo app before starting the final\u000aproject. After stumbling over a few kinks and rough edges, a weekend of\u000acoding yielded a <a href="/x/hackernews/">mobile Hacker News client</a>.</p>\u000a\u000a<p><img src="hackernews.png" alt="image" /></p>\u000a\u000a<h2>Loading JavaScript</h2>\u000a\u000a<p>jQuery Mobile is firmly rooted in the progressive enhancement philosophy\u000aof web application development. In practice, this means that the simple\u000amarkup that you write in it is processed by the framework's JavaScript\u000aand transformed into much more complex and JS-enhanced markup. In\u000aparticular, jQM rewrites <code>&lt;a href="page.html"&gt;&lt;/a&gt;</code> into an element that\u000aloads the contents of the page via an AJAX request. Unfortunately, CSS\u000aand JavaScript referenced by <code>page.html</code> is not loaded. </p>\u000a\u000a<p>To work around this, I've been using a makeshift JS loader which latches\u000aon to the jQM pageshow event so that whenever an HTML page loads, the\u000acorresponding JavaScript file also loads. To prevent the JS loader from\u000aloading already-loaded JavaScript whenever a page is reloaded, it\u000aemploys a cache. Still, the JS needs to be triggered when the page is\u000aloaded, so there's a way to bind a page load to a function. Here's some\u000asample usage of the ScriptCache:</p>\u000a\u000a<pre><code>// Create a new ScriptCache in the global scope\u000avar scriptCache = new ScriptCache();\u000a// Now when page.html loads, page.js will load\u000a\u000a// Also, each page can register with the scriptCache\u000ascriptCache.onPageLoad('page.html', pageInitFunction);\u000a\u000a// Each page.js looks like this:\u000a(function() {\u000a  var init = function() {...};\u000a\u000a  // Whenever this page is loaded, call init\u000a  scriptCache.onPageLoad('page.html', init);\u000a  // Call init the first time it's loaded too\u000a  init();\u000a})();\u000a</code></pre>\u000a\u000a<p>This approach was inspired by a discussion on the <a href="http://forum.jquery.com/topic/links-don-t-load-scripts">jQuery Forum</a>, and\u000aI really hope that there will be a better answer from readers or the jQM\u000adevelopers. The <a href="https://github.com/borismus/jQuery-Mobile-Hacker-News/blob/master/assets/www/scriptcache.js">ScriptCache loader is on github</a>.</p>\u000a\u000a<h2>Passing Parameters</h2>\u000a\u000a<p>Another issue I came across was parameter passing between pages. The jQM\u000aHacker News client needs to pass item IDs from the main page listing to\u000aitem pages. Unfortunately jQuery Mobile seems to have no provision for\u000adoing this. A couple approaches come to mind. One way is through a\u000aglobal variable parameter passing convention (possible since jQuery\u000apages aren't navigated to, but loaded with AJAX). The way I'm using is\u000athrough GET parameters after the hash. Thus the URL to an item page\u000alooks like this: http://example.com/index.html#item.html?id=82831.\u000aHowever, the pageshow event fires <em>just slightly</em> before the\u000awindow.location is updated with the requested URL. I'm currently working\u000aaround this with a <code>setTimeout</code>, but this is clearly unacceptable. This\u000aissue is being <a href="https://github.com/jquery/jquery-mobile/issues#issue/450/comment/543394">discussed on github</a>. </p>\u000a\u000a<p>There's also an issue passing parameters via the jQM changePage call:\u000a<code>$.mobile.changePage('page.html?key=value');</code>. The parameters are simply\u000aignored. Thus I was forced to use <code>window.location.href +=\u000a'#page.html?key=value';</code>, which only works if your window.location.href\u000ais the main application page.</p>\u000a\u000a<h2>Hacker News</h2>\u000a\u000a<p>Using the workarounds outlined above, I wrote an application that lets\u000ayou read and post to <a href="http://news.ycombinator.com">Hacker News</a>, a startup-oriented online\u000acommunity run by <a href="http://ycombinator.com">Y Combinator</a>. It uses the <a href="http://api.ihackernews.com">third party HN API</a>\u000aprovided by <a href="http://twitter.com/ronnieroller">@ronnieroller</a>, who has been very responsive on twitter\u000a(thanks!). For fetching posts and comments, the application relies on\u000athe JSONP methods of this API. It doesn't currently permit authenticated\u000aposting and voting on HN. </p>\u000a\u000a<p>For commenting and submitting links, I embed actual Hacker News post\u000apages in an iframe, working around the lack of support and avoiding\u000atrust issues. Unfortunately the <a href="http://code.google.com/p/android/issues/detail?id=12558">Android browser has a bug</a> which\u000aforces iframes to resize to the width of their contained textarea if one\u000aexists in the embedded page. This causes the width of the page to grow\u000awhich forces jQuery Mobile to relayout. The reference implementation\u000a(iOS) does not suffer from this issue. </p>\u000a\u000a<p>I'd be glad to get your feedback on this app. If you want to read the\u000acode or fork the project, it's available <a href="https://github.com/borismus/jQuery-Mobile-Hacker-News/tree/master/assets/www/">on github</a>. </p>\u000a\u000a<p>I've packed this application with PhoneGap, and currently use it as the\u000aHN reader on my phone. Existing HN applications on Android don't provide\u000ashare intent handlers (to share from the browser or your RSS reader).\u000aThis app provides share intent handling through WebIntents, an Android\u000aPhoneGap plugin that I've been working on. More on that next post, so\u000astay tuned!</p>\u000a
p891
tp892
Rp893
sg11
V/jquery-mobile-hacker-news
p894
sg13
Nsg14
I01
sg15
VjQuery mobile hacker news
p895
sg18
V\u000a\u000aI'm about to embark on a 3-week heads-down coding spree to finish off\u000athe final project of my masters.
p896
sS'snip'
p897
g5
(g6
g7
V<p>My first jQuery Mobile application (using Alpha 1). I found a few bugs and contributed a couple of plugins.</p>\u000a
p898
tp899
Rp900
sg23
g147
sg31
g894
sg148
(dp901
g150
S'Nov'
p902
sg152
S'November 17, 2010'
p903
sg154
I11
sg155
S'2010-11-17T09:00:00-00:00'
p904
sg157
I1290013200
sg158
I2010
sg159
I17
ssg61
g160
sg29
S'jquery-mobile-hacker-news'
p905
sS'categories'
p906
(lp907
S'web'
p908
asS'posted'
p909
g166
(S'\x07\xda\x0b\x11'
p910
tp911
Rp912
ssg32
S'content/posts/2010/jquery-mobile-hacker-news/index.md'
p913
sg34
F1433825781.0
sa(dp914
g2
(dp915
g26
g5
(g6
g7
V<p>Over this last year and a half, I spent a lot of time in airports,\u000aflying nearly 150,000 km (queue environmental angst). I lived in Europe\u000aand both coasts of the US, somehow getting a masters from CMU in the\u000aend. For posterity, I wanted to digitally summarize this extensive\u000aglobetrotting experience, but found no adequate applications. It was an\u000aexperience worth celebrating, and how better to celebrate than by\u000awriting some code...? So I wrote <a href="http://thattrip.appspot.com/">an application demo</a> that renders\u000atravel adventures as a tour in Google Earth. </p>\u000a\u000a<p>First you fill in a pretty silly madlib describing your trip: </p>\u000a\u000a<p><img src="madlib.png" alt="image" /> </p>\u000a\u000a<p>Then, you can view your trip in the browser using the [Google Earth\u000aplugin][], or download the tour and open it in the Google Earth\u000aapplication. Each trip gets a unique URL that can be shared with others.\u000aHere is a youtubified summary of my recent travels generated by this\u000aapp:</p>\u000a\u000a<iframe title="YouTube video player" width="600" height="480"\u000a  src="http://www.youtube.com/embed/N1y8C_w-7Uw" frameborder="0"\u000a  allowfullscreen></iframe>\u000a\u000a<h2>How it works</h2>\u000a\u000a<p>The madlib provides a list of places and transportation modes, the\u000aplaces resolving to (lat, long) coordinates, and the transportation\u000amodes affecting the transition effects between places. Airplane\u000atransitions are shown with the camera oriented north, in a bird's eye\u000aview, while ground transport transitions are more complex, facing in the\u000adirection of travel and changing pitch angle. The application builds KML\u000ain JavaScript and eventually loads it all in a Google Earth plugin:</p>\u000a\u000a<pre><code>// Build the KML\u000avar helper = new KMLHelper();\u000ahelper.processData(data.places, function() {\u000a  // Generate KML tour for this\u000a  data.kml = helper.kml();\u000a  data.distance = helper.distance();\u000a\u000a  var json = JSON.stringify(data);\u000a  // Save the new data to the database\u000a  $.ajax({...})\u000a});\u000a\u000a// Eventually render the KML (ge is a Google Earth instance)\u000a$.getJSON('/trips/' + id + '/', function(data) {\u000a  var kmlObject = ge.parseKml(data.kml);\u000a  ge.getFeatures().appendChild(kmlObject);\u000a});\u000a</code></pre>\u000a\u000a<p>Place-to-place transition effects are achieved by using <a href="http://code.google.com/apis/kml/documentation/touring.html">KML Tours</a>,\u000awhich provide powerful camera control requiring some computation, like\u000agetting the midpoint between (lat, long) pairs. Math for midpoint and\u000aother common geodesy operations are <a href="http://www.movable-type.co.uk/scripts/latlong.html">well documented</a> by Chris Veness,\u000awho also provides an implementation in the form of a\u000a<a href="http://www.movable-type.co.uk/scripts/latlon.js">handy JavaScript library</a>. </p>\u000a\u000a<p>In addition to destinations, travel paths and transitions, this demo\u000auses the <a href="http://code.google.com/apis/picasaweb/overview.html">picasaweb API</a> to get public picasaweb images as\u000aGroundOverlays near places you visited. While it's possible to get a\u000auser's photos tagged with a tag, there is, rather bizarrely, no way to\u000aget a specific user's images tagged with a given location. </p>\u000a\u000a<p>As usual, all of the code is available on <a href="https://github.com/borismus/That-Trip">the github</a>. It's running\u000alive on <a href="http://thattrip.appspot.com/">thattrip.appspot.com</a>, so try it out.</p>\u000a\u000a<h2>Observations</h2>\u000a\u000a<p>The Google Earth plugin provides what is probably the richest map view\u000aavailable on the web, but it is not without drawbacks. The mere fact\u000athat it's a plugin immediately shrinks the possible user base of any app\u000athat relies it and adds to load time. There's no way to pre-cache the\u000adata that will be shown on a tour, causing a lot of chunky terrain\u000aduring quick transitions. While KML is rich and makes it possible to\u000aembed images as ground and photo overlays, the web at large has far more\u000aoptions to handle images, through CSS transformations and animations or\u000aby embedding in a canvas or SVG element. </p>\u000a\u000a<p>This demo is very geo-centric, telling a story almost exclusively about\u000awhere you went, with the images added as an afterthought. In retrospect,\u000aa more compelling story is one about what you personally saw, shown in\u000atravel photos, with a geodesy cherry on top. Stay tuned for future\u000awork...</p>\u000a
p916
tp917
Rp918
sg11
V/kml-tours-google-earth
p919
sg13
Nsg14
I01
sg15
VKML tours in Google Earth
p920
sg18
V\u000a\u000aOver this last year and a half, I spent a lot of time in airports,\u000aflying nearly 150,000 km (queue environmental angst).
p921
sS'snip'
p922
g5
(g6
g7
V<p>My take on visualizing surprisingly large amounts of travel in 2010. I built a web application that uses Google Earth KML Tours for the purpose.</p>\u000a
p923
tp924
Rp925
sg23
g147
sg31
g919
sg148
(dp926
g150
S'Dec'
p927
sg152
S'December 25, 2010'
p928
sg154
I12
sg155
S'2010-12-25T09:00:00-00:00'
p929
sg157
I1293296400
sg158
I2010
sg159
I25
ssg61
g160
sg29
S'kml-tours-google-earth'
p930
sS'categories'
p931
(lp932
S'web'
p933
asS'posted'
p934
g166
(S'\x07\xda\x0c\x19'
p935
tp936
Rp937
ssg32
S'content/posts/2010/kml-tours-google-earth/index.md'
p938
sg34
F1433825786.0
sa(dp939
g2
(dp940
g26
g5
(g6
g7
V<p>I recently switched from shared hosting to a VPS, expecting to get an\u000aimmediate and automatic performance boost. I was overly optimistic and\u000aran into memory trouble right away. After endlessly struggling with\u000aApache and mod_php configuration, I was ready to give up. Then on a\u000awhim, I switched to nginx/fastcgi to see the average response time drop\u000afrom 1500ms to 300ms: </p>\u000a\u000a<p><img src="performance.png" alt="image" /> </p>\u000a\u000a<p><a href="http://www.webfaction.com">WebFaction</a> is a great shared hosting environment, providing tons of\u000afunctionality and impeccable customer support. However, shared hosting\u000ameans that multiple users share resources on a single physical machine,\u000aresulting in wildly fluctuating <a href="http://www.pingdom.com">site performance and uptime</a> at the\u000awhim of other clients. Also, I prefer to do system administration from\u000athe command line, but WebFaction provides a powerful but clunky\u000aweb-based administrative interface. All in all, despite my respect for\u000aWebFaction, I said goodbye and switched to <a href="http://www.slicehost.com">Slicehost</a>. </p>\u000a\u000a<p>I ordered my shiny new slice and plunged into configuration. I migrated\u000awordpress databases and set up Apache with mod<em>php. Everything seemed\u000ato work reasonably well until Apache ran for a few hours and began\u000aconsuming my memory allowance. The slice started thrashing and\u000aperformance fell to a crawl.  I found the culprit to be in my\u000ampm</em>prefork_module MaxClients and MaxRequestsPerChild configuration,\u000abut even after tweaking those, my slice was hitting the wall pretty\u000aquickly. I nearly returned to cushy WebFaction, where I had marginal\u000aperformance without the headache, but decided to experiment more. </p>\u000a\u000a<p>Nginx is a minimalist HTTP server written by <a href="http://sysoev.ru/en/">Igor Sysoev</a> for\u000a<a href="http://www.rambler.ru/">Rambler</a>. It's now being used by <a href="http://www.wordpress.com">Wordpress</a> and other high profile\u000asites. Seeking help from the internet, I eventually came across Thomasz\u000aSterna's <a href="http://tomasz.sterna.tv/2009/04/php-fastcgi-with-nginx-on-ubuntu/">php-fastcgi init script</a> and adapted it for my slice,\u000ausing</p>\u000a\u000a<pre><code>PHP_FCGI_CHILDREN=5\u000aPHP_FCGI_MAX_REQUESTS=100\u000a</code></pre>\u000a\u000a<p>for the low-memory environment. In addition, I installed the \u000a<a href="http://wordpress.org/extend/plugins/wp-super-cache/">WP Super Cache</a> plugin.</p>\u000a\u000a<p>For my <a href="http://www.guitarunleashed.com">django site</a>, I wrote an init script for \u000alaunching the django fcgi server via django's manage.py. Here's the\u000aimportant part:</p>\u000a\u000a<pre><code>MAXCHILDREN=5\u000aMAXSPARE=5\u000aMINSPARE=2 \u000a# ... \u000astart-stop-daemon --quiet --start \u005c\u000a  --pidfile $PIDFILE --chuid "$USER" \u005c\u000a  --exec /usr/bin/env -- python $SITEPATH/$SITENAME/manage.py runfcgi \u005c\u000a  --settings=settings \u005c\u000a  host=$HOST port=$PORT pidfile=$PIDFILE \u005c\u000a  maxchildren=$MAXCHILDREN maxspare=$MAXSPARE minspare=$MINSPARE\u000a</code></pre>\u000a\u000a<p>I'm happy with the performance both for django and php-powered sites.\u000aHowever, I'm just guessing when it comes to values of <code>MAXCHILDREN</code>,\u000a<code>MAXSPARE</code> and <code>MINSPARE</code>, <code>PHP_FCGI_CHILDREN</code> and\u000a<code>PHP_FCGI_MAX_REQUESTS</code>.</p>\u000a\u000a<p>Do you have insight on how to tweak these parameters? Is your VPS\u000ahosting configuration better?</p>\u000a
p941
tp942
Rp943
sg11
V/lightweight-wordpress-on-slicehost
p944
sg13
Nsg14
I01
sg15
VLightweight Wordpress on Slicehost
p945
sg18
V\u000a\u000aI recently switched from shared hosting to a VPS, expecting to get an\u000aimmediate and automatic performance boost.
p946
sS'snip'
p947
g5
(g6
g7
V<p>My experiences moving from the WebFaction shared host to the Slicehost VPS.</p>\u000a
p948
tp949
Rp950
sg23
g147
sg31
g944
sg148
(dp951
g150
S'Feb'
p952
sg152
S'February 26, 2010'
p953
sg154
I2
sg155
S'2010-02-26T09:00:00-00:00'
p954
sg157
I1267203600
sg158
I2010
sg159
I26
ssg61
g160
sg29
S'lightweight-wordpress-on-slicehost'
p955
sS'categories'
p956
(lp957
S'web'
p958
asS'posted'
p959
g166
(S'\x07\xda\x02\x1a'
p960
tp961
Rp962
ssg32
S'content/posts/2010/lightweight-wordpress-on-slicehost/index.md'
p963
sg34
F1433825792.0
sa(dp964
g2
(dp965
g26
g5
(g6
g7
V<p>Business card design is a tricky art. It's a fairly constrained space,\u000abut that's what design is all about, right? I'm ordering a personal set\u000aof <a href="http://us.moo.com/en/products/minicards.php">moo mini cards</a>. These are small, two sided prints. One side\u000acontains an image, and the other contains contact information. On the\u000aimage side, I'm putting snippets of travel photography. The other side\u000ais by default a conventional list of contact information, but moo\u000aconveniently allows it to be replaced by a custom image. </p>\u000a\u000a<p>Rather than having each field separately labeled, I tried to uncover as\u000amuch information as possible within my email address. Hidden inside are\u000amy first name, last name, website, and twitter account! Here's a minimal\u000adesign concept that tries to break it down. </p>\u000a\u000a<p><img src="business-card.png" alt="image" /> </p>\u000a\u000a<p>In my case, my email address contains all relevant info except the phone\u000anumber which I don't want to include anyway. I hope that this simple\u000aidea can inspire you to come up with something more polished. Looking\u000aforward to what you come up with! P.S. Thanks for the help, <a href="http://www.jennlu.com/">Jenn</a>!</p>\u000a
p966
tp967
Rp968
sg11
V/minimal-business-card-design
p969
sg13
Nsg14
I01
sg15
VMinimal business card design
p970
sg18
V\u000a\u000aBusiness card design is a tricky art.
p971
sS'snip'
p972
g5
(g6
g7
V<p>My quest to create the most minimalist business cards possible!</p>\u000a
p973
tp974
Rp975
sg23
g147
sg31
g969
sg148
(dp976
g150
S'Jul'
p977
sg152
S'July 21, 2010'
p978
sg154
I7
sg155
S'2010-07-21T09:00:00-00:00'
p979
sg157
I1279728000
sg158
I2010
sg159
I21
ssg61
g160
sg29
S'minimal-business-card-design'
p980
sS'categories'
p981
(lp982
S'design'
p983
asS'posted'
p984
g166
(S'\x07\xda\x07\x15'
p985
tp986
Rp987
ssg32
S'content/posts/2010/minimal-business-card-design/index.md'
p988
sg34
F1433825797.0
sa(dp989
g2
(dp990
g26
g5
(g6
g7
V<p>HTML5 games are really picking up. Casual Girl Gamer recently produced a\u000anice <a href="http://www.casualgirlgamer.com/articles/entry/28/The-Best-30-HTML-5-games/">list of impressive titles</a>. The modern web platform (namely,\u000afast javascript and canvas) is incredibly promising to a game developer.\u000aThe advantages that it brings are huge: no installation required, and\u000aubiquitous cross platform compatibility. I took a practical look at\u000agames in the HTML5 mobile space, taking <a href="http://lostdecadegamesapp.appspot.com/">Onslaught!</a>, a particularly\u000afun and well written game, and <a href="onslaught/">porting it to Android/iPhone</a>. </p>\u000a\u000a<p>Perhaps porting is too strong a word here. Onslaught! runs and performs\u000areasonably well on my Nexus One running Android 2.2.1. The only problem\u000ais that the game uses keyboard input, making it completely unplayable on\u000amobile devices. Luckily, the controls are quite simple, and only require\u000aa directional pad and two buttons. So I decided to build an on-screen\u000avirtual game controller, not unlike those found in many native iPhone\u000agames.  At first I was inclined to build the controls by extending the\u000agame itself (using the canvas element), but then decided that an\u000aHTML-based approach is better (since it saves the trouble of hit\u000adetection), and might even work as a generic controller for other games.</p>\u000a\u000a<p>Hoping to create a general game controller for Android/iPhone, I sought\u000ato generate keyboard events from JavaScript, based on touch input. While\u000athis is possible if your key event handlers are written in a particular\u000aframework (<a href="http://api.jquery.com/keydown/">say jQuery</a>), it seems to be generally impossible \u000a<a href="http://stackoverflow.com/questions/1601593/fire-tab-keypress-event-in-javascript">for security reasons</a>. I wrote an Onslaught!-specific <a href="onslaught/js/controller.js">controller</a>\u000athat should be easy to port to any other game. The controller simply\u000aembeds the Onslaught! game in it's original 640x480 resolution, which is\u000athen scaled to the device size using the <a href="http://www.quirksmode.org/mobile/viewports.html">meta viewport</a> element. The\u000aresult is a mobile game that's playable on Nexus One, and\u000anot-quite-playable on iPod Touch (2nd gen) due to slower JavaScript\u000aexecution. These are the only two devices I currently have access to, so\u000aI would appreciate it if you could [try it][porting it to\u000aAndroid/iPhone] on your Android or iOS device (in landscape mode) and\u000alet me know how it goes!</p>\u000a\u000a<p><a href="onslaught/"><img src="onslaught.png" alt="image" /></a> </p>\u000a\u000a<p>I ran into a few stumbling blocks as I was developing and testing this\u000aport, most of which involve the Android browser on Nexus One running\u000aAndroid 2.2.1.</p>\u000a\u000a<ol>\u000a<li>Very immature touch event (ontouchstart, ontouchend, etc) support.\u000aIn fact, the <strong>browser doesn't seem to support multi-touch at all</strong>\u000a(ie. only one touch can be registered at a time). In contrast,\u000aSafari for iOS supports multitouch events quite well. For complete\u000adetails, see this <a href="http://quirksmode.org/mobile/tableTouch.html">quirksmode writeup</a> and this <a href="http://code.google.com/p/android/issues/detail?id=11909">bug report</a>.</li>\u000a<li>The Android browser completely ignores many properties in the meta\u000aviewport element's content attribute. Specifically, the <strong>browser\u000adoesn't react to initial-scale, minimum-scale, maximum-scale and\u000awidth</strong>. As a result, I had to hack around this issue by abusing an\u000aAndroid-only property called <a href="http://developer.android.com/reference/android/webkit/WebView.html">target-densityDpi</a>. I suspect that I\u000amay be doing something wrong here, since it's a pretty fundamental\u000aissue. Still I logged a <a href="http://code.google.com/p/android/issues/detail?id=11912">bug</a>.</li>\u000a<li>Less significant but still noteworthy, the CSS <strong>:active selector\u000adoes not activate</strong> on the Android browser (at least for div\u000aelements). The reference implementation is iOS, where a touchstart\u000aevent on a div element causes it to become :active until a touchend\u000aevent.</li>\u000a</ol>\u000a\u000a<p>In the short term, HTML as a gaming platform is emerging as a real Flash\u000akiller. In the long term, I wouldn't be surprised if HTML games will be\u000awidely played on Mac, PC, TV console, and mobile phone platforms.\u000aWhatever happens, browsers will continue to be pushed to conform to\u000amodern specifications and perform ever better, making mobile rich web\u000aapplications more and more feasible. </p>\u000a\u000a<p>In closing, many thanks to <a href="http://blog.lostdecadegames.com/">Lost Decade Games</a> team for writing such a\u000asweet game and not obfuscating the JavaScript! Oh, and a reminder that\u000aif you're working on a HTML game, be sure to submit it to \u000a<a href="http://mozillalabs.com/gaming/2010/09/30/game-on-2010-is-here/">Mozilla's Game On</a> contest, then add some touch controls and submit\u000ait to this <a href="http://www.html5contest.com/">mobile HTML game contest</a>. Let the games begin!</p>\u000a
p991
tp992
Rp993
sg11
V/mobile-html-games
p994
sg13
Nsg14
I01
sg15
VAn onslaught of mobile HTML games
p995
sg18
V\u000a\u000aHTML5 games are really picking up.
p996
sS'snip'
p997
g5
(g6
g7
V<p>Porting Onslaught! to a mobile web-based game controller. Also featuring frustrations involving multi-touch on Android.</p>\u000a
p998
tp999
Rp1000
sg23
g147
sg31
g994
sg148
(dp1001
g150
S'Oct'
p1002
sg152
S'October 16, 2010'
p1003
sg154
I10
sg155
S'2010-10-16T09:00:00-00:00'
p1004
sg157
I1287244800
sg158
I2010
sg159
I16
ssg61
g160
sg29
S'mobile-html-games'
p1005
sS'categories'
p1006
(lp1007
S'web'
p1008
asS'posted'
p1009
g166
(S'\x07\xda\n\x10'
p1010
tp1011
Rp1012
ssg32
S'content/posts/2010/mobile-html-games/index.md'
p1013
sg34
F1433825805.0
sa(dp1014
g2
(dp1015
g26
g5
(g6
g7
V<p><a href="http://www.apple.com/ipod/nike/">Nike+</a> is a clever little system designed by Apple and Nike to infer\u000athe runner's speed and augment the running experience. The runner places\u000aa small chip in his shoe which transmits data to the iPod using a\u000aproprietary RF-based protocol. The chip contains a piezoelectric cell\u000awhich measures how long the foot exerted pressure on the ground.\u000aAccording to <a href="http://support.apple.com/kb/HT2293?viewlocale=en_US">Apple's FAQ</a>, this contact time is directly related to\u000ayour pace. In this post I provide a snippet of python code for\u000acollecting data through Sparkfun's adapter. </p>\u000a\u000a<p>Sparkfun <a href="http://www.sparkfun.com/commerce/tutorial_info.php?tutorials_id=41">dissected</a> the transmitter and receiver and currently sell a\u000a<a href="http://www.sparkfun.com/commerce/product_info.php?products_id=8245">Nike+ Serial to USB adapter</a>. There's been a number of <a href="http://www.sparkfun.com/commerce/tutorial_info.php?tutorials_id=135">notable</a>\u000a<a href="http://dub.washington.edu/pubs/46%0A">projects</a> since, and quite a lot of interest in the system for\u000ageneral hackery.</p>\u000a\u000a<p>I wanted to\u000ause Nike+ for my own project (a running bib that would automatically\u000adisplay the runner's speed on the back). Unfortunately, the only\u000aavailable implementations were in <a href="http://www.sparkfun.com/datasheets/DevTools/iPod/Nike_iPod_Serial.zip">Visual Basic</a> and <a href="http://rtadlock.blogspot.com/2009/06/some-perl-code-for-nikeipod-serial.html">perl</a>, neither\u000aof which work on Mac. Here's a small script for Python on Mac OS X to\u000acollect Nike+ data using SparkFun's adapter. </p>\u000a\u000a<pre><code>#!/usr/bin/env python\u000aimport serial\u000afrom hexbyte import *\u000a\u000adef readbytes(number):\u000a    buf = ''\u000a    for i in range(number):\u000a        byte = ser.read()\u000a        buf += byte\u000a\u000a    return buf\u000a\u000a# open the appropriate serial port\u000aser = serial.Serial('/dev/tty.usbserial-A6007uDh', 57600, bytesize=serial.EIGHTBITS)\u000a\u000a# send the following init string to the Nike+ device:\u000ainit1 = 'FF 55 04 09 07 00 25 C7'\u000aser.write(HexToByte(init1))\u000a\u000a# listen for the response string: FF 55 04 09 00 00 07 EC\u000aresponse1 = ByteToHex(readbytes(8))\u000aassert response1 == 'FF 55 04 09 00 00 07 EC'\u000a\u000a# send the second init string\u000ainit2 = 'FF 55 02 09 05 F0'\u000aser.write(HexToByte(init2))\u000a\u000a# listen for the response string: FF 55 04 09 06 00 25 C8\u000aresponse2 = ByteToHex(readbytes(8))\u000aassert response2 == 'FF 55 04 09 06 00 25 C8'\u000a\u000a# now we're ready to listen for actual data\u000aprint "nike+ initialized. listening for data"\u000a\u000awhile True:\u000a    byte = ser.read()\u000a    # if a byte is coming down the port,\u000a    if byte:\u000a        # get the rest of the message (34 chars)\u000a        message = byte + readbytes(33)\u000a        # and decipher it\u000a        data = {\u000a            'number': ByteToHex(message[11]),\u000a            'uid': ByteToHex(message[7:11]),\u000a            'data': ByteToHex(message[12:]),\u000a        }\u000a        print data\u000a</code></pre>\u000a\u000a<p>You'll also need <a href="https://github.com/borismus/Running-Gestures/blob/master/hexbyte.py">hexbyte.py</a>, which contains convenience conversion\u000amethods between binary and hex.</p>\u000a\u000a<p>I'm still unable to fully make sense of this data.  Firstly, each step\u000aseems to inexplicably generate 8 packets instead of one. Second, there\u000aare 22 bytes in the Nike+ data <a href="http://ipodlinux.org/wiki/Apple_Accessory_Protocol#Nike.2B_.28Mode_9.29">with an unknown structure</a>, probably\u000acontaining pressure duration data. If someone figures out how to make\u000asense of this please let me know!</p>\u000a\u000a<p><strong>Update (June 2013)</strong>: Dmitry Grinberg has published a <a href="http://dmitry.gr/index.php?r=05.Projects&amp;proj=05.%20Nike%20plus%20iPod">much more\u000athorough reverse engineering</a> blog post.</p>\u000a
p1016
tp1017
Rp1018
sg11
V/nike-hacking-with-python
p1019
sg13
Nsg14
I01
sg15
VNike+ hacking with python
p1020
sg18
V\u000a\u000a[Nike+][] is a clever little system designed by Apple and Nike to infer\u000athe runner's speed and augment the running experience.
p1021
sS'snip'
p1022
g5
(g6
g7
V<p>A deep dive into the Nike+ shoe sensor and USB dongle system, and how to get data from it using Python.</p>\u000a
p1023
tp1024
Rp1025
sg23
g147
sg31
g1019
sg148
(dp1026
g150
S'May'
p1027
sg152
S'May 18, 2010'
p1028
sg154
I5
sg155
S'2010-05-18T09:00:00-00:00'
p1029
sg157
I1274198400
sg158
I2010
sg159
I18
ssg61
g160
sg29
S'nike-hacking-with-python'
p1030
sS'categories'
p1031
(lp1032
S'physical'
p1033
asS'posted'
p1034
g166
(S'\x07\xda\x05\x12'
p1035
tp1036
Rp1037
ssg32
S'content/posts/2010/nike-hacking-with-python/index.md'
p1038
sg34
F1433825810.0
sa(dp1039
g2
(dp1040
g26
g5
(g6
g7
V<p>A little while ago I took an existing game called <a href="http://lostdecadegamesapp.appspot.com/">Onslaught!</a> by\u000a<a href="http://blog.lostdecadegames.com/">Lost Decade Games</a> and hacked it to use onscreen controls instead of\u000athe keyboard so that it could be enjoyed on mobile phones. This was\u000aintended as a proof of concept and not a polished product as I\u000aencountered many technical show-stoppers, described in the \u000a<a href="/mobile-html-games/">previous blog post</a>. I've been itching to experiment with <a href="http://www.phonegap.com/">PhoneGap</a> apps\u000ain the Android Market, so despite obvious issues, I decided to pack up my\u000aOnslaught! fork and upload it to the market. </p>\u000a\u000a<p>I expected there to be many gameplay issues with varying screen sizes\u000aand form factors. I strongly suspected my use of the target-densityDpi\u000awould break on non-Nexus One handsets. Indeed, my suspicions were\u000aconfirmed when, after a nominal $25 fee and a short setup, I uploaded\u000athe PhoneGap-created .apk to the Market.</p>\u000a\u000a<h2>Results</h2>\u000a\u000a<p>I've never published any application to any application store before. I\u000awas surprised to see Onslaught! available in the market within seconds\u000aof my submission, with a surge of new users and comments piling in\u000anearly immediately. Apparently the experience is particularly horrible\u000aon the Galaxy S and it doesn't work at all on HTC Hero. The day I\u000aunleashed Onslaught! unto the Market, I was contacted by someone from\u000a<a href="http://zeemote.com/">Zeemote</a>, who politely suggested that the game "could possibly\u000abenefit from the use of a joystick for improved control", and promptly\u000a<strong>sent me a Zeemote</strong> to experiment with. I haven't found the time yet,\u000abut thanks Zeemote, that was pretty sweet of you \u2013 nobody's ever sent me\u000afree stuff before! </p>\u000a\u000a<p><img src="stats.png" alt="image" /> </p>\u000a\u000a<p>Obviously this is a pretty embarrassing state of affairs, and I, like\u000aany slightly self respecting developer decided to remove the app from\u000athe market to hide the evidence! I also received a very friendly\u000a<strong>cease-and-desist</strong> style letter from the good folks at LDG (this is\u000aone of those times when the "ask for forgiveness, not permission"\u000aapproach has its drawbacks). Needless to say, I gladly pulled the game\u000aand apologized profusely to the developers.</p>\u000a\u000a<h2>PhoneGap</h2>\u000a\u000a<p>Creating an Android PhoneGap application out of a web application is\u000apretty simple. It's a matter of following a few steps outline on \u000a<a href="http://wiki.phonegap.com/w/page/16494774/Getting-started-with-Android-PhoneGap-in-Eclipse">this documentation page</a>. The process varies slightly for each platform and\u000awould surely become painful and tedious for developers supporting\u000amultiple platforms. Luckily the PhoneGap folks will soon have just the\u000athing to address this issue. <a href="http://build.phonegap.com/start">PhoneGap Build</a> is a service that\u000apromises to package and compile web apps for a variety of platforms.\u000aLooking forward to trying it out!</p>\u000a
p1041
tp1042
Rp1043
sg11
V/phonegap-games-android-market
p1044
sg13
Nsg14
I01
sg15
VPhoneGap games in the Android Market
p1045
sg18
V\u000a\u000aA little while ago I took an existing game called [Onslaught!][] by\u000a[Lost Decade Games][] and hacked it to use onscreen controls instead of\u000athe keyboard so that it could be enjoyed on mobile phones.
p1046
sS'snip'
p1047
g5
(g6
g7
V<p>On how I took Onslaught, wrapped it in PhoneGap published, it to the Android Market and got angry emails.</p>\u000a
p1048
tp1049
Rp1050
sg23
g147
sg31
g1044
sg148
(dp1051
g150
S'Nov'
p1052
sg152
S'November 11, 2010'
p1053
sg154
I11
sg155
S'2010-11-11T09:00:00-00:00'
p1054
sg157
I1289494800
sg158
I2010
sg159
I11
ssg61
g160
sg29
S'phonegap-games-android-market'
p1055
sS'categories'
p1056
(lp1057
S'web'
p1058
asS'posted'
p1059
g166
(S'\x07\xda\x0b\x0b'
p1060
tp1061
Rp1062
ssg32
S'content/posts/2010/phonegap-games-android-market/index.md'
p1063
sg34
F1433825815.0
sa(dp1064
g2
(dp1065
g26
g5
(g6
g7
V<p>I've been working on a couple of researchy projects involving gait\u000arecognition and running foot strike analysis. For my proof of concept, I\u000aturned to the wiimote, everyone's favorite physical interaction\u000aprototyping input device. Wiimotes are portable and rugged, and thus\u000awell suited to high-intensity activities like running. They attach\u000aeasily to legs with a physio band, although the elastic tension tends to\u000acut off circulation. No big deal, though... it's For Science! </p>\u000a\u000a<p>This article is not about attaching Wiis to legs (more on that at a\u000alater date!), but about communicating with the Wii remote using python.\u000aI started out by writing a Cocoa application to harvest accelerometer\u000adata using the WiiRemote framework provided by the <a href="http://darwiin-remote.sourceforge.net/">DarwiinRemote</a>\u000aproject.  After some objective-c iterations of my initial gait\u000arecognizer algorithm, I decided to port to python, an environment better\u000asuited for light prototyping. There's a few packages explicitly\u000adeveloped to integrate with the wiimote. <a href="http://stackoverflow.com/questions/481943/python-with-wiimote-using-pywiiuse-module">Pywiiuse</a> provides a\u000alightweight wrapper around the <a href="http://www.wiiuse.net/">wiiuse</a> library and does not work on\u000aOS X. An alternative, <a href="http://code.google.com/p/pywiimote/">pywiimote</a> claims to be multiplatform but\u000apointedly isn't. Here's the start of their code: </p>\u000a\u000a<pre><code>from ctypes import *\u000akernel = windll.kernel32\u000a</code></pre>\u000a\u000a<p>Having found no existing wiimote-specific python libraries that would\u000awork on my platform, I had no choice but to dig a little into the\u000abluetooth-based protocol that the wiimote uses. I found all the details\u000ain all their gory glory on the <a href="http://wiibrew.org/wiki/Wiimote#Accelerometer">wiibrew wiki</a>. The communication\u000aprotocol involves two open L2CAP sockets between the host and wiimote:\u000aone for reading and one for writing. After an initialization string is\u000asent over the write socket, the wiimote springs into life and sends a\u000astream of data on the read socket. In this data are accelerometer values\u000aand button presses. Here's a simple python snippet using the\u000a<a href="http://lightblue.sourceforge.net/">lightblue</a> library:</p>\u000a\u000a<pre><code>import sys, lightblue, hexbyte\u000a\u000aWIIMOTE_DEVICE_NAME = 'Nintendo RVL-CNT-01'\u000a\u000a# auto-discover nearby bluetooth devices\u000adevs = lightblue.finddevices(getnames=True, length=5)\u000a# find the one with the correct name\u000awiimote = [d for d in devs if d[1] == WIIMOTE_DEVICE_NAME] and d[0] or None\u000aif not wiimote:\u000a    print "No wiimotes found!"\u000a    sys.exit(1)\u000a\u000a# create a socket for writing control data\u000awrite_socket = lightblue.socket(lightblue.L2CAP)\u000awrite_socket.connect((wiimote, 0x11))\u000a\u000a# create a socket for reading accelerometer data\u000aread_socket = lightblue.socket(lightblue.L2CAP)\u000aread_socket.connect((wiimote, 0x13))\u000a\u000a# initialize the socket to the right mode\u000awrite_socket.send(hexbyte.HexToByte('52 12 00 33'))\u000a\u000a# start reading data from it\u000awhile 1:\u000a    byte = read_socket.recv(256 * 7)\u000a    data = hexbyte.ByteToHex(byte)\u000a    # do something interesting with the data\u000a    print data\u000a</code></pre>\u000a\u000a<p>You'll need <a href="https://github.com/borismus/Running-Gestures/blob/master/hexbyte.py">hexbyte.py</a> to run the above snippet. I hope\u000ayou (the wii remote wielding python fan) find this snippet useful. As a\u000aside note, if you've figured how to pair a wiimote with an android phone\u000aand released the code into the public domain, please let me know. Since\u000aAndroid 2.2 still doesn't ship with L2CAP APIs, I hit the wall.</p>\u000a
p1066
tp1067
Rp1068
sg11
V/prototyping-wii-remote-python
p1069
sg13
Nsg14
I01
sg15
VPrototyping with Wii remotes in python
p1070
sg18
V\u000a\u000aI've been working on a couple of researchy projects involving gait\u000arecognition and running foot strike analysis.
p1071
sS'snip'
p1072
g5
(g6
g7
V<p>A how-to about getting access to a wii remote using Python.</p>\u000a
p1073
tp1074
Rp1075
sg23
g147
sg31
g1069
sg148
(dp1076
g150
S'May'
p1077
sg152
S'May 28, 2010'
p1078
sg154
I5
sg155
S'2010-05-28T09:00:00-00:00'
p1079
sg157
I1275062400
sg158
I2010
sg159
I28
ssg61
g160
sg29
S'prototyping-wii-remote-python'
p1080
sS'categories'
p1081
(lp1082
S'physical'
p1083
asS'posted'
p1084
g166
(S'\x07\xda\x05\x1c'
p1085
tp1086
Rp1087
ssg32
S'content/posts/2010/prototyping-wii-remote-python/index.md'
p1088
sg34
F1433825821.0
sa(dp1089
g2
(dp1090
g26
g5
(g6
g7
V<p>Like many other runners, I like to listen to my music while training.\u000aEven with a playlist of running music, I often want to change the\u000acurrently playing track. There are currently two popular options of\u000adoing this: using the device itself, or using a headphone remote. My\u000aubiquitous computing project from last Spring explores a third option:\u000aimagine if your shoes had built-in accelerometers that allowed you to\u000askip mid-stride to change tracks. </p>\u000a\u000a<p>The most obvious way to change tracks is directly through the music\u000aplayer, but operating a touch screen while running is pretty annoying. A\u000amuch better way of changing tracks is by using a button attached to a\u000apair of headphones, but finding it, and then double clicking it is often\u000aa frustrating experience. Some clicks don't get registered, so one often\u000aends up triple-clicking, which skips <em>back</em> a track. My project explores\u000aa novel way of changing tracks: mid-stride skip. This gesture is\u000adetected by having an accelerometer in each shoe which tracks your\u000arunning patterns and detects when you perform the skip gesture. In\u000aaddition to being useful, this running gesture makes an otherwise\u000amonotonous activity more varied and enjoyable.</p>\u000a\u000a<p>I prototyped the skip-to-skip system with a wii remote attached to the\u000arunner's lower leg, as pictured above. A computer is paired to the\u000awiimote, collecting accelerometer data (especially in the axis\u000acorresponding to the runner's vertical movement). This communication is\u000aestablished using a wii library from a <a href="/prototyping-wii-remote-python/">previous post</a>. The naive\u000aalgorithm I use for detecting skips works as follows:</p>\u000a\u000a<ol>\u000a<li>Find peaks by looking at the 1st derivative (positive slope,\u000anegative slope pairs)</li>\u000a<li>Discard insignificant peak values (under a threshold)</li>\u000a<li>Compute distances between peaks</li>\u000a<li>Look at the last 5 distances, and compute the mode. That's the pace.</li>\u000a<li>Look for declinations from the pace characteristic of a skip.</li>\u000a</ol>\u000a\u000a<p>Once my gesture recognition code worked reasonably well, I ran a user\u000astudy to see how people liked this method compared to the touch screen\u000aand headphone remote. After a few hitches (including getting kicked out\u000aof the Madeira Tecnopolo), I managed to test the prototype with many of\u000amy Madeiran classmates (thanks guys!). Most people preferred the\u000askip-to-skip method over both the direct smartphone and headphone remote\u000amethods, which is promising.</p>\u000a\u000a<p>For more details on this project, check out the <a href="running-gestures-paper.pdf">paper</a> that\u000a<a href="http://dme.uma.pt/people/faculty/vassilis.kostakos/">Vassilis Kostakos</a> and I will present at <a href="http://www.ubicomp2010.org/">Ubicomp 2010</a> in\u000aCopenhagen. If you're interested in the source code, it's located on\u000a<a href="http://github.com/borismus/Running-Gestures">github</a>.</p>\u000a\u000a<p><strong>Update:</strong> here's the <a href="running-gestures-poster.pdf">poster</a>. The font-size is scary huge\u000aon A0!</p>\u000a
p1091
tp1092
Rp1093
sg11
V/skip-running-gesture
p1094
sg13
Nsg14
I01
sg15
VSkip to skip: a running gesture
p1095
sg18
V\u000a\u000aLike many other runners, I like to listen to my music while training.
p1096
sS'snip'
p1097
g5
(g6
g7
V<p>What if you had accelerometers in your shoes? Could mid-stride gestures replace the headphone remote for changing tracks?</p>\u000a
p1098
tp1099
Rp1100
sg23
g147
sg31
g1094
sg148
(dp1101
g150
S'Aug'
p1102
sg152
S'August 6, 2010'
p1103
sg154
I8
sg155
S'2010-08-06T09:00:00-00:00'
p1104
sg157
I1281110400
sg158
I2010
sg159
I6
ssg61
g160
sg29
S'skip-running-gesture'
p1105
sS'categories'
p1106
(lp1107
S'physical'
p1108
aS'music'
p1109
asS'posted'
p1110
g166
(S'\x07\xda\x08\x06'
p1111
tp1112
Rp1113
ssg32
S'content/posts/2010/skip-running-gesture/index.md'
p1114
sg34
F1433825830.0
sa(dp1115
g2
(dp1116
g26
g5
(g6
g7
V<p>Most people carry mobile phones, and many of those phones have\u000aintegrated bluetooth functionality. Some of these phones are in\u000adiscoverable mode, making them detectable by other bluetooth devices\u000a(Note: many new phones disable discoverable mode on a timer). By\u000abuilding and deploying such scanners in a bluetooth sensor network, we\u000acan collect a lot of interesting information about people's behavior in\u000apublic places. This post is about a minimal compelling application of\u000athis technology.</p>\u000a\u000a<p>Unfortunately we had just 3 makeshift internet-enabled bluetooth sensor\u000anodes at our disposal. Under this constraint, we brainstormed several\u000ainteresting scenarios where a bluetooth sensor network can be used to\u000acollect data, for example:</p>\u000a\u000a<ul>\u000a<li>People's movement patterns in a bar district</li>\u000a<li>Tourists following informational signs to points of interest</li>\u000a<li>Passengers riding a public bus system</li>\u000a<li>Travel between floors in a building</li>\u000a</ul>\u000a\u000a<p>We decided to study patterns of travel (ie. climbing stairs vs. taking\u000athe elevator) between two floors of a university building. We installed\u000athree sensors: two on separate floors, and the third on the stairwell\u000aconnecting them (blue circles represent bluetooth scanner nodes in the\u000abluetooth sensor network).</p>\u000a\u000a<p><img src="problem.png" alt="image" /> </p>\u000a\u000a<p>This is a specific example of a more general point-to-point problem with\u000atwo possible paths. From this simple setup, It's possible to compute\u000awhenever a trip was made from floor A to floor B, and whether or not the\u000atraveler took the stairs or the elevator. Suppose that there was a trip\u000afrom A to B. If the trip was also made through C, it must have been made\u000avia stairs, otherwise, it must have been made via elevator. Based on\u000athis basic data, we can infer a number of features, some of which are:</p>\u000a\u000a<ul>\u000a<li>Popularity of each route (ex. stairs more popular in the morning)</li>\u000a<li>Habit (ex. people that take the elevator always take the elevator)</li>\u000a<li>Temporal patterns (ex. some people consistently take the elevator in\u000athe morning)</li>\u000a<li>Duration of trips (ex. stairs down is faster, elevator up is faster)</li>\u000a<li>Waiting time at each terminal (ex. waiting times higher for\u000aelevator)</li>\u000a<li>Direction preferences (ex. many people prefer to take elevator up,\u000astairs down)</li>\u000a</ul>\u000a\u000a<p>Unfortunately this project finished before any data was actually\u000acollected! Luckily, the implementation is ready for use if you want to\u000atake over! The following describes a simple implementation using EeePCs\u000aor MacMinis:</p>\u000a\u000a<h2>Implementing the Sensor Network</h2>\u000a\u000a<p>The sensor network necessary for the inter-floor experiment consists of\u000aseveral components:</p>\u000a\u000a<p><img src="architecture.png" alt="image" /> </p>\u000a\u000a<p>Each scanner is a computer with a bluetooth modem and a wireless\u000aconnection. We used two Mac Minis running OS X and one ASUS EeePC\u000arunning Linux. Every computer executed a <a href="bluetooth_scanner.py">bluetooth scanner program</a>,\u000awhich scanned for bluetooth devices in the vicinity every 10 seconds,\u000aand updated its list of nearby device IDs. Whenever the list of nearby\u000adevices changed, it reported an ENTER or EXIT notification to the\u000acentral server via an HTTP POST. </p>\u000a\u000a<p>The central server runs a MySQL database to store all notifications from\u000athe scanners. Periodically, this <a href="trip_analyzer.py">analyzer program</a> crawls the\u000adatabase to extract trips from the data (ie. a device EXITs at one\u000asensor and subsequently ENTERs at another one). It also discerns between\u000atrips taken by stairs and by elevator. Once trips are extracted, they\u000aare appended to a Google Spreadsheet. This happens in "delayed\u000arealtime", since a trip can only be inferred once the start and endpoint\u000aare known.</p>\u000a\u000a<p>The final component is a custom visualization written using <a href="http://raphaeljs.com/">Raphael</a>\u000aand the <a href="http://code.google.com/apis/charttools/index.html">Google Visualization</a> framework. This JavaScript program\u000apolls the spreadsheet every 10 seconds to see if any new trips were\u000aadded. If new trips were found, the visualization would update itself,\u000arepresenting each trip as a circle moving from one terminal to another\u000aalong the stairs or elevator path. In addition, this visualization\u000astores a frequency chart over time to show how popular each of the\u000apossible paths are.</p>\u000a\u000a<p>For more information on related projects, check out <a href="http://dme.uma.pt/people/faculty/vassilis.kostakos/">Vassilis' work</a>\u000aat UMa.</p>\u000a
p1117
tp1118
Rp1119
sg11
V/stairs-elevator-bluetooth
p1120
sg13
Nsg14
I01
sg15
VStairs or elevator? Use bluetooth!
p1121
sg18
V\u000a\u000aMost people carry mobile phones, and many of those phones have\u000aintegrated bluetooth functionality.
p1122
sS'snip'
p1123
g5
(g6
g7
V<p>An exploration into using Bluetooth Sensor Networks for tracking patterns in behavior.</p>\u000a
p1124
tp1125
Rp1126
sg23
g147
sg31
g1120
sg148
(dp1127
g150
S'Sep'
p1128
sg152
S'September 30, 2010'
p1129
sg154
I9
sg155
S'2010-09-30T09:00:00-00:00'
p1130
sg157
I1285862400
sg158
I2010
sg159
I30
ssg61
g160
sg29
S'stairs-elevator-bluetooth'
p1131
sS'categories'
p1132
(lp1133
S'physical'
p1134
asS'posted'
p1135
g166
(S'\x07\xda\t\x1e'
p1136
tp1137
Rp1138
ssg32
S'content/posts/2010/stairs-elevator-bluetooth/index.md'
p1139
sg34
F1433825839.0
sa(dp1140
g2
(dp1141
g26
g5
(g6
g7
V<p>Every new post on this blog gets an unsolicited <a href="http://en.wikipedia.org/wiki/Pingback">pingback</a> from\u000a<a href="http://topsy.com">topsy</a>, a service that tracks which users mentioned the post on\u000atwitter (known informally as <a href="http://tomsucks.wordpress.com/2008/05/27/free-idea-tweetback/">tweetbacks</a>). On one hand, topsy is a\u000aparasite, using sites like mine to rise in search engine rankings, but\u000aon the other, it satisfies this blogger's curiosity to learn who reads\u000aand enjoys my posts enough to tweet about them. In this experiment, I've\u000aexposed tweetbacks directly on this blog using topsy's convenient \u000a<a href="http://otter.topsy.com">JSONP API</a>.</p>\u000a\u000a<p>The implementation is entirely in JavaScript, consisting of two\u000ascripts. The first is on the main post listing, showing the number of\u000atweets for each post using <code>http://otter.topsy.com/stats.js</code>. The second\u000ais on the post page itself, showing the tweets posted in response to the\u000apost using <code>http://otter.topsy.com/trackbacks.js</code>. Both scripts execute\u000aafter the page loads, so the only impact on page load time is the extra\u000akilobyte of JavaScript code. </p>\u000a\u000a<p>There are good reasons to use topsy over <a href="http://search.twitter.com">twitter search</a>, even though\u000aat first glance, they both provide similar information:</p>\u000a\u000a<ul>\u000a<li>Twitter search doesn't index old tweets</li>\u000a<li>Topsy distinguishes influential tweets (more on this later)</li>\u000a<li>JSONP API support</li>\u000a</ul>\u000a\u000a<p>It's important not to overwhelm readers by showing too many tweets.\u000aPopular blog posts can have hundreds or even thousands of tweets, and\u000ashowing all of them at once is a bad idea. Topsy associates an influence\u000avalue with each twitter user, giving them a certain weight, making it\u000aeasier to decide which tweets to show, and which to hide. My\u000aimplementation shows at most N tweets; if there are over N total tweets,\u000ait shows at most N 'influential' tweets, as per topsy's definition.</p>\u000a\u000a<pre><code>var MAX_TWEETS = 10;\u000avar BASE = 'http://otter.topsy.com/trackbacks.js?callback=?&amp;perpage=' + \u000a  MAX_TWEETS;\u000avar ALL = BASE + '&amp;url=';\u000avar INFL = BASE + '&amp;infonly=1&amp;url=';\u000a\u000afunction getTweets(url) {\u000a  $.getJSON(ALL + url, function(data) {\u000a    var response = data.response;\u000a    if (response.total &gt; MAX_TWEETS) {\u000a      $.getJSON(INFL + url, function(infl) {\u000a        processTweetList(infl.response.list);\u000a        var count = (infl.response.total &gt; MAX_TWEETS ? MAX_TWEETS : infl.response.total);\u000a        updateTweetCount(count, response.total);\u000a      });\u000a    } else if (response.total &gt; 0) {\u000a      processTweetList(response.list);\u000a      updateTweetCount(response.total);\u000a    }\u000a  });\u000a}\u000a</code></pre>\u000a\u000a<p>I also urlify the\u000atweet, converting all of the URLs and @mentions into <a\u005c&gt; elements using\u000athis function, I found the first regular expression somewhere on the\u000ainternet \u2013 I'd never write such a beast. </p>\u000a\u000a<pre><code>var URL_RE = /(\u005cb(https?|ftp|file):\u005c/\u005c/[-A-Z0-9+&amp;@#\u005c/%?=~_|!:,.;]*[-A-Z0-9+&amp;@#\u005c/%=~_|])/ig;\u000avar TWEET_RE = /@([A-Za-z0-9_]+)/g;\u000a\u000afunction urlify(text) {\u000a  return text.replace(URL_RE,"&lt;a href='$1'&gt;$1&lt;/a&gt;").\u000a              replace(TWEET_RE, "&lt;a href='http://twitter.com/$1'&gt;@$1&lt;/a&gt;");\u000a</code></pre>\u000a\u000a<p>The approach I took has some major advantages to over the <a href="http://yoast.com/wordpress/tweetbacks/">tweetback plugin</a> \u000afor wordpress:</p>\u000a\u000a<ul>\u000a<li>It works... the wordpress plugin doesn't</li>\u000a<li>No wordpress comment pollution since no wordpress comments are\u000acreated</li>\u000a<li>No server-side load since all comments are fetched in JS</li>\u000a<li>No wordpress required. Other blog engines or static sites work just\u000aas well</li>\u000a</ul>\u000a\u000a<p>A drawback of this approach is that it takes some time to run the JS,\u000awhich changes the DOM after the initial page load, resulting in a\u000ajarring experience. By using a fade effect to make twitter information\u000aappear gradually, I try to mitigate this problem. </p>\u000a\u000a<p>I conclude with a shameless plug: tweetbacks.js is running on this blog,\u000aso try it out by posting a tweet referring to this post's URL, and it\u000ashould appear in the list below, or if there are more than 10 tweets, at\u000aleast increment the count. If you find this concept interesting and\u000awould like to run it on your site, let me know and I'll pack\u000atweetbacks.js up into a jQuery plugin or something. Thanks for reading! </p>\u000a\u000a<p><strong>Update:</strong> I no longer use this code, in favor of the official twitter\u000abutton.</p>\u000a
p1142
tp1143
Rp1144
sg11
V/tweetbacks-in-javascript
p1145
sg13
Nsg14
I01
sg15
VTweetbacks in JavaScript
p1146
sg18
V\u000a\u000aEvery new post on this blog gets an unsolicited [pingback][] from\u000a[topsy][], a service that tracks which users mentioned the post on\u000atwitter (known informally as [tweetbacks][]).
p1147
sS'snip'
p1148
g5
(g6
g7
V<p>A wordpress widget that uses topsy to show all of the twitterverse mentions of your blog posts.</p>\u000a
p1149
tp1150
Rp1151
sg23
g147
sg31
g1145
sg148
(dp1152
g150
S'Dec'
p1153
sg152
S'December 16, 2010'
p1154
sg154
I12
sg155
S'2010-12-16T09:00:00-00:00'
p1155
sg157
I1292518800
sg158
I2010
sg159
I16
ssg61
g160
sg29
S'tweetbacks-in-javascript'
p1156
sS'categories'
p1157
(lp1158
S'web'
p1159
asS'posted'
p1160
g166
(S'\x07\xda\x0c\x10'
p1161
tp1162
Rp1163
ssg32
S'content/posts/2010/tweetbacks-in-javascript/index.md'
p1164
sg34
F1332684374.0
sa(dp1165
g2
(dp1166
g26
g5
(g6
g7
V<p>I signed up to do one month of paid research at CMU|Portugal before\u000aspring classes start. My task boils down to creating interesting\u000avisualizations. The bad news is that I have no experience visualizing\u000adata and the dataset I'm to visualize hasn't yet been collected.\u000aFortunately, I've always been <em>theoretically</em> interested in data\u000avisualization, so I was happy to have a solid excuse to explore the\u000asubject. All I needed was a sufficiently rich data set, mad skills and a\u000abit of inspiration.</p>\u000a\u000a<p>Lately, I've been pretty excited about squeezing some new potential out\u000aof Mechanical Turk. Part of my research involves finding patterns in\u000aMechanical Turk requester strategies. A few weeks ago, I began gathering\u000adata with a <a href="turk-visualizer/turkviz_scraper.py">python program</a> that extracts all scrape-able information\u000aabout every HIT group and stores it in a sqlite3 database. This is quite\u000aan interesting data set, so I pounced on the opportunity to visualize\u000ait, killing two birds with one stone.</p>\u000a\u000a<p>Due to lack of time, I decided to skip most of the \u000a<a href="http://www.amazon.com/Visual-Display-Quantitative-Information/dp/096139210X">visualization literature</a>. Instead, I found a great \u000a<a href="http://www.visualcomplexity.com/vc/">visualization project database</a> and started writing simple examples\u000ausing some popular\u000avisualization languages and frameworks. I began with <a href="http://processing.org/">Processing</a> and\u000a<a href="http://processingjs.org/">Processing.js</a> but quickly tired of the raster-based drawing model. I\u000aturned to SVG with <a href="http://raphaeljs.com/">Raphal</a> and jQuery and ended up building a simple\u000abubble chart. I had no specific visualization in mind, so my vague goal\u000awas to be able to visualize data in <a href="http://www.gapminder.org/">Hans Rosling</a>'s favorite\u000afive-dimensional (x, y, size, color, time) graph. Making this graph\u000aanimate, however, proved to be quite difficult.</p>\u000a\u000a<p><img src="raphael.jpg" alt="image" /> </p>\u000a\u000a<p>I turned to\u000athe internet for help, and Itai Raz <a href="http://www.youtube.com/watch?v=guhdYoPY3kM">explained to me</a> in his thick\u000aIsraeli accent that the Google Visualization API is pretty sweet.\u000aMoreover, it comes with the exact visualization I wanted to implement,\u000aapparently called the <a href="http://code.google.com/apis/visualization/documentation/gallery/motionchart.html">Motion Chart</a>. Happily, Google Spreadsheets\u000aexport data in the format that the visualization framework consumes.\u000aThus my problem was greatly reduced to one of analyzing and uploading\u000adata to a Google Spreadsheet. Turns out that this too is quite easy\u000ausing the python <a href="http://code.google.com/p/gdata-python-client/">GData framework</a>.</p>\u000a\u000a<p>I decided to visualize differences between requester strategies on\u000aMechanical Turk. Every time I scrape, I generate a Average Reward (x),\u000aAverage Allotted Time (y), Total Number of Hits (size) graph for the top\u000a50 requesters, and then upload it to a google spreadsheet. Here is the\u000a<a href="turk-visualizer/turkviz_scraper.py">python program</a> I wrote for this purpose. <a href="http://www.borismus.com/wp-content/uploads/2010/02/turk_requester_visualizer.html">The results</a> are\u000ainteresting and fun to play with. Google's Motion Chart visualization is\u000aincredibly powerful and flexible. I won't go in detail into findings\u000afrom the data since it's irrelevant to this largely technical discussion\u000aabout visualization technologies. I'll soon get my hands on the data I'm\u000aexpecting and create a custom visualization for it with the Google\u000aVisualization API. Stay chooned!</p>\u000a\u000a<p>[]: turk-visualizer/</p>\u000a
p1167
tp1168
Rp1169
sg11
V/visualizing-mturk-requester
p1170
sg13
Nsg14
I01
sg15
VVisualizing MTurk requesters
p1171
sg18
V\u000a\u000a\u000aI signed up to do one month of paid research at CMU|Portugal before\u000aspring classes start.
p1172
sS'snip'
p1173
g5
(g6
g7
V<p>A quick stab at scraping Mechanical Turk for statistics into a Google Spreadsheet, and then using Google's Visualization APIs to present that data in a meaningful way.</p>\u000a
p1174
tp1175
Rp1176
sg23
g147
sg31
g1170
sg148
(dp1177
g150
S'Feb'
p1178
sg152
S'February 6, 2010'
p1179
sg154
I2
sg155
S'2010-02-06T09:00:00-00:00'
p1180
sg157
I1265475600
sg158
I2010
sg159
I6
ssg61
g160
sg29
S'visualizing-mturk-requester'
p1181
sS'categories'
p1182
(lp1183
S'web'
p1184
aS'social'
p1185
asS'posted'
p1186
g166
(S'\x07\xda\x02\x06'
p1187
tp1188
Rp1189
ssg32
S'content/posts/2010/visualizing-mturk-requester/index.md'
p1190
sg34
F1433825848.0
sa(dp1191
g2
(dp1192
g26
g5
(g6
g7
V<p>Last month I took a work trip to So Paulo, Brazil. I gave four Chrome/HTML5\u000apresentations and talked to many engineers and designers over the course of the\u000aweek, trying hard not to sound like a broken record. Luckily Brazil has a lot\u000aof people so the audiences were different each time! Here's a link to my\u000a<a href="http://smustalks.appspot.com/brazil-11/">slides</a>, that I tweaked slightly depending on the audience.</p>\u000a\u000a<p>I'll be dumping all of my slide decks and/or talk videos to <a href="http://smustalks.appspot.com/">smustalks</a> on\u000aAppEngine. I've made the switch from building slides in Keynote to using this\u000afantastic HTML5 <a href="http://code.google.com/p/html5slides/">slide deck</a> template and my favorite <a href="http://www.vim.org/">text editor</a>.</p>\u000a\u000a<h2>Food</h2>\u000a\u000a<ul>\u000a<li>Delicious fruits of all varieties: exotic cashew, persimmon, star fruit, guyava, and way tastier bananas.</li>\u000a<li>Ridiculously massive portions of sashimi at japanese restaurants.</li>\u000a<li>The Rodizio we went to was super exquisite. Largest salad bar ever, including caviar (srsly).</li>\u000a</ul>\u000a\u000a<h2>Life</h2>\u000a\u000a<ul>\u000a<li>SP is a concrete jungle like nothing I've ever seen. The whole city is built up within a 60km radius.</li>\u000a<li>Chaotic traffic patterns and insane motorcyclists abound. Everyone loves to drive between lanes and honk gratuitously.</li>\u000a<li>Super bumpy roads date back to the dictatorship days. If you take the wrong exit, prepare to jump around.</li>\u000a<li>Helicopters fly all over town and sometimes land precariously close to unsuspecting window cleaners dangling from highrise roofs.</li>\u000a<li>Beware foul smelling rivers and slums on the way to Friday morning meetings!</li>\u000a</ul>\u000a\u000a<h2>Work</h2>\u000a\u000a<ul>\u000a<li>Super friendly people but my Portuguese skills failed to topple the language barrier.</li>\u000a<li>Long process to get into any office building. Some ask for ID, others ask for passport. Takes half an hour just to get in...</li>\u000a<li>Somehow, Google Brazil still feels like Google!</li>\u000a</ul>\u000a\u000a<p>After my work was done, I took some <a href="https://picasaweb.google.com/boris.smus/Brazil">travel photos</a> and heard\u000a<a href="https://picasaweb.google.com/boris.smus/Brazil#5612942830070183426">Samba da Minha Terra</a> live, and by so doing, joined the ranks of Don and\u000aMagdalena!</p>\u000a\u000a<pre><code>I've never sailed the Amazon,\u000aI've never reached Brazil;\u000aBut the Don and Magdalena,\u000aThey can go there when they will!\u000a\u000a            - R. Kipling\u000a</code></pre>\u000a
p1193
tp1194
Rp1195
sg11
V/brazil-trip
p1196
sg13
Nsg14
I01
sg15
VBrazil trip
p1197
sg18
V\u000a\u000a\u000aLast month I took a work trip to So Paulo, Brazil.
p1198
sS'snip'
p1199
g5
(g6
g7
V<p>Wherein I spoke to Brazilian web developers, ate strange fruits and heard some fantastic samba rock.</p>\u000a
p1200
tp1201
Rp1202
sg23
g147
sg31
g1196
sg148
(dp1203
g150
S'Jun'
p1204
sg152
S'June 21, 2011'
p1205
sg154
I6
sg155
S'2011-06-21T09:00:00-00:00'
p1206
sg157
I1308672000
sg158
I2011
sg159
I21
ssg61
g160
sg29
S'brazil-trip'
p1207
sS'categories'
p1208
(lp1209
S'web'
p1210
aS'chrome'
p1211
aS'travel'
p1212
asS'posted'
p1213
g166
(S'\x07\xdb\x06\x15'
p1214
tp1215
Rp1216
ssg32
S'content/posts/2011/brazil-trip/index.md'
p1217
sg34
F1433825600.0
sa(dp1218
g2
(dp1219
g26
g5
(g6
g7
V<p>Few people want to synchronize their burgeoning music libraries across\u000acomputers (SSDs are small), or even lift it into the clouds with\u000asomething like Amazon's Music Cloud (uploading would take forever). As a\u000aresult many are saying goodbye to iTunes and moving to web-based\u000astreaming music services like <a href="http://listen.grooveshark.com/">grooveshark</a>, <a href="http://www.last.fm">last.fm</a>,\u000a<a href="http://thesixtyone.com">thesixtyone</a>, <a href="http://rdio.com">rd.io</a> etc. This move has a significant UX drawback:\u000amany keyboards come with multimedia keys to control your music player,\u000abut these are useless if you use a web-based music player. This post\u000aaddresses this inconvenience with <a href="https://chrome.google.com/extensions/detail/cpgegiegacijlefhjkfodppcefjhgdeo/">Media Keys</a>, a Chrome extension\u000athat lets you assign keyboard shortcuts in Chrome control web-based\u000amusic players (currently thesixtyone only).</p>\u000a\u000a<h2>How it works</h2>\u000a\u000a<p>The extension relies on two injected <a href="http://code.google.com/chrome/extensions/content_scripts.html">content scripts</a>: <a href="https://github.com/borismus/Chrome-Media-Keys/blob/master/key.js">key.js</a>, a\u000akeyboard event listener injected into all pages and <a href="https://github.com/borismus/Chrome-Media-Keys/blob/master/t61.js">player.js</a>, a\u000amusic player controller, injected into the player page. </p>\u000a\u000a<p>Key.js intercepts keyboard commands (ex. the next song key), and if they\u000amatch bound music player keyboard shortcuts (ex. next song key =>\u000achange to the next song), it sends a message to player.js, which then\u000adoes the right thing (ex. changes to the next song) by simulating mouse\u000aclicks.  Sounds good on paper, but the snag here is that you can't\u000aeasily do direct tab-to-tab communication in Chrome extensions, except\u000apossibly through a long-lived port connection. However using long lived\u000aconnections doesn't make conceptual sense, since the lifespan of a tab\u000ais relatively short. </p>\u000a\u000a<p>So we go the long way with the help of a <a href="http://code.google.com/chrome/extensions/background_pages.html">background page</a>.</p>\u000a\u000a<ol>\u000a<li>On tab load, injected key.js messages the background page to\u000aretrieve media key bindings</li>\u000a<li>On music player load, injected player.js messages the background\u000apage, reporting its tab ID. The background page <a href="http://code.google.com/chrome/extensions/messaging.html#connect">opens a Port</a>\u000athrough which to communicate with the player page</li>\u000a<li>Matching keyboard events on all tabs send messages to the background\u000apage. The background page then relays those messages through the\u000aport to the music player</li>\u000a</ol>\u000a\u000a<p>Here's an abridged code sample from the <a href="https://github.com/borismus/Chrome-Media-Keys/blob/master/dispatch.html">background page</a>: </p>\u000a\u000a<pre><code>chrome.extension.onRequest.addListener(\u000a  function(request, sender, sendResponse) {\u000a    switch(request.type) {\u000a      case 'command':\u000a        try {\u000a          port.postMessage(request.command);\u000a        } catch(error) {\u000a          if (localStorage['autoload']) {\u000a            chrome.tabs.create({url: SITE_URL});\u000a          }\u000a        }\u000a        sendResponse({});\u000a        break;\u000a      case 'register':\u000a        chrome.tabs.getSelected(null, function(tab) {\u000a          port = chrome.tabs.connect(tab.id);\u000a          sendResponse({});\u000a        });\u000a        break;\u000a    }\u000a  }\u000a);\u000a</code></pre>\u000a\u000a<p>To summarize, this approach enables a client-side remote control for a\u000aspecific web application from any other page. This is potent stuff!</p>\u000a\u000a<h2>Try it out</h2>\u000a\u000a<p>Want to try it out? Media Keys works across all Chrome platforms.</p>\u000a\u000a<ol>\u000a<li>Install <a href="http://kevingessner.com/software/functionflip/">Function Flip</a> <em>(Mac only)</em></li>\u000a<li>Check previous, pause/play and next buttons (F6, F7, F8 here) <em>(Mac\u000aonly)</em></li>\u000a<li>Install the <a href="https://chrome.google.com/extensions/detail/cpgegiegacijlefhjkfodppcefjhgdeo">Media Keys</a> extension</li>\u000a<li>Open up the extension options, bind the keys and save</li>\u000a<li>Open up a new page and press the key bound to play</li>\u000a</ol>\u000a\u000a<p>I made a short screencast showing how to install configure and use the\u000aextension.</p>\u000a\u000a<iframe title="YouTube video player" width="600" height="368"\u000a  src="http://www.youtube.com/embed/SrfsnU2gSyI" frameborder="0"\u000a  allowfullscreen></iframe>\u000a\u000a<h2>Wish list</h2>\u000a\u000a<p>Just to recap, the keyboard shortcut binding pattern described above\u000ainjects a script into all tabs, which essentially listens to all key\u000aevents. A malicious developer could write a key logger watches username\u000aand password fields, correlates to the current domain and sends\u000aharvested data to some private server. </p>\u000a\u000a<p>There are also several limitations to this keyboard shortcut binding\u000apattern. It simply won't work in the following cases:</p>\u000a\u000a<ol>\u000a<li>Chrome is in the background</li>\u000a<li>Focus inside chrome is not on the page (ex. location bar)</li>\u000a<li>Chrome is on a special page (ex. new tab page) where content scripts\u000adon't get injected</li>\u000a<li>The current page intercepts keyboard events and stops propagation\u000a(ex. Google Docs)</li>\u000a</ol>\u000a\u000a<p>Keyboard shortcuts are super important to power users, and Chrome (OS)\u000asurely won't leave us in the dust, so I'm looking forward to helping\u000aaddress the security risks and practical limitations this approach as a\u000aChromium project contributor.</p>\u000a\u000a<h2>Share and enjoy</h2>\u000a\u000a<p>One last thing. If you've read my <a href="/chrome-extension-mashups/">previous post</a>, you know that I'm a\u000abig fan of <a href="http://thesixtyone.com">thesixtyone.com</a> so my initial implementation\u000aworks for this service only. Making it work for other music streaming\u000aservices is just a matter of creating a customized player.js file for\u000ayour favorite music app, and tweaking the manifest to inject the new\u000aplayer.js into the correct domain. Feel free to fork the \u000a<a href="https://github.com/borismus/Chrome-Media-Keys">project on github</a>.</p>\u000a
p1220
tp1221
Rp1222
sg11
V/chrome-media-keys
p1223
sg13
Nsg14
I01
sg15
VChrome media keyboard shortcuts
p1224
sg18
V\u000a\u000aFew people want to synchronize their burgeoning music libraries across\u000acomputers (SSDs are small), or even lift it into the clouds with\u000asomething like Amazon's Music Cloud (uploading would take forever).
p1225
sS'snip'
p1226
g5
(g6
g7
V<p>A Chrome extension that lets you bind keyboard shortcuts to control your favorite music player.</p>\u000a
p1227
tp1228
Rp1229
sg23
g147
sg31
g1223
sg148
(dp1230
g150
S'Apr'
p1231
sg152
S'April 1, 2011'
p1232
sg154
I4
sg155
S'2011-04-01T09:00:00-00:00'
p1233
sg157
I1301673600
sg158
I2011
sg159
I1
ssg61
g160
sg29
S'chrome-media-keys'
p1234
sS'categories'
p1235
(lp1236
S'chrome'
p1237
aS'web'
p1238
aS'keyboard'
p1239
aS'shortcuts'
p1240
asS'posted'
p1241
g166
(S'\x07\xdb\x04\x01'
p1242
tp1243
Rp1244
ssg32
S'content/posts/2011/chrome-media-keys/index.md'
p1245
sg34
F1433825614.0
sa(dp1246
g2
(dp1247
g26
g5
(g6
g7
V<p>I just skipped to the next Google Music track without leaving vim. Wanna play?\u000aHere's the <a href="https://github.com/downloads/borismus/keysocket/KeySocket.zip">app</a> and <a href="https://chrome.google.com/webstore/detail/fphfgdknbpakeedbaenojjdcdoajihik">Chrome extension</a>. To learn how it works, read on!</p>\u000a\u000a<h2>Script injection limitations</h2>\u000a\u000a<p><a href="/chrome-media-keys">Last time around</a> I implemented keyboard bindings by injecting a content\u000ascript into every tab in Chrome, capturing key events and sending them to a\u000abackground page. This approach has some serious performance drawbacks:</p>\u000a\u000a<ul>\u000a<li>Content scripts injected into each page.</li>\u000a<li>Background pages don't perform very well.</li>\u000a</ul>\u000a\u000a<p>And functional limitations:</p>\u000a\u000a<ul>\u000a<li>Won't work on special URLs like <code>chrome://</code> and <code>file://</code>.</li>\u000a<li>Won't work when the omnibox is focused.</li>\u000a<li>Requires chrome to be in the foreground.</li>\u000a</ul>\u000a\u000a<h2>Global key bindings and websockets</h2>\u000a\u000a<p>What we really want is global key bindings. I don't care where my keyboard\u000afocus happens to be right now, I just want to switch to the next freaking song!\u000aThis sort of thing requires OS-level event capture, which is functionality most\u000abrowsers don't come with. To get around this, I run a standalone app to capture\u000aglobal keys and run a websocket server to send these events to the browser.\u000aNote that this approach generalizes well to other use cases where functionality\u000ais not available in a browser, but can be more readily implemented natively.</p>\u000a\u000a<p>The obvious drawback to this approach is that it requires the user to run a\u000aseparate process to capture events.</p>\u000a\u000a<h2>Media key bindings in Cocoa and Python</h2>\u000a\u000a<p>Rogue Amoeba, maker of some popular OS X audio utilities, has a\u000a<a href="http://rogueamoeba.com/utm/2007/09/29/apple-keyboard-media-key-event-handling/">nice post</a> on their blog on capturing media keys from an OS X\u000aapplication. The basic idea is to subclass NSApplication and override the\u000asendEvent: selector:</p>\u000a\u000a<pre><code>- (void)sendEvent: (NSEvent*)event {\u000a  if( [event type] == NSSystemDefined &amp;&amp; [event subtype] == 8 ) {\u000a      // Event processing\u000a  }\u000a  [super sendEvent: event];\u000a}\u000a</code></pre>\u000a\u000a<p>Which in PyObjC results in the following equivalent code:</p>\u000a\u000a<pre><code>def sendEvent_(self, event):\u000a    if event.type() is NSSystemDefined and event.subtype() is 8:\u000a        # Event processing\u000a\u000a    NSApplication.sendEvent_(self, event)\u000a</code></pre>\u000a\u000a<p>It's pretty neat to be able to implement Cocoa apps without having to write a\u000asingle line of objective C. Writing a statusbar app with no dock item was\u000asurprisingly simple (though I have doubts that this works well for Lions and\u000aTigers and Bears). All that's required is to set <code>LSUIElement</code> to <code>true</code> in the\u000aInfo.plist.</p>\u000a\u000a<p>To package the whole PyObjC application, I wrote a setup.py script and used\u000a<a href="http://svn.pythonmac.org/py2app/py2app/trunk/doc/index.html">py2app</a>, which generates a Mac OS X .app bundle which, from a user's\u000aperspective is indistinguishable from an OS X app written in Objective C.</p>\u000a\u000a<h2>A WebSocket server in python</h2>\u000a\u000a<p>In addition to spawning off a Cocoa application and capturing events, of course\u000aI create a WebSocket server. WebSockets use a pretty simple protocol which can\u000aeasily be implemented using python sockets. I based my implementation heavily\u000aon <a href="https://gist.github.com/512987">this one</a>.</p>\u000a\u000a<p>Since a Cocoa application runs its own event loop which captures the\u000amain thread, the websocket listener needs to run in a separate thread:</p>\u000a\u000a<pre><code>class KeySocketServer(Thread):\u000a    def __init__(self):\u000a        self.server = websocket.WebSocketServer('localhost', 1337, KeySocket)\u000a        Thread.__init__(self)\u000a\u000a    def run(self):\u000a        self.server.listen()\u000a</code></pre>\u000a\u000a<p>The WebSocket standards are still evolving and implementers are, as ever,\u000ascrambling to catch up. The good news is that this means\u000a<a href="http://tools.ietf.org/html/draft-ietf-hybi-thewebsocketprotocol-06#section-4.6">binary support</a> is coming, which is a boon for games and other\u000aintensive network consumers. The bad news is that the latest Chrome canary (at\u000athe time of writing), requires the response to contain the\u000a<code>Sec-WebSocket-Accept</code> header, conforming to the\u000a<a href="http://tools.ietf.org/html/draft-ietf-hybi-thewebsocketprotocol-06">draft-ietf-hybi-thewebsocketprotocol-06</a>, which is incompatible with\u000athe python WebSocket code I'm currently using.</p>\u000a\u000a<p>On my wishlist is a robus python WebSockets implementation that supports\u000amultiple versions of the spec while it's still in flux.</p>\u000a\u000a<h2>Injected scripts</h2>\u000a\u000a<p>On the Chrome extension side, a script is injected into the web player\u000aapplication, which creates a WebSocket client that connects to the python\u000aserver on port 1337. When media keys are pressed, the python server sends\u000amessages to the JS clients and the injected JS simulates DOM events in the web\u000aplayer application, controlling music playback.</p>\u000a\u000a<h2>Try it out</h2>\u000a\u000a<p>If you listen to Google Music, thesixtyone or Grooveshark in Chrome on OS X and\u000awant global key bindings, please install the <a href="https://chrome.google.com/webstore/detail/fphfgdknbpakeedbaenojjdcdoajihik">extension</a> and\u000a<a href="https://github.com/downloads/borismus/keysocket/KeySocket.zip">application</a>. If you're feeling generous, contribute your time and love!\u000aI'd gladly take fixes for</p>\u000a\u000a<ul>\u000a<li>Key Socket servers for Linux and Windows</li>\u000a<li>Content scripts to control other web audio players</li>\u000a<li>Web Socket implementations that work with the new spec</li>\u000a</ul>\u000a\u000a<p>And of course, here's the <a href="https://github.com/borismus/keysocket">source</a>.</p>\u000a
p1248
tp1249
Rp1250
sg11
V/chrome-media-keys-revisited
p1251
sg13
Nsg14
I01
sg15
VGlobal chrome media keys with Key Socket
p1252
sg18
V\u000a\u000a\u000aI just skipped to the next Google Music track without leaving vim.
p1253
sS'snip'
p1254
g5
(g6
g7
V<p>An extension that lets you bind global keyboard shortcuts to control your favorite music player in chrome.</p>\u000a
p1255
tp1256
Rp1257
sg23
g147
sg31
g1251
sg148
(dp1258
g150
S'Jul'
p1259
sg152
S'July 28, 2011'
p1260
sg154
I7
sg155
S'2011-07-28T09:00:00-00:00'
p1261
sg157
I1311868800
sg158
I2011
sg159
I28
ssg61
g160
sg29
S'chrome-media-keys-revisited'
p1262
sS'categories'
p1263
(lp1264
S'chrome'
p1265
aS'web'
p1266
aS'keyboard'
p1267
aS'websockets'
p1268
aS'music'
p1269
asS'posted'
p1270
g166
(S'\x07\xdb\x07\x1c'
p1271
tp1272
Rp1273
ssg32
S'content/posts/2011/chrome-media-keys-revisited/index.md'
p1274
sg34
F1433825609.0
sa(dp1275
g2
(dp1276
g26
g5
(g6
g7
V<p>Like most people, I'm slowly lifting most of my work into the cloud.\u000aThis leads to a lot of time spent in the browser. Just how much, I'm not\u000areally sure. Enter <a href="https://chrome.google.com/extensions/detail/dbgohgmphghmoghphoiaghbopikmmgop/">Chronos</a>, a Chrome extension to track how much\u000atime you spend on each domain you visit. Chronos gives a per-day\u000abreakdown of time spent actively browsing. In addition to showing a\u000agraphical summary of domain frequency, you also get a total time spent\u000ain Chrome, and how much time your Chrome spends idle.</p>\u000a\u000a<p>Chronos, named after the Greek god of time, quietly sits and monitors\u000akeyboard and mouse events you generate. If you've been active recently,\u000athe domain you're visiting gets recorded. The data structure that stores\u000athis timing information persists on the client side using localStorage.\u000aThis data is never sent to any servers, so your browsing privacy is\u000apreserved. Chronos' visualization is built out of HTML divs. Just for\u000afun, if you become inactive in Chrome, the Chronos icon in the\u000aextensions toolbar fades out.</p>\u000a\u000a<p>Some features that I would find useful to\u000aadd to Chronos revolve around productivity and time management:</p>\u000a\u000a<ol>\u000a<li>Chronos makes it really obvious which sites consume most of your\u000atime. It would make sense to be able to enforce time limits spent on\u000asites, either by interfacing with an extension like <a href="https://chrome.google.com/extensions/detail/laankejkbhbdhmipfmgcngdelahlfoji">StayFocusd</a>,\u000aor by replicating that functionality.</li>\u000a<li>Many people find it helpful to be reminded to take breaks from\u000acomputing, either for RSI purposes or for general productivity.\u000aChronos already tracks activity levels in Chrome, so it could be\u000aaugmented to remind people to take breaks in a way similar to\u000a<a href="http://tech.inhelsinki.nl/antirsi/">AntiRSI</a>.</li>\u000a</ol>\u000a\u000a<p>Have a great idea to add to Chronos? Let me know, or add it yourself! As\u000ausual, the <a href="https://github.com/borismus/Chronos">source code</a> is on github.</p>\u000a
p1277
tp1278
Rp1279
sg11
V/chronos
p1280
sg13
Nsg14
I01
sg15
VChronos: Chrome browsing metrics
p1281
sg18
V\u000a\u000aLike most people, I'm slowly lifting most of my work into the cloud.
p1282
sS'snip'
p1283
g5
(g6
g7
V<p>Ever wondered how much time you spend in your browser?</p>\u000a
p1284
tp1285
Rp1286
sg23
g147
sg31
g1280
sg148
(dp1287
g150
S'Mar'
p1288
sg152
S'March 21, 2011'
p1289
sg154
I3
sg155
S'2011-03-21T09:00:00-00:00'
p1290
sg157
I1300723200
sg158
I2011
sg159
I21
ssg61
g160
sg29
S'chronos'
p1291
sS'categories'
p1292
(lp1293
S'web'
p1294
aS'chrome'
p1295
aS'statistics'
p1296
asS'posted'
p1297
g166
(S'\x07\xdb\x03\x15'
p1298
tp1299
Rp1300
ssg32
S'content/posts/2011/chronos/index.md'
p1301
sg34
F1433825621.0
sa(dp1302
g2
(dp1303
g26
g5
(g6
g7
V<p>Work marketplaces like MTurk are great for accomplishing small, well\u000adefined nuggets of work, such as labeling images and transcribing audio,\u000abut terrible for many more complex and labor intensive real world tasks.\u000aOver the last year, Robert Kraut, Niki Kittur and I formalized a general\u000aprocess of solving such complex problems using MTurk. We proposed three\u000abasic types of tasks and explored them in two neat experimental\u000aapplications. To facilitate these experiments, I implemented CrowdForge,\u000aa Django framework that takes output from MTurk HITs and uses it to\u000acreate new MTurk HITs. </p>\u000a\u000a<p>Solving complex problems on MTurk has always involved partitioning the\u000acomplex task into simpler sub-tasks.  CastingWords, one of the most\u000apopular MTurk requesters, transcribes long audio recordings by splitting\u000athem into overlapping segments, distributing the work among workers,\u000aperforming quality control and then recombining the transcription\u000afragments. CrowdForge formalizes this approach and takes it to the next\u000alevel. CrowdForge proposes the following task breakdown, roughly\u000ainspired by the MapReduce programming paradigm:</p>\u000a\u000a<ul>\u000a<li><strong>partition</strong> tasks split a problem into sub-problems (one to many)</li>\u000a<li><strong>map</strong> tasks solve a small unit of work (one to one)</li>\u000a<li><strong>reduce</strong> tasks combine multiple results into one (many to one)</li>\u000a</ul>\u000a\u000a<p><img src="crowdforge-simple.png" alt="image" /> </p>\u000a\u000a<p>CastingWords uses human intelligence only for their map tasks, which\u000aconsist of transcribing speech samples to text. Their partition task may\u000ainvolve an algorithm which seeks natural breaks in speech audio samples,\u000awhile the reduce task may involve programmatic stitching of audio\u000asamples. For sufficiently complex cases, however, algorithms may be\u000ainadequate, and the partitioning and reduction require human\u000aintelligence. Here are some experiments that illustrate this idea:</p>\u000a\u000a<h2>Writing Articles</h2>\u000a\u000a<p>In the first experiment, turkers generated encyclopedia-style articles\u000aon a given subject. The approach I took was to first generate an article\u000aoutline using an partition task, then for each heading in the outline,\u000ato collect facts on the topic, next to combine these facts into a\u000aparagraph, finally merging all of the paragraphs into a final article.\u000aThe following diagram illustrates this process for an article on New\u000aYork City: </p>\u000a\u000a<p><img src="crowdforge-article.png" alt="image" /> </p>\u000a\u000a<p>In one incarnation of this experiment, we used this approach to create\u000afive articles about New York City. Articles cost an average of $3.26 to\u000aproduce, required an average of 36 subtasks or HITs, included an average\u000aof 5.3 topics per article and consisted of an average of 658 words. As a\u000acomparison baseline, we created eight HITs which each requested one\u000aworker to write the full article, paying $3.26, the same amount required\u000afor the collaboratively written articles. </p>\u000a\u000a<p>We then evaluated the quality of all articles by asking a new set of\u000aworkers to each rate a single article based on use of facts, spelling\u000aand grammar, article structure, and personal preference. On average the\u000aarticles produced by the group were of rated on par with the Simple\u000aWikipedia article on the same topic, and higher than those produced\u000aindividually. See the paper for gritty details on the stats.</p>\u000a\u000a<h2>Product Comparisons</h2>\u000a\u000a<p>We did another experiment to prove the generality of the approach. We\u000aused CrowdForge to create purchase decision matrices to assist consumers\u000alooking to buy a new car. Given a short description of a consumer need,\u000awe created two partition tasks: one to decide which cars might be\u000aappropriate to consider, and one to decide which features the consumer\u000acares most about. This double partition resulted in an empty product\u000acomparison chart. Each cell in the chart then spawned a map task to\u000acollect related facts. Next, these facts are reduced into a sentence,\u000aresulting in a product comparison chart. Here's an excerpt: </p>\u000a\u000a<p><img src="crowdforge-purchase.png" alt="image" /></p>\u000a\u000a<p>The entire task was completed in 54 different HITs for a total cost of\u000a$3.70. When we tried to compare to the individual case, we had no\u000asuccess getting individuals to generate a similar product comparison\u000achart, even when offering more money than we paid the entire group.</p>\u000a\u000a<h2>Sweet Applications</h2>\u000a\u000a<p>MTurk is a one of my favorite tools for doing <a href="http://www.quora.com/What-are-the-most-creative-uses-of-Amazon-s-Mechanical-Turk">creative and novel</a>\u000aprojects. As illustrated, applying human intelligence to reduce and\u000apartition tasks, we can solve a new set of interesting problems. But\u000athere is much more to explore! For example, imagine collaborative\u000adrawing assignments in which a worker sketches out a picture, and\u000asubsequent workers refine the original picture by drawing sub-pictures\u000aor specific objects. Imagine requesting a trip plan and getting a\u000aresearched day-by-day itinerary of what to see and do. Imagine\u000apartitioning a Java class into methods, outsourcing the implementation\u000aand unit test implementations. </p>\u000a\u000a<p>CrowdForge is written as a Django application that communicates with\u000aMTurk servers using the <a href="http://code.google.com/p/boto/">Boto interface</a>, which is a Python framework\u000athat encapsulates the Amazon Web Services API. CrowdForge regularly\u000apolls MTurk and fires notifications whenever interesting things happen\u000a(a new result comes in for a HIT, all HITs of a certain type are\u000afinished, a HIT is expired or complete). CrowdForge can manage many\u000aflows, which encapsulate code that determines how to respond to these\u000aevents. For example, the article writing flow knows to create fact\u000acollection HITs for each outline topic once the outline is submitted. So\u000ato solve a new kind of complex problem, you would extend CrowdForge with\u000aa custom flow, like the following: </p>\u000a\u000a<p><img src="crowdforge-complex.png" alt="image" /></p>\u000a\u000a<p>"But Boris", you say, "how can I get my hands on this CrowdForge\u000aframework of yours?" Indeed, we just <a href="https://github.com/borismus/CrowdForge">released it</a> for non-commercial\u000ause under <a href="http://creativecommons.org/licenses/by-nc/3.0/">the Creative Commons license</a>. Let me know if you have any\u000aquestions, comments or interest in collaborating or maintaining the framework.</p>\u000a\u000a<p><strong>Update (February 2011):</strong> CrowdForge was covered in <a href="http://www.newscientist.com/article/mg20927985.800-silicon-supervisor-gets-the-job-done-online.html">NewScientist</a>,\u000ain a <a href="http://www.cmu.edu/homepage/society/2011/winter/crowdforge.shtml">CMU article</a>, and in a <a href="http://open.blogs.nytimes.com/2011/10/19/uist-2011-crowdsourcing-research/">NY Times blog</a>.</p>\u000a\u000a<p><strong>Update (November 2011)</strong>: The <a href="crowdforge-uist-11.pdf">full paper</a> was <a href="http://smustalks.appspot.com/crowdforge-11/">presented</a> at\u000a<a href="http://www.acm.org/uist/">UIST 2011</a> in Santa Barbara.</p>\u000a
p1304
tp1305
Rp1306
sg11
V/crowdforge
p1307
sg13
Nsg14
I01
sg15
VCrowdForge: crowdsourcing complex tasks
p1308
sg18
V\u000a\u000aWork marketplaces like MTurk are great for accomplishing small, well\u000adefined nuggets of work, such as labeling images and transcribing audio,\u000abut terrible for many more complex and labor intensive real world tasks.
p1309
sS'snip'
p1310
g5
(g6
g7
V<p>A MapReduce approach to Human Computation. I wrote a Django app for that.</p>\u000a
p1311
tp1312
Rp1313
sg23
g147
sg31
g1307
sg148
(dp1314
g150
S'Feb'
p1315
sg152
S'February 2, 2011'
p1316
sg154
I2
sg155
S'2011-02-02T09:00:00-00:00'
p1317
sg157
I1296666000
sg158
I2011
sg159
I2
ssg61
g160
sg29
S'crowdforge'
p1318
sS'categories'
p1319
(lp1320
S'social'
p1321
aS'web'
p1322
aS'crowdsourcing'
p1323
asS'posted'
p1324
g166
(S'\x07\xdb\x02\x02'
p1325
tp1326
Rp1327
ssg32
S'content/posts/2011/crowdforge/index.md'
p1328
sg34
F1433825628.0
sa(dp1329
g2
(dp1330
g26
g5
(g6
g7
V<p>After having dabbled in crowdsourcing research, I wanted to find a\u000acompelling MTurk application around which to build a real service that\u000awould simultaneously be useful, involve creative tasks, and be fun\u000aenough to capture my attention. Generating sound effects seemed to fit\u000athe bill. Creating sounds through imitation or collecting is creative,\u000afun and (ahem, quality aside) easy due to the ubiquity of microphones.\u000aEnter experimental service <a href="http://soundsourcing.com">soundsourcing.com</a>. </p>\u000a\u000a<p>First I had to verify some basic questions: Would turkers be willing to\u000acreate and submit sound effects? Would their output be sufficiently good\u000afor someone to use in practice? Would it be possible to have some\u000aquality control process to weed out the blatantly bad submissions?\u000aJumping into experimentation, I asked workers to imitate or record a cow\u000amooing, paying ten cents for their efforts. After 3 days I had a set of\u000amoos:</p>\u000a\u000a<p><object height="285" width="100%"><param name="movie"\u000a  value="http://player.soundcloud.com/player.swf?url=http%3A%2F%2Fapi.soundcloud.com%2Fplaylists%2F527826&amp;show_comments=false&amp;auto_play=false&amp;show_playcount=true&amp;show_artwork=true&amp;color=ff7700"></param><param\u000a  name="allowscriptaccess" value="always"></param> <embed\u000a  allowscriptaccess="always" height="285"\u000a  src="http://player.soundcloud.com/player.swf?url=http%3A%2F%2Fapi.soundcloud.com%2Fplaylists%2F527826&amp;show_comments=false&amp;auto_play=false&amp;show_playcount=true&amp;show_artwork=true&amp;color=ff7700"\u000a  type="application/x-shockwave-flash" width="100%"></embed></object></p>\u000a\u000a<p>Taking it up a notch, I requested the more obscure sound of a \u000a<a href="http://soundcloud.com/borismus/sets/turkish-trains-leaving-the-station/">train leaving the station</a>. There were many excellent submissions in\u000athe mix, although I suspect that many of them were not created by the\u000aworkers but found online despite my empty threat to verify their\u000aoriginality and reject. My favorite is probably this train, improvised\u000aon harmonica: </p>\u000a\u000a<p><object height="81" width="100%"><param name="movie" value="http://player.soundcloud.com/player.swf?url=http%3A%2F%2Fapi.soundcloud.com%2Ftracks%2F9563151"></param><param name="allowscriptaccess" value="always"></param> <embed allowscriptaccess="always" height="81" src="http://player.soundcloud.com/player.swf?url=http%3A%2F%2Fapi.soundcloud.com%2Ftracks%2F9563151" type="application/x-shockwave-flash" width="100%"></embed></object></p>\u000a\u000a<p>As usual, the hard problem with crowdsourcing is quality control. I\u000aapproach this problem with a rating step after the sound samples have\u000abeen submitted, in which turkers select their top three samples from the\u000asound set (shuffled in random order to avoid <a href="http://www.jstor.org/pss/3151704">order bias</a>). Each\u000ainstance of such a task can be done cheaply (I tried $0.05), so ordering\u000athe whole set in quality becomes easy. In the future I'd like to write\u000asome JS to verify that the turker in fact listened to all of the\u000asamples. Below is the rating form presented to MTurk users:</p>\u000a\u000a<p><img src="turk-rate.png" alt="image" /> </p>\u000a\u000a<p>When all of the pieces came together and some time remained, I built\u000a<a href="http://soundsourcing.com">soundsourcing.com</a> around this concept. I implemented the service\u000ausing <a href="http://www.djangoproject.com/">Django</a> and <a href="http://code.google.com/p/boto/">Boto</a>, reusing many concepts from the\u000a<a href="/crowdforge">CrowdForge</a> framework. The premise of the service is that people\u000aseeking very specific sound effects that they cannot find on existing\u000asound effects sites can use soundsourcing to get their custom sounds\u000abuilt to order. The basic flow of HITs on MTurk looks like this:</p>\u000a\u000a<p><img src="soundsourcing.jpg" alt="image" /> </p>\u000a\u000a<p>Since the cow and train experiments, I have collected <a href="http://soundsourcing.com/set/a-lion-roaring/">lion roars</a> and\u000asword slashes (submissions are in progress at the time of writing).\u000aPlease check out <a href="http://soundsourcing.com">the site</a>, request a sound sample\u000aand submit some feedback below. Mechanical Turkers, let's make some\u000anoise!!</p>\u000a
p1331
tp1332
Rp1333
sg11
V/crowdsourcing-sound
p1334
sg13
Nsg14
I01
sg15
VSoundsourcing: the sound of the crowd
p1335
sg18
V\u000a\u000aAfter having dabbled in crowdsourcing research, I wanted to find a\u000acompelling MTurk application around which to build a real service that\u000awould simultaneously be useful, involve creative tasks, and be fun\u000aenough to capture my attention.
p1336
sS'snip'
p1337
g5
(g6
g7
V<p>An experiment in which Mechanical Turkers generated sound effects on demand.</p>\u000a
p1338
tp1339
Rp1340
sg23
g147
sg31
g1334
sg148
(dp1341
g150
S'Feb'
p1342
sg152
S'February 20, 2011'
p1343
sg154
I2
sg155
S'2011-02-20T09:00:00-00:00'
p1344
sg157
I1298221200
sg158
I2011
sg159
I20
ssg61
g160
sg29
S'crowdsourcing-sound'
p1345
sS'categories'
p1346
(lp1347
S'social'
p1348
aS'music'
p1349
aS'web'
p1350
asS'posted'
p1351
g166
(S'\x07\xdb\x02\x14'
p1352
tp1353
Rp1354
ssg32
S'content/posts/2011/crowdsourcing-sound/index.md'
p1355
sg34
F1433825634.0
sa(dp1356
g2
(dp1357
g26
g5
(g6
g7
V<p>Chrome's Developer Tools have been getting much deserved love at this last\u000aGoogle I/O. Paul Irish and I started with an <a href="http://www.io-bootcamp.com/">bootcamp lab</a>, a hands-on \u000awalk through tweaking a web application using the developer tools. We handed\u000aout a cheatsheet to give developers an overview of available features.</p>\u000a\u000a<p><a href="https://github.com/borismus/DevTools-Lab/raw/master/cheatsheet/chromedev-cheatsheet.pdf"><img src="https://github.com/borismus/DevTools-Lab/raw/master/cheatsheet/chromedev-cheatsheet.jpg" alt="cheatsheet jpg" /></a></p>\u000a\u000a<p>The cheatsheet is available for download in <a href="https://github.com/borismus/DevTools-Lab/raw/master/cheatsheet/chromedev-cheatsheet.pdf">PDF</a> and\u000a<a href="https://github.com/borismus/DevTools-Lab/raw/master/cheatsheet/chromedev-cheatsheet.png">PNG</a>. </p>\u000a\u000a<p>Thanks to the awesome I/O organizers, we also gave away free HTML5 cake (as\u000apromised) after the session ended:</p>\u000a\u000a<p><img src="html5-cake.jpg" alt="html5 cake" /></p>\u000a\u000a<p>The developer tools were also featured at the <a href="http://www.youtube.com/watch?v=MiYND_zvIc0&amp;t=8m30s">start of the Chrome keynote</a>\u000awith a webkit-speech demo. Finally, Pavel Feldman (lead engineer for the dev\u000atools) and Paul gave an great <a href="http://www.youtube.com/watch?v=N8SS-rUEZPg">I/O talk</a>.</p>\u000a\u000a<p>One of the most promising aspects of the Chrome Developer Tools is that they\u000aare easy to extend with (experimental) <a href="http://code.google.com/chrome/extensions/trunk/experimental.devtools.html">chrome extension APIs</a>. I'm stoked to\u000asee what people create to make these developer tools even better!</p>\u000a
p1358
tp1359
Rp1360
sg11
V/devtools-cheatsheet
p1361
sg13
Nsg14
I01
sg15
VChrome developer tools cheatsheet
p1362
sg18
V\u000a\u000a\u000aChrome's Developer Tools have been getting much deserved love at this last\u000aGoogle I/O.
p1363
sS'snip'
p1364
g5
(g6
g7
V<p>I/O 2011 featured excellent Chrome Developer Tools coverage. New to Chrome DevTools? Check out this cheat sheet!</p>\u000a
p1365
tp1366
Rp1367
sg23
g147
sg31
g1361
sg148
(dp1368
g150
S'May'
p1369
sg152
S'May 13, 2011'
p1370
sg154
I5
sg155
S'2011-05-13T09:00:00-00:00'
p1371
sg157
I1305302400
sg158
I2011
sg159
I13
ssg61
g160
sg29
S'devtools-cheatsheet'
p1372
sS'categories'
p1373
(lp1374
S'web'
p1375
asS'posted'
p1376
g166
(S'\x07\xdb\x05\r'
p1377
tp1378
Rp1379
ssg32
S'content/posts/2011/devtools-cheatsheet/index.md'
p1380
sg34
F1433825641.0
sa(dp1381
g2
(dp1382
g26
g5
(g6
g7
V<p>Extension developers aren't given much freedom to modify Chrome's browser\u000achrome. Without resorting to changing the page itself, or using the new devtools\u000aextension APIs, there are two main ways of doing this. <a href="http://code.google.com/chrome/extensions/pageAction.html">Page actions</a>, which\u000areside in the omnibox, and <a href="http://code.google.com/chrome/extensions/browserAction.html">browser actions</a>, which are positioned to the\u000aright of the omnibox both of which are simple buttons with icons, click actions\u000aand hover states. Chrome conveniently <a href="http://code.google.com/chrome/extensions/browserAction.html#method-setIcon">provides an API</a> to dynamically change\u000athe icon of these buttons.</p>\u000a\u000a<p>You can do this by creating image data by hand or using the canvas API's\u000a<code>getImageData</code> function.:</p>\u000a\u000a<pre><code>var canvas = document.getElementById('canvas');\u000avar context = canvas.getContext('2d');\u000a// ...draw to the canvas...\u000avar imageData = context.getImageData(0, 0, 19, 19);\u000achrome.browserAction.setIcon({\u000a  imageData: imageData\u000a});\u000a</code></pre>\u000a\u000a<p>Note that this would be possible even without this chrome-specific API, by\u000ainstead using <a href="http://en.wikipedia.org/wiki/Data_URI_scheme">data URIs</a> to set the image. There's a GMail <a href="http://googlesystem.blogspot.com/2011/01/dynamic-gmail-favicon.html">labs plugin</a>\u000athat does this to badge the favicon with the unread email count.</p>\u000a\u000a<p>What makes this an interesting design domain is the limitation of the medium.\u000aLimited real estate (browser actions at 19x19 px, page actions at 16x16 px) adds\u000asignificant constraints. Still, one can show numbers, colors, small icons and\u000agraphs in this context, or even small amounts of text.</p>\u000a\u000a<h2>Applications of Dynamic Icons</h2>\u000a\u000a<p>There are extensions that use dynamic icons already, such as the\u000a<a href="https://chrome.google.com/webstore/detail/kpekpmmfocifmbnnoahnclccmjkckpcl">PageRank extension</a>, which effectively shows the Google PageRank for the\u000acurrent page right inside the browser action.</p>\u000a\u000a<p>Here are some other possibilities (not at all an exhaustive list):</p>\u000a\u000a<ul>\u000a<li>Create badges for page actions (which don't implement <code>setBadge*</code> calls).</li>\u000a<li>Icon of the weather forecast, click to toggle between days.</li>\u000a<li>Bandwidth meter: how large was the download of this page?</li>\u000a<li>A random profile pic of people that +1'ed the site.</li>\u000a</ul>\u000a\u000a<h2>The Smallest Music Visualizer</h2>\u000a\u000a<p>I wrote a sample extension to demonstrate the dynamic icon potential of Chrome\u000aextensions. This indispensible extension is a music visualizer that renders\u000ainside of a browser action button. I use the <a href="http://chromium.googlecode.com/svn/trunk/samples/audio/specification/specification.html">Web Audio API</a> to playback a\u000asong and analyse the audio stream, render the visualized audio spectrum with a\u000acanvas element and then transfer the resulting image data to the browser action\u000aicon.</p>\u000a\u000a<p><img src="music-vis.png" alt="screenshot" /></p>\u000a\u000a<p>Try out the <a href="https://chrome.google.com/webstore/detail/befnabfghcghgpmkjoalbecphdgdmick?hl=en">chrome extension</a> here, but note that it requires the Web Audio\u000aAPI flag to be enabled under about:flags (and a browser restart afterward).\u000aCheck out and fork the <a href="https://github.com/borismus/Music-Visualizer-Chrome-Extension">source on github</a>.</p>\u000a\u000a<h2>Learnings</h2>\u000a\u000a<p>This music visualizer extension loads an mp3 file when the extension\u000abackground page loads, which takes a certain amount of time. To provide a\u000abetter user experience, I was hoping to change the icon to reflect that the\u000afile was being loaded, and ran into two issues.</p>\u000a\u000a<p>The first issue was that when I tried to render a small string like "wait" in\u000athe icon, I wanted to use a custom <code>@font-face</code> embedded font, which is now\u000awell supported in CSS3. You can load CSS fonts</p>\u000a\u000a<pre><code>@font-face {\u000a  font-family: "Silkscreen"\u000a  src: url(slkscr.ttf);\u000a}\u000a</code></pre>\u000a\u000a<p>and then use them in a canvas:</p>\u000a\u000a<pre><code>context.font = "8px Silkscreen";\u000acontext.fillText('load');\u000a</code></pre>\u000a\u000a<p>When using custom fonts from HTML, the browser waits for the font to load, and\u000athen does a relayout. When using it from canvas, things get a bit tricky since\u000athe browser of course doesn't do font relayout for you and there's\u000aunfortunately no DOM event that fires when all embedded fonts finished loading.\u000aMore precisely, onload behaviors differ from browser to browser. Mozilla waits\u000afor all fonts to load before firing the event, WebKit doesn't. You can work\u000aaround this problem by assigning the custom font to a div, and observing the\u000adiv's width, which will change when the font loads (<a href="https://github.com/paulirish/font-face-detect/blob/master/isFontLoaded.js">codified</a>).</p>\u000a\u000a<p>Although I ultimately didn't use this font to show loading state, I recommend\u000achecking out the <a href="http://kottke.org/plus/type/silkscreen/">silkscreen font</a> for 8-bit style designs that lend\u000athemselves well to small resolution envirnoments. You can fit about 3x3\u000acharacters inside a 16x16 canvas:</p>\u000a\u000a<p><img src="silkscreen.png" alt="silkscreen" /></p>\u000a\u000a<p>To show that the extension is still loading, I went with a progress bar instead\u000aof a message. A second issue arose when I wanted to show the progress\u000abar animating while the mp3 loads. Unfortunately the Web Audio API doesn't\u000acurrently support asynchronous loading of files, so the UI thread gets blocked\u000aduring the <code>audioContext.createBuffer</code> call of this code snippet:</p>\u000a\u000a<pre><code>var request = new XMLHttpRequest();\u000arequest.onload = function() {\u000a  var audioBuffer = audioContext.createBuffer(request.response, false);\u000a}\u000a</code></pre>\u000a\u000a<p>Async loading of audio buffers is now a <a href="https://bugs.webkit.org/show_bug.cgi?id=61947">tracked issue</a> for you upvote in\u000athe webkit bug tracker. I thought of working around this with Web Workers, but\u000agave up early because of difficulties with passing objects between worker\u000athreads, and no shared memory options that would let workers access the\u000acontext of the main UI thread.</p>\u000a\u000a<p>Another interesting observation is that <code>requestAnimationFrame</code> does not work in\u000aa background page. I initially tried to use it to animate the music visualizer,\u000abut it didn't work. This is of course the API is designed to only callback when\u000athe calling page is in the foreground, and since the background page is never\u000aforegrounded, the callback never fires.</p>\u000a\u000a<p>That's it for me, now it's your turn! So, dearest reader, go forth and write\u000asome awesome Chrome extensions which tastefully use dynamic icons for page\u000aactions, browser actions, and favicons to make our browsing experience even\u000abetter.</p>\u000a
p1383
tp1384
Rp1385
sg11
V/dynamic-icons-chrome-extensions
p1386
sg13
Nsg14
I01
sg15
VDynamic chrome extension icons
p1387
sg18
V\u000a\u000a\u000aExtension developers aren't given much freedom to modify Chrome's browser\u000achrome.
p1388
sS'snip'
p1389
g5
(g6
g7
V<p>Adding a bit of pizzazz to your extension UIs with dynamic, maybe even animated browser and page actions.</p>\u000a
p1390
tp1391
Rp1392
sg23
g147
sg31
g1386
sg148
(dp1393
g150
S'Jun'
p1394
sg152
S'June 6, 2011'
p1395
sg154
I6
sg155
S'2011-06-06T09:00:00-00:00'
p1396
sg157
I1307376000
sg158
I2011
sg159
I6
ssg61
g160
sg29
S'dynamic-icons-chrome-extensions'
p1397
sS'categories'
p1398
(lp1399
S'web'
p1400
aS'chrome'
p1401
asS'posted'
p1402
g166
(S'\x07\xdb\x06\x06'
p1403
tp1404
Rp1405
ssg32
S'content/posts/2011/dynamic-icons-chrome-extensions/index.md'
p1406
sg34
F1433825647.0
sa(dp1407
g2
(dp1408
g26
g5
(g6
g7
V<p>Chrome\u2019s developer tools (also known as the WebKit inspector) are super useful\u000afor web developers. If you aren't ramped up already, take a look at this\u000a<a href="/devtools-cheatsheet">cheat sheet overview</a>. Also check out some of many online resources,\u000asuch as the <a href="http://code.google.com/chrome/devtools/docs/overview.html">official documentation</a>, which is quite readable, and this\u000a<a href="http://www.youtube.com/watch?v=nOEw9iiopwI">12 tricks screencast</a> from Paul Irish.</p>\u000a\u000a<p>But let\u2019s not get sidetracked here. I wasn't going to write about using the\u000adeveloper tools, it\u2019s about <strong>developing</strong> developer tools. There are two ways\u000ato go:</p>\u000a\u000a<ol>\u000a<li><p>You can extend the tools by writing chrome extensions that use the new\u000a<code>chrome.experimental.devtools</code> APIs. These recently created APIs are still\u000asubject to change, and marked experimental, so it\u2019s a bit more difficult to\u000adistribute and install them compared to other chrome extension APIs (more on\u000athis later).</p></li>\u000a<li><p>You can quite easily hack the devtools code yourself, to customize it to\u000ayour needs. Then, if you do a really good job, you can contribute your changes\u000aback to the chromium project and feel better for being such an awesome person.</p></li>\u000a</ol>\u000a\u000a<p>Let\u2019s dive in, starting from the beginning.</p>\u000a\u000a<h2>Developer tools extensions</h2>\u000a\u000a<p>Chrome provides <a href="http://code.google.com/chrome/extensions/trunk/experimental.devtools.html">three separate APIs</a> for extending the developer tools:</p>\u000a\u000a<ol>\u000a<li><p>Audits - add a new audit to the Audits tab in the developer tools:\u000a<img src="audit.png" alt="auditshot" /></p></li>\u000a<li><p>Panels - add a whole new panel to the developer tools: <img src="panel.png" alt="panelshot" /></p></li>\u000a<li><p>Resources - access a <a href="http://groups.google.com/group/http-archive-specification/web/har-1-2-spec">HAR file</a> containing known resources</p></li>\u000a</ol>\u000a\u000a<p>There are several <a href="http://code.google.com/chrome/extensions/trunk/samples.html#devtools">extension samples</a> available as starting points,\u000awhich cover all of the devtools extension API. The API is geared towards\u000aAudit-oriented extensions (HTML/CSS/Javascript validators, performance\u000aanalyzers) and Panel-oriented extensions (framework specific tooling, etc).\u000aYou can also write extensions that integrate more loosely with the developer\u000atools (perhaps only using the resources API).</p>\u000a\u000a<h3>My JSHint audit extension</h3>\u000a\u000a<p>I wanted to get my hands dirty, and wrote a <a href="http://jshint.com">JSHint</a>-based Javascript\u000avalidator extension. This extension uses the devtools audit API to create an\u000aaudit that checks all scripts (linked and inline) on the current page. Errors\u000aare shown per-script in the audit results view. I wanted to go further, and\u000aallow the user to click on the error and jump to the resource, but this wasn\u2019t\u000apossible with the developer tools extension API. To get around this limitation,\u000aI patched the developer tools with a pretty quick fix. More on modifying the\u000adevtools themselves later.</p>\u000a\u000a<p>Check out the JSHint audit extension <a href="https://github.com/borismus/jshint-extension">on github</a>.</p>\u000a\u000a<h3>Writing an extension</h3>\u000a\u000a<p>The first thing you should know about developer tools extensions, is that (at\u000athe time of writing), they are experimental. This means that you will need to\u000alaunch chrome with a special flag to use them. I use a shell script called\u000a<code>chrome-devtools</code> in my <code>~/bin</code> (which is in my <code>$PATH</code> of course):</p>\u000a\u000a<pre><code>/Applications/Google\u005c Chrome\u005c Canary.app/Contents/MacOS/Google\u005c Chrome\u005c Canary \u005c\u000a  --user-data-dir=/Users/smus/.chrome-devtools \u005c\u000a  --enable-experimental-extension-apis \u005c\u000a  --debug-devtools-frontend=/Users/smus/devtools_frontend\u000a</code></pre>\u000a\u000a<p>Let me explain these switches:</p>\u000a\u000a<ul>\u000a<li><p><code>--user-data-dir</code>: specifies a custom profile for your chrome (so you have a\u000afresh profile to deal with and don\u2019t accidentally clobber something important\u000ain your main chrome profile).</p></li>\u000a<li><p><code>--enable-experimental-extension-apis</code>: turns on experimental extension APIs.</p></li>\u000a<li><p>More on the last switch later.</p></li>\u000a</ul>\u000a\u000a<p>By the way, all of chrome\u2019s switches are explained on <a href="http://peter.sh/experiments/chromium-command-line-switches/">this page</a>,\u000aprovided by Peter Beverloo.</p>\u000a\u000a<p>Developer tools extensions are based on a devtools page, which gets loaded when\u000athe devtools open. You can specify this page in the manifest like other pages\u000a(such as background and options pages):</p>\u000a\u000a<pre><code>{\u000a  // manifest start\u000a  "devtools_page": "devtools.html",\u000a  // manifest end\u000a}\u000a</code></pre>\u000a\u000a<p>The API favors extensions that are built around an Audit or a Panel, and you\u000aare free to use the resources API, as well as the rest of the chrome extension\u000afeatures.</p>\u000a\u000a<p>One important note is that the devtools page has very limited access to chrome\u000aextension APIs, so you need to use <a href="http://code.google.com/chrome/extensions/messaging.html">messaging</a> and a background page to\u000aaccess the full chrome API.</p>\u000a\u000a<p>devtools.html:</p>\u000a\u000a<pre><code>// Send request to the background page\u000achrome.extension.sendRequest({}, function(response) {\u000a  // Handle response\u000a});\u000a</code></pre>\u000a\u000a<p>background.html:</p>\u000a\u000a<pre><code>chrome.extension.onRequest.addListener(function(request, sender, callback) {\u000a  // Call some chrome extension APIs. For example,\u000a  chrome.tabs.getSelected(null, function(tab) {\u000a    // Etc... and then callback to devtools.html\u000a    callback({data: tab.id});\u000a  });\u000a});\u000a</code></pre>\u000a\u000a<h3>Debugging devtools extensions</h3>\u000a\u000a<p>When writing Chrome extensions, you have the power of the Chrome developer\u000atools at your disposal. You can debug content scripts by inspecting the page\u000ainto which the Javascript has been injected. Background pages can be viewed by\u000arunning <code>chrome://extensions</code> in developer mode, and clicking on\u000abackground.html for your extension.</p>\u000a\u000a<p>However, when you write a developer tools extension, you rely on this\u000adevtools_page as well as the rest of the extension ecosystem. Debugging this\u000apage can get a bit meta \u2013 just inspect the devtools with the devtools!</p>\u000a\u000a<p><img src="meta.png" alt="meta" /></p>\u000a\u000a<p>Now you can inspect your devtools.html page, and debug away!</p>\u000a\u000a<h2>Changing the product itself</h2>\u000a\u000a<p>In addition to being highly extensible via Chrome extensions, the devtools are\u000aalso pretty easy to modify and tinker with. As you noticed earlier, the tools\u000aare a web application written in Javascript, CSS and HTML, and thus inspectable\u000aby the devtools themselves. This web application is hidden inside Chrome, but\u000aluckily Chrome makes it easy to run a custom version of it. Basically, it takes\u000athree steps:</p>\u000a\u000a<ol>\u000a<li>Download a <a href="http://commondatastorage.googleapis.com/chromium-browser-continuous/index.html">devtools frontend zip</a> (pre-packed version of\u000adevtools). You will need to drill down into a directory for\u000a<a href="http://commondatastorage.googleapis.com/chromium-browser-continuous/index.html?path=Mac/91508/">your platform</a>.</li>\u000a<li>Extract the zip to some directory, <code>DIR</code>.</li>\u000a<li>Invoke Chrome with the <code>--debug-devtools-frontend=DIR</code> switch, specifying\u000athe same directory as in the previous step.</li>\u000a</ol>\u000a\u000a<p>Since the <code>devtools_frontend.zip</code> has some dependencies on Chrome, your zip file\u000aand chrome version should be pretty close, or you may run into problems. You can\u000arun the exact same version by also downloading the <code>chrome-platform.zip</code>.</p>\u000a\u000a<p>Note: the Chromium project has a more detailed <a href="http://code.google.com/chrome/devtools/docs/contributing.html">instruction set</a> for\u000agetting started.</p>\u000a\u000a<h3>Making it your own</h3>\u000a\u000a<p>Now that you have your custom devtools frontend, you can make tweaks to the\u000aJS/CSS/HTML source, and see those changes in your development version of\u000achrome after you manually restart the browser.</p>\u000a\u000a<p>For example, a bunch of people wanted to get rid of the yellow highlight that\u000aappears overlaid in the browser window when hovering over DOM elements. At the\u000atime of writing, the method responsible for this is in <code>inspector.js</code> called\u000a<code>WebInspector.highlightDOMNode</code>. By applying this small patch, you can disable\u000athe default behavior.</p>\u000a\u000a<pre><code>1150,1151c1150\u000a&lt;     // Do not highlight the DOM node.\u000a&lt;     //this.highlightDOMNodeForTwoSeconds(nodeId);\u000a---\u000a&gt;     this.highlightDOMNodeForTwoSeconds(nodeId);\u000a</code></pre>\u000a\u000a<p>You can also easily make cosmetic tweaks to the devtools by changing the CSS.\u000aA lot of the base styles are implemented in <code>inspector.css</code>. For instance, for\u000apresentation purposes, the devtools are a bit small, so I find it useful to\u000aincrease the size of the devtools, which is a one line CSS change.</p>\u000a\u000a<p>There's already a ton of useful features packed into the devtools, but I bet\u000ayou can come up with some more awesomeness to add as an extension or chromium\u000apatch. Let me know if you do!</p>\u000a
p1409
tp1410
Rp1411
sg11
V/extending-chrome-developer-tools
p1412
sg13
Nsg14
I01
sg15
VExtending chrome developer tools
p1413
sg18
V\u000a\u000aChrome\u2019s developer tools (also known as the WebKit inspector) are super useful\u000afor web developers.
p1414
sS'snip'
p1415
g5
(g6
g7
V<p>Make the devtools/webkit inspector even better by writing devtools extensions and submitting Javascript patches.</p>\u000a
p1416
tp1417
Rp1418
sg23
g147
sg31
g1412
sg148
(dp1419
g150
S'Jul'
p1420
sg152
S'July 11, 2011'
p1421
sg154
I7
sg155
S'2011-07-11T09:00:00-00:00'
p1422
sg157
I1310400000
sg158
I2011
sg159
I11
ssg61
g160
sg29
S'extending-chrome-developer-tools'
p1423
sS'categories'
p1424
(lp1425
S'web'
p1426
aS'google'
p1427
aS'chrome'
p1428
asS'posted'
p1429
g166
(S'\x07\xdb\x07\x0b'
p1430
tp1431
Rp1432
ssg32
S'content/posts/2011/extending-chrome-developer-tools/index.md'
p1433
sg34
F1433825653.0
sa(dp1434
g2
(dp1435
g26
g5
(g6
g7
V<p>HTML5 games are here today, and rapidly increasing in complexity. Impressive\u000a<a href="http://madebyevan.com/webgl-water/">demos</a> are <a href="http://www.chromeexperiments.com/">everywhere</a>, and prominent titles like <a href="https://chrome.google.com/webstore/detail/ciamkmigckbgfajcieiflmkedohjjohh">Gun\u000aBros</a> and <a href="http://chrome.angrybirds.com">Angry Birds</a> prove that it's possible to\u000acreate competitive gaming experiences in the browser. Games like these are\u000apossible thanks largely to the modern web stack which includes WebGL, the Web\u000aAudio API, Web Sockets and others.</p>\u000a\u000a<p>Often forgotten, however, is the less sexy story of loading game assets. As the\u000aweb platform progresses and allows for increasingly complex games, game assets\u000a(ex. textures, movies, music and images) grow in size and number, and asset\u000amanagement becomes a sticking point for game developers.</p>\u000a\u000a<p>Let me share with you some truths:</p>\u000a\u000a<ol>\u000a<li>Modern games require gigabytes of assets (textures, movies, etc)</li>\u000a<li>Gamers don't like waiting for their game to load</li>\u000a<li>Browser gamers want to be able to play regardless of internet connectivity</li>\u000a</ol>\u000a\u000a<p>"But wait," you say, "I know! Just use the <a href="http://diveintohtml5.org/offline.html">Application Cache</a>\u000aand yer done!". Not so fast, dear reader... As described below, there are\u000aproblems with this approach, and I propose some solutions.</p>\u000a\u000a<h1>Problems with Application Cache</h1>\u000a\u000a<p>So you've started implementing your awesome asset loading solution using\u000aAppCache. The good news is that there are some useful tools to help you debug\u000aif you have taken this difficult route:</p>\u000a\u000a<ol>\u000a<li>You can get basic information about the site's app cache through the\u000aDeveloper Tools' <a href="http://code.google.com/chrome/devtools/docs/resources.html">resource panel</a>.</li>\u000a<li>You can view (and remove!) caches stored in Chrome by navigating to\u000a<code>chrome://appcache-internals/</code>.</li>\u000a</ol>\u000a\u000a<p>But let me be blunt: <strong>AppCache is annoying to deal with</strong>. If you've made a\u000asmall error in your cache manifest file, you'll quickly hit a brick\u000awall. I ran into an issue where I forgot to include a <code>NETWORK:</code>\u000afallback clause, and wasted hours trying to figure out why all of my\u000aXHRs were responding with status 0.</p>\u000a\u000a<p>Part of what makes AppCache difficult to debug is its very <strong>limited\u000aJavaScript API</strong>. Aside from letting you inspect the status of the entire cache\u000awith <code>window.applicationCache</code> and the <code>updateready</code> event, AppCache doesn't\u000agive us much to work with. There's no way to tell if a particular resource\u000awe're dealing with is cached or not and no programmatic way of clearing the\u000acache.</p>\u000a\u000a<p>AppCache takes a fully transactional approach to asset loading.  Either\u000athe cache is fully loaded, or fully unloaded. Compounding this issue,\u000ait's impossible to resume the download of an AppCache. Thus, if you have\u000aa large amount of assets, your user will have to <strong>wait a long time for\u000aeverything to be loaded</strong>, and if they reload, they will need to restart\u000atheir cache download.</p>\u000a\u000a<p>Lastly, you can only include one cache manifest per page, making it\u000a<strong>impossible to group assets</strong> into multiple bundles. There are hacks that\u000ause multiple iframes with different cache manifests to work around this\u000alimitation (used in <a href="http://chrome.angrybirds.com">Angry Birds</a>), but these are ugly!</p>\u000a\u000a<p>Ultimately, what we need is a well-thought-out Application Cache\u000aenhancement or replacement. Given how quickly web standards bodies move,\u000aI've started thinking a bit about a transitional solution.</p>\u000a\u000a<h1>Designing a game asset loader</h1>\u000a\u000a<p>An ideal asset loading solution requires some of these features:</p>\u000a\u000a<ol>\u000a<li>Granular asset loading. Load all, in groups, or individually.</li>\u000a<li>No asset size limits.</li>\u000a<li>Offline capability.</li>\u000a<li>Programatic control over assets.</li>\u000a</ol>\u000a\u000a<p>It makes sense to group assets in bundles and let the loader take care\u000aof the details. We can even create a custom manifest format, for\u000aexample, in JSON format:</p>\u000a\u000a<pre><code>{\u000a  "assetRoot": "./media/",   // The root of the assets.\u000a  "bundles": [{\u000a    "name": "core",          // A bundle definition.\u000a    "contents": [            // The contents within.\u000a      "theme.mp3",\u000a      "loading.jpg"\u000a    ]\u000a  }, {\u000a    "name": "level1",        // Multiple bundles defined.\u000a    "contents": [            // Note: order implicit since bundles\u000a      "L1/background.jpg",   // objects are stored in an array.\u000a      "L1/blip.wav"\u000a    ]\u000a  }, {\u000a    "name": "level2",\u000a    "contents": [\u000a      "L2/intro.mov"\u000a    ]\u000a  }],\u000a  "autoDownload": false      // If true, download all in order.\u000a}\u000a</code></pre>\u000a\u000a<p>With this manifest format in mind, sample API usage might look like\u000athis:</p>\u000a\u000a<pre><code>// Load the asset library.\u000avar gal = new GameAssetLoader('/path/to/gal.manifest');\u000a\u000a// Read the manifest and other good stuff.\u000agal.init(function() {\u000a  // When ready, download the bundle named 'core'.\u000a  gal.download('core');\u000a});\u000a\u000a// When the core assets are loaded.\u000agal.onLoaded('core', function(result) {\u000a  if (result.success) {\u000a    // Show a loading indicator.\u000a    document.querySelector('img').src = gal.get('loading.jpg');\u000a  }\u000a});\u000a\u000a// Check the progress of the download.\u000agal.onProgress('core', function(status) {\u000a  console.log('status:', status.current/status.total, '%');\u000a});\u000a</code></pre>\u000a\u000a<p>Note that although I've been using the name Game Asset Loader, this\u000aapproach can be used for loading any large non-game assets, such as for\u000aexample, a video or photo gallery.</p>\u000a\u000a<h1>Implementation details</h1>\u000a\u000a<p>Luckily, the modern web stack enables us to create a custom solution to\u000aaddress all of these requirements. By leveraging technologies such as\u000athe HTML5 Filesystem API or Indexed DB, we have programmatic access to\u000aa storage mechanism that we can use to build an asset loader described\u000ahere.</p>\u000a\u000a<p>I used the <a href="http://www.html5rocks.com/en/tutorials/file/filesystem/">Filesystem API</a> to implement a version of the asset\u000aloader. The code requests a large amount of persistent storage using the \u000a<a href="https://groups.google.com/a/chromium.org/group/chromium-html5/msg/5261d24266ba4366?dmode=source">Quota API</a>, which is undocumented, but works anyway:</p>\u000a\u000a<pre><code>// Get quota.\u000astorageInfo.requestQuota(window.PERSISTENT, quota,\u000a  onQuotaGranted, onError);\u000a\u000a// Callback when the quota API has granted quota\u000afunction onQuotaGranted = function(grantedBytes) {\u000a  // Save grantedBytes in the adapter\u000a  that.grantedBytes = grantedBytes;\u000a  // Once quota is grantedBytes, initialize a filesystem\u000a  requestFileSystem(window.PERSISTENT, grantedBytes, onInitFS, onError);\u000a};\u000a\u000a// Callback when the filesystem API has created a filesystem.\u000afunction onInitFS = function(fs) {\u000a  // Create a directory for the root of the assets.\u000a  fs.root.getDirectory(ROOT_DIR, {create: true}, function(dirEntry) {\u000a    that.root = dirEntry;\u000a  }, onError);\u000a};\u000a</code></pre>\u000a\u000a<p>The approach fetches assets with <code>XMLHttpRequest</code>, and stores them in the\u000afilesystem. All files in the filesystem are accessible via the <code>filesystem://</code>\u000aschema, and can be used as any other resource. This filesystem URL is returned\u000aby the library in the <code>get(path)</code> call.</p>\u000a\u000a<p>Note that the writable HTML5 filesystem API is currently available in Chrome\u000aonly, but that it's quite possible to use IndexedDB (supported in Firefox and\u000aIE10) as the data store.</p>\u000a\u000a<h1>Usage scenarios</h1>\u000a\u000a<p>The following section briefly describes what the game asset loader (GAL) does\u000ain several scenarios.</p>\u000a\u000a<p>Player goes to game.com which uses the game asset loader. The game calls\u000a<code>gal.download('core')</code> to download core assets and\u000a<code>gal.download('level1')</code> to load the first level into the player\u2019s\u000afilesystem. While the core bundle loads, the game displays a loading\u000aindicator. Once core is loaded, the game displays the main menu. As soon\u000aas the first level is loaded, the "Play now" button is enabled. As the\u000aplayer plays, the GAL downloads more of the levels in the background.</p>\u000a\u000a<p>Next time, the player tries playing offline. He goes to game.com, whose\u000acode is cached via AppCache, and loads GAL again. This time GAL knows it\u2019s\u000aoffline, looks up its manifest stored on the filesystem and doesn\u2019t try to\u000adownload new assets. The old assets still work though.</p>\u000a\u000a<p>Player is still offline, making good progress, and beats level 5, but\u000athere are no assets downloaded for level 6. Luckily, before starting\u000aeach level, the game calls <code>gal.download('levelBundle')</code> to make sure\u000athat the contents of that bundle are downloaded. The callback returns an\u000aerror and the game displays an error telling the player that he needs to\u000abe online to download the next level.</p>\u000a\u000a<p>So the player goes online and tries again. GAL re-downloads a manifest.\u000aNext, GAL tries re-downloading every asset that the JS requests. Luckily most\u000aof these assets are still in the browser cache, and won't be re-downloaded. The\u000aloader then saves all of the assets in the filesystem, clobbering old files\u000aindiscriminately. (This is bad, and needs to be fixed. Read on!)</p>\u000a\u000a<h1>Future work</h1>\u000a\u000a<p>In particular, re-downloading every asset while online is not desirable\u000abehavior, and we can't always rely on the browser cache for this. For\u000asmaller files, we can probably rely on ETag and Last-Modified headers\u000aand hopefully the browser won't re-download the files. However, the\u000a<strong>asset loader will still overwrite the asset in the filesystem, even if\u000ait's unmodified</strong>. This needs to be fixed. Large files are not likely to be\u000acached by the browser, so we will need more intelligent <strong>caching built into\u000athe asset loader itself</strong>.</p>\u000a\u000a<p>There are other edge cases that need to be considered, such as what happens\u000awhen an <strong>asset is removed from a manifest</strong>. Ideally if this occurs, it\u000a<strong>should also be removed from the filesystem</strong>, but this is not currently\u000aimplemented.</p>\u000a\u000a<p>I'm happy to release the <a href="https://github.com/borismus/game-asset-loader">source</a> under the permissive Apache 2\u000alicense and provide <a href="https://github.com/borismus/game-asset-loader/blob/master/tests/tests.js">unit tests</a> and a <a href="https://github.com/borismus/game-asset-loader/tree/master/tests/game">sample</a> project\u000afor your perusal. It's well documented and should be reasonably easy to\u000aunderstand. I've also made provisions to separate the core library\u000ainterface from the Filesystem-based implementation, making it even\u000aeasier to implement an Indexed DB adapter.</p>\u000a\u000a<p>Before I go, let me reiterate that this library isn't quite production ready,\u000abut a step in the right direction for facilitating real games on the web.\u000aPlease comment below if you have feedback on the idea, or are using the\u000alibrary to write a game of your own!</p>\u000a
p1436
tp1437
Rp1438
sg11
V/game-asset-loader
p1439
sg13
Nsg14
I01
sg15
VLoading large assets in modern HTML5 games
p1440
sg18
V\u000a\u000a\u000aHTML5 games are here today, and rapidly increasing in complexity.
p1441
sS'snip'
p1442
g5
(g6
g7
V<p>An HTML5 filesystem-based approach to loading game assets. Still a work in progress.</p>\u000a
p1443
tp1444
Rp1445
sg23
g147
sg31
g1439
sg148
(dp1446
g150
S'Sep'
p1447
sg152
S'September 22, 2011'
p1448
sg154
I9
sg155
S'2011-09-22T09:00:00-00:00'
p1449
sg157
I1316707200
sg158
I2011
sg159
I22
ssg61
g160
sg29
S'game-asset-loader'
p1450
sS'categories'
p1451
(lp1452
S'web'
p1453
aS'offline'
p1454
aS'games'
p1455
asS'posted'
p1456
g166
(S'\x07\xdb\t\x16'
p1457
tp1458
Rp1459
ssg32
S'content/posts/2011/game-asset-loader/index.md'
p1460
sg34
F1433825663.0
sa(dp1461
g2
(dp1462
g26
g5
(g6
g7
V<p>I've been thinking a bit about native and web applications, and how they can\u000a(or can't) coexist in the future. This topic has been steeping in my head for\u000amonths now, so here is a brain dump of some of my thoughts.<!--more--></p>\u000a\u000a<h2>On hybrid apps</h2>\u000a\u000a<p>Hybrid apps embed a browser, using a web view for part of their UI.\u000aThis is a very flexible definition for a very flexible beast. At one\u000aextreme are native apps that embed a Web View to render a small widget\u000a(written in HTML/CSS/JS) in the UI. On the other hand, you could have a\u000aweb application that provides just a native frame around the content\u000awhich is entirely implemented as a web app. The hybrid app spectrum\u000alooks something like this:</p>\u000a\u000a<pre><code>native &lt;--------------- hybrid apps ---------------&gt; web\u000aMail.app                GMail for iOS              GMail\u000a</code></pre>\u000a\u000a<p>Pieces of the web stack have long been useful as building blocks for\u000aapplications, but lately there has been a bloom in features under the\u000aHTML5 moniker. As a result, many developers are gravitating towards the\u000aright end of the hybrid app spectrum.</p>\u000a\u000a<p>There are several frameworks which target this niche by providing thin native\u000aframes, such as <a href="http://phonegap.com/">PhoneGap</a> and <a href="http://fluidapp.com/">Fluid</a>. There are three benefits to these\u000aframeworks, helping developers to:</p>\u000a\u000a<ol>\u000a<li>Make money: capitalize on AppStore and Market earnings.</li>\u000a<li>Use more features: get access to features not available from the web.</li>\u000a<li>Provide good platform-specific native integration.</li>\u000a</ol>\u000a\u000a<p>The first two are relatively well understood, so for the purposes of this post,\u000aI'm more interested in the third. Android PhoneGap apps are launched from the\u000ahome screen, Fluid Mac apps from spotlight or your dock, etc. Then you can\u000aswitch between these apps as if they were regular OS X applications.</p>\u000a\u000a<p>Even though the browser is a single app, it runs tons of applications\u000alike GMail, twitter, etc. The browser allows people to do two\u000afundamentally different things: view content on the web (sites), and <em>do\u000athings</em>, like listen to music, play games and create documents (apps).\u000aFor a deeper discussion on the distinctions between web apps and web\u000asites, check out James Pearce's article <a href="http://tripleodeon.com/2011/09/of-sites-and-apps/">Of Sites and Apps</a>.</p>\u000a\u000a<p>A pure web application does not rely on platform-specific native code\u000aand runs inside the browser, but still has to rely on a native frame\u000asolution to reap the native integration benefit frameworks like PhoneGap\u000aand Fluid provide.</p>\u000a\u000a<h2>Hybrid operating systems</h2>\u000a\u000a<p>Some browsers today have an explicit notion of apps. Without the proper OS\u000aintegration, this is confusing. Now your device (laptop, tablet, phone, tv) has\u000aapps, one of which is a browser, and the browser has apps too. So to launch an\u000aapp, users have to open the browser, and then launch what they want. This\u000areinforces the distinction between web and native apps in people's heads and\u000amakes for a very inelegant solution.</p>\u000a\u000a<p>One approach is for the OS to make the browser special in the operating system,\u000aallowing it also to manage installed web applications. Another approach is for\u000aoperating systems to implement their SDKs using HTML, CSS and JavaScript, but\u000awith non-standard APIs specific to the platform, such as WebOS. The extreme of\u000athis approach is Chrome OS, where browser and OS are indistinguishable.</p>\u000a\u000a<p>Like apps, operating systems vary in how much they embrace web applications.\u000aHere is an interesting spectrum to think about:</p>\u000a\u000a<pre><code>web agnostic &lt;------------ hybrid OS ------------&gt; apps are webapps\u000aiOS           Windows 8              Web OS               Chrome OS\u000a</code></pre>\u000a\u000a<p>Let me define what I mean by "hybrid OS". A hybrid OS is aware of the\u000apresence of not just native, but also web applications, and provides\u000aways of managing web apps, possibly alongside native ones. Windows 8 is an\u000aexample of a hybrid OS, employing the new Metro UI, while also supporting\u000aWindows 7-style UI. Palm's Web OS, has no native mode at all (except via\u000a<a href="https://developer.palm.com/content/api/dev-guide/pdk/overview.html">PDK plugins</a>), and all apps are web apps (though they require the use of a\u000aspecial set of JavaScript libraries).</p>\u000a\u000a<p>Web apps today are actively used, in some cases, even <a href="http://www.readwriteweb.com/archives/financial_times_proves_html5_can_beat_native_mobil.php">surpassing</a> native\u000aapps in popularity. Unfortunately, users of web apps are stuck in their browser\u000awhich is confusing and limiting, given the current landscape of native\u000aapplications. Hybrid operating systems make explicit the idea that in the end,\u000aweb apps are just apps.</p>\u000a\u000a<h2>Future</h2>\u000a\u000a<blockquote>\u000a  <p>The second goal of PhoneGap is for the project to cease to exist. This is not\u000a  a nihilistic sentiment... -- <a href="http://wiki.phonegap.com/w/page/46311152/apache-callback-proposal">Apache Callback Proposal</a></p>\u000a</blockquote>\u000a\u000a<p>My goal is for web apps to become compelling enough to force OS creators to\u000ahybridize their platforms. In other words, I'd like to see rightward movement\u000ain both the app and OS spectrums. As more hybrid operating systems emerge, we\u000aget closer to PhoneGap's second goal.</p>\u000a
p1463
tp1464
Rp1465
sg11
V/hybrid-operating-systems
p1466
sg13
Nsg14
I01
sg15
VHybrid operating systems
p1467
sg18
V\u000aI've been thinking a bit about native and web applications, and how they can\u000a(or can't) coexist in the future.
p1468
sg4
V<p>I've been thinking a bit about native and web applications, and how they can\u000a(or can't) coexist in the future. This topic has been steeping in my head for\u000amonths now, so here is a brain dump of some of my thoughts.
p1469
sg23
g147
sg31
g1466
sg148
(dp1470
g150
S'Nov'
p1471
sg152
S'November 28, 2011'
p1472
sg154
I11
sg155
S'2011-11-28T09:00:00-00:00'
p1473
sg157
I1322499600
sg158
I2011
sg159
I28
ssg61
g160
sg29
S'hybrid-operating-systems'
p1474
sS'categories'
p1475
(lp1476
S'web'
p1477
asS'posted'
p1478
g166
(S'\x07\xdb\x0b\x1c'
p1479
tp1480
Rp1481
ssg32
S'content/posts/2011/hybrid-operating-systems/index.md'
p1482
sg34
F1433825668.0
sa(dp1483
g2
(dp1484
g26
g5
(g6
g7
V<p>A few weekends ago I went to the jQuery Conference held at the MS campus in\u000aMountain View. And I took notes!</p>\u000a\u000a<p>Overall trends about the jQuery community:</p>\u000a\u000a<ul>\u000a<li><p>People are writing more complex apps on top of jQuery and there\u000ais a widely understood need for MVC frameworks, such as <a href="http://documentcloud.github.com/backbone/">Backbone.js</a>, \u000a<a href="http://knockoutjs.com/">Knockout.js</a> and <a href="http://javascriptmvc.com/">JavaScript MVC</a>.</p></li>\u000a<li><p>Feature detection is important!</p>\u000a\u000a<ul>\u000a<li>Polyfill - replicates standard feature with a compatible API</li>\u000a<li>Shim - provides its own API for a future feature</li>\u000a</ul></li>\u000a<li><p>Serious need for templating systems. Boris Moore showed a very performant\u000ademo of jQuery Templates. Many other templating systems exist as well,\u000alike one built into <a href="http://documentcloud.github.com/underscore/">underscore.js</a> and <a href="http://mustache.github.com/">mustache.js</a>.</p></li>\u000a<li><p>Many new mobile performance tools: <a href="http://www.blaze.io/">blaze.io</a> -- a tool that gives a\u000ageneral overview of a site's performance, <a href="http://pcapperf.appspot.com/">pcapperf</a> -- a web performance\u000aanalyzer that uses tcpdump output from mobile device activity, and <a href="http://jdrop.org/">jDrop</a>\u000a-- a service that lets you capture large amounts of data on your mobile\u000adevice and then analyze it on the desktop web browser.</p></li>\u000a<li><p>People are rallying around <a href="http://jshint.com/">JSHint</a>, a fork of Crockford's <a href="http://www.jslint.com/">JSLint</a>\u000aproject, but with more configurable JavaScript sanitation rules.</p></li>\u000a<li><p>Haters gotta hate. Everybody seems to get a kick out of hating Douglas\u000aCrockford. Give the nice opinionated man a break and go write some\u000aJavaScript.</p></li>\u000a</ul>\u000a\u000a<p>I went to a bunch of talks, and I took the most notes for during this talk:</p>\u000a\u000a<h2>State of jQuery</h2>\u000a\u000a<p>John Resig talked about a bunch of changes to the project structure, largely\u000airrelevant to jQuery library consumers. He also covered some of many jQuery 1.6\u000aimprovements:</p>\u000a\u000a<ul>\u000a<li>Rewrite of <code>attr()</code> and <code>val()</code>. For example, <code>attr('val', false)</code> removes\u000athe attribute</li>\u000a<li>Separate <code>prop()</code> from <code>attr()</code>. Indeed!</li>\u000a<li><code>$('input:focus')</code> gets focused input box across platforms</li>\u000a<li>Significant performance boosts:\u000a<ul>\u000a<li><code>attr()</code> performance ~85% faster, <code>val()</code> ~150% faster, <code>data()</code> ~115%\u000afaster</li>\u000a</ul></li>\u000a<li>Integration with requestAnimationFrame for animations</li>\u000a<li><code>$.map(Object, function)</code> now works (as it does for Arrays)</li>\u000a</ul>\u000a\u000a<p>Pro tip: jQuery automatically parses serialized JSON if it's included as the value \u000aof a HTML5 data attribute. Example: <code>&lt;header data-array="[0,1,2]"&gt;</code> then \u000a<code>$('header').data('array')[1] == 1</code></p>\u000a\u000a<h2>State of jQuery Mobile</h2>\u000a\u000a<p>Mobile matters. 5.3 billion mobile subscriptions (cf. global population of 6.8\u000abillion), 10 billion web-enabled mobile devices.</p>\u000a\u000a<p>John Resig also touched on jQuery Mobile, and then Scott Jehl and Todd Parker went \u000ainto a lot more detail.</p>\u000a\u000a<ul>\u000a<li>Navigation model now uses the <a href="https://developer.mozilla.org/en/DOM/Manipulating_the_browser_history">history API</a> for hash-less URLs.</li>\u000a<li>jQM minified and packed is ~18kb!</li>\u000a<li>Nice gallery of goodness at <a href="http://www.jqmgallery.com/">jQuery Mobile Gallery</a>\u000a<ul>\u000a<li>Including <a href="http://www.barackobama.com/m/">Obama's mobile site</a>!</li>\u000a</ul></li>\u000a<li>Media queries\u000a<ul>\u000a<li>Useful as a browser support cutoff heuristic.</li>\u000a<li>CSS classes added based on media queries, facilitating simpler styles</li>\u000a<li>Uses <a href="https://github.com/scottjehl/Respond">Respond.js</a>, a polyfill for browsers that don't support media queries</li>\u000a</ul></li>\u000a<li>Philosophy: easily brandable cross-device experience</li>\u000a<li>All builtin views are ARIA-enabled</li>\u000a</ul>\u000a\u000a<p>Pro tip: mouse events in some mobile browsers are on a <a href="http://cubiq.org/remove-onclick-delay-on-webkit-for-iphone">300ms delay</a> to allow\u000athe browser to interpret user's gestures. jQuery Mobile includes a fix for\u000athis!</p>\u000a\u000a<h2>Prototyping Tools in jQuery</h2>\u000a\u000a<p>Super useful and informative set of tools!</p>\u000a\u000a<p>MockJAX is a library that simulates a server.</p>\u000a\u000a<ul>\u000a<li>Intercepts and simulates AJAX calls\u000a<ul>\u000a<li>Define a URL structure and a response structure</li>\u000a</ul></li>\u000a<li>Can define responses as a function.</li>\u000a<li>Can simulate error responses.</li>\u000a<li>Useful for unit testing as well!</li>\u000a</ul>\u000a\u000a<p>MockJSON: create fake JSON on demand</p>\u000a\u000a<ul>\u000a<li>A way to generate random-ish JSON</li>\u000a<li>For example, <code>{'age|0-99'}</code> outputs <code>{'age': randint_between_0_and_99}</code></li>\u000a</ul>\u000a\u000a<p>Amplify: abstraction layer for all data</p>\u000a\u000a<ul>\u000a<li>Abstracts away shifting server-side APIs</li>\u000a<li>amplify.request.define can define a data store. </li>\u000a</ul>\u000a\u000a<p>For example:</p>\u000a\u000a<pre><code>amplify.request.define("list", "ajax", {\u000a  url: "/todo/",\u000a  dataType: "json",\u000a  type: "GET"\u000a});\u000a</code></pre>\u000a
p1485
tp1486
Rp1487
sg11
V/jquery-conference
p1488
sg13
Nsg14
I01
sg15
VjQuery conference 2011
p1489
sg18
V\u000a\u000a\u000aA few weekends ago I went to the jQuery Conference held at the MS campus in\u000aMountain View.
p1490
sS'snip'
p1491
g5
(g6
g7
V<p>I went to the Bay Area jQuery Conference and learned some interesting things.</p>\u000a
p1492
tp1493
Rp1494
sg23
g147
sg31
g1488
sg148
(dp1495
g150
S'Apr'
p1496
sg152
S'April 26, 2011'
p1497
sg154
I4
sg155
S'2011-04-26T09:00:00-00:00'
p1498
sg157
I1303833600
sg158
I2011
sg159
I26
ssg61
g160
sg29
S'jquery-conference'
p1499
sS'categories'
p1500
(lp1501
S'web'
p1502
aS'conference'
p1503
asS'posted'
p1504
g166
(S'\x07\xdb\x04\x1a'
p1505
tp1506
Rp1507
ssg32
S'content/posts/2011/jquery-conference/index.md'
p1508
sg34
F1433825672.0
sa(dp1509
g2
(dp1510
g26
g5
(g6
g7
V<p>In mobile development, it's often easier to start prototyping on the desktop\u000aand then tackle the mobile-specific parts on the devices you intend to support.\u000aMulti-touch is one of those features that's difficult to test on the desktop, since\u000amost desktops didn't have multi-touch hardware, and thus desktop browsers don't\u000ahave touch event support. Things are different today (you hear every mother say). \u000aMost new Macs, for example, ship with multi-touch capable input of some sort.\u000aUnfortunately the browsers haven't really caught up yet.</p>\u000a\u000a<p>Enter Fajran Iman Rusadi, who released a <a href="https://github.com/fajran/npTuioClient">npTuioClient</a> NPAPI plugin with a\u000aJavaScript wrapper. Unfortunately this library provides a non-standard API to\u000amulti-touch, which is not ideal for developers that want to write their\u000amulti-touch application on desktop and then run the same code on their mobile\u000adevices without modifications.</p>\u000a\u000a<h2>Browser Patches</h2>\u000a\u000a<p>As HTML5 grows up, browser vendors struggle to stay current up with the growing\u000avariety of specifications. The result is <a href="http://caniuse.com/">uneven feature support</a> across\u000abrowsers and a complex problem for web developers.</p>\u000a\u000a<p>The web development community has rallied around <strong>shims</strong> and <strong>polyfills</strong>\u000afor the solution. These are bizarre terms that I find confusing and so will\u000adefer to <a href="http://remysharp.com/2010/10/08/what-is-a-polyfill/">Remy Sharp to define</a>. The basic idea of both is to fill in\u000afunctionality that's missing in the browser implementation.</p>\u000a\u000a<p>Since we now have a well established <a href="https://dvcs.w3.org/hg/webevents/raw-file/tip/touchevents.html">touch events specification</a> working\u000agroup at the W3C, I wrote <a href="https://github.com/borismus/MagicTouch">MagicTouch.js</a>, a multi-touch polyfill thatlets\u000ayou, the developer, write the same code, test it on your desktop browser and\u000athen, run it on your real device. Totally tubular!</p>\u000a\u000a<p>MagicTouch.js still relies on the npTuioClient plugin, it just creates\u000aspec-compatible touch events. Incidentally, here's how you can trigger custom\u000aDOM events:</p>\u000a\u000a<pre><code>var event = document.createEvent('CustomEvent');\u000a// Initialize the event, make it bubble up and possible to cancel\u000aevent.initEvent('touchstart', true, true);\u000a// Assign properties to the event\u000aevent.touches = touchArray;\u000a...\u000a// Get the element associated with the event\u000avar element = document.elementFromPoint(...);\u000a// Assign the element\u000aevent.target = element;\u000a// Finally, dispatch the event\u000aelement.dispatchEvent(event);\u000a</code></pre>\u000a\u000a<p>Note that this approach to create custom DOM events is not cross-browser\u000acompatible. I only tested in Chrome.</p>\u000a\u000a<h2>Installation</h2>\u000a\u000a<p>Here how to get multi-touch web events working in Chrome for Mac:</p>\u000a\u000a<ol>\u000a<li>Download and install the <a href="https://github.com/fajran/npTuioClient#readme">npTuioClient NPAPI plugin</a>\u000ainto <code>~/Library/Internet Plug-Ins/</code>.</li>\u000a<li>Download the <a href="https://github.com/fajran/tongseng/downloads">TongSeng TUIO tracker</a> for Mac\u2019s MagicPad and start the\u000aserver</li>\u000a<li>Download <a href="https://github.com/borismus/MagicTouch">MagicTouch.js</a> and include both the script and the plugin in your\u000aapp.</li>\u000a</ol>\u000a\u000a<p>The code for this is as follows:</p>\u000a\u000a<pre><code>&lt;head&gt;\u000a  ...\u000a  &lt;script src="/path/to/magictouch.js"&gt;&lt;/script&gt;\u000a&lt;/head&gt;\u000a&lt;body&gt;\u000a  ...\u000a  &lt;object id="tuio" type="application/x-tuio" style="width:0; height:0;"&gt;\u000a    TUIO Plugin failed to load\u000a  &lt;/object&gt;\u000a&lt;/body&gt;\u000a</code></pre>\u000a\u000a<p>...and you're off to the races! Your multi-touch code will now work. Try out\u000athis <a href="https://github.com/borismus/MagicTouch/blob/master/samples/tracker.html">finger tracking demo</a> on either your multi-touch mobile device or your\u000anewly patched desktop browser.</p>\u000a\u000a<h2>Future Steps</h2>\u000a\u000a<p>As you saw, MagicTouch.js takes some effort to set up initially, requires\u000ayou to use an <code>&lt;object&gt;</code> in the HTML, and also needs you to run a separate\u000aprocess for intercepting touch events. While we can't quite get away without\u000ahaving to run another process, we can eliminate the NPAPI plugin by using the\u000a<a href="http://dev.w3.org/html5/websockets/">WebSocket API</a> to communicate to that process.</p>\u000a\u000a<p>If you're interested in multi-touch mobile web development, check out this\u000a<a href="http://www.html5rocks.com/mobile/touch.html">article on html5rocks.com</a>.</p>\u000a
p1511
tp1512
Rp1513
sg11
V/multi-touch-browser-patch
p1514
sg13
Nsg14
I01
sg15
VMulti-touch for your desktop browser
p1515
sg18
V\u000a\u000a\u000aIn mobile development, it's often easier to start prototyping on the desktop\u000aand then tackle the mobile-specific parts on the devices you intend to support.
p1516
sS'snip'
p1517
g5
(g6
g7
V<p>Prototyping multi-touch applications? Simulate spec-compatible touch events without a mobile device.</p>\u000a
p1518
tp1519
Rp1520
sg23
g147
sg31
g1514
sg148
(dp1521
g150
S'May'
p1522
sg152
S'May 2, 2011'
p1523
sg154
I5
sg155
S'2011-05-02T09:00:00-00:00'
p1524
sg157
I1304352000
sg158
I2011
sg159
I2
ssg61
g160
sg29
S'multi-touch-browser-patch'
p1525
sS'categories'
p1526
(lp1527
S'web'
p1528
asS'posted'
p1529
g166
(S'\x07\xdb\x05\x02'
p1530
tp1531
Rp1532
ssg32
S'content/posts/2011/multi-touch-browser-patch/index.md'
p1533
sg34
F1433825680.0
sa(dp1534
g2
(dp1535
g26
g5
(g6
g7
V<p>One day I had some friends over at my house introducing me some cool iPad\u000agames. One of the games was Osmos, developed by an Canadian indie studio called\u000a<a href="http://www.hemispheregames.com">Hemisphere Games</a>. You control a little blob that floats in 2D space, and\u000athe only thing your blob can do is shoot pieces of itself in a given\u000adirection, which propels it in the opposite direction. The rules of the game\u000aare simple, the main rule being that when two blobs collide, the larger one\u000awill consume the smaller one. The rest of the rules pretty much follow directly\u000afrom conservation of mass and energy. <a href="http://www.youtube.com/watch?v=pso6UBicLWU">See for yourself</a> - it's way\u000abetter than it sounds!</p>\u000a\u000a<p>Osmos really caught my attention because of its simple but engaging gameplay,\u000ameditative pace and distinct <strong>lack of multiplayer support</strong>, which struck me\u000aas a potentially very interesting problem to tackle. And so, Osmus (mu for\u000amultiplayer) was born as a browser-based multiplayer Osmos clone.</p>\u000a\u000a<h2>How it works</h2>\u000a\u000a<p>When a browser navigates to the osmus landing page, the server sends the new\u000aclient the current state of its universe, which is composed of blobs with\u000arandomized velocities. At this point, the client can passively watch the game\u000aprogress, but of course, can also join the game as a player controlled blob.\u000aOnce a player joins, he can click or tap (on mobile devices) the canvas to\u000ashoot off a new blob.</p>\u000a\u000a<p>As the game progresses, the server decides when someone (possibly one of the\u000aautonomous blobs) is victorious, at which point, players are notified and the\u000agame is restarted.</p>\u000a\u000a<p>The rest of this post is about some development-related details. I've had to take the game offline because it was too expensive for me to keep running on my VPS.\u000aHowever I did record a video:</p>\u000a\u000a<iframe width="640" height="360" src="http://www.youtube.com/embed/NiPZK3g_i1M" frameborder="0" allowfullscreen></iframe>\u000a\u000a<h2>Game architecture</h2>\u000a\u000a<p>I wrote osmus to be split into distinct, loosely coupled components both to\u000amake the codebase more approachable for other contributors, and to make it easy\u000ato experiment with interchangeable technologies.</p>\u000a\u000a<p><img src="osmus-architecture.png" alt="architecture" /></p>\u000a\u000a<p>Osmus uses a shared game engine that runs in both the browser and on\u000athe server. The engine is a simple state machine whose primary function\u000ais to compute the next game state as a function of time using the rules of\u000aphysics defined within.</p>\u000a\u000a<pre><code>Game.prototype.computeState = function(delta) {\u000a  var newState = {};\u000a  // Compute a bunch of stuff based on this.state\u000a  return newState;\u000a}\u000a</code></pre>\u000a\u000a<p>This is a pretty narrow definition of a <em>Game Engine</em>. In the game developer\u000aworld, what's typically meant by a game engine may include anything from a\u000arenderer, sound player, networking layer, <a href="http://en.wikipedia.org/wiki/Game_engine">etc</a>. In this case, I've\u000amade very clear divisions between these components, and the osmus game core\u000aonly includes just the physical state machine, so that both client and server\u000acan compute the next states and be reasonably synchronized in time.</p>\u000a\u000a<p>The client is composed of three main components: a renderer, input\u000amanager and sound manager. I built a very simple canvas-based renderer\u000athat draws blobs as red circles, and player blobs are green ones. My\u000acolleague <a href="http://twitter.com/kurrik">Arne Roomann-Kurrik</a> wrote an alternative\u000a<a href="https://github.com/mrdoob/three.js/">three.js</a> based renderer with some epic shaders and shadows.</p>\u000a\u000a<p>The sound manager handles playback of both sound effects and background music\u000a(taken from <a href="http://feryl.bandcamp.com/album/8-bit-magic-a-module-chiptune-collection">8-bit Magic</a>). The current implementation uses audio tags,\u000awith two <code>&lt;audio&gt;</code> elements, one for the background music channel, and one for\u000athe sound effects. There are known limitations of this approach, but given the\u000amodularity of my implementation, the sound implementation can be swapped out\u000afor one that uses Chrome's <a href="https://dvcs.w3.org/hg/audio/raw-file/tip/webaudio/specification.html">Web Audio API</a>, for example.</p>\u000a\u000a<p>Finally, the input manager handles mouse events, but can be replaced with one\u000athat uses touch instead, for a mobile version. In the mobile context, it will\u000alikely make sense to use CSS3 transformations instead of canvas, since CSS3 is\u000ahardware accelerated on iOS, while HTML5 canvas still isn't, and WebGL is not\u000aimplemented.</p>\u000a\u000a<p>Speaking of mobile, I was happily surprised that osmus works pretty well on\u000aiPad, especially on an iPad 2 running the latest iOS version. This is really\u000agreat, and one of the tangible benefits of writing games for the open web.</p>\u000a\u000a<h2>Networking is hard</h2>\u000a\u000a<p>From a networking perspective, a game is a rather ambitious project that\u000arequires seamless real-time synchronization between clients. Because of this,\u000abidirectional client-server communication is essential. In the modern web\u000astack, this is provided by <a href="http://dev.w3.org/html5/websockets/">Web Sockets</a> which supply a thin layer\u000aabove TCP and hide a lot of gory details from the implementer. To further hide\u000anetwork stack details, I use the <a href="http://socket.io/">socket.io</a> library, which provides a dead\u000asimple event-driven abstraction for the whole thing. Unfortunately there's\u000acurrently no support for binary data, which would greatly compress message\u000asize, perhaps by an order of two magnitudes in the case of osmos.</p>\u000a\u000a<p>From a bit of research which included this <a href="http://www.youtube.com/watch?v=zj1qTrpuXJ8">nice talk from Rob Hawkes</a>, it\u000abecame clear that to have any sort of shared experience, the simplest model is\u000ato have the true game state on the server, and have clients periodically sync\u000awith it. The main trade off here is synchronization quality vs. network\u000atraffic required.</p>\u000a\u000a<p>On one extreme, a game can be written by having the game logic entirely on the\u000aserver and sending updates (or perhaps even <a href="http://www.onlive.com/">simply screenshots</a>) to\u000athe client at 60 FPS, but this is generally not feasible due to the sheer\u000aamount of bandwidth required for this model. On the opposite extreme, you can\u000aimagine a network architecture in which clients connect, get initial state, and\u000aare then largely autonomous.</p>\u000a\u000a<p>In practice, there is a happy medium in which many multiplayer games fall,\u000awhich means replicating non-trivial code in both the client and the server.\u000aLuckily, now that we're in the ubiquitous JavaScript era, there is no longer a\u000aneed to duplicate functionality, but can instead share code by writing the game\u000aengine in JavaScript, and then running it in both a browser on the client, and\u000ain <a href="http://nodejs.org/">node.js</a> on the server.</p>\u000a\u000a<p>There's a lot more to be written about writing the multiplayer bits of osmus,\u000awhich will hopefully turn into a more detailed article at some point in the\u000afuture.</p>\u000a\u000a<h2>Shared JS modules</h2>\u000a\u000a<p>As mentioned earlier, osmus uses a physics engine that's shared between clients\u000aand the server. One might imagine that sharing JavaScript code between the two\u000awould be a breeze, but it's not that simple.</p>\u000a\u000a<p>Module loaders are a mess. There's the <a href="http://www.commonjs.org/">CommonJS spec</a>,\u000a<a href="http://requirejs.org/">RequireJS library</a> and node.js require system, none of which play\u000anicely together. If you want to share code between client and server (one of\u000athe big wins of JS on the server) without a module loader, you can use this\u000asomewhat hacky pattern:</p>\u000a\u000a<pre><code>(function(exports) {\u000a\u000avar MyClass = function() { /* ... */ };\u000avar myObject = {};\u000a\u000aexports.MyClass = MyClass;\u000aexports.myObject = MyObject;\u000a\u000a})(typeof global === "undefined" ? window : exports);\u000a</code></pre>\u000a\u000a<p>This hack relies on the fact that node.js defines a <code>global</code> object while the\u000abrowser does not. With the hack, node.js <code>require()</code> will be happy, and you can\u000aalso include the file in a <code>&lt;script&gt;</code> tag without polluting your namespace,\u000aassuming of course, that no other JS pollutes your namespace with a\u000a<code>window.global</code> object!</p>\u000a\u000a<p>Unfortunately this approach only works well for one shared module. As soon as\u000ayou have multiple modules depending on each other (via <code>require</code>s in node-land,\u000aand globals in browser-land), the difference between node's namespacing and\u000abrowser's inclusion becomes painfully apparent and requires more hacky\u000aworkarounds.</p>\u000a\u000a<p>Another approach is to use <a href="http://substack.net/posts/24ab8c/browserify-browser-side-require-for-your-node-js">browserify</a> to bundle all JS and emulate\u000arequires in the browser. This approach relies on node.js to serve the\u000agenerated JS, which is not ideal, since static files should be served by a\u000a<a href="http://nginx.net/">webserver</a> optimized for the purpose. However node.js + browserify can\u000abe configured to compile JS that can be served statically without relying on\u000anode to serve it. This approach introduces some overhead:</p>\u000a\u000a<ol>\u000a<li>Extra build step for deploying.</li>\u000a<li>Performance overhead of whatever mechanism browserify uses to support\u000a<code>require()</code> calls.</li>\u000a</ol>\u000a\u000a<p>Overall this approach sounds better to me, and I hope to try it out in a future\u000aversion of osmus.</p>\u000a\u000a<h2>Your turn</h2>\u000a\u000a<p>Today I'm releasing <a href="http://o.smus.com/">osmus</a> as a completely open source HTML5 game. Feel\u000afree to <a href="https://github.com/borismus/osmus">fork it</a> to your heart's content. Oh, and for other game related\u000agoodness, check out this article on <a href="http://www.html5rocks.com/en/tutorials/canvas/performance/">HTML5 canvas performance</a> recently\u000aposted on html5rocks.</p>\u000a
p1536
tp1537
Rp1538
sg11
V/multiplayer-html5-games-with-node
p1539
sg13
Nsg14
I01
sg15
VDeveloping multiplayer HTML5 games with node.js
p1540
sg18
V\u000a\u000a\u000aOne day I had some friends over at my house introducing me some cool iPad\u000agames.
p1541
sS'snip'
p1542
g5
(g6
g7
V<p>Introducing osmus, a multiplayer HTML5 game written with web sockets, canvas and a game engine that runs shared code on the client and server.</p>\u000a
p1543
tp1544
Rp1545
sg23
g147
sg31
g1539
sg148
(dp1546
g150
S'Aug'
p1547
sg152
S'August 30, 2011'
p1548
sg154
I8
sg155
S'2011-08-30T09:00:00-00:00'
p1549
sg157
I1314720000
sg158
I2011
sg159
I30
ssg61
g160
sg29
S'multiplayer-html5-games-with-node'
p1550
sS'categories'
p1551
(lp1552
S'web'
p1553
aS'games'
p1554
aS'multiplayer'
p1555
asS'posted'
p1556
g166
(S'\x07\xdb\x08\x1e'
p1557
tp1558
Rp1559
ssg32
S'content/posts/2011/multiplayer-html5-games-with-node/index.md'
p1560
sg34
F1433825687.0
sa(dp1561
g2
(dp1562
g26
g5
(g6
g7
V<p>Now that the Vancouver Canucks are in the NHL Finals, a special visit to\u000aVancouver is in order. Unfortunately, the recently posted schedule really\u000asucks. There\u2019s just one weekend game, and the rest are spaced in such a way\u000athat I can only hope to see Game 5 (if it happens) and either Game 4 or Game 6\u000a(if it happens) if I manage to work from home half of the week. The question\u000ais: which game to choose? I want to be in Vancouver when the Canucks take the\u000acup!</p>\u000a\u000a<h2>Yay math!</h2>\u000a\u000a<p>Being super nerdy, I decided to throw math at the problem. Brushing up on\u000aprobability, I calculated the odds of a series ending at each game (4-7). For a\u000aseries to end in game 4, one team has to win all four games. The odds of this,\u000agiven a 50% chance for each team to win, is (1/2)^4. Since there are two teams,\u000amultiply by two, resulting in a 1/8 chance. You can do similar calcuations to\u000aget the following odds:</p>\u000a\u000a<pre><code>p(4)   p(5)   p(6)   p(7)\u000a0.125  0.25   0.3125 0.3125\u000a</code></pre>\u000a\u000a<p>Based on this distribution, it seems that game 6 is more likely to see the end\u000aof the series than game 4, but then I would risk missing a game 4 victory.\u000a<a href="http://www.ted.com/talks/arthur_benjamin_s_formula_for_changing_math_education.html">Arthur Benjamin</a> would be proud of me (unless my numbers are wrong).</p>\u000a\u000a<h2>A bit of history</h2>\u000a\u000a<p>Historical data tells a different story. As it turns out, the NHL is really\u000aold, dating back to 1927, but the league switched to best-of-7 scoring in 1939.\u000aWikipedia has a great chronology of NHL playoffs, complete with scores and\u000abrackets.</p>\u000a\u000a<p>I analyzed series scores from all playoff games in the last 25 years (since\u000a1985) by fetching raw wikipedia articles via wget and running them through a\u000a[python script]. Older records can also be found on wikipedia, but they are\u000aburied inside other pages, in harder to parse formats.</p>\u000a\u000a<pre><code>for year in {1985..2011} do\u000a  wget -O ${year} "http://en.wikipedia.org/w/index.php?title=${year}_Stanley_Cup_playoffs&amp;action=raw"\u000adone\u000a</code></pre>\u000a\u000a<p>This yielded the following distribution for playoff games, based on 383\u000amatches:</p>\u000a\u000a<pre><code>p(4)   p(5)   p(6)   p(7)\u000a0.16   0.24   0.33   0.27\u000a</code></pre>\u000a\u000a<p>For finals games only, the distribution is more skewed, probably in part due to\u000aa shortage of data (just NN finals games from the wikipedia page), but finals\u000agame history is probably a better predictor for finals games than all playoffs\u000agames, so this is worthwhile:</p>\u000a\u000a<pre><code>p(4)   p(5)   p(6)   p(7)\u000a0.28   0.24   0.27   0.21\u000a</code></pre>\u000a\u000a<p>Here are all three distributions plotted in a graph:</p>\u000a\u000a<p><img src="nhl-series-odds.png" alt="graph" /></p>\u000a\u000a<h2>Analyzing...</h2>\u000a\u000a<p>The most striking thing about this graph is the huge difference between finals\u000ahistory and stats, especially in the 4 game series scenario. Perhaps some teams\u000ajust buckle under the pressure, while their opponents remain steadfast. Or this\u000ais just a statistically insignificant fluke that can be attributed to having an\u000ainsufficiently large sample of finals matches.</p>\u000a\u000a<p>As expected, the historical distributions are skewed toward the series\u000afinishing in Game 4 due to unequal strength between teams. In other words, if\u000aone team is stronger than another, then the odds of that team winning each game\u000awould be greater, making the series shorter on average.</p>\u000a\u000a<h2>Decision!</h2>\u000a\u000a<p>Armed with numbers, I can decide whether to pick Game 4 or Game 6. Here are the\u000aodds of seeing the Canucks (yeah yeah, maybe the Bruins) win in games 4 or 5,\u000acompared to 5 or 6, as calculated from each of the three methods:</p>\u000a\u000a<pre><code>         Game 4 or 5            Game 5 or 6\u000aStats        38%                    58%\u000aPlayoffs     40%                    57%\u000aFinals       52%                    51%\u000a</code></pre>\u000a\u000a<p>Interesting. Given how annoying it would be to miss a Game 4 victory and how\u000afavorable the finals history-based odds look, I\u2019ll go for 4 and 5. Now to try\u000ato work from home, book flights and bask in Canuck glory!</p>\u000a
p1563
tp1564
Rp1565
sg11
V/nhl-finals-numbers
p1566
sg13
Nsg14
I01
sg15
VCrunching numbers for the NHL finals
p1567
sg18
V\u000a\u000a\u000aNow that the Vancouver Canucks are in the NHL Finals, a special visit to\u000aVancouver is in order.
p1568
sS'snip'
p1569
g5
(g6
g7
V<p>Doing some math to decide when to visit Vancouver in order to make the best of the Canucks final</p>\u000a
p1570
tp1571
Rp1572
sg23
g147
sg31
g1566
sg148
(dp1573
g150
S'May'
p1574
sg152
S'May 31, 2011'
p1575
sg154
I5
sg155
S'2011-05-31T09:00:00-00:00'
p1576
sg157
I1306857600
sg158
I2011
sg159
I31
ssg61
g160
sg29
S'nhl-finals-numbers'
p1577
sS'categories'
p1578
(lp1579
S'math'
p1580
aS'travel'
p1581
asS'posted'
p1582
g166
(S'\x07\xdb\x05\x1f'
p1583
tp1584
Rp1585
ssg32
S'content/posts/2011/nhl-finals-numbers/index.md'
p1586
sg34
F1433825694.0
sa(dp1587
g2
(dp1588
g26
g5
(g6
g7
V<p>Applications that access online services often need to access a user's private\u000adata. Chrome Extensions are no different. OAuth has emerged as the standard way\u000aof letting users share their private resources across sites without having to\u000ahand out their usernames and passwords. There is already a very nice \u000a<a href="http://code.google.com/chrome/extensions/tut_oauth.html">OAuth library for Chrome Extensions</a> that aims to simplify some of \u000athe pains that developers face when authorizing against OAuth endpoints.</p>\u000a\u000a<p>Since this library was written, the OAuth standard enjoyed a version bump\u000a(<a href="http://oauth.net/2/">OAuth 2.0</a>) which greatly simplifies the flow by no longer requiring\u000acryptography in the client. Also, some adventurous companies (notably\u000aGoogle, Facebook and others) have actually implemented OAuth 2.0\u000aendpoints. At the time of writing, OAuth 2.0 is still a draft spec, but is\u000anearing completion, and Chrome Extensions need some love.</p>\u000a\u000a<p>You may be wondering why you even need an OAuth 2.0 library in the first place.\u000aAs Aaron Parecki pointed out in his <a href="http://www.slideshare.net/aaronpk/the-current-state-of-oauth-2">Current State of OAuth 2</a>\u000apresentation at <a href="http://opensourcebridge.org/">Open Source Bridge</a> in Portland, OAuth 2 is very\u000amuch a moving target. The spec is not yet finalized, and there are 16 versions\u000aof it (although the most popular seems to be v10). Also, today's OAuth 2\u000aimplementations diverge from the spec in varying degrees, adding to the\u000adeveloper pain. The reason this library needs to be chrome extension-specific\u000ais that unfortunately Chrome extensions can't directly use the OAuth 2.0\u000aserver-side or client-side flows because they live at <code>chrome-extension://</code>\u000aURLs.</p>\u000a\u000a<p>When writing the OAuth 2.0 library for Chrome extensions, I had some goals in\u000amind:</p>\u000a\u000a<ol>\u000a<li>Support a variety of OAuth 2.0 providers that implement the spec</li>\u000a<li>Allow one app/extension to use multiple different OAuth 2.0 endpoints</li>\u000a<li>Avoid background pages for performance reasons</li>\u000a</ol>\u000a\u000a<h2>The OAuth 2.0 Library</h2>\u000a\u000a<p>There's a bit of setup involved if you'd like to create a Chrome extension that\u000aconnects to an OAuth 2 endpoint. This brief tutorial will guide you through\u000aconnecting to Google's APIs.</p>\u000a\u000a<p>Register your application with an OAuth 2.0 endpoint that you'd like to\u000ause. If it's a Google API you're calling, go to the <a href="https://code.google.com/apis/console/">Google APIs</a> page,\u000acreate your application and note your client ID and client secret. For more\u000ainfo on this, check out the <a href="http://code.google.com/apis/accounts/docs/OAuth2.html">Google OAuth 2.0</a> docs. When you setup your\u000aapplication, you will be asked to provide redirect URI(s). Please provide the\u000aURI that corresponds to the service you're using.</p>\u000a\u000a<p>Here's a table that will come in handy:</p>\u000a\u000a<p><style>\u000a  #impls { margin-left: -100px; }\u000a  #impls td, #impls th { border: 1px solid #999 }\u000a  #impls td { padding: 5px }\u000a</style></p>\u000a\u000a<table id="impls">\u000a  <tr>\u000a    <th>Adapter</th>\u000a    <th>Redirect URI</th>\u000a    <th>Access Token URI</th>\u000a  </tr>\u000a  <tr>\u000a    <td>google</td>\u000a    <td>http://www.google.com/robots.txt</td>\u000a    <td>https://accounts.google.com/o/oauth2/token</td>\u000a  </tr>\u000a  <tr>\u000a    <td>facebook</td>\u000a    <td>http://www.facebook.com/robots.txt</td>\u000a    <td>https://graph.facebook.com/oauth/access_token</td>\u000a  </tr>\u000a  <tr>\u000a    <td>github</td>\u000a    <td>https://github.com/robots.txt</td>\u000a    <td>https://github.com/login/oauth/access_token</td>\u000a  </tr>\u000a</table>\u000a\u000a<h4>Step 1: Copy library</h4>\u000a\u000a<p>You will need to copy the <a href="https://github.com/borismus/oauth2-extensions/tree/master/lib">oauth2 library</a> into your chrome extension\u000aroot into a directory called <code>oauth2</code>.</p>\u000a\u000a<h4>Step 2: Inject content script</h4>\u000a\u000a<p>Then you need to modify your manifest.json file to include a content script\u000aat the redirect URL used by the Google adapter. The "matches" redirect URI can\u000abe looked up in the table above:</p>\u000a\u000a<pre><code>"content_scripts": [\u000a  {\u000a    "matches": ["http://www.google.com/robots.txt*"],\u000a    "js": ["oauth2/oauth2_inject.js"],\u000a    "run_at": "document_start"\u000a  }\u000a],\u000a</code></pre>\u000a\u000a<h4>Step 3: Allow access token URL</h4>\u000a\u000a<p>Also, you will need to add a permission to Google's access token granting URL,\u000asince the library will do an XHR against it. The access token URI can be looked\u000aup in the table above as well.</p>\u000a\u000a<pre><code>"permissions": [\u000a  "https://accounts.google.com/o/oauth2/token"\u000a]\u000a</code></pre>\u000a\u000a<h4>Step 4: Include the OAuth 2.0 library</h4>\u000a\u000a<p>Next, in your extension's code, you should include the OAuth 2.0 library:</p>\u000a\u000a<pre><code>&lt;script src="/oauth2/oauth2.js"&gt;&lt;/script&gt;\u000a</code></pre>\u000a\u000a<h4>Step 5: Configure the OAuth 2.0 endpoint</h4>\u000a\u000a<p>And configure your OAuth 2 connection by providing clientId, clientSecret and\u000aapiScopes from the registration page. The authorize() method may create a new\u000apopup window for the user to grant your extension access to the OAuth2\u000aendpoint.</p>\u000a\u000a<pre><code>var googleAuth = new OAuth2('google', {\u000a  client_id: '17755888930840',\u000a  client_secret: 'b4a5741bd3d6de6ac591c7b0e279c9f',\u000a  api_scope: 'https://www.googleapis.com/auth/tasks'\u000a});\u000a\u000agoogleAuth.authorize(function() {\u000a  // Ready for action\u000a});\u000a</code></pre>\u000a\u000a<h4>Step 6: Use the access token</h4>\u000a\u000a<p>Now that your user has an access token via <code>auth.getAccessToken()</code>, you can\u000arequest protected data by adding the accessToken as a request header</p>\u000a\u000a<pre><code>xhr.setRequestHeader('Authorization', 'OAuth ' + myAuth.getAccessToken())\u000a</code></pre>\u000a\u000a<p>or by passing it as part of the URL (depending on the server implementation):</p>\u000a\u000a<pre><code>myUrl + '?oauth_token=' + myAuth.getAccessToken();\u000a</code></pre>\u000a\u000a<p><strong>Note</strong>: if you have multiple OAuth 2.0 endpoints that you would like to\u000aauthorize with, you can do that too! Just inject content scripts and add\u000apermissions for all of the providers you would like to authorize with.</p>\u000a\u000a<p>I've provided <a href="https://github.com/borismus/oauth2-extensions/tree/master/samples">some sample extensions</a> that use this library to help\u000ayou get started.</p>\u000a\u000a<h2>Varying OAuth implementations</h2>\u000a\u000a<p>Writing this library for one OAuth 2.0 endpoint was pretty straightforward.\u000aThe issues came when branching out to support multiple OAuth 2.0 server\u000aimplementations which comply to various degrees with differing versions of the\u000aspec.</p>\u000a\u000a<p>Facebook was the worst offender here. They claim to be an OAuth 2.0\u000aimplementation in line with v10, but are actually quite far from it. Here are\u000asome of the issues:</p>\u000a\u000a<ul>\u000a<li><p>Token request method is GET instead of POST.</p></li>\u000a<li><p>Token response is some strange form encoded format instead of JSON.</p></li>\u000a<li><p><a href="http://developers.facebook.com/docs/authentication/permissions/">List of scopes</a> (aka "extended permissions") was really hard to\u000afind.</p></li>\u000a<li><p>Apparently to get a user's favorite music, you need the <code>user_likes</code>\u000apermission. Facebook, please <a href="http://forum.developers.facebook.net/viewtopic.php?pid=283691">fix your docs</a>.</p></li>\u000a<li><p>No refresh tokens but they have an offline_access permission which makes your\u000aaccess token expire later. This is ridiculous!</p></li>\u000a</ul>\u000a\u000a<p>Twitter doesn't even have an OAuth 2.0 API. <a href="http://dev.twitter.com/anywhere">@Anywhere</a> does not\u000acount. Some good <a href="http://www.quora.com/Why-isnt-Twitter-implementing-OAuth-2-0-just-like-Facebooks">questions</a> on <a href="http://www.quora.com/When-is-Twitter-going-to-implement-OAuth-2-0">quora</a> about this.</p>\u000a\u000a<p>Still there are a lot of services that <em>DO</em> implement OAuth 2.0, such as\u000aFoursquare, Gowalla, Windows Live, Salesforce, Soundcloud and many others.</p>\u000a\u000a<h2>Extending the Library</h2>\u000a\u000a<p>To mitigate differences between OAuth 2.0 implementations, I implemented the\u000a<a href="http://en.wikipedia.org/wiki/Adapter_pattern">Adapter pattern</a>. Doing this encapsulates protocol differences\u000ain a separate adapter module for each server implementation.</p>\u000a\u000a<p>The library comes with adapters for Google, Facebook and Github. These adapters\u000aare located in the <a href="https://github.com/borismus/oauth2-extensions/tree/master/lib/adapters">adapters directory</a> here. If you would like to\u000acontribute your own adapter, please take a look at the sample adapter and then\u000a<a href="https://github.com/borismus/oauth2-extensions">fork the project</a>, submit a pull request, and I'll try to add\u000ait to the project.</p>\u000a\u000a<p>Also, please let me know if you experience problems using this library and\u000awe'll sort them out! The best way to do this is via <a href="http://github.com/borismus">github</a> or\u000a<a href="http://twitter.com/borismus">twitter</a>.</p>\u000a
p1589
tp1590
Rp1591
sg11
V/oauth2-chrome-extensions
p1592
sg13
Nsg14
I01
sg15
VOAuth 2.0 from chrome extensions
p1593
sg18
V\u000a\u000a\u000aApplications that access online services often need to access a user's private\u000adata.
p1594
sS'snip'
p1595
g5
(g6
g7
V<p>A JavaScript library that handles OAuth 2.0 for you, with a dead simple API. Comes with adapters for Google, Facebook and Github.</p>\u000a
p1596
tp1597
Rp1598
sg23
g147
sg31
g1592
sg148
(dp1599
g150
S'Jul'
p1600
sg152
S'July 7, 2011'
p1601
sg154
I7
sg155
S'2011-07-07T09:00:00-00:00'
p1602
sg157
I1310054400
sg158
I2011
sg159
I7
ssg61
g160
sg29
S'oauth2-chrome-extensions'
p1603
sS'categories'
p1604
(lp1605
S'web'
p1606
aS'google'
p1607
aS'chrome'
p1608
asS'posted'
p1609
g166
(S'\x07\xdb\x07\x07'
p1610
tp1611
Rp1612
ssg32
S'content/posts/2011/oauth2-chrome-extensions/index.md'
p1613
sg34
F1433825699.0
sa(dp1614
g2
(dp1615
g26
g5
(g6
g7
V<p>This post is about video capture in Chrome that doesn't rely on any\u000aexternal dependencies like Flash (no fun), NPAPI (not supported on\u000aChrome OS) and Native Client (not <em>yet</em> supported on Chrome OS).</p>\u000a\u000a<p>I take screenshots all the time for bug reporting, image editing, etc.\u000aOn OS X, this functionality is conveniently built in, and available\u000athrough <code>Command</code> - <code>Shift</code> - <code>4</code>. As a web denizen, I find it very\u000auseful to auto-upload these captures to a remote server, so I wrote this\u000a<a href="https://github.com/borismus/screencapture-www">minimal image uploader</a> which replaces the default behavior\u000aon OS X to capture the screenshot, and also uploads it to a picture\u000ahosting service.</p>\u000a\u000a<p>Taking video capture of various UIs is also immensely useful for showing\u000ademonstrations, complex interactions, and subtle bugs. I recently\u000are-discovered that QuickTime on OS X comes with this functionality built\u000ain. Prior to that I used (paid) ScreenFlow, which also has very nice\u000adimension cropping and time dilation features.</p>\u000a\u000a<p>What if we're on a web-only device, such as a Chromebook running Chrome\u000aOS? There is a still screenshotting API, but capturing video is less\u000atrivial. I've released an extension that captures and play backs video\u000acaptures inside Chrome, and also lets you share stills to Picasa (using\u000athe OAuth 2 <a href="http://smus.com/oauth2-chrome-extensions">extension library</a>). It's available on the\u000a<a href="https://chrome.google.com/webstore/detail/omahgjnmfgeeeoekegajhndkncocoofd">webstore</a>, and the source is on <a href="https://github.com/borismus/chrome-screencast">github</a>. Read on to\u000alearn how it works, and see how you can help.</p>\u000a\u000a<h1>Screenshots in Chrome</h1>\u000a\u000a<p>Chrome provides the <a href="http://code.google.com/chrome/extensions/tabs.html#method-captureVisibleTab"><code>captureVisibleTab</code></a> extension API for taking\u000aa screenshot of a tab. It requires host permissions on the page, but as\u000ausual the <all_urls> permission will enable the API across all pages\u000a(with some exceptions). A few successful extensions, such as\u000a<a href="http://awesomescreenshot.com/">Awesome Screenshot</a>, use this API and allow\u000acropping, annotation and sharing of screen grabs.</p>\u000a\u000a<p>What if you want to capture video of a tab? Chrome provides no\u000apre-existing API for this purpose, however, we can piggyback on the\u000astill screenshot API, executing it repeatedly from the background page\u000afor every frame we want to capture:</p>\u000a\u000a<pre><code>var images = [];\u000avar FPS = 30;\u000avar QUALITY = 50;\u000atimer = setInterval(function() {\u000a  chrome.tabs.captureVisibleTab(null, {quality: QUALITY},\u000a    function(img) {\u000a      images.push(img);\u000a    });\u000a}, 1000 / FPS);\u000a</code></pre>\u000a\u000a<p>As we capture, we store the base64-encoded strings representing video\u000aframes in an array. Once we're done capturing, we can simulate video\u000aplayback by rapidly swapping the images in and out:</p>\u000a\u000a<pre><code>var background = chrome.extension.getBackgroundPage();\u000atimer = setInterval(function() {\u000a  if (currentIndex &gt;= images.length - 1) {\u000a    pause();\u000a    return;\u000a  }\u000a  setIndex(currentIndex + 1);\u000a  updateSliderPosition();\u000a}, 1000 / background.FPS);\u000a</code></pre>\u000a\u000a<p>This approach turns out to be surprisingly efficient, with the extension\u000abeing able to capture at 30 FPS on a MacBook Air, and 10 FPS on a\u000aChromebook without too much noticeable slowdown.</p>\u000a\u000a<p>Note that we rely on a fixed FPS for ease of implementation, however one\u000acould imagine using <code>requestAnimationFrame</code> and tracking the variable\u000aframe rate so that the playback speed is reasonable. However, there are\u000adefinitely precision issues with JavaScript's timers, so this is a much\u000amore challenging approach.</p>\u000a\u000a<p>So we can capture and playback videos inside the browser, but getting it\u000aout of the browser is another matter entirely. As a temporary measure,\u000amy colleague <a href="http://greenido.wordpress.com">Ido Green</a> built a screen stitching service which\u000aencodes multiple images into a movie using ffmpeg. Ideally, of course,\u000awe would encode in the browser. Perhaps a JavaScript video encoder could\u000abe implemented, though the performance may be too poor for practical\u000ause. Alternatively, a ffmpeg Native Client-based approach might be\u000asuitable, especially given that ffmpeg has <a href="http://code.google.com/p/naclports/source/browse/trunk/src/libraries/ffmpeg-0.5/">already been ported</a>.</p>\u000a\u000a<h1>Free ideas</h1>\u000a\u000a<p>There are a few logical next steps for this sample. As already\u000amentioned, encoding video in the browser is a top priority, but there\u000aare a slew of other interesting directions, some of which can be seen as\u000afeatures, and others as separate products.</p>\u000a\u000a<ul>\u000a<li><p>The <code>captureVisibleTab</code> API doesn't track the mouse cursor. This could\u000abe done by injecting an overlay onto the current page and tracking\u000amousemove and click events. This data could then either be drawn onto\u000aa canvas context, or encoded separately as <code>mouseData</code>, and then drawn\u000awith JavaScript at playback time.</p></li>\u000a<li><p>Cropping the video dimensions, modifying the video time schedule\u000a(speedup, slowdown, truncation) and annotation are all desired\u000avideo-editing class features that could be implemented by treating\u000aimages as canvases.</p></li>\u000a<li><p>A compelling use case for this technology is creating screen sharing\u000asessions for demos and presentations. Thus, it would be very useful to\u000astream the video to a server, and broadcast it to multiple clients in\u000areal time. <a href="http://updates.html5rocks.com/2011/08/What-s-different-in-the-new-WebSocket-protocol">Binary websockets</a> are now available in Chrome,\u000aand this could be a great application.</p></li>\u000a<li><p>Audio annotations on screen captures make perfect sense, and are\u000awidely supported by desktop applications. APIs for sound capture have\u000abeen a <a href="http://www.w3.org/2009/dap/">long time coming</a>, but finally we may have an answer\u000avia the <a href="http://www.webrtc.org/">WebRTC</a> ecosystem, and the <code>getUserMedia</code> call.</p></li>\u000a</ul>\u000a\u000a<p>By the way, I've switched to exclusively using Markdown for all of my\u000apublished writing, and wrote an <a href="https://github.com/borismus/markdown-preview">markdown preview</a> for Chrome\u000ato make my life a bit easier.</p>\u000a
p1616
tp1617
Rp1618
sg11
V/screen-capture-for-chrome-os
p1619
sg13
Nsg14
I01
sg15
VScreen video capture for Chrome OS
p1620
sg18
V\u000a\u000a\u000aThis post is about video capture in Chrome that doesn't rely on any\u000aexternal dependencies like Flash (no fun), NPAPI (not supported on\u000aChrome OS) and Native Client (not *yet* supported on Chrome OS).
p1621
sS'snip'
p1622
g5
(g6
g7
V<p>An effective way of creating video screen captures in Chrome without relying on a plugin.</p>\u000a
p1623
tp1624
Rp1625
sg23
g147
sg31
g1619
sg148
(dp1626
g150
S'Oct'
p1627
sg152
S'October 4, 2011'
p1628
sg154
I10
sg155
S'2011-10-04T09:00:00-00:00'
p1629
sg157
I1317744000
sg158
I2011
sg159
I4
ssg61
g160
sg29
S'screen-capture-for-chrome-os'
p1630
sS'categories'
p1631
(lp1632
S'web'
p1633
aS'google'
p1634
aS'chrome'
p1635
asS'posted'
p1636
g166
(S'\x07\xdb\n\x04'
p1637
tp1638
Rp1639
ssg32
S'content/posts/2011/screen-capture-for-chrome-os/index.md'
p1640
sg34
F1332684374.0
sa(dp1641
g2
(dp1642
g26
g5
(g6
g7
V<p>I've been a member of the Chrome developer relations team for the last 9\u000amonths, where one aspect of my job is developer support. I actively\u000amonitor the chromium-extensions@chromium.org list, answering questions\u000aabout developing Chrome extensions. Thankfully, there are many others that\u000ahelp me in this endeavor, both from the Chrome team and from within the\u000adeveloper community itself, notably a certain <a href="http://goo.gl/di3kR">PhistucK</a>, the current\u000arecord holder for the number of messages posted to the list.</p>\u000a\u000a<p>Over the last years, another channel of questions has emerged:\u000a<a href="http://stackoverflow.com/">Stack Overflow (SO)</a>. SO is a great Q&amp;A community for\u000asoftware developers, providing many advantages over ordinary discussion\u000agroups. Google has recognized this and sponsored SO tags,\u000asuch as <a href="http://stackoverflow.com/questions/tagged/google-chrome">google-chrome</a>, and <a href="http://stackoverflow.com/questions/tagged/google-chrome-extension">google-chrome-extension</a>. As I\u000aoutlined in an mail to the group, we are now including SO as\u000aan official Chrome extension developer support channel.</p>\u000a\u000a<!--more-->\u000a\u000a<p>As part of optimizing the support workflow, I wrote a Chrome extension\u000awhich monitors questions with certain tags on SO and other Stack\u000aExchange sites. This extension implements a browser action which gets\u000abadged with the number of unreviewed questions.</p>\u000a\u000a<p>The extension UI uses the same visual style as other Google apps, and\u000ablends well into Chrome. I think it works reasonably well from a design\u000aperspective, both in the options page and in a browser action:</p>\u000a\u000a<p><img src="stack-screenshot.png" alt="screenshot" /></p>\u000a\u000a<p>If you're curious to see how the extension is implemented, the source\u000a<a href="https://github.com/borismus/Question-Monitor-for-Stack-Exchange">is available</a> free of charge! This extension was implemented in\u000a<a href="http://code.google.com/closure/">Closure-style</a> JavaScript.</p>\u000a\u000a<p>If you support developers on Stack Overflow, please <a href="https://chrome.google.com/webstore/detail/bnnkhapbhkejookmhgpgaikfdoegkmdp">install it</a>,\u000agive me constructive feedback and consider monitoring the\u000a<a href="http://stackoverflow.com/questions/tagged/google-chrome-extension">google-chrome-extension</a> tag :)</p>\u000a
p1643
tp1644
Rp1645
sg11
V/stack-exchange-question-notifier
p1646
sg13
Nsg14
I01
sg15
VStack Exchange question notifier
p1647
sg18
V\u000aI've been a member of the Chrome developer relations team for the last 9\u000amonths, where one aspect of my job is developer support.
p1648
sg4
V<p>I've been a member of the Chrome developer relations team for the last 9\u000amonths, where one aspect of my job is developer support. I actively\u000amonitor the chromium-extensions@chromium.org list, answering questions\u000aabout developing Chrome extensions. Thankfully, there are many others that\u000ahelp me in this endeavor, both from the Chrome team and from within the\u000adeveloper community itself, notably a certain <a href="http://goo.gl/di3kR">PhistucK</a>, the current\u000arecord holder for the number of messages posted to the list.</p>\u000a\u000a<p>Over the last years, another channel of questions has emerged:\u000a<a href="http://stackoverflow.com/">Stack Overflow (SO)</a>. SO is a great Q&amp;A community for\u000asoftware developers, providing many advantages over ordinary discussion\u000agroups. Google has recognized this and sponsored SO tags,\u000asuch as <a href="http://stackoverflow.com/questions/tagged/google-chrome">google-chrome</a>, and <a href="http://stackoverflow.com/questions/tagged/google-chrome-extension">google-chrome-extension</a>. As I\u000aoutlined in an mail to the group, we are now including SO as\u000aan official Chrome extension developer support channel.</p>\u000a\u000a
p1649
sg23
g147
sg31
g1646
sg148
(dp1650
g150
S'Nov'
p1651
sg152
S'November 15, 2011'
p1652
sg154
I11
sg155
S'2011-11-15T09:00:00-00:00'
p1653
sg157
I1321376400
sg158
I2011
sg159
I15
ssg61
g160
sg29
S'stack-exchange-question-notifier'
p1654
sS'categories'
p1655
(lp1656
S'web'
p1657
aS'chrome'
p1658
asS'posted'
p1659
g166
(S'\x07\xdb\x0b\x0f'
p1660
tp1661
Rp1662
ssg32
S'content/posts/2011/stack-exchange-question-notifier/index.md'
p1663
sg34
F1433825711.0
sa(dp1664
g2
(dp1665
g26
g5
(g6
g7
V<p>I just returned to San Francisco after having a blast at my second SXSW.\u000aLast time I attended, two years ago, I spent most of my time attending\u000asessions and networking. This year I was mostly on the other side:\u000apreparing talks, staffing booths and organizing events. This year's\u000ahighlight was definitely the LEGO MINDSTORMS hackathon, held as part of\u000a<a href="http://extraordinary-hackers.appspot.com/hackathon.html">The League of Extraordinary Hackers</a> event put on by Google and\u000aothers, that I helped Chris Messina organize. We modeled the event after\u000a<a href="http://breadpig.com/2011/01/18/hack-club-1-total-success/">Breadpig's Hack Club</a>, but scaled it up. The event was so awesome\u000athat here I am blogging about it! </p>\u000a\u000a<p><a href="hackathon.jpg"><img src="hackathon-small.jpg" alt="hackathon image" /></a></p>\u000a\u000a<p>At 7pm, 12 teams of 3-6 people began building MINDSTORMS sumo robots\u000awhich would then compete in a double elimination tournament. We had\u000amixed skill levels, from complete novices to people working at National\u000aInstruments, the company that builds the LEGO MINDSTORMS software. Teams\u000awere given 4 hours to build their robots out of two complete NXT sets\u000aand additional LEGO parts. During battle, their autonomous robots were\u000aplaced back to back, starting with a 180 degree turn before trying to\u000apush each other out of the ring. The battles started at 11:30pm, and\u000awent on for about an hour.</p>\u000a\u000a<iframe title="YouTube video player" width="600" height="480"\u000a  src="http://www.youtube.com/embed/-oELrCuDrrE" frameborder="0"\u000a  allowfullscreen></iframe>\u000a\u000a<p>We had a great turnout of spectators and builders on the third floor of\u000athe <a href="http://www.speakeasyaustin.com/">Speakeasy</a>. The crowd was engaged the whole time, thanks to the\u000alively narration of MC <a href="http://twitter.com/tielure">@tielure</a>, which is quite surprising, given\u000athe amount of party hopping that usually ensues during SXSW. Here's the\u000afinal round (Tiger Blood vs. Beam Me Up):</p>\u000a\u000a<iframe title="YouTube video player" width="600" height="480"\u000a  src="http://www.youtube.com/embed/3EJD6M0zDbw" frameborder="0"\u000a  allowfullscreen></iframe>\u000a\u000a<p>At the end of the day, we made hundreds of geeks happy, and donated\u000asomething on the order of $15,000 of LEGOs to charity. I got to work\u000awith tons of awesome people, including <a href="http://twitter.com/chrismessina">@chrismessina</a>, <a href="http://twitter.com/kcpike">@kcpike</a>,\u000a<a href="http://twitter.com/StevenCanvin">@StevenCanvin</a>, <a href="http://twitter.com/Chrysaora">@Chrysaora</a> and many others. The event even got\u000asome good press on <a href="http://mashable.com/2011/03/13/league-of-extraordinary-hackers/">mashable</a>. </p>\u000a\u000a<p>My only regret is not having my real camera with me, so if you have high\u000aquality footage of the event that I haven't included in this post,\u000aplease let me know. Looking forward to next time!</p>\u000a
p1666
tp1667
Rp1668
sg11
V/sxsw-mindstorms-hackathon
p1669
sg13
Nsg14
I01
sg15
VSXSW LEGO MINDSTORMS hackathon
p1670
sg18
V\u000a\u000aI just returned to San Francisco after having a blast at my second SXSW.
p1671
sS'snip'
p1672
g5
(g6
g7
V<p>My excellent adventures organizing a hackathon-party at South by Southwest.</p>\u000a
p1673
tp1674
Rp1675
sg23
g147
sg31
g1669
sg148
(dp1676
g150
S'Mar'
p1677
sg152
S'March 17, 2011'
p1678
sg154
I3
sg155
S'2011-03-17T09:00:00-00:00'
p1679
sg157
I1300377600
sg158
I2011
sg159
I17
ssg61
g160
sg29
S'sxsw-mindstorms-hackathon'
p1680
sS'categories'
p1681
(lp1682
S'conference'
p1683
aS'physical'
p1684
asS'posted'
p1685
g166
(S'\x07\xdb\x03\x11'
p1686
tp1687
Rp1688
ssg32
S'content/posts/2011/sxsw-mindstorms-hackathon/index.md'
p1689
sg34
F1433825721.0
sa(dp1690
g2
(dp1691
g26
g5
(g6
g7
V<p>I went to UIST 2011 in Santa Barbara and presented our research on\u000a<a href="http://crowdforge.com">CrowdForge</a>.</p>\u000a\u000a<p>Here's a sample of some of the great work that was presented this year, in the\u000a3 research areas that interest me most: crowdsourcing/human computation,\u000amobile physical computing, and music.<!--more--></p>\u000a\u000a<h1>Crowdsourcing</h1>\u000a\u000a<p>Notable work in both workflow-oriented approaches (Jabberwocky, CrowdForge,\u000aPlateMate) and synchronous collaborative approaches (Crowds in seconds,\u000aCollabode, Real-time).</p>\u000a\u000a<h3>PlateMate</h3>\u000a\u000a<ul>\u000a<li>Presents a workflow-based crowdsourcing nutritional analysis from food\u000aphotographs.</li>\u000a<li>Implemented in Django/Python, same as CrowdForge</li>\u000a<li>Found out about CrowdForge 90% of the way into the research.</li>\u000a</ul>\u000a\u000a<p><img src="http://crowdresearch.org/blog/wp-content/uploads/2011/09/dinner.jpg" alt="platemate" /></p>\u000a\u000a<h3>Real-time crowd control</h3>\u000a\u000a<ul>\u000a<li>Nice research from Jeff Bigham's group.</li>\u000a<li>Compares different strategies of merging input from different users.</li>\u000a<li>Reminded me of my <a href="/android-powered-mindstorms">twitter mindstorms project</a>.</li>\u000a</ul>\u000a\u000a<iframe width="560" height="315" src="http://www.youtube.com/embed/P_Tqn-3BF_I" frameborder="0" allowfullscreen></iframe>\u000a\u000a<h3>Crowds in two seconds</h3>\u000a\u000a<ul>\u000a<li>Great research from MIT crowdsourcing folks (Michael Bernstein, Rob Miller)</li>\u000a<li>Retainer: keep workers "on tap" - ready to work by paying $0.30/hour.</li>\u000a<li>Rapid refinement: crowd algorithm to narrow search space to accelerate.</li>\u000a<li>Cool application: take movies, crowdsource the best moment.</li>\u000a</ul>\u000a\u000a<iframe width="560" height="315" src="http://www.youtube.com/embed/9IICXFUP6MM" frameborder="0" allowfullscreen></iframe>\u000a\u000a<h3>The Jabberwocky programming environment</h3>\u000a\u000a<ul>\u000a<li>Really ambitious MTurk like project.</li>\u000a<li>Combine different worker types from different spheres (social groups, paid workers, machines)</li>\u000a<li>Full runtime stack (Dog high level language, ManReduce low level language, Dormouse runtime)</li>\u000a<li>Great potential to mix different spheres of workers, but a rather intimidating project... scope creep!</li>\u000a</ul>\u000a\u000a<p>More in the <a href="http://kamvar.org/assets/papers/jabberwocky.pdf">Jabberwocky</a> paper.</p>\u000a\u000a<h3>Real-time collaborative coding in a web IDE</h3>\u000a\u000a<ul>\u000a<li>More awesome work from MIT (Max Goldman, Greg Little, Rob Miller)</li>\u000a<li>Web-based Java EtherPad editor for collaboration</li>\u000a<li>Main problem: others leave code in semi-working state. When to sync?</li>\u000a<li>Idea: automatic error-aware integration. Auto-sync code when it compiles (or - tests pass)</li>\u000a</ul>\u000a\u000a<p>More on the <a href="http://groups.csail.mit.edu/uid/collabode/">Collabode</a> site.</p>\u000a\u000a<h1>Mobile physical computing</h1>\u000a\u000a<p>Interesting work in mobile virtual and projected UI spaces.</p>\u000a\u000a<h3>PocketTouch</h3>\u000a\u000a<ul>\u000a<li>Great work from Microsoft Research to let you use your phone through fabric (while in the pocket)</li>\u000a<li>Since orientation in-pocket is unclear, there's an orientation setting gesture (not sold on this)</li>\u000a<li>Unfortunately typical front pocket jeans work rather poorly.</li>\u000a</ul>\u000a\u000a<iframe width="560" height="315" src="http://www.youtube.com/embed/fHSDpE0kTag" frameborder="0" allowfullscreen></iframe>\u000a\u000a<h3>Multi-user Interaction with Handheld Projectors</h3>\u000a\u000a<ul>\u000a<li>Most projector-based systems require a fixed place. This one is fully mobile.</li>\u000a<li>System produces visible projection (for image) and invisible projection (for data)</li>\u000a<li>Camera tracks location of nearby projections</li>\u000a<li>Cool applications: virtual boxing, content transfer, 3D model viewing.</li>\u000a</ul>\u000a\u000a<p><img src="http://www.disneyresearch.com/_images/459-00B516CF.jpg" alt="concept" /></p>\u000a\u000a<p>To get a better sense of the project, take a look at <a href="http://www.disneyresearch.com/research/projects/hci_sidebyside_drp.htm">this video</a>.</p>\u000a\u000a<h3>Imaginary phone</h3>\u000a\u000a<ul>\u000a<li>Lets you control a mobile phone device without taking it out of your pocket.</li>\u000a<li>Maps iPhone controls to the palm of your hand similar to <a href="http://en.wikipedia.org/wiki/Guidonian_hand">Guidonian hand</a></li>\u000a<li>Touchscreen progression: fingers replaced styli. Next step: palm replaces phone (bit of a stretch)</li>\u000a</ul>\u000a\u000a<iframe width="560" height="315" src="http://www.youtube.com/embed/aCARtauIS50" frameborder="0" allowfullscreen></iframe>\u000a\u000a<h3>OmniTouch</h3>\u000a\u000a<ul>\u000a<li>Shoulder mounted depth camera/projector.</li>\u000a<li>Works for all sorts of surfaces. Heavy math for tracking and projecting.</li>\u000a<li>Tracks "hover" (haven't seen this notion before) and "touch" states.</li>\u000a</ul>\u000a\u000a<iframe width="560" height="315" src="http://www.youtube.com/embed/Pz17lbjOFn8" frameborder="0" allowfullscreen></iframe>\u000a\u000a<h1>Music</h1>\u000a\u000a<p>I had the pleasure of hearing <a href="https://ccrma.stanford.edu/~ge/">Ge Wang</a> speak\u000aabout some of his older projects, including <a href="http://www.youtube.com/watch?v=RhCJq7EAJJA">Ocarina</a>,\u000a<a href="http://www.youtube.com/watch?v=KetWJi0zou0">Leaf Trombone</a>, the Stanford Mobile orchestra, and others. Notably I\u000ahadn't really seen the <a href="http://chuck.cs.princeton.edu/doc/language/">ChucK language in action</a>, and would be\u000ainterested in seeing if the <a href="http://www.html5rocks.com/en/tutorials/webaudio/intro/">Web Audio API</a> could support this sort of thing.\u000aBrowser-based ChucK, anyone?</p>\u000a\u000a<p>The last piece of research that I really enjoyed was called "onNote: Playing\u000aprinted music scores as a musical instrument". The idea here was to use OMR\u000atechniques for markerless tracking of sheet music using positioning and using a\u000afinger for pointing. The other neat application here was to support\u000acompositional remixing by literally cutting up sheet music and splicing it back\u000atogether.</p>\u000a\u000a<iframe width="420" height="315" src="http://www.youtube.com/embed/fGOk16Cnq7c" frameborder="0" allowfullscreen></iframe>\u000a\u000a<p>This was my first UIST, and though I'm unlikely to have new research to submit\u000afor UIST 2012, I'm seriously considering going anyway, just to stay on top of\u000athe great work that this vibrant community generates.</p>\u000a
p1692
tp1693
Rp1694
sg11
V/uist-2011
p1695
sg13
Nsg14
I01
sg15
VUIST 2011 greatest hits
p1696
sg18
V\u000a\u000aI went to UIST 2011 in Santa Barbara and presented our research on\u000a[CrowdForge][crowdforge].
p1697
sg4
V<p>I went to UIST 2011 in Santa Barbara and presented our research on\u000a<a href="http://crowdforge.com">CrowdForge</a>.</p>\u000a\u000a<p>Here's a sample of some of the great work that was presented this year, in the\u000a3 research areas that interest me most: crowdsourcing/human computation,\u000amobile physical computing, and music.
p1698
sg23
g147
sg31
g1695
sg148
(dp1699
g150
S'Oct'
p1700
sg152
S'October 20, 2011'
p1701
sg154
I10
sg155
S'2011-10-20T09:00:00-00:00'
p1702
sg157
I1319126400
sg158
I2011
sg159
I20
ssg61
g160
sg29
S'uist-2011'
p1703
sS'categories'
p1704
(lp1705
S'research'
p1706
aS'conference'
p1707
asS'posted'
p1708
g166
(S'\x07\xdb\n\x14'
p1709
tp1710
Rp1711
ssg32
S'content/posts/2011/uist-2011/index.md'
p1712
sg34
F1433825725.0
sa(dp1713
g2
(dp1714
g26
g5
(g6
g7
V<p>I've used <a href="http://wordpress.com/">Wordpress</a> for a couple of years now as my web publishing platform of\u000achoice. I customized it a little bit, and made a <a href="/new-design">custom theme</a> for it. But \u000anothing is perfect, and neither is wordpress. As a result of relatively heavy\u000ause, I've collected a list of things I don't much like about it:</p>\u000a\u000a<ul>\u000a<li>Can't upload source code since it restricts allowed file types.</li>\u000a<li>Can't easily edit files with a regular text editor.</li>\u000a<li>Painful theme customization.</li>\u000a<li>PHP makes it unpleasant to extend.</li>\u000a<li>Constant security upgrades required.</li>\u000a</ul>\u000a\u000a<p>All of these things separately are perhaps small and insignificant, but\u000atogether add up to create a certain barrier to blogging.</p>\u000a\u000a<h2>Static files</h2>\u000a\u000a<p>A blog is mostly static. Why should my article be stored in a database? It's hardly \u000aever queried or searched. Comments are perhaps the exception here, but services\u000alike <a href="http://disqus.com/">Disqus</a> implement commenting with embeddable JavaScript. And in general,\u000adynamic parts can be handled in client-side code.</p>\u000a\u000a<p>So I looked to static site generators. There's about <a href="http://stackoverflow.com/questions/186290/best-static-website-generator">a million</a> of these,\u000athe most prominent of which is <a href="https://github.com/mojombo/jekyll">Jekyll</a>, written by the github co-founder. \u000aUltimately I chose <a href="https://github.com/lakshmivyas/hyde/">Hyde</a> for it's use of <a href="http://docs.djangoproject.com/en/dev/ref/templates/builtins/">Django templates</a> and Python.</p>\u000a\u000a<h2>Migrating the old blog</h2>\u000a\u000a<p>The old blog was written in HTML, but I wanted to switch to a format that can\u000abe written faster, such as Markdown, Textile, or many others. To do this I used\u000aa tool called exitwp, which parses the Wordpress export and generates Markdown\u000afiles appropriate for Jekyll. I <a href="https://github.com/borismus/exitwp">forked exitwp</a> and hacked it to generate\u000afiles better suited to Hyde. Thanks <a href="http://thomas.jossystem.se/">Thomas Frssman</a> for exitwp.</p>\u000a\u000a<p>I did spent a fair amount of time tweaking the export, replacing Smart YouTube URLs\u000awith real YouTube embeds, and eliminating Syntax Highlighter markup.</p>\u000a\u000a<h2>Customizing hyde</h2>\u000a\u000a<p>Hyde is basically built on top of Django templates, so is customizable with\u000atemplate tags and filters, written in Python. It comes with a bunch of them,\u000aincluding ones that parse markdown and other structured text formats into HTML.</p>\u000a\u000a<p>Surprisingly I didn't need to customize Hyde much, although I did fix a couple of \u000abugs, which the author, <a href="http://ringce.com/">Lakshmi Vyas</a> accepted. Anyway, I feel much more\u000afuture proof now than with Wordpress, since I can easily write extensions in\u000aPython, or in the worst case take my Markdown-formatted blog posts and easily\u000aexport them to many different formats.</p>\u000a\u000a<h2>Dynamic stuff</h2>\u000a\u000a<p>The bit of custom JS for the site layout is written using jQuery.</p>\u000a\u000a<p>I use the excellent <a href="http://softwaremaniacs.org/soft/highlight/en/">highlight.js</a> plugin to highlight code snippets. The\u000ajQuery <a href="http://timeago.yarp.com/">timeago</a> plugin does a great job of converting absolute dates (April\u000a1, 2010) to relative dates (about a year ago). Finally, Disqus powers comments,\u000awhich I migrated from the wordpress database.</p>\u000a\u000a<h2>New design</h2>\u000a\u000a<p>In my great wisdom, I decided that since there's so much flux in the blog, why not\u000aadd some more chaos by switching to a new layout too? Here's the old layout:</p>\u000a\u000a<p><img src="old-design.png" alt="old" /></p>\u000a\u000a<p>I recently heard from sage advice from some experienced writers. Among things\u000athat stuck were:</p>\u000a\u000a<ul>\u000a<li>Optimal posting time is 9am PST</li>\u000a<li>Writing personal posts is OK sometimes</li>\u000a<li>Show personal information in the sidebar</li>\u000a</ul>\u000a\u000a<p>The last part is expressed in the new design. Overall the redesign was an\u000aexercise in typography, CSS3 features and <a href="http://www.alistapart.com/articles/responsive-web-design/">responsive layout</a>. </p>\u000a\u000a<p><a href="http://www.google.com/webfonts">Google fonts</a> is a nice collection of web fonts. I decided to switch from\u000aMyriad (used on borismus.com) to a serif for the main body for a\u000achange. I ultimately went with <a href="http://www.google.com/webfonts/list?family=PT+Serif">PT Serif</a>, which seemed like a nice\u000aimprovement to Georgia, which is probably my favorite serif web font.</p>\u000a\u000a<p>I used a bunch (too many?) CSS3 features on the new site. Most of these are\u000aCSS box-shadows, gradients, transformations and transitions. I used <a href="http://sass-lang.com/">SCSS</a>.</p>\u000a\u000a<p>Also, following the responsive layout philosophy, I wanted the site to scale well\u000ato a number of different resolutions. For example, the sidebar moves to the end of the \u000aarticles if the window is too narrow. Also, in the <a href="/projects">projects page</a>, the number of \u000acolumns of projects is flexible. This is achieved through <a href="http://www.w3.org/TR/css3-mediaqueries/">CSS media queries</a>.</p>\u000a\u000a<p>For posterity, here is the new layout at the time of writing:</p>\u000a\u000a<p><img src="new-design.png" alt="new" /></p>\u000a\u000a<h2>New domain</h2>\u000a\u000a<p>I also recently bought <a href="http://smus.com">smus.com</a> from a fellow in Germany, and will be migrating \u000amy site there. I'll keep the <a href="http://borismus.com">borismus.com</a> blog intact for a while, but will \u000aplace an annoying banner there, and disable commenting. I guess I should also \u000aswitch the feedburner feed to point to smus.com as well.</p>\u000a\u000a<p>Not long ago, I wrote about how switching from webfaction and apache to slicehost\u000aand NGINX greatly <a href="/lightweight-wordpress-on-slicehost/">enhanced page load time</a>. Well, this seems to have happened again.\u000aOver the same time period, the average response time of my wordpress instance was \u000a<strong>934ms</strong>:</p>\u000a\u000a<p><img src="old-perf.png" alt="old perf" /></p>\u000a\u000a<p>Over the same time frame, my static blog, hosted on the same machine, responded on \u000aaverage in <strong>371ms</strong>:</p>\u000a\u000a<p><img src="new-perf.png" alt="new perf" /></p>\u000a\u000a<p>Oh, finally, the site is fully open source <a href="http://github.com/borismus/smus.com">on github</a>. </p>\u000a\u000a<h2>Thanks</h2>\u000a\u000a<p>Thanks to a bunch of people:</p>\u000a\u000a<ul>\u000a<li>Sol, Kat and Jon for giving useful design feedback!</li>\u000a<li><a href="http://nlevin.com/">Noah Levin</a> for awesome CSS3 tweaks</li>\u000a<li>Steve Losh for his <a href="http://stevelosh.com/blog/2010/01/moving-from-django-to-hyde/">Hyde migration</a> write-up</li>\u000a</ul>\u000a\u000a<p>You, for reading this post and continuing to read this blog. Until next time!</p>\u000a
p1715
tp1716
Rp1717
sg11
V/wordpress-to-hyde
p1718
sg13
Nsg14
I01
sg15
VFrom Wordpress to Hyde
p1719
sg18
V\u000a\u000a\u000aI've used [Wordpress][] for a couple of years now as my web publishing platform of\u000achoice.
p1720
sS'snip'
p1721
g5
(g6
g7
V<p>or, how I stopped worrying and switched this blog to a Django static site generator.</p>\u000a
p1722
tp1723
Rp1724
sg23
g147
sg31
g1718
sg148
(dp1725
g150
S'Apr'
p1726
sg152
S'April 20, 2011'
p1727
sg154
I4
sg155
S'2011-04-20T09:00:00-00:00'
p1728
sg157
I1303315200
sg158
I2011
sg159
I20
ssg61
g160
sg29
S'wordpress-to-hyde'
p1729
sS'categories'
p1730
(lp1731
S'web'
p1732
asS'posted'
p1733
g166
(S'\x07\xdb\x04\x14'
p1734
tp1735
Rp1736
ssg32
S'content/posts/2011/wordpress-to-hyde/index.md'
p1737
sg34
F1332684374.0
sa(dp1738
g2
(dp1739
g26
g5
(g6
g7
V<p>When developing in a particular environment, say Android or Cocoa, we\u000aare subconsciously aware that the APIs are essentially fixed and beyond\u000aour control. There is no effective mechanism to go and tell Apple that\u000asome method is poorly named, or tell the Android team how much you wish\u000athe Audio APIs were nicer to use.</p>\u000a\u000a<p>The web, however, is built by many different companies and individuals,\u000agiving consumers of the platform (web developers) a unique chance to\u000aalso become contributors to its evolution. Rather than griping about how\u000abroken something on the web is, remember that you can play a part in\u000afixing it!</p>\u000a\u000a<!--more-->\u000a\u000a<h2>Current flow: "top down"</h2>\u000a\u000a<p>Today, features get added to the web platform as a result of competition\u000abetween browsers and some standardization work. The web developer has\u000alittle say in the process until perhaps the very end. Worse, many spec\u000aauthors and browser engineers are not web developers, so they have no\u000aintuition or experience as to what the web platform actually needs.\u000aHere's how it works today:</p>\u000a\u000a<ol>\u000a<li>Browser vendor proposes way for solving a problem and implements it\u000ain their browser. (browser vendor)</li>\u000a<li>In some cases it's concurrently introduced as a W3C specification.\u000a(spec author)</li>\u000a<li>Other browser vendors implement the feature. (browser vendor)</li>\u000a<li>Much later, someone implements a JS wrapper on top of feature to make\u000athe feature practical for web developers to use. (web developer)</li>\u000a</ol>\u000a\u000a<p>Some of the negative features of this flow are:</p>\u000a\u000a<ul>\u000a<li>Takes a long time before any developer feedback is received.</li>\u000a<li>New features have partial support for a very long time.</li>\u000a<li>Innovation is very browser vendor/spec writer centric.</li>\u000a<li>Very slow cycle overall.</li>\u000a<li>Little room for incremental improvements.</li>\u000a</ul>\u000a\u000a<h2>Ideal flow "forward polyfills"</h2>\u000a\u000a<p>Rather than sticking to the top down flow, web developers can be\u000aincluded in the loop. While forming organizations like\u000a<a href="http://www.w3.org/community/coremob/">CoreMob</a> might be a good first step, we can have a more direct\u000aimpact as well. The following alternative flow can work pretty well in\u000aconjunction with the status quo, described above.</p>\u000a\u000a<p>The basic idea is to hit the ground running with a prototype developed\u000ain JavaScript, get early feedback and then propose to W3C when you are\u000aready.</p>\u000a\u000a<ol>\u000a<li>Propose a sane API that developers can use &amp; implement it on top of\u000aexisting API(s). (web developer)</li>\u000a<li>Solicit adoption and iterate on feedback. (web developer)</li>\u000a<li>Push through standardization. (spec author)</li>\u000a<li>Implement natively in browsers. (browser vendor)</li>\u000a</ol>\u000a\u000a<p>This flow has many benefits over the first one.</p>\u000a\u000a<h3>Prototypes have a tighter feedback loop</h3>\u000a\u000a<p>By releasing a JavaScript library quickly, you immediately get feedback\u000afrom developers. As you tweak your API, you can incorporate real world\u000afeedback. This can be a very short cycle, especially with the help of\u000atools like github, where consumers of your library can give you\u000asuggestions and fix your bugs!</p>\u000a\u000a<h3>Useful out of the gate</h3>\u000a\u000a<p>By the time you are ready to propose your library to be standardized as\u000aa core web specification, you already have a reference implementation\u000awith unit tests. These can then be used by browser vendors as they\u000aimplement the specification. This is much more useful to have than\u000athe abstract pseudocode that the W3C specs currently provide, and should\u000alead to a more consistent set of browser implementations.</p>\u000a\u000a<p>Even more importantly, you know that what you are proposing is something\u000auseful because that real web developers have already used it! This is\u000aalready better than many APIs currently offered by the web platform,\u000asome of which has seen little developer uptake.</p>\u000a\u000a<h3>A polyfill at launch</h3>\u000a\u000a<p>By the time the first browsers start implementing the spec, you already\u000ahave a polyfill for developers to use &mdash; the library you wrote in\u000astep 1. This polyfill should feature detect for the presence of the API,\u000aand if not present, load the functionality via the JavaScript library.</p>\u000a\u000a<h2>Limitation: some things need more than JavaScript</h2>\u000a\u000a<p>Not all features can be implemented in JavaScript. In fact, some of the\u000amost exciting ones require browser-level innovation because JavaScript\u000ais heavily sandboxed (eg. contacts API, access to new sensors). Still,\u000amany new features <strong>can</strong> be implemented, such as new layout models (eg.\u000aflexbox, new storage APIs, responsive image solutions).</p>\u000a\u000a<p>Even if you can't provide a JavaScript implementation, there are ways to\u000aprototype these features to see if they are useful or not, giving\u000asome of the benefits describe above. PhoneGap and other WebView wrappers\u000acan be easily instrumented with plugins that bring native functionality\u000ato the web. More ambitiously, if you can stomach the learning curve,\u000acontribute to open source browsers like WebKit/Chromium and Firefox!</p>\u000a\u000a<h2>Limitation: JS library to spec is uncharted territory</h2>\u000a\u000a<p>One of the missing pieces in the "forward polyfill" approach I advocate\u000afor in this post is that it can still be very difficult to get your\u000avoice heard without being a browser vendor or spec author.</p>\u000a\u000a<p>Anecdotally I've sent a few messages to the <a href="http://goo.gl/hrBvS">www-style@\u000alist</a>, without many tangible results. Other web developers\u000asuch as, roughly following the tactic I described earlier, have\u000aexperienced similar frustrations in the recent discussions <a href="http://www.webmonkey.com/2012/05/browsers-at-odds-with-web-developers-over-adaptive-images/">regarding\u000ahigh DPI images</a>.</p>\u000a\u000a<h2>Conclusion</h2>\u000a\u000a<p>This post is a summary of my thinking around most of my recent web\u000adevelopment work: projects like <a href="https://github.com/borismus/pointer.js">pointer.js</a>, <a href="https://github.com/borismus/physical-units">physical units</a>\u000aand the <a href="https://github.com/borismus/srcset-polyfill/">srcset-polyfill</a> all have the same mission in mind: to\u000acease to exist by having their functionality replaced by the web\u000aplatform itself. The most notable example of such a project is PhoneGap,\u000awhich explicitly states self destruction as its now famous <a href="http://phonegap.com/2012/05/09/phonegap-beliefs-goals-and-philosophy/">second\u000agoal</a>.</p>\u000a\u000a<p>It may seem strange to develop projects with such a nihilistic purpose.\u000aBut keep in mind, that in this case it's not just the code that counts,\u000abut the ideas behind it. We would be better off with many of these\u000athings in the web platform: an implementation of pointer events,\u000aphysical units, and a good set of device APIs.</p>\u000a\u000a<p>I'm not arguing that the current spec-and-browser-first flow should be\u000areplaced. I'm merely suggesting that there is an alternative out\u000athere that involves web developers; one worthy of exploration by\u000athose of us that aren't in the business of writing browsers but\u000acare enough about the web platform to try to make a change.</p>\u000a\u000a<p><strong><em>UPDATE</em></strong>: At Paul Irish's great suggestion, please post your\u000acomments in <a href="https://plus.google.com/115694705577863745195/posts/fMyCkBYvHRi">this Google+ thread</a>.</p>\u000a
p1740
tp1741
Rp1742
sg11
V/how-the-web-should-work
p1743
sg13
Nsg14
I01
sg15
VHow the web should work
p1744
sg18
VWhen developing in a particular environment, say Android or Cocoa, we\u000aare subconsciously aware that the APIs are essentially fixed and beyond\u000aour control.
p1745
sg4
V<p>When developing in a particular environment, say Android or Cocoa, we\u000aare subconsciously aware that the APIs are essentially fixed and beyond\u000aour control. There is no effective mechanism to go and tell Apple that\u000asome method is poorly named, or tell the Android team how much you wish\u000athe Audio APIs were nicer to use.</p>\u000a\u000a<p>The web, however, is built by many different companies and individuals,\u000agiving consumers of the platform (web developers) a unique chance to\u000aalso become contributors to its evolution. Rather than griping about how\u000abroken something on the web is, remember that you can play a part in\u000afixing it!</p>\u000a\u000a
p1746
sg23
g147
sg31
g1743
sg148
(dp1747
g150
S'Sep'
p1748
sg152
S'September 12, 2012'
p1749
sg154
I9
sg155
S'2012-09-12T09:00:00-00:00'
p1750
sg157
I1347465600
sg158
I2012
sg159
I12
ssg61
g160
sg29
S'how-the-web-should-work'
p1751
sS'posted'
p1752
g166
(S'\x07\xdc\t\x0c'
p1753
tp1754
Rp1755
ssg32
S'content/posts/2012/how-the-web-should-work.md'
p1756
sg34
F1347792976.0
sa(dp1757
g2
(dp1758
g26
g5
(g6
g7
V<p>Although I love Apple's design aesthetic and ability to consistently\u000achurn out amazing hardware, I'm never quite comfortable fully embracing\u000ait. The reasons have to do with platform fertility, or how well suited a\u000aplatform is to incremental platform innovations and new platform\u000acreation. Onward!</p>\u000a\u000a<!--more-->\u000a\u000a<h2>Personal computing</h2>\u000a\u000a<p>The evolution of personal computing is generally accepted to look something\u000alike the following:</p>\u000a\u000a<pre><code>Mainframe -&gt; Minicomputer -&gt; Personal computer\u000a</code></pre>\u000a\u000a<p>The above relationship can be interpreted in at least two ways:</p>\u000a\u000a<ol>\u000a<li>The former was disrupted by the latter.</li>\u000a<li>The former was used to design the latter.</li>\u000a</ol>\u000a\u000a<p>Taking the first interpretation, we are in the middle (closer to the end, I\u000areckon) of another disruptive innovation:</p>\u000a\u000a<pre><code>Personal computer -&gt; Tablet\u000a</code></pre>\u000a\u000a<p>Indeed, compared to laptops, tablets seem to be a better way for the\u000ageneral public to use computers. The touch interface is more intuitive,\u000aand the physical form factor is better suited to casual use. For the\u000apurposes of this post, let's assume for argument's sake that tablets\u000aovertake personal computing in the future. The interesting thing is what\u000ahappens next:</p>\u000a\u000a<pre><code>Tablet -&gt; (next disruptive innovation)\u000a</code></pre>\u000a\u000a<p>Let's go back to the second interpretation: <code>b was designed using a</code>\u000a(or, in some biblical sense, b begat a). In other words, designers and\u000aengineers used a mainframe to invent minicomputers, a minicomputer to\u000ainvent PCs, and a PC to build tablets. We can call this "platform\u000afertility" just for fun. Up to now, said professionals could use\u000athe latest general purpose computers to do their job, but this may be\u000achanging.</p>\u000a\u000a<p>The tablet disruption was enabled by its predecessor, the PC, being\u000aflexible and extensible enough, and thus well suited as a prototyping\u000aplatform. Linux, OS X and even Windows are all very flexible platforms,\u000aintended to work with a variety of software toolkits and external\u000adevices. These desktop platforms never imposed restrictions like\u000asoftware signing, and even if they did, you the power user could always\u000aoverride.</p>\u000a\u000a<h2>Peril of a closed platform</h2>\u000a\u000a<p>I don't want to argue about the semantics of the word "open", but\u000aregardless of your religious dispositions, we can all agree that Apple\u000ais not, nor has any pretenses to be associated with, that word. I'm not\u000amaking judgements here, it's just how they roll. The API surface is\u000acarefully designed to give developers the right amount of flexibility,\u000abut not more. AppStore is explicitly a sandbox, and if you don't play by\u000athe rules, you lose your playground privileges.</p>\u000a\u000a<p>So imagine for a minute that iPad swept the tablet market (shouldn't be\u000ahard given the <a href="http://www.appleinsider.com/articles/12/05/04/ipad_tablet_market_share_will_dip_to_50_by_2017_study_says.html">current market distribution</a>). From a pragmatic\u000auser's perspective, this is fine, even good! iOS is a very\u000awell-integrated platform, working across all shiny Apple products, and\u000ausers are generally pretty happy with the interface and overall\u000aexperience. From a curious developer's perspective, however, things\u000aare a bit different.</p>\u000a\u000a<p>As an iOS developer that wants to improve the platform experience,\u000ahowever, you are pretty much stuck with how things are. You can't\u000areplace the lock screen, can't write long-running applications that read\u000ain accelerometer data in the background, can't customize your home\u000ascreen launcher, etc.</p>\u000a\u000a<p>The problem is exacerbated when you set out to try to invent the next\u000athing. How do you interface with your new stereo camera rig? How many\u000ahurdles do you have to overcome to make it possible to interface with\u000ayour new smart watch? How about a pair of smart contact lenses? How do\u000ayou get raw USB access? Bluetooth? Ad-hoc wireless? Granted, the further\u000ayou venture away from the platform core, the less help you would expect\u000afrom it. This is generally where you climb down a layer of abstraction -\u000afor example, to <a href="http://developer.android.com/sdk/ndk/index.html">NDK in Android</a>. Without such an option, you're\u000aleft dead in the water.</p>\u000a\u000a<h2>A Litmus test</h2>\u000a\u000a<p>Unless iPads become more hackable or other, more developer-friendly\u000atablets emerge as serious competitors, laptop computers will become\u000aspecialized tools for software professionals while tablets supplant\u000alaptops for the rest of thepublic.</p>\u000a\u000a<p>Platforms inherently restrict the developer in some sense, placing them\u000ain a box delineated by the APIs that the platform provides. An\u000ainteresting way to examine this box is with this notion of "platform\u000afertility", centering around the question:</p>\u000a\u000a<p><strong>Can this platform beget future platforms?</strong></p>\u000a\u000a<p>Whether your idea of the next platform is incremental (for example, a\u000abetter lock screen) or a fundamental disruptive innovation (for example,\u000asmart glasses), the answer to the above question for the current state\u000aof iOS is a resounding no. Sorry bro!</p>\u000a
p1759
tp1760
Rp1761
sg11
V/platform-fertility
p1762
sg13
Nsg14
I01
sg15
VPlatform fertility: open for innovation?
p1763
sg18
VAlthough I love Apple's design aesthetic and ability to consistently\u000achurn out amazing hardware, I'm never quite comfortable fully embracing\u000ait.
p1764
sg4
V<p>Although I love Apple's design aesthetic and ability to consistently\u000achurn out amazing hardware, I'm never quite comfortable fully embracing\u000ait. The reasons have to do with platform fertility, or how well suited a\u000aplatform is to incremental platform innovations and new platform\u000acreation. Onward!</p>\u000a\u000a
p1765
sg23
g147
sg31
g1762
sg148
(dp1766
g150
S'Jun'
p1767
sg152
S'June 6, 2012'
p1768
sg154
I6
sg155
S'2012-06-06T09:00:00-00:00'
p1769
sg157
I1338998400
sg158
I2012
sg159
I6
ssg61
g160
sg29
S'platform-fertility'
p1770
sS'posted'
p1771
g166
(S'\x07\xdc\x06\x06'
p1772
tp1773
Rp1774
ssg32
S'content/posts/2012/platform-fertility.md'
p1775
sg34
F1338969220.0
sa(dp1776
g2
(dp1777
g26
g5
(g6
g7
V<p><a href="http://emberjs.com/">Ember</a> and <a href="http://documentcloud.github.com/backbone/">Backbone</a> are both promising JavaScript frameworks but have\u000acompletely different philosophies. In this post, I'll compare the two, both\u000afrom a practical and philosophical perspective. I'll defer to real world\u000aexperience with Backbone and <a href="http://sproutcore.com">SproutCore</a> (Ember's predecessor), as well as\u000abasic experiments with Ember (full disclosure: haven't built a large Ember app\u000ayet). I'll also supplement claims with quotes from a fantastic <a href="http://goo.gl/t7gHG">conversation</a>\u000afrom Freenode <a href="irc://irc.freenode.net/#documentcloud">#documentcloud</a> on February 3rd, 2011. For quote\u000acontext, <code>wycats</code> is <a href="http://yehudakatz.com/">Yehuda Katz</a>, one of the lead developers on Ember, and\u000a<code>jashkenas</code> is <a href="http://ashkenas.com/">Jeremy Ashkenas</a>, one of the lead developers on Backbone.\u000a<!--more--></p>\u000a\u000a<p>Before I go into too much detail, it's pretty clear that both frameworks have\u000athe same goal: to help developers build apps.</p>\u000a\u000a<blockquote>\u000a  <p>1:09 PM <strong>wycats</strong> backbone apps are apps</p>\u000a</blockquote>\u000a\u000a<p>So we have a roughly apples-to-apples comparison. Let me dive in and talk about\u000asome philosophical differences:</p>\u000a\u000a<h2>Backbone at a glance</h2>\u000a\u000a<p>Backbone is a minimalist framework that builds on top of ideas from\u000ajQuery to give some structure to web applications. It introduces\u000aconcepts of views, models, a restful sync interface, routers, etc, in\u000asurprisingly little code. Backbone is the darling of microframework\u000alovers, who highly value small framework size and don't want to commit\u000ato a single "full stack" solution.</p>\u000a\u000a<p>Backbone is incredibly flexible, and doesn't impose how your views and\u000amodels should actually interact. The main benefit is that it adds some\u000astructure to the app and provides convenient ways of listening to DOM\u000aevents and turning them into application events.</p>\u000a\u000a<blockquote>\u000a  <p>1:15 PM <strong>wycats</strong> backbone is 600loc <br />\u000a  1:16 PM <strong>wycats</strong> "here's how you should think about your app structure"</p>\u000a</blockquote>\u000a\u000a<p>Since Backbone is so small, it leaves a lot of decisions up to the\u000adeveloper. This is both a blessing (flexibility, works for me, etc) and\u000aa curse, since support for many things is missing.</p>\u000a\u000a<blockquote>\u000a  <p>12:32 PM <strong>wycats</strong> jashkenas: backbone is 600 lines of code <br />\u000a  12:32 PM <strong>wycats</strong> jashkenas: the idea that there are things missing in it that\u000a  are common should not be controversial</p>\u000a</blockquote>\u000a\u000a<h2>Ember at a glance</h2>\u000a\u000a<p>Ember has a very different history. It's an evolution of SproutCore,\u000awhich is a complete web application solution. Ember takes the core parts\u000aof SproutCore - two-way data binding, computed properties, tight\u000atemplate integration, and strips the rest off into sub-modules. Things\u000athat come in module format are a data serialization layer (via <a href="https://github.com/emberjs/data">data</a>),\u000arouting (via <a href="https://github.com/emberjs-addons/sproutcore-routing">routes</a>). Ember's out of the box functionality is\u000aactually smaller than Backbone's, but it provides a higher level\u000aabstraction.</p>\u000a\u000a<p>I used SproutCore back in 2008 while working as an engineer on\u000aiWork.com. It was a SproutCore pre-1.0 release, and things were a bit\u000arough. Ember seems to have come clean in many ways, presenting a more\u000aconsistent template data binding solution via Handlebars, and being a\u000alot less monolithic SproutCore once was.</p>\u000a\u000a<p>Ember aims to provide a full solution in an opinionated way. Thus,\u000ato get the most of the framework, developers must do things in a certain\u000astyle. While you're not forced to use Handlebars, it's the only way to\u000aget some of the compelling features the framework provides (eg. for\u000atwo-way data binding). Many of the auxillary modules such as ember-data\u000aare designed to fit will into the existing structure of the framework.</p>\u000a\u000a<blockquote>\u000a  <p>1:22 PM <strong>wycats</strong> tbranyen: ember is an end-to-end framework built on\u000a  top of more modular components</p>\u000a</blockquote>\u000a\u000a<p>There is a sentiment from many people, including Backbone's founder that\u000amuch of what Ember provides is over-engineered:</p>\u000a\u000a<blockquote>\u000a  <p>12:28 PM <strong>jashkenas</strong> It's Backbone's take that Ember's more complex\u000a  data binding model, intermediate controllers, run loop etc. ... are\u000a  all interesting approaches, but are <em>not</em> usually helpful in building\u000a  a real site.</p>\u000a</blockquote>\u000a\u000a<h2>Different audiences</h2>\u000a\u000a<blockquote>\u000a  <p>12:30 PM <strong>jashkenas</strong> wycats: and I find quite the opposite --\u000a  self-selecting sample pools, as you'd expect.</p>\u000a</blockquote>\u000a\u000a<p>So to summarize at a high level, there are two camps (and excuse me\u000awhile I generalize a lot).</p>\u000a\u000a<ol>\u000a<li><p>Lovers of micro-frameworks. JavaScript hackers seeking extra\u000astructure in their slightly complex apps. These people are\u000acomfortable mixing and matching frameworks, solving problems as they\u000acome, and just want to get started quickly.</p></li>\u000a<li><p>Software Engineers that are used to a deep abstraction layer and a\u000afull service stack. These people are probably coming from native app\u000adevelopment and want to write very complex applications on the web.</p></li>\u000a</ol>\u000a\u000a<p>Basically, these two developers have different needs, come from\u000adifferent programming cultures, and are maybe even writing different\u000aapplications.</p>\u000a\u000a<blockquote>\u000a  <p>12:31 PM <strong>wycats</strong> jashkenas: so then it's fair to say that for <em>some</em>\u000a  people, Ember's approach is overkill</p>\u000a</blockquote>\u000a\u000a<p>Andrew says it best:</p>\u000a\u000a<blockquote>\u000a  <p>12:36 PM <strong>andrewdeandrade</strong> It's really all about values. Occasionally\u000a  I get frustrated by things backbone.js doesn't do and occasionally I\u000a  get frustrated by things rails does that are hard to undo. My personal\u000a  preference is to have a framework not do something and implement it\u000a  myself than have a framework do something and figure out how to do the\u000a  opposite. That's me. I know people who feel differently</p>\u000a</blockquote>\u000a\u000a<h2>A more detailed comparison</h2>\u000a\u000a<p>After engineering with SproutCore, writing an app with Backbone, and\u000awriting small amounts of Ember (mostly samples to get a feel for data\u000abindings, etc), I've got some sense of the issues that you will run into\u000awhen developing a moderately complex app. I'll go over some, and\u000asupplement them with quotes from framework authors.</p>\u000a\u000a<blockquote>\u000a  <p>1:28 PM <strong>wycats</strong> jashkenas: do you disagree that the pattern "listen\u000a  for these properties, and when any of them changes, trigger observers"\u000a  is very common? <br />\u000a  1:26 PM <strong>jashkenas</strong> that's correct -- you listen for changes to the source data and render computed values. not hard. <br />\u000a  1:26 PM <strong>wycats</strong> jashkenas: yes\u2026 a pattern that happens sufficiently often that it's good to abstract</p>\u000a</blockquote>\u000a\u000a<p>In general, Ember seeks to find common problems that developers face,\u000aand solve them in an opinionated way. Backbone, on the other hand,\u000aleaves it to developers solve their own problems in the way that works\u000afor them.</p>\u000a\u000a<h3>Templates</h3>\u000a\u000a<p>While Ember in theory lets you pick which templating engine to use, you\u000alose a lot of benefits if you're not using Handlebars.</p>\u000a\u000a<blockquote>\u000a  <p>1:29 PM <strong>jashkenas</strong> forcing your users to use logic-less templates is <em>incredibly</em> constraining. <br />\u000a  1:30 PM <strong>tomdale</strong> jashkenas: ember supports any templating language you'd like. you just don't get auto-updating <br />\u000a  1:30 PM <strong>jashkenas</strong> right, exactly.</p>\u000a</blockquote>\u000a\u000a<p>In practice, my last project used Mustache because it was logic-less.\u000aSuch templating engines are my preference, and I'm happy to be\u000aconstrained to a pretty good templating system if that means access to\u000apowerful features, cleaner application code and less boilerplate.</p>\u000a\u000a<p>I don't think that Ember should be toting choice of template as a big\u000abenefit. Like any application framework, Ember is a contract between\u000adeveloper and framework author. You write in our style and we'll give\u000ayou powerful features. In an ideal world, this lets developers focus on\u000atheir app instead of dealing with middleware.</p>\u000a\u000a<h3>Views</h3>\u000a\u000a<p>view decomposition is one of the first tasks a front end developer\u000afaces. They need to decide which parts of their app will be implemented\u000awith what view. Should each list item be its own view? Should the list\u000abe a single monolithic view?</p>\u000a\u000a<p>In my experience writing Backbone apps, views are very primitive and\u000atend to cause issues. There's no support for any sort of view nesting,\u000awhich is totally critical for large applications with complex UIs. In\u000acontrast, Ember provides an easy way of nesting views inside one\u000aanother.</p>\u000a\u000a<p>The other thing you'll notice with Backbone is that there's a lot of\u000amicro-management required when building Backbone views. They need to\u000abe properly cleaned up by hand, otherwise you end up with <a href="http://stackoverflow.com/questions/7125402/backbone-bind-multi-event-to-one-button-after-i-new-view-multi-times">zombie\u000aviews</a> bound to events, or events that <a href="http://stackoverflow.com/questions/7348988/backbone-js-events-not-firing-after-re-render">don't fire at all</a>.</p>\u000a\u000a<p>Some backbone projects, such as the <a href="https://github.com/tbranyen/backbone.layoutmanager">layout manager</a>, aim to remedy\u000asome of these limitations by creating a Layout abstraction, that allows\u000anested views and handles a lot of the rendering. This is a very\u000ainteresting project, but I haven't tried it yet.</p>\u000a\u000a<h3>MVC?</h3>\u000a\u000a<p>Ember is a traditional MVC framework, where it's clear which parts are\u000athe view, the controller and the model. Backbone on the other hand, is\u000aexplicitly not an MVC. It never even claims to be!</p>\u000a\u000a<p>In fact, if you read the <a href="http://documentcloud.github.com/backbone/">main page</a>, you'll notice that\u000athere's no mention of controllers anywhere. Don't get me wrong: Backbone\u000astill gives your applications structure, but has no opinion on the\u000agluing layer between data and presentation. I think Backbone once had\u000acontrollers, but they were renamed to routers, designed primarily for\u000ahandling URLs and history/pushState.</p>\u000a\u000a<p>In my experience, there's very much a need for a controller when writing\u000aeven moderately complex apps. You're presented with several options:</p>\u000a\u000a<ol>\u000a<li>Write controller code in views</li>\u000a<li>Write controller code in models</li>\u000a<li>Write controller code in a router</li>\u000a<li>Write your own controller infrastructure</li>\u000a</ol>\u000a\u000a<p>If you care about separation of concerns, none of these options are\u000areally acceptable.</p>\u000a\u000a<h3>Data and servers</h3>\u000a\u000a<p>Backbone packs a huge punch in a small package. It comes with\u000a<code>Backbone.sync</code>, which lets you fully customize how you want to interact\u000awith the server.</p>\u000a\u000a<p>Following the CRUD pattern, Backbone lets you specify the response\u000aformat of an Read, but unfortunately doesn't allow you to fully\u000acustomize how you would like to serialize Create and Update and Delete\u000apayloads for calling your server-side API. For my last project, I ended\u000aup just completely redefining Backbone.sync, which is very powerful, but\u000aI needed to write a lot of boilerplate to make it work reasonably.</p>\u000a\u000a<p>The problem with Backbone's approach is that it doesn't separate two\u000aparts of data stores: internal collection management and the interface\u000awith the backend API.</p>\u000a\u000a<p>Ember data, on the other hand, has the concept of API adapters, which\u000alet you specify the interface the server. Additionally it has a\u000aDataStore model. I haven't experimented with this in Ember yet, but\u000aSproutCore's version of this worked quite well.</p>\u000a\u000a<h3>Performance considerations</h3>\u000a\u000a<p>When I worked on iWork.com, we definitely had some issues with\u000aSproutCore performance for large amounts of data. The problem was that\u000awe couldn't really optimize our code because we were locked into the way\u000aSproutCore does things, so our way forward was to patch SproutCore\u000aitself to address some of the performance issues, or break out of the\u000aframework and implement the performance intensive part manually.</p>\u000a\u000a<p>Jeremy voices this concern here:</p>\u000a\u000a<blockquote>\u000a  <p>1:33 PM <strong>jashkenas</strong> finally, and perhaps most importantly, embers tight\u000a  coupling of handlebars-ui-with-very-specific-bindings-to-ember-models is\u000a  trouble, performance-wise. You can't build the really intensive parts of\u000a  your UI with that level of binding / dom tweaking.</p>\u000a</blockquote>\u000a\u000a<p>And I agree with him. It's very legitimate concern for large apps, and\u000aYehuda didn't answer adequately in my opinion.</p>\u000a\u000a<blockquote>\u000a  <p>1:34 PM <strong>wycats</strong> jashkenas: exposing the performance question to the\u000a  user is trouble <br />\u000a  1:34 PM <strong>wycats</strong> over the long haul Ember will be able to heuristically\u000a  decide how bulky to update <br />\u000a  1:34 PM <strong>jashkenas</strong> sufficiently smart compiler, eh? <br />\u000a  1:34 PM <strong>wycats</strong> jashkenas: :P  </p>\u000a</blockquote>\u000a\u000a<p>I'm a bit concerned about this point and would love to hear a less\u000ahandwavy answer from Yehuda.</p>\u000a\u000a<h2>Final thoughts (Backbone)</h2>\u000a\u000a<p>My conclusion from writing an app with Backbone, and attending a Bocoup\u000atraining workshop, is that Backbone by itself is not sufficient for\u000abuilding complex web apps. You will invariably go one of two directions:</p>\u000a\u000a<ol>\u000a<li>Engineer a lot of stuff on top of it, or</li>\u000a<li>Use existing Backbone plugins of various maturity and hope they work\u000awell together.</li>\u000a</ol>\u000a\u000a<p>Based on my experience with jQuery, and the mess of ensuing plugins, The\u000alatter seems overly optimistic. So basically, be prepared to write a lot\u000aof extra <a href="https://github.com/tbranyen/backbone-boilerplate">boilerplate</a> code. But if you're a JavaScript developer, you\u000acan handle that!</p>\u000a\u000a<p>In practice, it's very difficult to remain productive if you're writing\u000aboth an app and a framework at the same time. Unfortunately this was my\u000atendency when using Backbone. I hate reinventing the wheel. Especially\u000aif it's kind of lopsided.</p>\u000a\u000a<p>That said, Backbone is fantastic for mid-to-low complexity applications\u000athat want to maintain structured code.</p>\u000a\u000a<h2>Final thoughts (Ember)</h2>\u000a\u000a<p>Ember, on the other hand, forces you into its way of doing things. This\u000ais a framework with opinions that gives you less flexibility. However,\u000aif you grit your teeth a little bit and buy in, you'll be exposed to a\u000awell thought out set of libraries that work well together.</p>\u000a\u000a<p>I really like the features Ember offers, and its philosophy of finding\u000acommon problems developers will face and solving them. One thing I\u000aanticipate is that many of the non-core Ember modules are immature.\u000aHowever, just having these modules that are designed to work together is\u000aa boon for serious application developers.</p>\u000a\u000a<p>I'm still concerned that Ember applications may be stuck if dealing with\u000aparticularly hairy custom view situations, or large amounts of data, but\u000aI'll have a better sense of the limitations soon.</p>\u000a
p1778
tp1779
Rp1780
sg11
V/backbone-and-ember
p1781
sg13
Nsg14
I01
sg15
VBackbone and ember
p1782
sg18
V\u000a\u000a[Ember][] and [Backbone][] are both promising JavaScript frameworks but have\u000acompletely different philosophies.
p1783
sg4
V<p><a href="http://emberjs.com/">Ember</a> and <a href="http://documentcloud.github.com/backbone/">Backbone</a> are both promising JavaScript frameworks but have\u000acompletely different philosophies. In this post, I'll compare the two, both\u000afrom a practical and philosophical perspective. I'll defer to real world\u000aexperience with Backbone and <a href="http://sproutcore.com">SproutCore</a> (Ember's predecessor), as well as\u000abasic experiments with Ember (full disclosure: haven't built a large Ember app\u000ayet). I'll also supplement claims with quotes from a fantastic <a href="http://goo.gl/t7gHG">conversation</a>\u000afrom Freenode <a href="irc://irc.freenode.net/#documentcloud">#documentcloud</a> on February 3rd, 2011. For quote\u000acontext, <code>wycats</code> is <a href="http://yehudakatz.com/">Yehuda Katz</a>, one of the lead developers on Ember, and\u000a<code>jashkenas</code> is <a href="http://ashkenas.com/">Jeremy Ashkenas</a>, one of the lead developers on Backbone.\u000a
p1784
sg23
g147
sg31
g1781
sg148
(dp1785
g150
S'Feb'
p1786
sg152
S'February 15, 2012'
p1787
sg154
I2
sg155
S'2012-02-15T09:00:00-00:00'
p1788
sg157
I1329325200
sg158
I2012
sg159
I15
ssg61
g160
sg29
S'backbone-and-ember'
p1789
sS'categories'
p1790
(lp1791
S'web'
p1792
asS'posted'
p1793
g166
(S'\x07\xdc\x02\x0f'
p1794
tp1795
Rp1796
ssg32
S'content/posts/2012/backbone-and-ember/index.md'
p1797
sg34
F1433825556.0
sa(dp1798
g2
(dp1799
g26
g5
(g6
g7
V<p>There are many technical decisions to make when writing web applications. I've\u000acome back to writing modern web applications lately, and wanted to consolidate\u000asome scattered thoughts that I\u2019ve recorded over the course of my development\u000acycle. In particular, this post is about the set of frameworks that I found to\u000abe instrumental in developing my most recent project. I'll go over some of the\u000amost important framework types, each of which could be expanded into an article\u000ain its own right. This is not meant to be an extensive comparison of existing\u000aofferings, just a slice of technologies that I experimented with most recently.\u000a<!--more--></p>\u000a\u000a<p>Although my focus is on mobile first, I think that this set of technologies can\u000abe applied to web apps in general. All of my decisions and data points were\u000amade with a few requirements in mind:</p>\u000a\u000a<ul>\u000a<li>JavaScript only (CoffeeScript, Dart, are definitely worth a serious look, but\u000acause an explosion in choice which I wanted to avoid)</li>\u000a<li>Must work well in modern browsers (iOS 5, Android 4)</li>\u000a</ul>\u000a\u000a<h2>Picking an MVC</h2>\u000a\u000a<p>The model view controller pattern has been in use in native UI app development\u000afor decades. The basic idea is to separate the data layer (storage,\u000acommunication, data) from the presentation layer (UI, animation, input). There\u000aare other similar patterns such as MVVM (Model View ViewModel), but the main idea\u000ais to have well-defined separation between the presentation and data layers for\u000acleaner code and ultimately long-term velocity:</p>\u000a\u000a<p><img src="separation.jpg" alt="separation" /></p>\u000a\u000a<p>There are tons of offerings of JavaScript model view controller frameworks.\u000aSome, such as <a href="http://documentcloud.github.com/backbone/">Backbone.js</a> and <a href="http://spinejs.com/">Spine.js</a> are written in pure\u000acode, while others like <a href="http://knockoutjs.com/">Knockout.js</a> and <a href="http://angularjs.org/#/">Angular</a> rely on\u000aDOM data attribute binding. Relying on HTML5 data DOM attributes feels wrong\u000afor an MVC system, whose whole point is to separate view and data. This\u000aexcludes the Knockout and Angular frameworks. Spine.js is easier with\u000aCoffeeScript, which we exclude based on my initial requirements.</p>\u000a\u000a<p>Backbone.js has been around for longer than most (except perhaps JavaScriptMVC,\u000aseems like a dead project), and also features a growing open source community.\u000aFor my app stack, I went with Backbone.js. For more information about picking\u000aan MVC, check out <a href="http://addyosmani.github.com/todomvc/">TodoMVC</a>, which implements the same Todo\u000aapplication using different MVC frameworks. Also see this\u000a<a href="http://codebrief.com/2012/01/the-top-10-javascript-mvc-frameworks-reviewed/">MVC framework comparison</a>, which strongly favors the\u000a<a href="http://emberjs.com/">Ember.js</a>, a relative newcomer to the scene. I haven\u2019t yet had a chance\u000ato play with it, but it\u2019s on my list.</p>\u000a\u000a<h2>Picking a templating engine</h2>\u000a\u000a<p>To build a serious application on the web, you inevitably build up large DOM\u000atrees. Rather than using JavaScript APIs to manipulate the DOM, it can be much\u000asimpler and more efficient to write HTML using a string-based template instead.\u000aGenerally speaking JS templates have evolved to use this at-first strange\u000aconvention of embedding the template content inside script tags: <code>&lt;script\u000a  id="my-template" type="text/my-template-language"&gt;...&lt;/script&gt;</code>. The basic\u000apattern of use for all template engines is to load the template as a string,\u000aconstruct template parameters and then run the template and parameters through\u000athe templating engine.</p>\u000a\u000a<p>Backbone.js depends on <a href="http://documentcloud.github.com/underscore/">Underscore.js</a>, which ships with a somewhat\u000alimited templating engine with verbose syntax. There are other options\u000aavailable, including <a href="http://api.jquery.com/category/plugins/templates/">jQuery Templates</a>, <a href="http://handlebarsjs.com/">Handlebars.js</a>,\u000a<a href="http://mustache.github.com/">Mustache.js</a> and many others. jQuery Templates have been deprecated\u000aby the jQuery team, so I did not consider this option. Mustache is a\u000across-language templating system, featuring simplicity and a deliberate\u000adecision to support as little logic as possible. Indeed, the most complex\u000aconstruct in Mustache is a way to iterate an array of objects. Handlebars.js\u000abuilds heavily on Mustache, adding some nice features such as template\u000aprecompilation and in-template expressions. For my purposes I didn\u2019t need these\u000aextra features, and chose Mustache.js as my templating platform.</p>\u000a\u000a<p>In general, my impression is that the existing templating frameworks are quite\u000aminimal and comparable in features, so the decision is largely a matter of\u000apersonal preference.</p>\u000a\u000a<h2>Picking a CSS Framework</h2>\u000a\u000a<p>CSS frameworks are essential tools that extend CSS\u2019s feature set with\u000aconveniences such as variables, a way to create hierarchical CSS selectors, and\u000asome more advanced features. This essentially creates a new language: an\u000aaugmented version of CSS (let\u2019s call it CSS++). For development ease, some\u000aframeworks implement a JavaScript CSS++ interpreter in the browser, while other\u000aframeworks let you monitor a CSS++ file and compile it whenever there are any\u000achanges made. All CSS frameworks should provide command line tool to compile\u000aCSS++ down to CSS for deployment.</p>\u000a\u000a<p>As with templating languages, there are many choices all of which do very\u000asimilar things. My choice was motivated by personal syntax preference, and I\u000aprefer <a href="http://sass-lang.com/">SCSS</a> because it avoids weird syntax like <code>@</code>. One drawback of SCSS is\u000athat it doesn\u2019t ship with a JavaScript interpreter (there is an <a href="https://github.com/bmavity/scss-js">unofficial one</a>\u000athat I haven\u2019t tried), but does come with a command line watcher. Other similar\u000aCSS frameworks include <a href="http://lesscss.org/">LESS</a> and <a href="http://learnboost.github.com/stylus/">Stylus</a>.</p>\u000a\u000a<h2>How to layout views</h2>\u000a\u000a<p>HTML5 provides a variety of ways to layout content, and MVC frameworks provide\u000ano opinion about which of these layout technologies to use, leaving the\u000asometimes difficult decision to you, the developer.</p>\u000a\u000a<p>Generally speaking, relative positioning is appropriate for documents, but\u000afalls apart for apps. Absolute positioning should be avoided, as should tables,\u000aclearly. Many web developers have turned to the float property to align\u000aelements, but this is suboptimal for building application views, since it\u2019s not\u000aoptimized for app-like layouts, which results in many odd problems and <a href="http://stackoverflow.com/questions/8554043/what-actually-is-clearfix">infamous\u000aclearfix hacks</a>.</p>\u000a\u000a<p>After much experimentation with various web layout technologies over the years,\u000aI think that a combination of fixed positioning and flexbox model is ideal for\u000amobile web applications. I use fixed positioning for UI elements that are fixed\u000aon the screen (headers, sidebars, footers, etc). The flex box model is great\u000afor laying out stacked views on the page (horizontally or vertically). It\u2019s the\u000aonly CSS box model explicitly optimized for interface design, quite similar to\u000aAndroid\u2019s LinearLayout manager. For more information about the flexbox model,\u000aread <a href="http://www.html5rocks.com/en/tutorials/flexbox/quick/">Paul's article</a> and note that this spec is being replaced by a\u000a<a href="http://www.w3.org/TR/css3-flexbox/">new, non-backwards compatible version</a>.</p>\u000a\u000a<h3>Adaptive Web Apps</h3>\u000a\u000a<p>One final section on this matter: I\u2019m a strong proponent of creating\u000adevice-specific user interfaces. This means re-writing parts of your view code\u000afor different form factors. Luckily, the MVC pattern makes it relatively easy\u000ato reuse a single model for multiple views (eg. tablet and phone).</p>\u000a\u000a<p>Flipboard for iOS demonstrates this idea very well, giving tablet and phone\u000ausers a highly tailored experience for each device form factor.</p>\u000a\u000a<p><img src="flipboard-phone.jpg" alt="flipboard-phone" />\u000aThe phone UI is optimized for vertical swipes, allowing single hand use.</p>\u000a\u000a<p><img src="flipboard-tablet.jpg" alt="flipboard-tablet" />\u000aTablet UI works well for two hands holding the device on opposite sides.</p>\u000a\u000a<h2>Input considerations</h2>\u000a\u000a<p>On mobile, the main way users interact with your application is by touching the\u000ascreen with their fingers. This is quite different from mouse-based\u000ainteraction, since there are 9 additional points to track on the screen, which\u000ameans developers need to move away from mouse events when writing mobile apps.\u000aIn addition, mouse events on mobile have the problem of clicks being delayed by\u000a300ms (there is a well-known <a href="http://code.google.com/mobile/articles/fast_buttons.html">touch-based workaround</a>). For more information\u000aabout using these events in mobile browsers, see <a href="http://www.html5rocks.com/en/mobile/touch.html">my touch events article</a>.</p>\u000a\u000a<p>It\u2019s not enough to just <code>s/mousedown/touchstart/</code> all of your event handlers.\u000aThere is a completely new set of gestures that users have come to expect on\u000atouch devices, such as swipes to, for example, navigate through lists of\u000aimages. Though Apple has a little-known <a href="http://developer.apple.com/library/safari/#documentation/UserExperience/Reference/GestureEventClassReference/GestureEvent/GestureEvent.html#//apple_ref/doc/uid/TP40009353">gestures API</a>, there is no open spec for\u000adoing gesture detection on the web. We really need a JavaScript library to do\u000agesture detection, for some of the <a href="http://www.lukew.com/touch/TouchGestureGuide.pdf">more common gestures</a>.</p>\u000a\u000a<h2>How to make it work offline</h2>\u000a\u000a<p>For an app to work offline, you need two things to be true:</p>\u000a\u000a<ol>\u000a<li>Assets are available (via AppCache, Filesystem API, etc)</li>\u000a<li>Data is available (via LocalStorage, WebSQL, IndexedDB, etc)</li>\u000a</ol>\u000a\u000a<p>In practice, building offline apps on the web is a difficult problem. Generally\u000aspeaking offline functionality should be built into your app from the\u000abeginning. It\u2019s especially difficult to offline-ify an existing web application\u000awithout significant code rewriting. Additionally, there are often unknown\u000astorage limits for various offline technologies, and undefined behavior for\u000awhat happens when those limits are exceeded. Finally, there are problems with\u000atechnologies in the offline technology stack, most notably AppCache, as I\u000aoutlined in a <a href="http://smus.com/game-asset-loader">previous post</a>.</p>\u000a\u000a<p>A very interesting approach to write truly offline-capable apps is to go\u000a\u201coffline first\u201d. In other words, write everything as if you have no internet\u000aconnection, and implement a syncing layer that synchronizes data when an\u000ainternet connection exists. In the Backbone.js MVC model, this can fit nicely\u000aas a custom <code>Backbone.sync</code> adapter.</p>\u000a\u000a<h2>Unit testing</h2>\u000a\u000a<p>It\u2019s hard to unit test your UI. However, since you\u2019re using an MVC, the model\u000ais completely isolated from the UI and as a result, easy to test. <a href="http://docs.jquery.com/QUnit">QUnit</a> is\u000aquite a nice option, especially because it allows to unit test asynchronous\u000acode using it\u2019s <a href="http://docs.jquery.com/QUnit/start#decrement">start() and stop()</a> methods.</p>\u000a\u000a<h2>Signing off</h2>\u000a\u000a<p>To summarize, I used Backbone.js for MVC, Mustache.js for templating, SCSS for\u000aa CSS framework, CSS Flexbox to render views, custom touch events and QUnit for\u000aunit testing to write my mobile web application. For offline support, I\u2019m still\u000aexperimenting with various technologies and will hopefully follow up with more\u000ainformation a future post. While I strongly believe in the need for each class\u000aof tool (eg. MVC) outlined here, I also believe that many of the specific\u000atechnologies I described here are interchangeable (eg. Handlebars and\u000aMustache).</p>\u000a\u000a<p><strong>One more thing</strong>: yesterday (on January 17th, 2012), <a href="http://walmartlabs.github.com/thorax/">Thorax</a> was\u000aannounced. This is a Backbone-based set of libraries very similar in spirit to\u000awhat I describe in this post. I've yet to investigate it in any depth, but the\u000aname is great :)</p>\u000a\u000a<p>Use a similar set of frameworks? Have a personal favorite? Think I\u2019m missing an\u000aimportant type of framework? Let me know!</p>\u000a
p1800
tp1801
Rp1802
sg11
V/mobile-web-app-tech-stack
p1803
sg13
Nsg14
I01
sg15
VA mobile web application tech stack
p1804
sg18
V\u000aThere are many technical decisions to make when writing web applications.
p1805
sg4
V<p>There are many technical decisions to make when writing web applications. I've\u000acome back to writing modern web applications lately, and wanted to consolidate\u000asome scattered thoughts that I\u2019ve recorded over the course of my development\u000acycle. In particular, this post is about the set of frameworks that I found to\u000abe instrumental in developing my most recent project. I'll go over some of the\u000amost important framework types, each of which could be expanded into an article\u000ain its own right. This is not meant to be an extensive comparison of existing\u000aofferings, just a slice of technologies that I experimented with most recently.\u000a
p1806
sg23
g147
sg31
g1803
sg148
(dp1807
g150
S'Jan'
p1808
sg152
S'January 18, 2012'
p1809
sg154
I1
sg155
S'2012-01-18T09:00:00-00:00'
p1810
sg157
I1326906000
sg158
I2012
sg159
I18
ssg61
g160
sg29
S'mobile-web-app-tech-stack'
p1811
sS'categories'
p1812
(lp1813
S'web'
p1814
aS'mobile'
p1815
asS'posted'
p1816
g166
(S'\x07\xdc\x01\x12'
p1817
tp1818
Rp1819
ssg32
S'content/posts/2012/mobile-web-app-tech-stack/index.md'
p1820
sg34
F1332684374.0
sa(dp1821
g2
(dp1822
g26
g5
(g6
g7
V<p><strong>Update (August 7, 2013): Pointer.js is deprecated. Please use the\u000a<a href="https://github.com/Polymer/PointerEvents">PointerEvents polyfill</a> instead.</strong></p>\u000a\u000a<p>Mouse will soon cease to be the dominant input method for computing,\u000athough it will likely remain in some form for the forseeable future.\u000aTouch is the heir to the input throne, and the web needs to be ready.\u000aUnfortunately, the current state of input on the web is... you guessed\u000ait: a complete mess! There are two separate issues:</p>\u000a\u000a<ol>\u000a<li>No unified story between mouse, touch, and other spatial input.</li>\u000a<li>Poor support for complex gestures, especially needed for touch.</li>\u000a</ol>\u000a\u000a<p>I'll look at each in a bit of detail, and then show you\u000a<a href="https://github.com/borismus/pointer.js">pointer.js</a>. </p>\u000a\u000a<!--more-->\u000a\u000a<h2>Lack of unified touch and mouse system</h2>\u000a\u000a<p>Most web developers should care about providing a good experience on\u000aboth mouse and touch interfaces. This is increasingly true with\u000acrossover mouse-touch devices like the Transformer prime, and upcoming\u000aWindows 8 laptops.</p>\u000a\u000a<p>Here's what you end up with if you want to support touch and mouse\u000aevents on the web today:</p>\u000a\u000a<pre><code>$(window).mousedown(function(e) { down(e.pageY); });\u000a$(window).mousemove(function(e) { move(e.pageY); });\u000a$(window).mouseup(function() { up(); });\u000a\u000a// Setup touch event handlers.\u000a$(window).bind('touchstart', function(e) {\u000a  e.preventDefault();\u000a  down(e.originalEvent.touches[0].pageY);\u000a});\u000a$(window).bind('touchmove', function(e) {\u000a  e.preventDefault();\u000a  move(e.originalEvent.touches[0].pageY);\u000a});\u000a$(window).bind('touchend', function(e) {\u000a  e.preventDefault();\u000a  up();\u000a});\u000a</code></pre>\u000a\u000a<p>The above is a bunch of boilerplate code that does absolutely nothing!\u000aYou end up having to manually wrangle two completely different models\u000ainto one.</p>\u000a\u000a<p>Microsoft is taking a very smart approach with IE10 to address this\u000aissue by <a href="http://blogs.msdn.com/b/ie/archive/2011/09/20/touch-input-for-ie10-and-metro-style-apps.aspx">introducing pointer events</a>. The idea is to\u000aconsolidate all input that deals with one or more points on the screen\u000ainto a single unified model.</p>\u000a\u000a<p>Unfortunately, it's not being proposed as a standardized spec.\u000aAlso, because it's not universally available, it will be yet another\u000athing developers need to support (if they want to support Windows\u000a8/Metro apps). So now our sample above gets <a href="http://blogs.msdn.com/b/ie/archive/2011/10/19/handling-multi-touch-and-mouse-input-in-all-browsers.aspx">even more\u000aboilerplate</a>, with at least three more calls like the following:</p>\u000a\u000a<pre><code>$(window).bind('MSPointerDown', function(e) {\u000a  // Extract x, y, and call shared handler.\u000a});\u000a$(window).bind('MSPointerMove', function(e) {\u000a  // Extract x, y, and call shared handler.\u000a});\u000a$(window).bind('MSPointerUp', function(e) {\u000a  // Extract x, y, and call shared handler.\u000a});\u000a</code></pre>\u000a\u000a<p>Although their intentions are good, this approach potentially makes the\u000asituation (hopefully temporarily) worse.</p>\u000a\u000a<h2>Touch gestures need to be easy</h2>\u000a\u000a<p>Touch UIs often involve gestures that aren't easy for developers to\u000aimplement, such as pinch-zooming and rotation. However, on the web, due\u000ato the simplicity of <a href="https://dvcs.w3.org/hg/webevents/raw-file/tip/touchevents.html">the touch events</a>, even implementing\u000asomething as simple as a button <a href="http://code.google.com/mobile/articles/fast_buttons.html">is non-trivial</a>.\u000aImplementing more complex gesture recognizers on top of the primitive\u000a<code>touch*</code> events is even less trivial.</p>\u000a\u000a<p>Frameworks like <a href="http://dev.sencha.com/deploy/touch/examples/production/kitchensink/index.html#demo/touchevents">Sencha Touch</a> and <a href="http://eightmedia.github.com/hammer.js/">Hammer.js</a> come to\u000athe rescue to address the lack of gestures, however these both have\u000aproblems. Sencha comes as a complete package, and it's impossible to use\u000atheir gesture recognizer without using their whole framework (or\u000aspending considerable effort trying to pull it out). Hammer.js, on the\u000aother hand, doesn't actually implement gesture recognition for\u000apinchzoom, but instead relies on the touch spec providing non-standard\u000a<code>rotation</code> and <code>scale</code> values <a href="http://developer.apple.com/library/safari/#documentation/UserExperience/Reference/TouchEventClassReference/TouchEvent/TouchEvent.html#//apple_ref/doc/uid/TP40009358">pioneered by Apple</a>.</p>\u000a\u000a<p>Microsoft has a gesture layer on top of their consolidated pointer\u000amodel. This makes sense as an approach to take. True, certain gestures\u000aonly make sense for touch, and it's easy to distinguish the input type\u000ausing the <code>event.pointerType</code> API. That said, with a unified model,\u000athere can be new gestures that span multiple input modalities, like\u000a<a href="#">this research</a> suggests.</p>\u000a\u000a<h2>Pointer.js - A solution to both problems</h2>\u000a\u000a<p>The solution to this problem is to write another library, tag on a <code>.js</code>\u000ato the end of the name, get everyone to use it, prove that it's very\u000auseful, and have browsers and spec implement it natively. Once this\u000ais spec'ed, approved, and widely implemented, it should just be a matter\u000aof removing the script tag!</p>\u000a\u000a<p><img src="pointer.js-architecture.png" alt="Pointer.js architecture." /></p>\u000a\u000a<p>Pointer.js consolidates pointer-like input models across browsers and\u000adevices. It provides the following:</p>\u000a\u000a<ul>\u000a<li>Events: <code>pointerdown, pointermove, pointerup</code></li>\u000a<li>Event payload class: <code>originalEvent, pointerType, getPointerList()</code></li>\u000a<li>Pointer class: <code>x, y, type</code></li>\u000a</ul>\u000a\u000a<p>To use it, simply include <code>pointer.js</code> in your web page. This\u000aautomatically rigs <code>addEventListener</code> with support for <code>pointer*</code> and\u000a<code>gesture*</code> events.</p>\u000a\u000a<p>Try some simple pointer.js demos:</p>\u000a\u000a<ul>\u000a<li><a href="http://borismus.github.com/pointer.js/demos/draw.html">Multi-touch drawing</a></li>\u000a<li><a href="http://borismus.github.com/pointer.js/demos/basic-pointers.html">Pointer event logger</a></li>\u000a<li><a href="http://borismus.github.com/pointer.js/demos/basic-gestures.html">Gesture event logger</a> (supports scale, longpress and doubletap)</li>\u000a</ul>\u000a\u000a<p>For more info about the library, check it out at\u000a<a href="https://github.com/borismus/pointer.js">https://github.com/borismus/pointer.js</a>. Contributions in the form of\u000apull requests are most welcome: more demos using pointer events,\u000aunimplemented gesture recognizers for common gestures, like swipe and\u000arotation, and tweaks to the system itself.</p>\u000a
p1823
tp1824
Rp1825
sg11
V/mouse-touch-pointer
p1826
sg13
Nsg14
I01
sg15
VGeneralized input on the cross-device web
p1827
sg18
V**Update (August 7, 2013): Pointer.
p1828
sg4
V<p><strong>Update (August 7, 2013): Pointer.js is deprecated. Please use the\u000a<a href="https://github.com/Polymer/PointerEvents">PointerEvents polyfill</a> instead.</strong></p>\u000a\u000a<p>Mouse will soon cease to be the dominant input method for computing,\u000athough it will likely remain in some form for the forseeable future.\u000aTouch is the heir to the input throne, and the web needs to be ready.\u000aUnfortunately, the current state of input on the web is... you guessed\u000ait: a complete mess! There are two separate issues:</p>\u000a\u000a<ol>\u000a<li>No unified story between mouse, touch, and other spatial input.</li>\u000a<li>Poor support for complex gestures, especially needed for touch.</li>\u000a</ol>\u000a\u000a<p>I'll look at each in a bit of detail, and then show you\u000a<a href="https://github.com/borismus/pointer.js">pointer.js</a>. </p>\u000a\u000a
p1829
sg23
g147
sg31
g1826
sg148
(dp1830
g150
S'Jun'
p1831
sg152
S'June 14, 2012'
p1832
sg154
I6
sg155
S'2012-06-14T09:00:00-00:00'
p1833
sg157
I1339689600
sg158
I2012
sg159
I14
ssg61
g160
sg29
S'mouse-touch-pointer'
p1834
sS'posted'
p1835
g166
(S'\x07\xdc\x06\x0e'
p1836
tp1837
Rp1838
ssg32
S'content/posts/2012/mouse-touch-pointer/index.md'
p1839
sg34
F1375937485.0
sa(dp1840
g2
(dp1841
g26
g5
(g6
g7
V<p>One of my ongoing projects is called smusique, a mobile web-based sheet\u000amusic viewer. The application was designed for tablets, enabling a\u000arich browsing experience through sheet music. Though some sheet\u000amusic is encumbered by licenses, most of the classic stuff is legally\u000aavailable online via sheet music databases such as the <a href="http://imslp.org">Petrucci\u000aLibrary</a> at <a href="http://imslp.org">imslp.org</a>.\u000a<!--more--></p>\u000a\u000a<p>While the focus of my project was to design and develop a delightful\u000atablet-based interface for sheet music, I needed to solve some technical\u000aproblems:</p>\u000a\u000a<ol>\u000a<li>Figure out how to deal with PDFs in the browser.</li>\u000a<li>Create or find a database with a large enough corpus of searchable\u000asheet music.</li>\u000a</ol>\u000a\u000a<p>Read on to find out how I solved these problems using the AppEngine\u000aconversion API and through a Chrome extension that does client-side\u000ascraping.</p>\u000a\u000a<h2>Dealing with PDFs</h2>\u000a\u000a<p>Since digital sheets are largely in PDF format, I needed some solution\u000afor showing PDFs inside a web user interface. Unfortunately\u000ashowing PDFs inline is pretty much impossible using today's web. Ideally,\u000athe browser would have support for something like the following:</p>\u000a\u000a<pre><code>&lt;img src="something.pdf" page="3" /&gt;\u000a</code></pre>\u000a\u000a<p>But this functionality simply doesn't exist in a widely supported\u000afasion. Given how important showing PDFs is for my application, I\u000aneeded a workaround. A few options came to mind:</p>\u000a\u000a<ol>\u000a<li>Attempt to show PDFs in an iframe, and programatically scroll to the\u000aright position based on the page number.</li>\u000a<li>Convert PDFs on the client side using something like <a href="http://andreasgal.com/2011/06/15/pdf-js/">PDFJS</a></li>\u000a<li>Build my own PDF conversion server</li>\u000a</ol>\u000a\u000a<p>I experimented briefly with the first approach and unsurprisingly found\u000amixed levels of browser support for PDF rendering inside iframes. Some mobile\u000abrowsers allowed PDFs to be opened within iframes, but exhibited\u000aunexpected scaling behavior, could not be programmatically scrolled, or\u000aboth.</p>\u000a\u000a<p>Did not seriously consider PDFJS as a solution, judging that running PDF\u000aconversion in a mobile web browser would be prohibitively slow.</p>\u000a\u000a<p>Just as I was bracing to take the plunge and write my own\u000a<a href="http://www.imagemagick.org/">ImageMagick</a>-based conversion API running on Django/Slicehost, I\u000adiscovered the very new <a href="http://code.google.com/appengine/docs/python/conversion/overview.html">AppEngine conversion API</a>.</p>\u000a\u000a<h2>The conversion API</h2>\u000a\u000a<p>Turns out that AppEngine provides a new experimental <a href="http://code.google.com/appengine/docs/python/conversion/overview.html">file format\u000aconversion API</a>. It allows you to map from a variety of input file\u000aformats to a variety of outputs, including PDF to PNGs (one per page),\u000awhich is exactly what I wanted. The full list of conversion paths is\u000aavailable in <a href="http://code.google.com/appengine/docs/python/conversion/overview.html">the docs</a>. Some of the more exciting\u000afeatures include image to text conversions via OCR, and generating images\u000afrom HTML.</p>\u000a\u000a<p>My conversion was really straight forward to implement:</p>\u000a\u000a<pre><code>def convert_pdf(self, pdf_data):\u000a    """Converts PDF to PNG images. Returns an array of PNG data."""\u000a    asset = conversion.Asset('application/pdf', pdf_data, 'sheet.pdf')\u000a    conversion_request = conversion.Conversion(asset, 'image/png')\u000a    result = conversion.convert(conversion_request)\u000a    if result.assets:\u000a        return [asset.data for asset in result.assets]\u000a    else:\u000a        raise Exception('Conversion failed: %d %s'\u000a                % (result.error_code, result.error_text))\u000a</code></pre>\u000a\u000a<p>Note that the API methods recently changed (maybe in AppEngine 1.6?)\u000afrom <code>conversion.ConversionRequest</code> to <code>conversion.Conversion</code>. I should\u000ahave expected breaking changes since the conversion API is still\u000aexperimental, but it stumped me for a little while anyway.</p>\u000a\u000a<p>The API works pretty well, but doesn't provide much meaningful feedback\u000aif running in the dev server. So far I've only managed to overload the\u000aconversion service a few times with very large (eg. ~100 page) PDFs.\u000aThat said, a lot of sheet music is incredibly long, especially in an\u000aorchestral setting. So, if I was doing this for production, I would\u000aprobably need to build a custom solution.</p>\u000a\u000a<p>One other caveat with this approach is that I have no idea how much\u000amoney I would be charged for using this service. Conversion is quite\u000aintensive, and given the recently increased rates, I would be wary.</p>\u000a\u000a<p>Once the images are converted, I upload them to an Amazon S3 instance\u000awhere I keep my data. To do this, I use <a href="http://aws.amazon.com/code/134">S3.py</a>, a really simple\u000alibrary for interacting with Amazon S3:</p>\u000a\u000a<pre><code>def upload_helper(self, path, data, contentType):\u000a    conn = S3.AWSAuthConnection(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)\u000a    options = {'x-amz-acl': 'public-read', 'Content-Type': contentType}\u000a    response = conn.put(DEFAULT_BUCKET, path, S3.S3Object(data), options)\u000a    return URL_FORMAT % {'bucket': DEFAULT_BUCKET, 'path': path}\u000a</code></pre>\u000a\u000a<p>The full code for the AppEngine server is open source on <a href="https://github.com/borismus/smusique-uploader">github</a>.</p>\u000a\u000a<h2>Client-side scraping</h2>\u000a\u000a<p>Unfortunately IMSLP doesn't provide a useful API out of the box. I\u000awasn't ambitious enough to create a full API for it, but needed some\u000ainterim solution. I figured that for a demo, it wouldn't be such a\u000aterrible experience to have to seed the database with the repertoire you\u000awere interested in if it was easy enough to do.</p>\u000a\u000a<p>Ordinarily, one might write a little scraper utility in their favorite\u000ascripting language. However, scraping HTML from python (or any other\u000ascripting language for that matter) is really not my favorite activity.\u000aAdditionally, relying on the command line excludes the target audience\u000a(musicians) for this application.</p>\u000a\u000a<p>As a workaround, I came across a potentially interesting idea:\u000aclient-side scraping with a Chrome extension. Let me explain.</p>\u000a\u000a<p>Suppose you want to scrape part of a corpus of data that's available\u000athrough a website, but want to let the user decide which parts they are\u000ainterested in. Simply use a Chrome extension that injects code into the\u000atarget page, fetches the interesting part of the DOM using selectors and\u000aperhaps jQuery for convenience, and then uploads the data to some\u000aserver. I used <a href="http://code.google.com/chrome/extensions/content_scripts.html">content scripts</a> for this purpose. In the manifest, the\u000aentry looks as follows:</p>\u000a\u000a<pre><code>"content_scripts": [{\u000a  "matches": ["http://imslp.org/wiki/*"],\u000a  "js": ["imslp.js"],\u000a  "css": ["imslp.css"]\u000a}]\u000a</code></pre>\u000a\u000a<p>Then, <code>imslp.js</code> does the scraping, which can happen automatically as a\u000auser navigates through a page, or by adding extra elements to the page.\u000aThis simple IMSLP scraper creates a "send to smusique" button beside\u000aeach PDF on IMSLP:</p>\u000a\u000a<p><img src="http://i.imgur.com/6YhZ2.png" alt="screenshot" /></p>\u000a\u000a<p>Once clicked, <code>imslp.js</code> gets the URL to fetch, extracts all of the meta\u000adata of the current piece via CSS selectors, and makes a cross-domain\u000arequest to the converter with all of this information.</p>\u000a\u000a<p>This approach is much harder to prevent than traditional, server side\u000ascraping. Although setting UserAgent restrictions is futile, many sites\u000ause JavaScript to render their content, and serve up a content-free\u000apage. In contrast, there's very little a content provider can do to\u000aprotect themselves from client side scraping, making this a very\u000apowerful technique. If the content is in the DOM, it can be extracted.</p>\u000a\u000a<p>The source of the extension is somewhat messy but available also on\u000a<a href="https://github.com/borismus/smusique-extension">github</a>.</p>\u000a
p1842
tp1843
Rp1844
sg11
V/pdf-conversion-client-side-scraping
p1845
sg13
Nsg14
I01
sg15
VPDF conversion and client-side scraping
p1846
sg18
V\u000a\u000aOne of my ongoing projects is called smusique, a mobile web-based sheet\u000amusic viewer.
p1847
sg4
V<p>One of my ongoing projects is called smusique, a mobile web-based sheet\u000amusic viewer. The application was designed for tablets, enabling a\u000arich browsing experience through sheet music. Though some sheet\u000amusic is encumbered by licenses, most of the classic stuff is legally\u000aavailable online via sheet music databases such as the <a href="http://imslp.org">Petrucci\u000aLibrary</a> at <a href="http://imslp.org">imslp.org</a>.\u000a
p1848
sg23
g147
sg31
g1845
sg148
(dp1849
g150
S'Feb'
p1850
sg152
S'February 8, 2012'
p1851
sg154
I2
sg155
S'2012-02-08T09:00:00-00:00'
p1852
sg157
I1328720400
sg158
I2012
sg159
I8
ssg61
g160
sg29
S'pdf-conversion-client-side-scraping'
p1853
sS'categories'
p1854
(lp1855
S'web'
p1856
aS'extension'
p1857
asS'posted'
p1858
g166
(S'\x07\xdc\x02\x08'
p1859
tp1860
Rp1861
ssg32
S'content/posts/2012/pdf-conversion-client-side-scraping/index.md'
p1862
sg34
F1433825570.0
sa(dp1863
g2
(dp1864
g26
g5
(g6
g7
V<p>There's an increasing variety of devices in use today. Even generally\u000arectangular touch enabled devices vary hugely in their physical sizes,\u000aaspect ratios, pixel densities, etc.</p>\u000a\u000a<p>One thing that remains constant across these devices are their users.\u000aTechnologies come and go every year, but people stay the same. Existing\u000aform factors: <a href="http://en.wikipedia.org/wiki/Smart_device">pads, tabs and boards</a> still make sense, and\u000awill continue to do so for the forseeable future. As a result,\u000aergonomic considerations like touch target sizing, readable text and\u000aimage size remain constant. Fingers will be fingers and eyes will be\u000aeyes! Our bodies are firmly rooted in the physical world, and the\u000ainterfaces we create should reflect that.</p>\u000a\u000a<!--more-->\u000a\u000a<p>Take touch targets, for example. Apple's <a href="http://developer.apple.com/library/ios/#DOCUMENTATION/UserExperience/Conceptual/MobileHIG/Characteristics/Characteristics.html">human interface guidelines\u000arecommend</a> minimum touch target size of 44x44pt for iOS. On\u000aan iPhone, 44pt is about 0.25in. On an iPad, 44pt is about 0.3in, physically\u000alarger because of different device pixel density. Even Apple has such\u000adiscrepancies despite a very simple device landscape. Apple devices only\u000acome in two configurations: iPhone and iPad (and their retina\u000acounterparts).</p>\u000a\u000a<p>The situation gets far more complex when we look at mobile devices in\u000ageneral, which is the world of mobile web developers. In an ideal world,\u000aimages and fonts should be scalable and all units should be physical.\u000aI built a <a href="https://github.com/borismus/physical-units">prototype</a> that illustrates roughly how this\u000amodel would work. Onward for the gory details.</p>\u000a\u000a<h2>Device variance</h2>\u000a\u000a<p>Here's a brief sampling of pixel density and aspect ratio for a few\u000apopular devices (data <a href="http://en.wikipedia.org/wiki/List_of_displays_by_pixel_density">from wikipedia</a>):</p>\u000a\u000a<pre><code>Device          DPI   Aspect\u000a============================\u000aiPhone*         163      3:2\u000aiPad*           132      4:3\u000aNexus S*        117      5:3\u000aGalaxy Nexus*   158     16:9\u000aGalaxy Note*    142      8:5\u000aGalaxy Tab      149      8:5\u000aT-Prime         149      8:5\u000aLumia 900       217     10:6\u000a</code></pre>\u000a\u000a<p>The ones with a <code>*</code> denote double density devices. In other words, the\u000aphysical pixel density is double the value indicated, but the browser\u000agenerally reports the resolution to be this halved value. For more\u000ainformation about this practice, read <a href="http://www.quirksmode.org/blog/archives/2010/04/a_pixel_is_not.html">a pixel is not a pixel</a>.</p>\u000a\u000a<p>Thus, in practice, a 44x44px button rendered with a reasonable viewport:\u000a<code>&lt;meta name="viewport" content="width=device-width"&gt;</code> will vary in\u000adimensions between 0.20in on the Lumia 900 to nearly double, 0.38in on\u000athe Nexus S.</p>\u000a\u000a<p>Further complicating things is a somewhat obscure fact that pixels might\u000anot be square at all. The term for it is <a href="http://en.wikipedia.org/wiki/Pixel_aspect_ratio">pixel aspect\u000aratio</a>. As far as I can tell, this is only a concern when\u000adisplays aren't using their native resolutions, which luckily is a\u000ararity in mobile devices.</p>\u000a\u000a<h2>The problem with pixels</h2>\u000a\u000a<p><a href="http://www.lukew.com/ff/entry.asp?1085">Many HIGs</a> recommend minimum touch target sizes in\u000apixels. What really matters is the size of human fingers, which as we\u000aestablished earlier, don't grow and shrink as a function of the device\u000ayou happen to be using.</p>\u000a\u000a<p>Microsoft, anticipating a more fragmented ecosystem of devices with\u000avaried densities and screen sizes, is recommending a physical size\u000aapproach instead, which makes more sense for touch targets. They suggest\u000aa 9x9mm recommended lower bound, and an absolute minimum of 7x7mm. This\u000ajives well with established research, such as MIT's <a href="http://touchlab.mit.edu/publications/2003_009.pdf">Touch Lab study of\u000aHuman Fingertips to Investigate the Mechanics of Tactile Sense</a>,\u000awhich found that the average human finger pad is 10-14mm and the average\u000afingertip is 8-10mm in diameter.</p>\u000a\u000a<h2>Benefits of physical units</h2>\u000a\u000a<p>As a developer, I'd like to just say "this button is 40x9mm" and have\u000athat actually map to real physical dimensions. If I could ensure usable\u000asizes for all of my touch targets, that solve this problem across all\u000adevices, once and for all!</p>\u000a\u000a<p>Physical units also make feature-based device detection far more\u000areliable. Currently, the best we can do is basically guess what sort of\u000adevice is visiting your site, using heuristics like device resolution\u000a(in magic CSS pixels that automatically get scaled). Using this\u000aapproach, it's hard to distinguish between laptops and tablets, for\u000aexample. It's even more challenging to distinguish phones from 7"\u000atablets, or 7" tablets from 10" ones. Approaches like <a href="https://github.com/borismus/device.js">device.js</a> would\u000abenefit greatly.</p>\u000a\u000a<p>Finally, as you saw, using pixels results in a huge variance in font\u000asize, resulting in bad text readability in general. With physical units,\u000ayou can set a good default baseline size for text to ensure readability.</p>\u000a\u000a<p>Of course, in all of these cases, it's possible to manually rescale the\u000aview in order to get the desired zoom level that's readable and\u000atouchable, but the whole point is to have a system that behaves well by\u000adefault.</p>\u000a\u000a<h2>Scaling well</h2>\u000a\u000a<p>So, in this world of physical sizes for everything, virtually everything\u000aneeds to be scaled to the appropriate size. This includes text, images,\u000aand general layout (eg.  sizes, coordinates, etc). This means having\u000aassets, fonts and a layout engine that's capable of scaling well,\u000awithout loss of visual fidelity.</p>\u000a\u000a<p>General layout is pretty easy to do - all you need is to convert real\u000aunits into pixels for actually rendering. Text gets a bit tricker, since\u000amost fonts are defined only at certain key-sizes, and scaling to\u000afractional sizes might not work perfectly. Scaling images, of course, is\u000aa whole separate topic. Rasters are very difficult to scale down without\u000acompromising quality, and scaling up creates a blurry or pixelated\u000alooking image. The obvious solution is to use scalable image formats. On\u000athe web, this basically means SVG, which is <a href="http://caniuse.com/#search=svg">quite well\u000asupported</a> across browsers, but does have its own\u000aidiosyncrasies.</p>\u000a\u000a<h3>Vectors and rasters</h3>\u000a\u000a<p>Still, even with vector images, there are potential arguments to be made\u000aabout the superiority of pixel-perfect assets, since the designer has\u000aabsolute control to decide their assets at the pixel level.\u000aUnfortunately, the pixel-perfect approach causes a lot of pain for\u000adesigners, who are routinely forced to create multiple versions of the\u000asame asset. On iOS, you specify both the regular and retina asset. On\u000aAndroid, you specify four: <code>ldpi</code>, <code>mdpi</code>, <code>hdpi</code> and <code>xhdpi</code>. These\u000adon't actually get scaled, but the closest one gets served based on the\u000adevice DPI. This means that it's impossible to create assets in an exact\u000aphysical size. Here's an excerpt from the <a href="http://developer.android.com/guide/practices/screens_support.html">Android docs</a>:</p>\u000a\u000a<p><img src="android-dpi.png" alt="Android assets and device DPI" /></p>\u000a\u000a<p>On the web, you can't realistically hope for pixel perfection because of\u000athe vast variety of devices and browsers. Designers need to embrace the\u000amedium, and do as well as they can given the constraints of the web.\u000aScaling the same image (vector or raster) may not be adequate for other\u000areasons. In some cases, especially with icons, you want to specify\u000aassets with different levels of detail depending on the physical size.\u000aCheck out a few <a href="http://mrgan.tumblr.com/post/708404794/ios-app-icon-sizes">great</a> <a href="http://www.pushing-pixels.org/2011/11/04/about-those-vector-icons.html">posts</a> on this subject.</p>\u000a\u000a<blockquote>\u000a  <p>It\u2019s simply not possible to create excellent, detailed icons which can\u000a  be arbitrarily scaled to very small dimensions while preserving clarity.</p>\u000a</blockquote>\u000a\u000a<p><img src="http://farm7.static.flickr.com/6224/6311957505_6f15b6f925.jpg" alt="Varying levels of details depending on size" /></p>\u000a\u000a<p>That said, there's no reason why icons with appropriate levels of detail\u000afor smaller and larger sized screens shouldn't use the same scalable,\u000aphysical approach. In the above example, using scaled assets would let\u000athe designer create 4 instead of 5 separate assets, since the 128px and\u000a64px versions are the same, just scaled.</p>\u000a\u000a<p>It's not all flowers and sausages, though. Vectors are inherently less\u000aefficient to deal with, since they require the intermediate step of\u000arasterization before they can be blitted to the screen buffer. This is\u000aexpensive, but not a show stopper. Optimizing this issue is a topic that\u000aprobably warrants a whole article on its own, but one major win could be\u000ato rasterize once depending on your device DPI, and then cache the\u000arasterized asset so that you don't need to rasterize the vector every\u000atime you render.</p>\u000a\u000a<h2>Doing it on the web</h2>\u000a\u000a<p>If you want to figure out your phone's exact physical dimensions in a\u000abrowser based application, you're out of luck. Even units that share a\u000aname with physical units (like <code>inch</code>, <code>cm</code>, etc) don't actually render\u000athat way. Here's a <a href="http://lewisnyman.github.com/Where-are-our-absolute-units--Demo/">test page</a> that renders some text with\u000aCSS inches. If you measure the actual output on your laptop or phone,\u000ayou'll notice that it's not a real inch.</p>\u000a\u000a<p>Now, you could imagine doing some monkey patching to enable physical\u000aunits using JavaScript, but that's also impossible to do cleanly since\u000ayou need to know either the true device DPI or the physical size of\u000athe screen, neither of which is queriable in any way on the web.\u000aJavaScript does provide <code>window.devicePixelRatio</code>, but that only reports\u000athe scale factor for pixel density, which is used for retina and other\u000adisplays where CSS pixels differ from physical pixels.</p>\u000a\u000a<h2>How does it feel?</h2>\u000a\u000a<p>Even though it's impossible to get physical size or DPI from a browser,\u000aI wanted to try it out, to see if this would work in practice. Here's a\u000a<a href="https://github.com/borismus/physical-units">prototype library</a> that uses entirely physical units\u000afor everything, and fully scalable assets. All objects shown on <a href="http://borismus.github.com/physical-units/controls.html">this\u000acontrols page</a> should render to the correct physical size on\u000athe following devices: iPhone, Galaxy Nexus, iPad, MacBook Air 13". Go\u000aahead and pull out a ruler (I did) and measure - the sizes should be\u000aexact across the supported devices.</p>\u000a\u000a<p>Here's a screenshot of a <a href="http://borismus.github.com/physical-units/sample.html">basic phone UI</a> built with only physical\u000aunits and percentages on an iPhone 4S and Galaxy Nexus:</p>\u000a\u000a<p><img src="android-ios-physical.png" alt="Phones with a UI built using physical units" /></p>\u000a\u000a<h3>Implementation details</h3>\u000a\u000a<p>As mentioned, there's no way to get DPI in the browser. My prototype\u000aworks because I've hard-coded the physical DPI for each supported device\u000abased on user agent matching. It's a terrible hack and not a great\u000aoption for production, even with more complete coverage.</p>\u000a\u000a<p>The prototype currently only works by specifying sizes in absolute\u000aunits. It currently only supports inline styles, and not CSS rules. For\u000aexample, one can now write the DOM like:</p>\u000a\u000a<pre><code>&lt;button style="width: 9mm;"&gt;Nice touch target&lt;/button&gt;\u000a</code></pre>\u000a\u000a<p>I ran into a few small implementation details along the way that I\u000athought might be worth mentioning.</p>\u000a\u000a<p>Firstly, by default, nginx doesn't serve SVG with the correct mime type.\u000aSo, visiting the asset URL just downloads the asset rather than\u000arendering it, and embedding the asset in an <code>img</code> tag doesn't work. The\u000afix is <a href="http://stackoverflow.com/questions/3695409/nginx-offers-of-downoload-svg-instead-of-showing-it">simple and documented</a>: add the correct mime type to an nginx\u000aconfiguration file.</p>\u000a\u000a<p>Secondly, I was a bit disappointed not to find many pre-downloadable SVG\u000aicon sets, but did discover <a href="http://raphaeljs.com/icons/">this resource</a> which has SVG\u000aicons as pathes. To use them it's just a matter of pasting the path\u000aspecification into an SVG <code>path</code> element, as follows:</p>\u000a\u000a<pre><code>&lt;svg width="32" height="32" xmlns="http://www.w3.org/2000/svg"&gt;\u000a  &lt;path d="your-pathspec-goes-here" /&gt;\u000a&lt;/svg&gt;\u000a</code></pre>\u000a\u000a<p>Though unstated, the icons on the above site seem to all be 32x32 units\u000a(Note: not pixels).</p>\u000a\u000a<p>Lastly, since CSS has units that look like physical ones (cm, mm, in,\u000aetc), I wanted to create a new unit, say truemm, truein, etc.\u000aUnfortunately, Chrome (and perhaps other browsers, haven't tested)\u000adoesn't support units that it doesn't recognize. As a result, I had to\u000apiggyback and override existing units. This approach would break the\u000abehavior of sites that use for some unknown-to-me reason, but\u000aintroducing new units prefixed by <code>true</code> is verbose and confusing for\u000abeginners. If you know of any reasons why a developer today would be\u000ausing CSS mms/inches, please let me know.</p>\u000a\u000a<h2>Angular units</h2>\u000a\u000a<p>Rob <a href="http://lists.w3.org/Archives/Public/www-style/2012Feb/0948.html">writes</a>:</p>\u000a\u000a<blockquote>\u000a  <p>The only use-cases for truemm are when you need content matching the\u000a  size of some real-world object, e.g. a ruler, or a life-size image, or\u000a  a human fingertip. That's it.</p>\u000a</blockquote>\u000a\u000a<p>I disagree. It so happens that generally touch devices are used at a\u000arelatively fixed distance from screen to user. This makes physical units\u000auseful for not just touch targets, but also text and image readability.</p>\u000a\u000a<p>But in the same breath, Rob makes a great point:</p>\u000a\u000a<blockquote>\u000a  <p>Ask yourself, "do I really want this content to be the same physical\u000a  size on a phone and a wall projector?" If the answer is yes, use\u000a  truemm, otherwise don't.</p>\u000a</blockquote>\u000a\u000a<p>Indeed, there are cases where physical units aren't ideal. The\u000aconditions for this are roughly:</p>\u000a\u000a<ol>\u000a<li>Unknown distance from viewer to screen.</li>\u000a<li>Unknown screen size.</li>\u000a</ol>\u000a\u000a<p>In this case, the reasonable thing to do is adopt an angular unit, which\u000awould be able to scale depending on viewing distance from the screen\u000a(and of course DPI too).  This might mean that you use degrees, <a href="http://en.wikipedia.org/wiki/Minute_of_arc">minutes\u000aof arc</a>, or some arbitrary angular unit "au" (not to be confused\u000awith <a href="http://en.wikipedia.org/wiki/Astronomical_unit">AU</a>).</p>\u000a\u000a<p>The idea of angular units isn't new. In fact, surprisingly, the CSS spec\u000afeatures it quite prominently in the definition of CSS pixels. A <a href="http://inamidst.com/stuff/notes/csspx">recent\u000aarticle</a> on this topic reminded that pixels are actually\u000aspecified to be angular in nature.</p>\u000a\u000a<blockquote>\u000a  <p>The reference pixel is the visual angle of one pixel on a device with\u000a  a pixel density of 96dpi and a distance from the reader of an arm's\u000a  length. For a nominal arm's length of 28 inches, the visual angle is\u000a  therefore about 0.0213 degrees. For reading at arm's length, 1px thus\u000a  corresponds to about 0.26 mm (1/96 inch). -- <a href="http://www.w3.org/TR/CSS21/syndata.html#length-units">CSS spec</a></p>\u000a</blockquote>\u000a\u000a<p>There are a couple of problem with this.</p>\u000a\u000a<ol>\u000a<li><p>Poor naming: calling such a unit a pixel is incredibly confusing.</p></li>\u000a<li><p>Lack of flexibility: this approach assumes a 96dpi display and a\u000afixed viewing distance. As previously discussed, the former is a bad\u000aassumption, and in the case of large format viewing, so is the latter.</p></li>\u000a</ol>\u000a\u000a<p>Introducing true angular units that can be configured based on display\u000adistance from the observer would be a boon for designers and engineers\u000aworking on software for TVs and other large-format devices. Some default\u000aviewing distance could be provided, but developers should have a means\u000aof specifying a custom viewing distance (potentially inferred from\u000adevice hardware, as described in <a href="http://www.chrisharrison.net/index.php/Research/LeanAndZoom">this research</a>).</p>\u000a\u000a<h2>Next steps</h2>\u000a\u000a<p>Browser vendors need to start supporting true physical units. One path\u000amight be creating all new units (eg. <code>truemm</code>, <code>truein</code>) or redefining\u000athe old ones to be use absolute values, which in many cases may be what\u000athe developers actually intended.</p>\u000a\u000a<p>These units should also be usable in media queries, so that it's\u000apossible to do things like:</p>\u000a\u000a<pre><code>(max-device-width: 4in)\u000a</code></pre>\u000a\u000a<p>This will enable a more robust way to do device detection via media\u000aqueries a la <a href="https://github.com/borismus/device.js">device.js</a>.</p>\u000a\u000a<p>There appear to be some efforts from Mozilla to have a media query for\u000aDPI, via <a href="https://developer.mozilla.org/en/CSS/resolution">resolution</a>. This is a good start, but there should\u000aalso be a way to get DPI directly via JavaScript, otherwise enterprising\u000adevelopers will need to resort to binary searching through possible DPIs\u000avia <code>window.matchMedia</code>, which is just ridiculous.</p>\u000a\u000a<p>The introduction of a true angular unit with a configurable viewing\u000adistance would be amazing for a web that works well on large-format\u000adisplays.</p>\u000a\u000a<p>Ultimately, UI developers need to understand the serious problems with\u000apixels because this problem isn't going away any time soon.</p>\u000a
p1865
tp1866
Rp1867
sg11
V/physical-units
p1868
sg13
Nsg14
I01
sg15
VLet's get physical (units)
p1869
sg18
VThere's an increasing variety of devices in use today.
p1870
sg4
V<p>There's an increasing variety of devices in use today. Even generally\u000arectangular touch enabled devices vary hugely in their physical sizes,\u000aaspect ratios, pixel densities, etc.</p>\u000a\u000a<p>One thing that remains constant across these devices are their users.\u000aTechnologies come and go every year, but people stay the same. Existing\u000aform factors: <a href="http://en.wikipedia.org/wiki/Smart_device">pads, tabs and boards</a> still make sense, and\u000awill continue to do so for the forseeable future. As a result,\u000aergonomic considerations like touch target sizing, readable text and\u000aimage size remain constant. Fingers will be fingers and eyes will be\u000aeyes! Our bodies are firmly rooted in the physical world, and the\u000ainterfaces we create should reflect that.</p>\u000a\u000a
p1871
sg23
g147
sg31
g1868
sg148
(dp1872
g150
S'May'
p1873
sg152
S'May 2, 2012'
p1874
sg154
I5
sg155
S'2012-05-02T09:00:00-00:00'
p1875
sg157
I1335974400
sg158
I2012
sg159
I2
ssg61
g160
sg29
S'physical-units'
p1876
sS'posted'
p1877
g166
(S'\x07\xdc\x05\x02'
p1878
tp1879
Rp1880
ssg32
S'content/posts/2012/physical-units/index.md'
p1881
sg34
F1433825576.0
sa(dp1882
g2
(dp1883
g26
g5
(g6
g7
V<p>I re-designed this site using the <a href="http://www.google.com/webfonts/specimen/PT+Sans">PT Sans font</a>, aiming for\u000aappealing typography for optimal readability. Interestingly,</p>\u000a\u000a<blockquote>\u000a  <p>PT Sans is based on Russian sans serif types of the second part of the\u000a  20th century, but at the same time has distinctive features of\u000a  contemporary humanistic designs.</p>\u000a</blockquote>\u000a\u000a<p>Since visitors are increasingly coming from a variety of devices, I also\u000acreated three variants of the site for small, medium and large screens\u000avia media queries:</p>\u000a\u000a<!--more-->\u000a\u000a<pre><code>@media screen and (max-width: $small) {/* small */}\u000a@media screen and (min-width: $small) and (max-width: $large) {/* medium */}\u000a@media screen and (min-width: $large) {/* large */}\u000a</code></pre>\u000a\u000a<p>Note that the above is valid <a href="http://sass-lang.com/">SCSS</a> (as of version 3.2)! To try it out,\u000ayou may need to install the prerelease via <code>gem install sass --pre</code>.</p>\u000a\u000a<p>As usual, the layout of this site is largely inspired by designers far\u000amore skilled than I, including: <a href="http://viljamis.com/">http://viljamis.com/</a> and\u000a<a href="http://www.markboulton.co.uk/">http://www.markboulton.co.uk/</a>.</p>\u000a\u000a<h2>New engine</h2>\u000a\u000a<p>This blog can now also double as a micro blog, where I can easily <a href="/archive/links">post\u000alinks</a> in a social network-agnostic way. Similarly, it now hosts\u000a<a href="/archive/talks">all of my talks</a>, which used to live on\u000a<a href="http://smustalks.appspot.com">http://smustalks.appspot.com</a>. It's also a lot easier for me to create\u000anew posts.</p>\u000a\u000a<p>All of these structural changes are largely due to me building a\u000acompletely new static site generator which addresses a lot of my pains\u000ausing other static generators. I'll write a separate article about this\u000aengine when I'm ready to release it to the public.</p>\u000a\u000a<h2>No more comments</h2>\u000a\u000a<p>Since the start of this blog, I've struggled with the concept of\u000acomments on my site and in on blogs in general. Even with services like\u000adisqus, which try to integrate your blog commentator personality into\u000aone place, I feel that in practice, commenting on the web is very hard\u000ato keep in one place, and the most interesting discussions end up\u000adistributed in many pockets such as Hacker News, twitter, etc. Having\u000athe in-blog comments as well only makes things messier.</p>\u000a\u000a<p>Other problems include:</p>\u000a\u000a<ul>\u000a<li>Leaving a comment right after reading content encourages unconsidered\u000aresponses, since readers haven't had time to process the content.</li>\u000a<li>Lately my blog has been generating a lot of spam comments that I need\u000ato moderate. I don't have time for that.</li>\u000a<li>People tend to ask for technical support in the comments, which adds\u000avery little value for other readers.</li>\u000a<li>Disqus slows down page load time, and presents a UI that's not easily\u000askinnable.</li>\u000a</ul>\u000a\u000a<p>For a more in-depth analysis about blog comments, see Matt Gemmell's\u000athorough post <a href="http://mattgemmell.com/2011/11/29/comments-off/">on this subject</a>.</p>\u000a\u000a<p>I've disabled comments on all posts. That said, I still want to hear\u000ayour ideas and feedback, and engage in discussion around topics I'm\u000aobviously interested in (enough to write about!)</p>\u000a\u000a<h2>Evolving designs</h2>\u000a\u000a<p>This is the fourth iteration of my blog's design. I apparently do a\u000are-design every year, since I started in 2008.</p>\u000a\u000a<p><style>\u000aarticle img { border: 1px solid gray; }\u000a</style></p>\u000a\u000a<p><img src="v1.png" alt="original un-design" /></p>\u000a\u000a<p>This version was a small modification done to an existing wordpress\u000atheme.</p>\u000a\u000a<p><img src="v2.png" alt="first iteration" /></p>\u000a\u000a<p>I really liked this version for a long time, since it was so clean and\u000aminimal. Unfortunately the header took way too much space, and the\u000atypography left something to be desired.</p>\u000a\u000a<p><img src="v3.png" alt="second iteration" /></p>\u000a\u000a<p>This was the first "responsive" version. It had too many gimmicky,\u000anon-standard design elements.</p>\u000a\u000a<p><img src="v4.png" alt="current iteration" /></p>\u000a\u000a<p>I'm reasonably happy with the current version, especially the typography\u000ain the main body.</p>\u000a\u000a<p>And strangely excited about the engine that powers this blog too\u000a(codenamed <em>Lightning</em>, which generates a lot of static, and is also\u000avery fast! Ha, get it?). Once released, I will write another meta-blog\u000apost about it. For now, though, I wrote a bit about Lightning\u000a<a href="/site">here</a>.</p>\u000a
p1884
tp1885
Rp1886
sg11
V/redesign-2012
p1887
sg13
Nsg14
I01
sg15
VNew design
p1888
sg18
VI re-designed this site using the [PT Sans font][ptsans], aiming for\u000aappealing typography for optimal readability.
p1889
sg4
V<p>I re-designed this site using the <a href="http://www.google.com/webfonts/specimen/PT+Sans">PT Sans font</a>, aiming for\u000aappealing typography for optimal readability. Interestingly,</p>\u000a\u000a<blockquote>\u000a  <p>PT Sans is based on Russian sans serif types of the second part of the\u000a  20th century, but at the same time has distinctive features of\u000a  contemporary humanistic designs.</p>\u000a</blockquote>\u000a\u000a<p>Since visitors are increasingly coming from a variety of devices, I also\u000acreated three variants of the site for small, medium and large screens\u000avia media queries:</p>\u000a\u000a
p1890
sg23
g147
sg31
g1887
sg148
(dp1891
g150
S'Mar'
p1892
sg152
S'March 30, 2012'
p1893
sg154
I3
sg155
S'2012-03-30T09:00:00-00:00'
p1894
sg157
I1333123200
sg158
I2012
sg159
I30
ssg61
g160
sg29
S'redesign-2012'
p1895
sS'posted'
p1896
g166
(S'\x07\xdc\x03\x1e'
p1897
tp1898
Rp1899
ssg32
S'content/posts/2012/redesign-2012/index.md'
p1900
sg34
F1433825581.0
sa(dp1901
g2
(dp1902
g26
g5
(g6
g7
V<p>These days, I write a lot more code, and my projects have increased in\u000acomplexity. Often, a single application brings many kinds of data\u000asources and bleeding edge web features together. On the other hand, most\u000aof what I build are prototypes, which need to be churned out quickly,\u000awork reliably in demos, and look/feel good.</p>\u000a\u000a<p>MVC frameworks help write UI code much more quickly, but there are\u000adrawbacks too: there are too many to choose from, they don't\u000ainteroperate with one another, and if you want to release parts of your\u000acode to the open source community, only those developers that use the\u000asame framework will benefit. The solution is to create a clear\u000aseparation between core application logic and MVC UI code. This way you\u000acan reuse a lot of code and reduce switching costs.</p>\u000a\u000a<!--more-->\u000a\u000a<p>Before jumping in, here's a diagram of the approach:</p>\u000a\u000a<p><img src="mvc-js-layers.svg" alt="diagram" /></p>\u000a\u000a<p>The rest of the post is about interop problems with JS MVC frameworks,\u000aand a closer look reasons for taking the above approach.</p>\u000a\u000a<h2>MVC frameworks are great</h2>\u000a\u000a<p>One of my key requirements is to be able to build user interfaces\u000a<strong>quickly</strong>. This means that writing in pure JavaScript is out of the\u000aquestion, because it simply doesn't provide a high enough layer of\u000aabstraction for highly interactive user interfaces, and reduces to\u000aspaghetti code in just a couple of days. Instead of raw JS, use an model\u000aview controller (MVC) framework.</p>\u000a\u000a<p>My current weapon of choice is <a href="http://emberjs.com/">Ember.js</a>, mostly because it\u000amakes many decisions that I agree with, providing easy two-way binding,\u000aa sane templating syntax, observers and computed properties. All-in-all,\u000ait is pretty effective for whipping up consistent UIs quickly.</p>\u000a\u000a<p>But there are problems with buying into an MVC framework.</p>\u000a\u000a<h2>JS libraries are like insects</h2>\u000a\u000a<p>JavaScript frameworks are like insects. There are thousands of them,\u000athey move very quickly, and generally have very short life spans.</p>\u000a\u000a<p>If you've ever taken a hiatus from client-side web development, you were\u000aprobably overwhelmed with the amount of new stuff available when you\u000areturned. The flipside is that many of the frameworks you were familiar\u000awith could have easily disappeared. To be fair, some winners have\u000aemerged in the past, most notably in the utility frameworks, where\u000ajQuery has risen to the top, and Prototype has fallen to obscurity. In\u000aMVC frameworks, however, there are many interesting contenders and, as I\u000awrote earlier, still <a href="http://smus.com/backbone-and-ember/">no clear winner</a>.</p>\u000a\u000a<p>So at this stage, going all-in in on a framework may be a bad idea,\u000abecause what if the community moves on to something else? What if the\u000aframework developers get bored, stop caring or cease maintenance for\u000asome other reasons? I have this feeling all the time, despite framework\u000aauthors promises to the contrary. And this is the case even though I\u000abuild mostly prototypes with relatively short life spans.</p>\u000a\u000a<h2>Switching cost between MVC frameworks is high</h2>\u000a\u000a<p>Once you bite the bullet and decide to invest in a framework, you often\u000ahave no easy way to move your code out of it. If you pick Backbone, but\u000adecide mid-cycle that it's not for you, you are in for a world of hurt:</p>\u000a\u000a<p>Not only do your Models and Views not share the same base classes, they\u000adon't even use the same <strong>class system</strong>. Backbone and Ember provide\u000atheir own class systems that are not compatible. This is a ridiculous\u000aproblem to have, and one unique to JavaScript, which provides a\u000aprototypal inheritance system which is so inconvenient, there are about\u000a<a href="http://goo.gl/hqAHD">a million libraries</a> that add OO-style classes to the\u000alanguage.</p>\u000a\u000a<ol>\u000a<li>Those who use or invent a custom class system in JavaScript that\u000alooks more like traditional OO.</li>\u000a<li>Those who don't believe in classes, or think that JavaScript provides\u000aenough through prototypal inheritance.</li>\u000a</ol>\u000a\u000a<p>For the reasons outlined above, I'm very much in favor of the former\u000aoption: having a language-level class abstraction. This seems to be\u000a<a href="http://h3manth.com/content/classes-javascript-es6">coming soon in ECMAScript 6</a>, and will basically provide\u000asyntactic sugar on top of prototypal inheritance. Having a consistent\u000aclass and module system is one of the main reasons why languages like\u000a<a href="http://www.dartlang.org/">Dart</a>, <a href="http://www.typescriptlang.org/">TypeScript</a> and <a href="http://coffeescript.org/">Coffeescript</a> are increasingly\u000aappealing to me.</p>\u000a\u000a<h2>Coding to a framework restricts your audience</h2>\u000a\u000a<p>Because of a lack of interoperability between frameworks and their class\u000asystems, if you write non-UI code using a framework, only users of that\u000aframework will use your code. It's very unlikely that someone building\u000aan Ember application will want to use your library that uses Backbone\u000aobjects. </p>\u000a\u000a<p>Often your collaborators may have varied tastes and prefer one framework\u000aover another, but including multiple MVC frameworks in the same\u000aapplication gets messy quickly. If you have core functionality that you\u000awant to release, release it in pure JavaScript, not as a jQuery plugin,\u000aor Ember module. Of course use prototypal inheritance and proper\u000aabstraction (or at least, as proper as JS can provide).</p>\u000a\u000a<h2>Solution: defensive architecture</h2>\u000a\u000a<p>To avoid framework and class-system lock-in, I have taken a slightly\u000adifferent approach to developing with JavaScript MVC frameworks. It\u000aaffords the convenience of building a UI with MVC, but keeps the core\u000aof the application flexible.</p>\u000a\u000a<p>The basic idea is to separate the core functionality of the application\u000afrom the user interface into two separate layers. With this separation,\u000ayou can implement the two layers differently:</p>\u000a\u000a<ol>\u000a<li><p>Build the base layer using pure JavaScript prototypal inheritance.\u000aThis is the part you write with the intention of keeping for later.\u000aThis base layer will need an API that you will want to spend a bit of\u000atime honing.  To make the separation crystal clear, you can think of the\u000aUI as a client that uses this API as if it were on the server. This way\u000ayou can avoid creating leaky abstractions.</p></li>\u000a<li><p>Use an MVC framework to implement the UI, and call into the base\u000alayer directly. This lets you move quickly and focus entirely on writing\u000athe user interface. This architecture lets you build your UI on a solid\u000afoundation and avoid getting stuck.</p></li>\u000a</ol>\u000a\u000a<h2>Benefits of this approach</h2>\u000a\u000a<p>You get many benefits by taking this approach:</p>\u000a\u000a<ol>\u000a<li><p>If you want to scrap your existing UI and write a new one very\u000aquickly, you can easily do this and still reuse large chunks of your\u000alogic.</p></li>\u000a<li><p>Clear layer separation leads to more maintainable code.</p></li>\u000a<li><p>Easy to ship the underlying core functionality as a library or\u000astandalone module.</p></li>\u000a<li><p>Easy to write unit tests for the core functionality of the\u000aapplication. Unit tests aren't well suited to user interface code\u000aanyway.</p></li>\u000a</ol>\u000a\u000a<h2>This is why we can't have nice things</h2>\u000a\u000a<p>The lack of a widely used class system is ridiculous. The sheer number\u000aof different JS class systems is a clear signal that this is a big\u000aomission in the language. Similarly for MVC frameworks. A renewed\u000ainterest in JavaScript MVC shows that the web platform needs something\u000abuilt-in to address this problem.</p>\u000a\u000a<p>All other widely used programming languages provide a consistent class\u000asystem, and popular platforms provide a framework for separating\u000aapplication logic and user interface. Until these things come to the\u000aweb, I'll continue to have second thoughts about embracing any\u000aparticular MVC framework or custom class system.</p>\u000a
p1903
tp1904
Rp1905
sg11
V/reusable-js-mvc-frameworks
p1906
sg13
Nsg14
I01
sg15
VReusable JavaScript for MVC frameworks
p1907
sg18
VThese days, I write a lot more code, and my projects have increased in\u000acomplexity.
p1908
sg4
V<p>These days, I write a lot more code, and my projects have increased in\u000acomplexity. Often, a single application brings many kinds of data\u000asources and bleeding edge web features together. On the other hand, most\u000aof what I build are prototypes, which need to be churned out quickly,\u000awork reliably in demos, and look/feel good.</p>\u000a\u000a<p>MVC frameworks help write UI code much more quickly, but there are\u000adrawbacks too: there are too many to choose from, they don't\u000ainteroperate with one another, and if you want to release parts of your\u000acode to the open source community, only those developers that use the\u000asame framework will benefit. The solution is to create a clear\u000aseparation between core application logic and MVC UI code. This way you\u000acan reuse a lot of code and reduce switching costs.</p>\u000a\u000a
p1909
sg23
g147
sg31
g1906
sg148
(dp1910
g150
S'Nov'
p1911
sg152
S'November 6, 2012'
p1912
sg154
I11
sg155
S'2012-11-06T09:00:00-00:00'
p1913
sg157
I1352221200
sg158
I2012
sg159
I6
ssg61
g160
sg29
S'reusable-js-mvc-frameworks'
p1914
sS'posted'
p1915
g166
(S'\x07\xdc\x0b\x06'
p1916
tp1917
Rp1918
ssg32
S'content/posts/2012/reusable-js-mvc-frameworks/index.md'
p1919
sg34
F1433825589.0
sa(dp1920
g2
(dp1921
g26
g5
(g6
g7
V<p>About a year ago, I wrote an overview of many of the different <a href="http://www.html5rocks.com/en/mobile/high-dpi/">responsive\u000aimage approaches</a> in an HTML5Rocks article, all of which try to solve the\u000afundamental problem:</p>\u000a\u000a<p><strong>Serve the optimal image to the device.</strong></p>\u000a\u000a<p>Sounds simple, but the devil's in the details. For the purposes of this here\u000adiscussion, I will focus on optimal image size and fidelity, and much to your\u000achagrin, will completely ignore the art direction component of the problem.</p>\u000a\u000a<p>Even for tackling screen density, a lot of the solutions out there involve a\u000alot of extra work for web developers. I'll go into two solutions (client and\u000aserver side) on the horizon that serve the right density images. In both cases,\u000aall you need to do is:</p>\u000a\u000a<pre><code>&lt;img src="img.jpg"/&gt;\u000a</code></pre>\u000a\u000a<!--more-->\u000a\u000a<h2>Nobody cares about responsive images (that much)</h2>\u000a\u000a<p>Let me start with an underlying problem: for one reason or another, most developers\u000adon't really care that much about responsive images. Even if left unsolved,\u000athe images still get to their destination, they're just a little crummier than\u000athey should be. If fidelity doesn't matter much to you and your app, then no\u000abig deal.</p>\u000a\u000a<p>Others may not even know about the problem. If you're not a high density screen\u000auser, you may have not been disappointed by the gulf in quality between crisp\u000aimages in native apps and blurry images in web apps. Some applications may prioritize\u000aperformance over fidelity, and want to deliberately send low resolution images.</p>\u000a\u000a<p><strong>A vast majority of devs know about the problem, but are just waiting for a\u000asolution that works well</strong>. We're all inherently lazy and in my opinion, a\u000areasonable solution is one that requires little to no extra work.</p>\u000a\u000a<h2>Good solutions require almost no extra work</h2>\u000a\u000a<p>How can we serve the optimal image to the device with as little work as\u000apossible?  One approach is to always serve a highly compressed but high density\u000aimage, as I outlined in <a href="http://www.html5rocks.com/en/mobile/easy-high-dpi-images/">Easy High DPI Images</a> on HTML5Rocks. This\u000aapproach is better than nothing, but isn't really optimal since you end up\u000asending high density images to low density screens.</p>\u000a\u000a<p>Two promising standards are on the horizon to wider adoption: the <code>srcset</code>\u000aattribute for <code>img</code> elements, and the <code>CH</code> client hint header.</p>\u000a\u000a<h3>Solution 1: Client-side build step with srcset &amp; friends</h3>\u000a\u000a<p>The <code>srcset</code> attribute <a href="https://www.webkit.org/blog/2910/improved-support-for-high-resolution-displays-with-the-srcset-image-attribute/">recently landed in WebKit</a>, and it looks like\u000aothers will follow. Though it's more terse than <code>&lt;picture&gt;</code> and friends, <code>srcset</code>\u000astill requires quite a bit of extra work to implement:</p>\u000a\u000a<pre><code>&lt;img src="img.jpg" srcset="img-1.5x.jpg 1.5x, img-2x.jpg 2x, img-3x.jpg 3x"&gt;\u000a</code></pre>\u000a\u000a<p><a href="http://www.w3.org/TR/css4-images/#image-set-notation"><code>image-set</code></a> is the CSS equivalent, and looks quite similar.\u000aUnfortunately it requires even more work:</p>\u000a\u000a<pre><code>selector {\u000a  background-image: url(img.jpg);\u000a  background-image: -webkit-image-set(\u000a      url(img-1.5x.jpg) 1.5x, url(img-2x.jpg) 2x, url(img-3x.jpg) 3x);\u000a  background-image: -moz-image-set(\u000a      url(img-1.5x.jpg) 1.5x, url(img-2x.jpg) 2x, url(img-3x.jpg) 3x);\u000a  background-image: -ms-image-set(\u000a      url(img-1.5x.jpg) 1.5x, url(img-2x.jpg) 2x, url(img-3x.jpg) 3x);\u000a  background-image: -o-image-set(\u000a      url(img-1.5x.jpg) 1.5x, url(img-2x.jpg) 2x, url(img-3x.jpg) 3x);\u000a  /* Hehe, moar prefixes! */\u000a}\u000a</code></pre>\u000a\u000a<p>Phew! After you have exploded your markup, you need to generate multiple images\u000aof different sizes and decide on appropriate compression levels for each.</p>\u000a\u000a<p>You'll notice that this extra work is very formulaic. It almost looks like it\u000acould be automated! Let's skip the busywork and write our web pages like we do\u000atoday, specifying a very high quality asset (eg. 3x), and running a build\u000ascript. In your markup, all you need to do is:</p>\u000a\u000a<pre><code>&lt;img src="img.jpg" /&gt;\u000a</code></pre>\u000a\u000a<p>or</p>\u000a\u000a<pre><code>selector {\u000a  background-image: url(img.jpg);\u000a}\u000a</code></pre>\u000a\u000a<p>This magic time-saving script would need to do two things. First, it\u000agenerates images:</p>\u000a\u000a<ol>\u000a<li>Find all image files on the site.</li>\u000a<li>Downsize all image files to the right size depending on desired density breakpoints (eg. <code>1x, 1.5x, 2x, 3x</code>).</li>\u000a<li>Name the images according to some convention (eg. <code>${image}-${density}.${format}</code>).</li>\u000a</ol>\u000a\u000a<p>Image resizing already has a <a href="http://addyosmani.com/blog/generate-multi-resolution-images-for-srcset-with-grunt/">grunt-based solution</a>, and many\u000aothers will surely follow. The second part is rewriting the HTML and\u000aCSS. Here's how it works:</p>\u000a\u000a<ol>\u000a<li>Parse all image references from HTML (eg. <code>img</code>) and CSS (eg. <code>background</code>,\u000a<code>background-image</code>).</li>\u000a<li>Augment all HTML <code>img</code> elements with the right srcset. Augment all CSS\u000a<code>background</code> and <code>background-image</code> properties with the right (and prefixed)\u000aimage-set value.</li>\u000a</ol>\u000a\u000a<p>Now we're talking! And all you need to do is provide one set of high quality\u000aimage assets and add this script to your build step (you have a build step,\u000aright?). Keep writing those <code>&lt;img src&gt;</code>s!</p>\u000a\u000a<h3>Solution 2: Server-side build step with Client-Hints</h3>\u000a\u000a<p>The <a href="http://tools.ietf.org/html/draft-grigorik-http-client-hints-00">Client-Hints proposal</a> (CH) is another promising (read: minimal\u000adeveloper effort required) future direction that would help solve the\u000aresponsive image problem on the server. Ilya Grigorik goes into much\u000amore detail in <a href="http://www.igvita.com/2013/08/29/automating-dpr-switching-with-client-hints/">his post</a>.</p>\u000a\u000a<p>Currently, the main thing a server has to identify a client is its\u000aUser-Agent (UA) header. The UA header is insufficient to infer basic\u000athings like display density, even in conjunction with a <a href="http://en.wikipedia.org/wiki/WURFL">UA\u000adatabase</a>. CH is a new header used to pass information to the\u000aserver about the user agent.  With it, you can specify the\u000a<code>devicePixelRatio</code> (DPR) of your device explicitly:</p>\u000a\u000a<pre><code>CH: dpr=2\u000a</code></pre>\u000a\u000a<p>Once browsers send this CH header, you can imagine some really simple\u000aserver-side logic to serve the best asset for the DPR specified. You will need\u000aeither a smart image generator (and cache) on the server, or a build script for\u000agenerating images at different densities. Luckily this build script is the same\u000aas the first half of solution 2, so less work for us! Once the images are\u000agenerated, it's just a matter of producing the right redirects based on the CH\u000aheader, which Ilya provides his <a href="http://www.igvita.com/2013/08/29/automating-dpr-switching-with-client-hints/">article</a>.</p>\u000a\u000a<p>One benefit of solving this problem server side is that it's universal\u000aand completely transparent to the client. A drawback to the first\u000a(client-side) solution is that it will not work when setting <code>&lt;img src&gt;</code>\u000awith JavaScript, although this can be remedied easily with a loader that\u000ayou use to specify the image asset. In practice, instead of specifying\u000athe image asset directly, you would need to go through a small image URL\u000arewriter. Imagine something like this:</p>\u000a\u000a<pre><code>var imagePath = images.get('img.jpg');\u000a// imagePath is now img-2x.jpg if on a 2x display.\u000aimageEl.src = imagePath;\u000a</code></pre>\u000a\u000a<p>Another benefit of the server-side approach is that there's no need for\u000aparsing HTML and CSS (the second part of the build step) which can be\u000atricky and error prone.</p>\u000a\u000a<h2>Both solutions are good</h2>\u000a\u000a<p>In summary, both solutions have merit, and since <code>srcset</code> has <a href="https://www.webkit.org/blog/2910/improved-support-for-high-resolution-displays-with-the-srcset-image-attribute/">momentum\u000aalready</a>, it should be standardized and broadly supported as soon as\u000apossible. Many designers may not have access to server side configuration, so\u000afor them the client-side build script would make sense. Conversely, many\u000adevelopers that have access to server-side image generators and advanced\u000acaching techniques should take advantage of Client-Hints once it's\u000aavailable, which <a href="https://groups.google.com/a/chromium.org/forum/#!topic/blink-dev/c38s7y6dH-Q">may be soon</a>!</p>\u000a\u000a<p>Now, to write that build script... Any volunteers?</p>\u000a
p1922
tp1923
Rp1924
sg11
V/responsive-image-workflow
p1925
sg13
Nsg14
I01
sg15
VResponsive image workflow
p1926
sg18
VAbout a year ago, I wrote an overview of many of the different [responsive\u000aimage approaches][h5r1] in an HTML5Rocks article, all of which try to solve the\u000afundamental problem:\u000a\u000a**Serve the optimal image to the device.
p1927
sg4
V<p>About a year ago, I wrote an overview of many of the different <a href="http://www.html5rocks.com/en/mobile/high-dpi/">responsive\u000aimage approaches</a> in an HTML5Rocks article, all of which try to solve the\u000afundamental problem:</p>\u000a\u000a<p><strong>Serve the optimal image to the device.</strong></p>\u000a\u000a<p>Sounds simple, but the devil's in the details. For the purposes of this here\u000adiscussion, I will focus on optimal image size and fidelity, and much to your\u000achagrin, will completely ignore the art direction component of the problem.</p>\u000a\u000a<p>Even for tackling screen density, a lot of the solutions out there involve a\u000alot of extra work for web developers. I'll go into two solutions (client and\u000aserver side) on the horizon that serve the right density images. In both cases,\u000aall you need to do is:</p>\u000a\u000a<pre><code>&lt;img src="img.jpg"/&gt;\u000a</code></pre>\u000a\u000a
p1928
sg23
g147
sg31
g1925
sg148
(dp1929
g150
S'Sep'
p1930
sg152
S'September 9, 2013'
p1931
sg154
I9
sg155
S'2013-09-09T09:00:00-00:00'
p1932
sg157
I1378742400
sg158
I2013
sg159
I9
ssg61
g160
sg29
S'responsive-image-workflow'
p1933
sS'posted'
p1934
g166
(S'\x07\xdd\t\t'
p1935
tp1936
Rp1937
ssg32
S'content/posts/2013/responsive-image-workflow.md'
p1938
sg34
F1378651351.0
sa(dp1939
g2
(dp1940
g26
g5
(g6
g7
V<p>I just got back from Scotland, where I had the pleasure of attending\u000aUIST 2013 in St. Andrews. This was my second time attending, and again\u000ait was incredibly engaging and interesting content. I was impressed\u000aenough to take notes just like <a href="http://smus.com/uist-2011/">my last UIST in 2011</a>. What\u000afollows are my favorite talks with demo videos. I grouped them into\u000atopics of interest: gestural interfaces, tangibles and GUIs.</p>\u000a\u000a<!--more-->\u000a\u000a<h3>Quadrotor Tricks</h3>\u000a\u000a<p>UIST kicked off with a very compelling demos from Rafaello D'Andrea,\u000aprofessor at ETH, co-founder of Kiva. He currently works on the <a href="http://www.flyingmachinearena.org/">flying\u000amachine arena</a>, a lab at ETH working on quadrotor control systems.</p>\u000a\u000a<p>I really liked the flight assembled architecture idea: a building\u000aassembled by quadrotors.</p>\u000a\u000a<iframe width="560" height="315" src="//www.youtube.com/embed/JnkMyfQ5YfY" frameborder="0" allowfullscreen></iframe>\u000a\u000a<p>Rafaello also showed off a kinect controlled quadrotor. A pointing\u000ainterface to control quadrotors. Other highlights included the ability\u000ato place the quadrotor with your hand, and simulating environments like\u000acontrolled gravity, virtual walls, springs, and damped oscillations.</p>\u000a\u000a<h3>Mime: Compact, Low Power 3D Gesture Sensing</h3>\u000a\u000a<p>An MIT Media Lab group presented a pretty neat approach for gesture\u000atracking combining time-of-flight and RGB cameras. The approach is\u000acompact enough to be embedded on a HUD device like Google Glass.</p>\u000a\u000a<p>The specs are impressive: 100 FPS, sub-centimeter resolution, low-power\u000a(45 mW). Showed glasses hardware with 3 cameras (baseline = face) and an\u000aIR LED. Here's roughly how it works:</p>\u000a\u000a<ol>\u000a<li>Illuminate scene with IR. Backscatter light captured by cameras.</li>\u000a<li>Time-of-flight approach. Source <code>s(t)</code> and response <code>r_n(t)</code>. Look\u000afor time-shifted waveforms.</li>\u000a<li>...Lots of crazy math reducing to convex optimization...</li>\u000a</ol>\u000a\u000a<p>Applications presented were a bit limited, mostly focused on in-air\u000awriting and drawing. They also presented some cringe-worthy menu\u000anavigation. The last and most obvious application was games.</p>\u000a\u000a<h3>Gaze Locking: Passive Eye Contact Detection for Human\u2013Object Interaction</h3>\u000a\u000a<p>Surprisingly insightful project from Columbia based on a simple idea:\u000agaze tracking is hard. Knowing WHERE the user is looking is very\u000adifficult, but knowing IF the user is looking is much easier. I loved\u000athe approach of <a href="http://blog.kenperlin.com/?p=13296">solving the simpler problem</a>.</p>\u000a\u000a<p>Detector approach:</p>\u000a\u000a<ol>\u000a<li>Eye corner detection</li>\u000a<li>Geometric rectification</li>\u000a<li>Mask eye area</li>\u000a<li>Extract features from 96x26px rectangle.</li>\u000a<li>PCA + MDA compression</li>\u000a<li>Binary classifier (gaze locked or not).</li>\u000a</ol>\u000a\u000a<p>They also generated a Gaze Data set (6K images). The detector actually\u000adoes better than human vision. Works well from 18m away, though the\u000apresenter claimed there was no degradation as a function of distance,\u000awhich was very suspicious.</p>\u000a\u000a<p>They also presented a series of compelling applications:</p>\u000a\u000a<ul>\u000a<li>Human-object interaction (very cool video of iPads powering on based\u000aon gaze).</li>\u000a<li>Ad analytics (wow, incredible potential for Google/Signs team).</li>\u000a<li>Sort/filter images by eye contact (as a measure of photo quality).</li>\u000a<li>Gaze-triggered photography (when everyone is looking at the camera).</li>\u000a</ul>\u000a\u000a<p>More info on <a href="http://www.cs.columbia.edu/CAVE/projects/gaze_locking/index.php">the lab's site</a>.</p>\u000a\u000a<h3>BodyAvatar: Creating 3-D Avatars with Your Body and Imagination</h3>\u000a\u000a<p>Setting your avatar in video games is annoying. You basically go through\u000aa wizard based on a GUI. This delightful implementation from Microsoft\u000aResearch uses your body to build your character's avatar. Creation\u000abegins from the first person, as you create a general skeleton for the\u000aavatar. Then the perspective changes to third person as you add\u000acustomizations using gestures. The final stage lets you paint your\u000aavatar from the third person perspective.</p>\u000a\u000a<iframe width="560" height="315" src="//www.youtube.com/embed/yU2Ai18tft4" frameborder="0" allowfullscreen></iframe>\u000a\u000a<p>They also showed some impressive demos of stepping into limbs for\u000aparticularily complex models (eg. butterfly with 6 limbs). Super cool!</p>\u000a\u000a<h3>Sauron: Embedded Single-Camera Sensing of Printed Physical User Interfaces</h3>\u000a\u000a<p>Excellent work from Berkeley showing how a single camera can drive a\u000awhole printed physical UI. The idea is that you 3D print an object,\u000ainsert a camera and have a fully functional input device.</p>\u000a\u000a<p>Sauron simulates full motions of all components, ensures that everything\u000ais visible via ray casting. One problem is that you can't always see the\u000awhole interior. So Sauron modifies the design by extruding inputs,\u000aadding mirrors.</p>\u000a\u000a<p>A good question was asked about doing the same for output. Using a\u000atransparent material you might also be able to light up specific areas\u000aof the prototype, but apparently 3d printers can't print\u000atransparent/translucent plastics. Cool future work might be to design\u000amobile tangibles that snap to a phone and use the phone's camera.</p>\u000a\u000a<iframe width="560" height="315" src="//www.youtube.com/embed/GNdCnmm-cw8" frameborder="0" allowfullscreen></iframe>\u000a\u000a<p>Ok, that's all for vision and gestures. Now on to tangibles:</p>\u000a\u000a<h3>PneUI: pneumatically activated soft materials</h3>\u000a\u000a<p>Ishii's group presented nature-inspired interfaces that are\u000atransformative and responsive. Using mostly air pockets, they set out to\u000acreate tangible UIs inspired by soft marine organisms. Some examples of\u000athe applications:</p>\u000a\u000a<ol>\u000a<li><p>Curvature: folding wristband/phone. Wraps up when placed on wrist.\u000aUnwraps when used as a tablet. Pulsates shape changes to indicate\u000aincoming calls.</p></li>\u000a<li><p>Volume-change based interfaces with underlying origami substructure.\u000aApplication: origami accordion with variable height and input.</p></li>\u000a<li><p>Micro + macro elastomers to create transformable textures.\u000aApplication: "feel" GPS on the steering wheel rather than see/hear.</p></li>\u000a</ol>\u000a\u000a<iframe src="//player.vimeo.com/video/63591283" width="500" height="281" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>\u000a\u000a<p>Mind = blown.</p>\u000a\u000a<h3>Paper Generators: Harvesting Energy from Touching, Rubbing and Sliding</h3>\u000a\u000a<p>Disney research presented a way of harvesting energy from interaction,\u000aprimarily for popup book-type applications. Based mostly on static\u000aelectricity, they used teflon, which has low electron affinity. Rubbing\u000ait on paper causes a discharge. Rubbing generates 500 A, 1200 V.\u000aTapping generates 60 mW.</p>\u000a\u000a<p>The approach is easy to build, printable with conductive ink cartridges.\u000aIn addition to rubbing, showed a bunch of different widgets that can\u000agenerate electricity - buttons, cranks, </p>\u000a\u000a<p>Approach 1: direct energy usage. (eg. animations on e-ink displays.)\u000aApproach 2: store and release if more energy is needed. (eg. actuate servos.)</p>\u000a\u000a<iframe width="560" height="315" src="//www.youtube.com/embed/4WaUcXSfPTg" frameborder="0" allowfullscreen></iframe>\u000a\u000a<h3>Touch &amp; Activate: Adding Interactivity via Active Acoustic Sensing</h3>\u000a\u000a<p>Tsukuba University presented a very cool paper on adding acoustic\u000asensing to hard objects using contact mics and speakers. The basic idea\u000ais that touching an object changes its bounding conditions, depending on\u000ahow it is touched. </p>\u000a\u000a<p>The way it works is they vibrate objects at a wide frequency range and\u000acapture the response.</p>\u000a\u000a<ol>\u000a<li>Attach contact speaker and microphone.</li>\u000a<li>Make the object vibrate, doing a sweep signal from 20-40 KHz (inaudible).</li>\u000a<li>Vibration response determined by object properties.</li>\u000a<li>Extract features via FFT</li>\u000a<li>Classify via SVM</li>\u000a</ol>\u000a\u000a<p>Applications:</p>\u000a\u000a<ul>\u000a<li>Simple music player based on duplo blocks.</li>\u000a<li>Interactive animal body</li>\u000a<li>Grasp recognition system for phone using a case.</li>\u000a</ul>\u000a\u000a<iframe width="560" height="315" src="//www.youtube.com/embed/XgxXi6w8IQc" frameborder="0" allowfullscreen></iframe>\u000a\u000a<p>How cool is that? Anyway, now for something a bit more traditional:</p>\u000a\u000a<h3>Transmogrifiers: Casual Manipulations of Visualizations</h3>\u000a\u000a<p>University of Calgary presented their awesome visualization toolkit.\u000aTheir goal is to enable exploration and manipulation of data that is\u000astored in images with no underlying data.</p>\u000a\u000a<p>The idea is to pick a "lens" shape which acts as a template and is\u000aplaced on an image. Also provide an output shape to serve as the target.</p>\u000a\u000a<p>Applications:</p>\u000a\u000a<ul>\u000a<li><a href="http://upload.wikimedia.org/wikipedia/commons/5/5a/1862_Johnson_and_Ward_Map_or_Chart_of_the_World%27s_Mountains_and_Rivers_-_Geographicus_-_MtsRvrs-j-1861.jpg">Tracing rivers to 1D</a> to compare their lengths.</li>\u000a<li>Mutate data chart types (eg. ring chart ==&gt; bar chart)</li>\u000a</ul>\u000a\u000a<iframe width="560" height="315" src="//www.youtube.com/embed/S1Roi2NOmx8" frameborder="0" allowfullscreen></iframe>\u000a\u000a<h3>Content-based Tools for Editing Audio Stories</h3>\u000a\u000a<p>A Berkeley PhD student showed his project, which aims to edit audio\u000astories (radio shows, podcasts, audio books) at a semantic level much\u000ahigher than the current industry standard (waveforms). Not that\u000atechnically challenging, just a really cool idea. Might be a very\u000acompelling product.</p>\u000a\u000a<p>Cool interactions:</p>\u000a\u000a<ul>\u000a<li>Edit speech (eg. copy, paste) in a text editor.</li>\u000a<li>Lets you pick sentences from a list of takes.</li>\u000a<li>Insert breaths and pauses where needed.</li>\u000a<li>Retarget music by segmenting song by beats and automatically finding music change points.</li>\u000a<li>Specify speech emphasis points manually, and use them as alignment points to music change points.</li>\u000a</ul>\u000a\u000a<iframe width="560" height="315" src="//www.youtube.com/embed/RHtI4G5L31w" frameborder="0" allowfullscreen></iframe>\u000a\u000a<p>Here's to <a href="https://twitter.com/ACMUIST/status/390958095939407872">next UIST</a>. Hang loose!</p>\u000a
p1941
tp1942
Rp1943
sg11
V/uist-2013
p1944
sg13
Nsg14
I01
sg15
VUIST 2013 highlights
p1945
sg18
VI just got back from Scotland, where I had the pleasure of attending\u000aUIST 2013 in St.
p1946
sg4
V<p>I just got back from Scotland, where I had the pleasure of attending\u000aUIST 2013 in St. Andrews. This was my second time attending, and again\u000ait was incredibly engaging and interesting content. I was impressed\u000aenough to take notes just like <a href="http://smus.com/uist-2011/">my last UIST in 2011</a>. What\u000afollows are my favorite talks with demo videos. I grouped them into\u000atopics of interest: gestural interfaces, tangibles and GUIs.</p>\u000a\u000a
p1947
sg23
g147
sg31
g1944
sg148
(dp1948
g150
S'Oct'
p1949
sg152
S'October 25, 2013'
p1950
sg154
I10
sg155
S'2013-10-25T09:00:00-00:00'
p1951
sg157
I1382716800
sg158
I2013
sg159
I25
ssg61
g160
sg29
S'uist-2013'
p1952
sS'posted'
p1953
g166
(S'\x07\xdd\n\x19'
p1954
tp1955
Rp1956
ssg32
S'content/posts/2013/uist-2013.md'
p1957
sg34
F1433825443.0
sa(dp1958
g2
(dp1959
g26
g5
(g6
g7
V<p>As usual, I want two conflicting things. Firstly, I want to own the\u000acontent I write, and control how it is authored. My weapon of choice is\u000aMacVim and <a href="https://github.com/borismus/lightning">Lightning</a>, a static blog engine I wrote to\u000aaddress my <a href="http://smus.com/site/">very specific requirements</a>: </p>\u000a\u000a<p><img src="composing-long-post.png" alt="Composing a blog post with Lightning" /></p>\u000a\u000a<p>Secondly, I want people to read the things I write and follow the\u000astories that I link to, since it feels good, and sometimes generates\u000ainteresting discussions. I wrote a Mac GUI that automates link blogging\u000aand <a href="http://indiewebcamp.com/POSSE">POSSE style</a> cross-posting to social networks.</p>\u000a\u000a<!--more-->\u000a\u000a<p>Before I carry on, let me take you back to a simpler time. A time before\u000aJustin Bieber, Black Eyed Peas and social networks. Here's how blogging\u000aused to work: writers wrote to their blogs, and readers subscribed to\u000athem. They would read blog posts from RSS feed readers. Some people ran\u000alink blogs that were kind of like twitter -- short updates discussing a\u000alink. It's was a very clean, federated model. Unfortunately that's not\u000ahow things work today. I have already lamented this fact in a <a href="http://smus.com/really-simple-social-syndication/">previous\u000ablog post</a>.</p>\u000a\u000a<p>For this site, RSS feed readership is a tiny fraction of the inbound\u000atraffic for new posts. Most of non-search traffic comes from social\u000anetworks. As it turns out, the value of these networks is (wait for it)\u000ain their network effect! It only takes a few prominent reshares to have\u000aa post become relatively widely read. By not seeding your content to\u000asocial networks, you lose that benefit.</p>\u000a\u000a<h2>Cross-posting to social networks is hard</h2>\u000a\u000a<p>To seed content to social networks, we look to POSSE. From the\u000a<a href="http://indiewebcamp.com/POSSE">indiewebcamp.com</a> definition:</p>\u000a\u000a<blockquote>\u000a  <p>POSSE is an acronym/abbreviation for Publish (on your) Own Site,\u000a  Syndicate Elsewhere. It's a Syndication Model where the flow involves\u000a  posting your content on your own domain first, then syndicating out\u000a  copies to 3rd party services with perma(short)links back to the\u000a  original version.</p>\u000a</blockquote>\u000a\u000a<p>The problem is that posting to social networks is a pain in the rear.\u000aEach network has its own limitations (eg. 140 characters on Twitter,\u000anetwork-specific markup on G+). There are services which cross-post to\u000athese networks, but they tend to sweep the subtle differences between\u000athe networks under the carpet. For example, services like\u000a<a href="http://manageflitter.com/">ManageFlitter</a> and <a href="http://friendsplus.me/">Friends+Me</a> can cross-post from G+ to\u000aTwitter, but if the post is too long to fit in 140 chars, they include a\u000alink back to the original G+ post. I find cross-linking between social\u000anetworks to be questionable, so I have stopped using such tools.</p>\u000a\u000a<h2>Making link-style updates easier</h2>\u000a\u000a<p>A while ago, I realized that my use of social networks is remarkably\u000aclose to a link blog. While I'll sometimes @reply/comment and +1/star\u000athings, I hardly ever post broadcast-style updates without a URL.</p>\u000a\u000a<p>About a year ago, I added a special "link" type of content on this blog,\u000aspecially for this purpose. This type of content is just like a post,\u000aexcept one link is prominently shown as the title of the post, and the\u000apost itself is focused on commentary about the link. My plan is to use\u000athis type of content more, whenever I want to broadcast a URL and have\u000asomething to say about it.</p>\u000a\u000a<p>So to scratch my itch, I made a little GUI that automates creating a\u000alink entry on the local static blog (with commentary). After the link is\u000aposted and deployed publicly, it also broadcasts the content to\u000asupported social networks, sometimes linking back to the link page on\u000athis site.</p>\u000a\u000a<p><img src="lightning-link.png" alt="Lightning linker Mac GUI" /></p>\u000a\u000a<p>As you can see, there are three fields: the URL, a title (pre-populated\u000afrom the <code>&lt;body&gt;&lt;title&gt;</code> of the URL), and the body of the post (in\u000aMarkdown), all of which are clearly visible in the link page.\u000aTechnically, posting to social networks is easy enough. If they provide\u000aa write API, it's just a matter of doing the OAuth dance and hanging on\u000ato an access token to authorize requests. A more interesting question is\u000ahow to re-arrange the above three fields to form social network updates.</p>\u000a\u000a<h2>Posting to other networks</h2>\u000a\u000a<p>Once the link is posted on the site, Lightning Link has a set of\u000aheuristics to decide what to post to each supported social network. This\u000acan be quite challenging if you are faced with a strict character limit.</p>\u000a\u000a<p>Here are some options I considered for Twitter:</p>\u000a\u000a<ol>\u000a<li>Truncated plaintext body, followed by the URL</li>\u000a<li>Title colon space, the truncated plaintext body, and then the URL</li>\u000a<li>Title and URL</li>\u000a</ol>\u000a\u000a<p>With Option 1, the truncated plaintext body doesn't necessarily reflect\u000athe main idea of the commentary on the link, since the body can be much\u000alonger than 140 chars, and I might just be warming up :) Option 2 leaves\u000avery little room for the body at all, except for about half of a\u000asentence. I went with Option 3, which lends itself well to short,\u000aTwitter-style updates.</p>\u000a\u000a<p>Posting to G+ is relatively easy: take the title and slap on the\u000aplaintextified (from markdown) body, while attaching the URL. </p>\u000a\u000a<p>The other question is about URLs. There are two URLs of relevance in\u000aeach link blog post:</p>\u000a\u000a<ol>\u000a<li>The one on the link blog (eg. <a href="http://smus.com/link/2013/not-terrible-javascript-modules/">http://smus.com/link/2013/not-terrible-javascript-modules/</a>), and</li>\u000a<li>The linked material (eg. <a href="http://github.com/substack/node-browserify">http://github.com/substack/node-browserify</a>).</li>\u000a</ol>\u000a\u000a<p>My approach is to use the linked material directly (2) if the comments\u000acan fit entirely into the space alotted by the social network, falling\u000aback to the link blog URL (1).</p>\u000a\u000a<p>The Lightning Link app isn't general enough for me to recommend unless\u000ayou either like pain, use lightning already, or have a static blog very\u000asetup similar to mine, with a "link" type of post. If you'd still like\u000ato try it, the <a href="https://github.com/borismus/lightning/tree/master/link">code is on github</a>. If you have a similar\u000alink-blogging approach with automatic syndication to social networks,\u000atell me about it!</p>\u000a
p1960
tp1961
Rp1962
sg11
V/easier-link-blogging
p1963
sg13
Nsg14
I01
sg15
VEasier link blogging
p1964
sg18
VAs usual, I want two conflicting things.
p1965
sg4
V<p>As usual, I want two conflicting things. Firstly, I want to own the\u000acontent I write, and control how it is authored. My weapon of choice is\u000aMacVim and <a href="https://github.com/borismus/lightning">Lightning</a>, a static blog engine I wrote to\u000aaddress my <a href="http://smus.com/site/">very specific requirements</a>: </p>\u000a\u000a<p><img src="composing-long-post.png" alt="Composing a blog post with Lightning" /></p>\u000a\u000a<p>Secondly, I want people to read the things I write and follow the\u000astories that I link to, since it feels good, and sometimes generates\u000ainteresting discussions. I wrote a Mac GUI that automates link blogging\u000aand <a href="http://indiewebcamp.com/POSSE">POSSE style</a> cross-posting to social networks.</p>\u000a\u000a
p1966
sg23
g147
sg31
g1963
sg148
(dp1967
g150
S'Jul'
p1968
sg152
S'July 29, 2013'
p1969
sg154
I7
sg155
S'2013-07-29T09:00:00-00:00'
p1970
sg157
I1375113600
sg158
I2013
sg159
I29
ssg61
g160
sg29
S'easier-link-blogging'
p1971
sS'posted'
p1972
g166
(S'\x07\xdd\x07\x1d'
p1973
tp1974
Rp1975
ssg32
S'content/posts/2013/easier-link-blogging/index.md'
p1976
sg34
F1433825541.0
sa(dp1977
g2
(dp1978
g26
g5
(g6
g7
V<p>Imagine this: you start conducting as if you were in front of a great\u000aorchestra, and music fades in out of thin air, matching your tempo and time\u000asignature. Your nuanced gestures can indicate changes in intensity, and of\u000acourse affect the speed of the piece. You'd first need to learn some\u000abasic conducting patterns, like these:</p>\u000a\u000a<p><img src="conduct-time-signatures.png" alt="Conducting patterns for various time signatures" /></p>\u000a\u000a<!--more-->\u000a\u000a<p>You would also have to wait for me to finish this project, which uses a LEAP\u000amotion device (or your trackpad), the Web Audio API, and some signal processing\u000ato achieve a scaled back version of the idea described above.</p>\u000a\u000a<h2>Prototype</h2>\u000a\u000a<p>The current prototype lets you control the tempo of a song called "Phantom"\u000afrom the excellent <a href="http://www.parovstelar.com/">Parov Stelar</a>. You can do this by making simple\u000aconducting patterns, similar to the 2/4 pattern above. In practice, you can use\u000aany pattern in which your hand oscillates between two points in space to play\u000awith this prototype. You can even use your mouse instead of a LEAP motion\u000adevice. Just click in to enable pointer lock. This will ensure that your mouse\u000awill always be focused inside your browser.</p>\u000a\u000a<p>I built a visualizer which is an 8-bit inspired frequency graph which\u000aalso shows directional changes as pulsating red dots, and clusters which\u000aflash to the beat.</p>\u000a\u000a<p><a href="http://borismus.github.io/gestural-music-direction/"><img src="screenshot.png" alt="Screenshot of leap conductor" /></a></p>\u000a\u000a<p>If you'd like to try it live, the <a href="http://borismus.github.io/gestural-music-direction/">demo lives here</a>.</p>\u000a\u000a<h2>Handling input</h2>\u000a\u000a<p>With a LEAP device plugged in, the prototype maps the palm's 3D center\u000ato 2D. It works just as well with just a trackpad or mouse attached to\u000ayour computer, which directly outputs 2D coordinates. The input handling\u000aalgorithm then uses the resulting (x, y) pairs to do roughly the\u000afollowing:</p>\u000a\u000a<ul>\u000a<li><p>First, track positions and first (velocity) and second (acceleration)\u000aorder history, including times. Store in a ring buffer, which is\u000aimplemented in <a href="https://github.com/borismus/gestural-music-direction/blob/master/js/ring-buffer.js">ring-buffer.js</a>.</p></li>\u000a<li><p>Extract sudden changes of direction based on heuristics related to\u000avelocity and acceleration history.</p></li>\u000a<li><p>Cluster directional changes using K-means or similar clustering\u000aalgorithm which is implemented in <a href="https://github.com/borismus/gestural-music-direction/blob/master/js/clusterizer.js">clusterizer.js</a>. I run this\u000aK-means implementation with 3 values of k in [2, 3, 4] and pick the\u000aone with the lowest error. I've also build a standalone\u000a<a href="http://borismus.github.io/gestural-music-direction/cluster.html">clustering test page</a> with the following output:</p></li>\u000a</ul>\u000a\u000a<p><img src="cluster.png" alt="Clustering algorithm visualization" /></p>\u000a\u000a<ul>\u000a<li><p>To calculate tempo, pick a cluster and calculate mode of the deltas\u000abetween adjacent points.</p></li>\u000a<li><p>The time signature is just the number of clusters over 4 (for the\u000asimple 2/4, 3/4 and 4/4 patterns).</p></li>\u000a</ul>\u000a\u000a<h2>Changing tempo in real-time</h2>\u000a\u000a<p>Once we have an idea of what pattern the user is creating with their hands, we\u000aneed to match up the song to the pattern, and continuously adapt the song's\u000aplayback rate to the user's motions.</p>\u000a\u000a<p>The Web Audio API makes it dead simple to change the playback rate of an audio\u000abuffer for a source node'ss entire duration. However, things get a bit\u000atrickier if this rate changes continuously over time. Chris Wilson\u000adescribes a scheduling technique which addresses this exact problem in\u000ahis <a href="http://www.html5rocks.com/en/tutorials/audio/scheduling/">"Tale of Two Clocks" HTML5Rocks article</a>. You can also see\u000aa simple version of it inaction in his <a href="http://www.html5rocks.com/en/tutorials/audio/scheduling/goodmetronome.html">metronome demo</a>.</p>\u000a\u000a<p>I used this idea to do a bit of granular synthesis on an audio buffer. I\u000aschedule a bit of the buffer into the future, at the current tempo. As the\u000atempo changes, new bits of the buffer are scheduled at a different\u000aplaybackRate. I keep track of how far into the buffer we've gone and use that\u000aas the grainOffset. Here's some code that illustrates this (but see the\u000a<a href="https://github.com/borismus/gestural-music-direction/blob/master/js/music-player.js">variable rate music player</a> for the full code):</p>\u000a\u000a<pre><code>MusicPlayer.prototype.loop_ = function() {\u000a  // Schedule the next bar if it's not yet scheduled.\u000a  while (this.nextNoteTime &lt; audioContext.currentTime + this.scheduleAheadTime) {\u000a    this.scheduleSegment_(this.grainOffset, this.nextNoteTime);\u000a    this.nextNote_();\u000a  }\u000a}\u000a\u000aMusicPlayer.prototype.scheduleSegment_ = function(grainOffset, time) {\u000a  // Get the part of the buffer that we're going to play.\u000a  var source = audioContext.createBufferSource();\u000a  source.buffer = this.buffer;\u000a  source.connect(audioContext.destination);\u000a\u000a  var rate = this.getPlaybackRate_();\u000a  source.playbackRate.value = rate;\u000a\u000a  var secondsPerBeat = 60.0 / this.tempo;\u000a  source.noteGrainOn(time, grainOffset, secondsPerBeat * rate);\u000a}\u000a\u000aMusicPlayer.prototype.nextNote_ = function() {\u000a  // Advance current note and time by a 16th note...\u000a  var secondsPerBeat = 60.0 / this.tempo;\u000a  // Notice this picks up the CURRENT tempo value to calculate beat length.\u000a  this.nextNoteTime += secondsPerBeat;\u000a  // Get the next grain.\u000a  var rate = this.getPlaybackRate_();\u000a  this.grainOffset += secondsPerBeat * rate;\u000a}\u000a</code></pre>\u000a\u000a<p>In practice, I'm unfortunately hitting some rounding errors, so the\u000agrains aren't stitched together as seamlessly as I wanted. You can\u000asometimes hear artifacts if you slow the tempo way down.</p>\u000a\u000a<h2>A work in progress</h2>\u000a\u000a<p>My initial idea was to use <a href="http://developer.echonest.com/">The Echo Nest</a> to pick the right song\u000a(based on time signature and tempo), and then stream that song from some\u000astreaming music service. Unfortunately it's quite hard to get at PCM versions\u000aof tracks from Rdio and Spotify. That said, it can be <a href="https://github.com/oampo/AmbientCloud">done with\u000aSoundcloud</a>. Long story short, the prototype currently only\u000asupports one song.</p>\u000a\u000a<p>A time signature recognizer is mainly useful for classical music, since so much\u000aof popular music is in common time (with <a href="http://twentytwowords.com/2011/05/18/6-pop-songs-in-unusual-time-signatures/">rare exceptions of popular music with\u000acomplex time signatures</a>). But applying simple transformations\u000alike changing the tempo just feels wrong for complex music without a very\u000aobvious rhythmic structure.</p>\u000a\u000a<p>Lastly, LEAP's palm tracking is still quite noisy (even after drastic\u000aimprovements to palm tracking as of <a href="https://developer.leapmotion.com/blog/sdk-0-7-7-released-new-palm-tracking-and-gesture-settings">SDK 0.7.7</a>). Also, the\u000abay windows in my living room lets in tons of infrared light which often puts\u000athe device into a low fidelity tracking mode.</p>\u000a\u000a<p>As always, let me know what you think, and of course, feel free to fork\u000aand evolve on <a href="https://github.com/borismus/gestural-music-direction">github</a>.</p>\u000a
p1979
tp1980
Rp1981
sg11
V/gestural-music-direction
p1982
sg13
Nsg14
I01
sg15
VGestural music direction
p1983
sg18
VImagine this: you start conducting as if you were in front of a great\u000aorchestra, and music fades in out of thin air, matching your tempo and time\u000asignature.
p1984
sg4
V<p>Imagine this: you start conducting as if you were in front of a great\u000aorchestra, and music fades in out of thin air, matching your tempo and time\u000asignature. Your nuanced gestures can indicate changes in intensity, and of\u000acourse affect the speed of the piece. You'd first need to learn some\u000abasic conducting patterns, like these:</p>\u000a\u000a<p><img src="conduct-time-signatures.png" alt="Conducting patterns for various time signatures" /></p>\u000a\u000a
p1985
sg23
g147
sg31
g1982
sg148
(dp1986
g150
S'May'
p1987
sg152
S'May 3, 2013'
p1988
sg154
I5
sg155
S'2013-05-03T09:00:00-00:00'
p1989
sg157
I1367596800
sg158
I2013
sg159
I3
ssg61
g160
sg29
S'gestural-music-direction'
p1990
sS'posted'
p1991
g166
(S'\x07\xdd\x05\x03'
p1992
tp1993
Rp1994
ssg32
S'content/posts/2013/gestural-music-direction/index.md'
p1995
sg34
F1433825538.0
sa(dp1996
g2
(dp1997
g26
g5
(g6
g7
V<p><style>\u000aarticle img.border {\u000a  margin: 0 auto;\u000a  max-width: 100%;\u000a  box-shadow: inset 0 0 10px #999\u000a}\u000a</style>\u000a<a name="problem"></a></p>\u000a\u000a<p>Have you seen the <a href="http://extensiblewebmanifesto.org/">extensible web manifesto</a>? It's the\u000aformalization of a recent trend in web standards: a tendency towards\u000alower level APIs. Lower levels of abstraction enable developers to build\u000amore on top of a solid foundation. By going down a level of abstraction\u000ain the web platform, web developers can contribute to the platform\u000aitself in a more fundamental way, working along with browser vendors and\u000aspec writers. This is <a href="/how-the-web-should-work/">how the web should work</a>.</p>\u000a\u000a<p>But there is a big missing piece in the extensible web vision. Our\u000abeloved platform is stuck in a constrictive security sandbox. The "drive\u000aby" web's security philosophy is that users of the web should be able to\u000afeel safe on any webpage they visit. While very important for the well\u000abeing of web denizens, it prevents developers from using increasingly\u000aimportant features enjoyed by native platforms such as access to\u000acontacts, TCP/UDP sockets, interfaces to external USB/Bluetooth devices.\u000aBreaching this sandbox is a huge barrier for the web as a compelling\u000aapplication platform.</p>\u000a\u000a<p>Some recent features, such as <code>getUserMedia</code>, which gives web developers\u000aaccess to the audio and video streams of your device's camera and\u000amicrophone, have started to break out of the sandbox. There are two\u000aapproaches to this problem today: (a) infobars and (b) packaged apps. In\u000athe rest of this post I'll describe why these are bad solutions,\u000adeconstruct them down into small pieces and then glue the pieces back\u000atogether. The ultimate goal is a modest proposal for installable web\u000aapps. Read on for my take on the background of the problem, or skip\u000aahead to read my <a href="#solution">illustrated proposal</a> for fixing it.</p>\u000a\u000a<!--more-->\u000a\u000a<h2>The extensible web is a good idea</h2>\u000a\u000a<p>There are many recent examples of the extensible web philosophy across\u000amany areas of the web. Because of the low level nature of WebGL and Web\u000aAudio, these technologies open up a wide variety of applications to be\u000abuilt. Under the hood, these APIs are relatively thin wrappers around\u000aunderlying native technology, not compromising performance (much) while\u000aadding developer usability. General purpose low level computing\u000atechnologies like <a href="http://asmjs.org/">asm.js</a> and <a href="https://developers.google.com/native-client/">NaCl</a> enable computationally\u000aintensive algorithms to run far more efficiently. </p>\u000a\u000a<p>Finally, frameworks like <a href="http://www.polymer-project.org/">Polymer</a> use other kinds of low level\u000aAPIs like <a href="http://www.youtube.com/watch?v=fqULJBBEVQE">web components</a> and <a href="https://github.com/Polymer/mdv">model-driven views</a> to let\u000adevelopers invent new types of HTML elements with custom functionality. </p>\u000a\u000a<h2>Sandbox vs. low level APIs</h2>\u000a\u000a<p>Restrictive web security makes a lot of sense. You should never have to\u000aworry about malicious or careless developers erasing files from your\u000alocal filesystem, even if you frequent the most notorious <code>.ru</code> domains!\u000aThat said, if the web is to be a viable application platform that stands\u000aa chance against native platform, it needs to have access to certain\u000adata that is sensitive.</p>\u000a\u000a<p>Many of the APIs that align with the extensible web philosophy have\u000aalready tested the bounds of the web's sandbox. Some have resulted in\u000asecurity vulnerabilities, such as 2011's <a href="http://blog.chromium.org/2011/07/using-cross-domain-images-in-webgl-and.html">cross-domain WebGL texture\u000aattack</a>. Others have required extending the web platform\u000awith additional levels of security. The earliest of these is probably\u000athe geolocation API. More recent additions include <code>getUserMedia</code>, which\u000agives developers a stream of the microphone and camera. These APIs could\u000aobviously lead to very serious privacy breaches if turned on by default\u000aon all web pages. I don't want the Russians knowing where I live, or\u000aeavesdropping on my conversations (NSA already knows).</p>\u000a\u000a<p>The "drive-by" web solves this problem through infobars. I will explain\u000alater why this is a terrible idea. The other solution is packaged web\u000aapps. Packaging circumvent the web completely by copying the\u000adistribution model of native apps, bundling your whole application into\u000aa locally downloaded zip. This model also features an installation step\u000awhich sometimes also grants additional permissions up front. Both of\u000athese so-called solutions reduce the likelihood of new low level APIs\u000afrom coming to the web platform.</p>\u000a\u000a<h2>Infobars are a bad user experience</h2>\u000a\u000a<p>First, look at this:</p>\u000a\u000a<p><img src="infobar-apocalypse.png"/></p>\u000a\u000a<p>This gem, courtesy of <a href="http://persistent.info">Mihai Parparita</a>, is my favorite\u000aexplanation for why infobars suck. You can immediately see several\u000aproblems stemming from the obvious fact that the infobar model does not\u000ascale well. In the infobar world, each feature requires its own\u000apermission, leading to far too many stacked dialogs that are just ugly.\u000aFrom a usability perspective, your users have to click through each one\u000aof the Allow/OK dialogs before they can do anything with your\u000aapplication. If you then reload the page, many of these infobars will\u000aagain return to haunt you, forcing you to click OK five more times\u000abefore being able to use the webapp.</p>\u000a\u000a<p>In some cases, your browser might remember that you accepted an infobar,\u000aand choose not to show it to you again. For example, this happens if you\u000agrant <code>getUserMedia</code> access on an HTTPS site, selecting the "save this\u000apreference" option. This is remembered on a per-domain basis, and in\u000aChrome, is available via <code>Preferences -&gt; Content Settings</code>. In general,\u000aconditions for when exactly the browser remembers how you responded to\u000aan infobar are unclear and underdefined.</p>\u000a\u000a<p>There are also some less obvious issues with the infobar model. Because\u000ainfobars are non-modal, users often don't realize that they have to\u000aaccept them before they can use the webapp's functionality. For example,\u000aif you have a <a href="http://webcamtoy.com">photo booth application</a>, it will be\u000acompletely useless until you accept the "access to video stream"\u000ainfobar, yet many of your users may not notice the infobar at the top of\u000ayour browser window. If you attempt to draw your users' attention to the\u000ainfobar via some illustration, you may end up pointing to the Deny\u000abutton by accident because of variations in placement across various\u000abrowsers and browser versions.</p>\u000a\u000a<p>To summarize, infobars are broken in the following ways:</p>\u000a\u000a<ol>\u000a<li>Does not scale with number of permissions.</li>\u000a<li>Visually jarring at scale. Sometimes not visually obvious enough.</li>\u000a<li>Permission granting should often be modal.</li>\u000a<li>Inconsistent persistence, poor management.</li>\u000a</ol>\u000a\u000a<p>Part of the problem might be addressable via a <a href="https://code.google.com/p/chromium/issues/detail?id=250797">visual\u000arefresh</a> of infobars (as Paul Neave suggests), but I\u000asuspect that a broader rethink of the problem is in order. Until this is\u000aresolved, many new low level APIs will increases Mihai's stack of\u000aapocalyptic infobars, reducing their chance of coming to the platform in\u000athe first place.</p>\u000a\u000a<h2>Packaged apps...</h2>\u000a\u000a<p>Packaged apps are an odd marriage between native app distribution and\u000aweb technologies. The packaged web app model consists of a few moving\u000aparts:</p>\u000a\u000a<ul>\u000a<li>A directory for discovering and installing apps (eg. Firefox\u000aMarketplace, Chrome Web Store).</li>\u000a<li>A set of platform-specific APIs built on top of the web platform for\u000ause in these apps.</li>\u000a<li>A manifest describing each app, which can specify permissions to\u000aenable either the above platform specific APIs or restricted open web APIs.</li>\u000a</ul>\u000a\u000a<p>There are certainly benefits to this approach, such as a sane offline\u000astory, since all of the assets of the application can be packaged\u000atogether into a bundle that is downloaded at install time, circumventing\u000apainful technology like <a href="appcache.png">AppCache</a>. There is a clear install\u000astep, during which you can grant an application permissions beyond the\u000ascope of the open web platform. Also, it's very easy to add features to\u000apackaged apps, since there are no annoying standards to worry about,\u000aamirite?</p>\u000a\u000a<h2>...are bad for the web</h2>\u000a\u000a<p>Unfortunately, packaged web apps provide the worst of both worlds,\u000acombining relatively poor web developer ergonomics with the longer\u000adevelopment and distribution cycle of native apps. Also, many of the\u000adrawbacks of packaging are at odds with the philosophy of the open web\u000aplatform.</p>\u000a\u000a<p>The first and most obvious problem is the lack of URLs for packaged\u000aapps. URLs are critically important as unique identifiers for content\u000afound on the web. They are great for sharing content, indexing, and\u000abookmarking. Secondly, there is no standard packaged app format across\u000aplatforms, which means that the packaging formats and APIs available are\u000acompletely different between Chrome, Firefox, and other packaged app\u000aproviders. This cross-platform aspect is the main economic reason to\u000adevelop for the web. Another drawback is that each of these packaged app\u000avendors has its own app store, sometimes complete with approval\u000aprocesses similar to the much reviled App Store approval flow. </p>\u000a\u000a<p>Lastly, once a browser vendor has a packaged app model, it's very\u000atempting for them to just implement new low level features there and not\u000aon the open web. This effectively lifts the pressure for browser vendors\u000ato go through the pain of standardization. The standard response can now\u000abe "just go build a packaged app". A summary of these issues with packaged\u000aapps:</p>\u000a\u000a<ol>\u000a<li>No URLs</li>\u000a<li>Not cross platform</li>\u000a<li>Dependent on centralized directories</li>\u000a<li>Vendors have an excuse to punt on adding new features to the web\u000aplatform.</li>\u000a</ol>\u000a\u000a<p>Packaged apps are at odds with the web. To the unintiated, it feels as\u000aif their inventors slapped web technology on top of the Apple app store\u000amodel. I know that there are some legitimate, security-motivated reasons\u000afor their decisions, but believe that these are surmountable.</p>\u000a\u000a<h2>Installable web apps</h2>\u000a\u000a<p>If you have an iOS device at your disposal, take a look at\u000a<a href="http://forecast.io/">forecast.io</a>. Forecast.io is an example of an <a href="http://blog.forecast.io/its-not-a-web-app-its-an-app-you-install-from-the-web/">app you install\u000afrom the web</a>. This approach is interesting because it combines\u000athe best of both worlds. On one hand, you retain the benefits of the\u000aweb: URLs, cross-linking, lack of centralized control. On the other, you\u000aget the benefit of elevated permissions.</p>\u000a\u000a<p>A benefit of this approach is that there is a clear install step during\u000awhich you can request additional permissions, which is a natural place\u000afor breaking out of the web's sandbox in a user-friendly manner. The\u000aresult of installation is a homescreen shortcut, which is both a launch\u000aconvenience, and a way of managing permissions. Removing that shortcut\u000acan also mean revoking special permissions for that application.</p>\u000a\u000a<p>Another benefit is that there is no centralized appstore - you can\u000adiscover apps in the same way that you discover the web today - through\u000asearch engines, links in your email inbox, feed readers and through any\u000aother URL-based sharing scheme. There is no reason to conflate\u000ainstallation with the presence of a centralized directory. Google search\u000ais already revealing apps in search results. If you search for "Angry\u000aBirds", you will find both the iOS and Android versions on the first\u000apage.</p>\u000a\u000a<p><a name="solution"></a></p>\u000a\u000a<h1>Proposal: apps you can install from the web</h1>\u000a\u000a<p>So far I've described the <a href="#problem">problem</a>: a major barrier to the\u000avision of the extensible web: there is no good way of getting outside of\u000athe sandbox. I have been complaining a lot without providing any\u000aconstructive answers.</p>\u000a\u000a<p>In order to keep things constructive, the second half of the post\u000aproposes a solution to get us out of the sandbox. There's a whole world\u000aout there! Here is my birds-eye-view of the install-from-the-web world:</p>\u000a\u000a<p><img src="flow.png" alt="flow" /></p>\u000a\u000a<p>This diagram is intended to be general enough to work across\u000aoperating systems and device types, but the mocks themselves will be\u000asketched out with a phone form factor in mind. We'll be installing\u000a<code>app.io</code>, a mobile app that lets you leave audio notes.</p>\u000a\u000a<p><img src="screen1.png" class="border" /></p>\u000a\u000a<p><em>Screen 1: App.io example.</em></p>\u000a\u000a<h2>An API for installing webapps</h2>\u000a\u000a<p>This can be done with an iOS-style approach (and corresponding Chrome\u000afor Android <a href="https://code.google.com/p/chromium/issues/detail?id=153066">feature request</a>), which presents a generic UI\u000afor adding apps to the homescreen (see Screen 2).</p>\u000a\u000a<p><img src="screen2.png" class="border" /></p>\u000a\u000a<p><em>Screen 2: Add via browser button.</em></p>\u000a\u000a<p>There are trade-offs between having a button or an opt-in developer\u000a<strong>API for installing web apps</strong>. With a button, any URL can be added to\u000athe home screen, which may not make sense. But with an API, the\u000adeveloper has to provide an explicit call to action for you to install\u000atheir app. The button UX will always be consistent, since it's part of\u000athe browser. An API-based install path may be ugly or spammy. However,\u000aan API can also provide a consistent experience across browsers without\u000athe need for guessing where each browser places the button. Many\u000aforecast.io-style apps on iOS have callouts on the page pointing to the\u000abutton in the browser chrome which would be broken if another browser\u000ahad a different method of adding to homescreen.</p>\u000a\u000a<p>My opinion is that button- and API-based approaches both have a place.\u000aFor webapps that make more sense installed, the API can be a nice touch.\u000aOther pages might be useful as webapps without their developer realizing\u000ait, so the button-based approach is useful there.</p>\u000a\u000a<p>How would the installation API look like? A JavaScript-based API only\u000acallable on user action, similar to how audio playback in mobile\u000abrowsers prevents the annoying situation where visiting a page\u000aautomatically prompts you to install it. Installing a webapp should\u000acome with a default set of permissions above and beyond what the web\u000aplatform provides.</p>\u000a\u000a<pre><code>var button = document.querySelector('button#install');\u000abutton.addEventListener('click', window.app.requestInstall);\u000a</code></pre>\u000a\u000a<p>You should also be able to request additional permissions at\u000ainstall-time. For example, to request installation and audio capture,\u000athe following code should work:</p>\u000a\u000a<pre><code>window.app.requestInstall({permissions: ['audioCapture']});\u000a</code></pre>\u000a\u000a<p>This action should also result in a standard browser-specific dialog to\u000aaccept installation, showing which permissions have been requested\u000a(Screen 3).</p>\u000a\u000a<p><img src="screen3.png" class="border" /></p>\u000a\u000a<p><em>Screen 3: Confirm installation.</em></p>\u000a\u000a<p>Once accepted, a launcher shortcut should be created (Screen 4). Two\u000apieces of metadata are necessary for this launcher:</p>\u000a\u000a<ol>\u000a<li><p>Icon, which should first look for a large enough version of the\u000a<a href="http://en.wikipedia.org/wiki/Favicon#HTML5_recommendation_for_icons_in_multiple_sizes">multiresolution favicon</a> as determined by the UA. If none\u000aexists, it should look for the <a href="http://goo.gl/6Qdi3">apple-touch-icon</a> in the\u000a<code>&lt;head&gt;</code>. If none is specified, a screenshot of the page can be used\u000aas in iOS.</p></li>\u000a<li><p>Title, which can be extracted from the <code>&lt;title&gt;</code> element in the head.\u000aIf none is specified, the user can be prompted to input their own\u000atitle.</p></li>\u000a</ol>\u000a\u000a<p><img src="screen4.png" class="border" /></p>\u000a\u000a<p><em>Screen 4: New launcher added to the home screen.</em></p>\u000a\u000a<h2>Launching in standalone mode</h2>\u000a\u000a<p>iOS already has an <strong>API to know if a webapp was launched in standalone\u000amode</strong> (ie. from the launcher) or if it was opened from a browser. This\u000afunctionality is available via <code>window.navigator.standalone</code>. It also\u000aopens the app in full-screen mode.</p>\u000a\u000a<p>Other vendors should standardize and implement similar functionality.\u000aFor example, something like <code>window.app.standalone</code>, if only for naming\u000aconsistency could be implemented, and a polyfill provided for the Apple\u000aspec. It would also make sense to launch homescreen apps in full screen,\u000aproviding the same UX as the full-screen API (Screen 5):</p>\u000a\u000a<p><img src="screen5.png" class="border" /></p>\u000a\u000a<p><em>Screen 5: App launched in standalone mode.</em></p>\u000a\u000a<h2>Requesting additional permissions</h2>\u000a\u000a<p>Apps might need additional permissions that go beyond the default\u000abaseline of permissions granted to the app at install time. Access to\u000ayour camera would fall into this bucket. An <strong>API call to request extra\u000apermissions</strong> might look like the following:</p>\u000a\u000a<pre><code>window.app.requestExtraPermissions(['videoCapture']);\u000a</code></pre>\u000a\u000a<p>Running this command would also require user-initiation and prompt a\u000amodal optional permissions dialog (Screen 6) similar to the one seen at\u000ainstallation. After granting it, the associated API call (in this case,\u000a<code>getUserMedia</code>) can be invoked without incurring any infobars.</p>\u000a\u000a<p><img src="screen6.png" class="border" /></p>\u000a\u000a<p><em>Screen 6: Additional permissions request.</em></p>\u000a\u000a<h2>Removing installed web apps and extra permissions</h2>\u000a\u000a<p>If the installed webapp has a native launcher, removing the launcher can\u000ado this implicitly. There should also be a browser- or system- UI\u000asimilar to existing app management interfaces that lets you remove\u000ainstalled apps, or revoke granted permissions.</p>\u000a\u000a<h2>That's it folks</h2>\u000a\u000a<p>So there you have it: my strawman fixing the security model of the web,\u000awhich, as I outlined at the <a href="#problem">beginning of this post</a>, is\u000acritically important to address for the continued success of the web.\u000aTo recap, the solution consists of an API surface in the <code>window.app</code>\u000anamespace, and a number of new screens that are part of the installation\u000aprocess.</p>\u000a\u000a<p>If tackled, this could solve one of the most important issues on the web\u000atoday. Otherwise, we may find ourselves in a place where the web\u000aplatform is irrelevant to application developers, who will just build\u000afor packaged platforms.</p>\u000a\u000a<p>I'm not silly enough to think that this proposal is the ultimately\u000acorrect and best solution for elevated priveledges on the open web.\u000aThere is a huge amount of work required to refine the flow, think of all\u000aof the edge cases, implement it across browsers, etc. The above is just\u000aa draft to re-ignite the web permissions discussion that died several\u000ayears ago. Please blog something in response or in the worst case, tweet\u000aor email your opinion. Looking forward to hearing from you.</p>\u000a\u000a<h1>Update: important links</h1>\u000a\u000a<p><strong>July 18, 2013</strong>: Several people have pointed out that I've missed some\u000aimportant links.  My public apologies!</p>\u000a\u000a<p><a href="https://developers.google.com/chrome/apps/docs/developers_guide">Chrome hosted apps</a> are a somewhat similar concept,\u000abut suffered from security issues that still need to be resolved to make\u000athis proposal a reality. There was even an effort to make hosted web\u000aapps installable from the web, called <a href="http://blog.persistent.info/2011/07/theres-web-app-for-that-site.html">CRX-less web\u000aapps</a> (preserved on Mihai's blog), which today is little\u000amore than a <a href="http://code.google.com/intl/en-US/chrome/apps/docs/no_crx.html">broken link</a>.</p>\u000a\u000a<p>To my knowledge, the most active project along the lines of this\u000aproposal is the <a href="https://developer.mozilla.org/en-US/docs/Web/Apps">Open Web Apps</a> work from Firefox OS. </p>\u000a
p1998
tp1999
Rp2000
sg11
V/installable-webapps
p2001
sg13
Nsg14
I01
sg15
VInstallable webapps: extend the sandbox
p2002
sg18
V<style>\u000aarticle img.
p2003
sg4
V<p><style>\u000aarticle img.border {\u000a  margin: 0 auto;\u000a  max-width: 100%;\u000a  box-shadow: inset 0 0 10px #999\u000a}\u000a</style>\u000a<a name="problem"></a></p>\u000a\u000a<p>Have you seen the <a href="http://extensiblewebmanifesto.org/">extensible web manifesto</a>? It's the\u000aformalization of a recent trend in web standards: a tendency towards\u000alower level APIs. Lower levels of abstraction enable developers to build\u000amore on top of a solid foundation. By going down a level of abstraction\u000ain the web platform, web developers can contribute to the platform\u000aitself in a more fundamental way, working along with browser vendors and\u000aspec writers. This is <a href="/how-the-web-should-work/">how the web should work</a>.</p>\u000a\u000a<p>But there is a big missing piece in the extensible web vision. Our\u000abeloved platform is stuck in a constrictive security sandbox. The "drive\u000aby" web's security philosophy is that users of the web should be able to\u000afeel safe on any webpage they visit. While very important for the well\u000abeing of web denizens, it prevents developers from using increasingly\u000aimportant features enjoyed by native platforms such as access to\u000acontacts, TCP/UDP sockets, interfaces to external USB/Bluetooth devices.\u000aBreaching this sandbox is a huge barrier for the web as a compelling\u000aapplication platform.</p>\u000a\u000a<p>Some recent features, such as <code>getUserMedia</code>, which gives web developers\u000aaccess to the audio and video streams of your device's camera and\u000amicrophone, have started to break out of the sandbox. There are two\u000aapproaches to this problem today: (a) infobars and (b) packaged apps. In\u000athe rest of this post I'll describe why these are bad solutions,\u000adeconstruct them down into small pieces and then glue the pieces back\u000atogether. The ultimate goal is a modest proposal for installable web\u000aapps. Read on for my take on the background of the problem, or skip\u000aahead to read my <a href="#solution">illustrated proposal</a> for fixing it.</p>\u000a\u000a
p2004
sg23
g147
sg31
g2001
sg148
(dp2005
g150
S'Jun'
p2006
sg152
S'June 25, 2013'
p2007
sg154
I6
sg155
S'2013-06-25T09:00:00-00:00'
p2008
sg157
I1372176000
sg158
I2013
sg159
I25
ssg61
g160
sg29
S'installable-webapps'
p2009
sS'posted'
p2010
g166
(S'\x07\xdd\x06\x19'
p2011
tp2012
Rp2013
ssg32
S'content/posts/2013/installable-webapps/index.md'
p2014
sg34
F1433825531.0
sa(dp2015
g2
(dp2016
g26
g5
(g6
g7
V<p>Many new devices come with unexpected connectivity - often a WiFi\u000aconnection that enables them to connect to a hotspot and the larger\u000ainternet. <a href="http://www.nest.com/">Nest</a>, a smart thermostat, was one of the first\u000acommercial products to do this. Many more indie projects are following\u000asuit, with an explosion of kickstarters like this <a href="http://www.kickstarter.com/projects/limemouse/lifx-the-light-bulb-reinvented">teleoperated\u000alight</a>, <a href="http://www.withings.com/">connected scale</a> or this <a href="http://supermechanical.com/twine/">general purpose\u000aconnected sensor</a>. The idea of an Internet of Things, in which\u000aevery appliance and object is somehow connected, has long been popular\u000ain academic circles, and this time around it feels like we're actually\u000aclose.</p>\u000a\u000a<p>If we think of these physical devices/appliances as web services with\u000aAPIs, we can mash them up just like we did in the early days of the web,\u000acreating applications that are more useful than the sum of their parts.\u000aIn this post I argue for using the web as the medium to tie everything\u000atogether, describe a simple architecture for building networked physical\u000adevices and build a web lamp controlled by an arduino.</p>\u000a\u000a<!--more-->\u000a\u000a<h2>Web meets ubiquitous computing</h2>\u000a\u000a<p>The projects I mentioned above don't use proprietary home automation\u000aprotocols like X10.  Instead, they establish connections via the\u000ainternet. Nest registers itself with a central server (operated by Nest\u000ainc) over WiFi. You can then control your thermostat by logging into\u000atheir website at nest.com.</p>\u000a\u000a<p>There's no need for heavyweight ISO666 standardization efforts. This is\u000aa very good thing, because it's really hard for anyone (including our\u000aindustry) to agree on something new. Instead, we reuse a proven protocol\u000athat everyone's already agreed on: HTTP.</p>\u000a\u000a<h2>An Internet of too many things</h2>\u000a\u000a<p>Nest also provides Android and iOS apps to control your thermostat. But\u000awhat happens when you have your thermostat, light bulbs, and home\u000atheater, scanner and <a href="http://www.google.com/cloudprint/learn/">printer</a> all connected up to the internet in\u000athis fashion? Too many apps! Imagine you have all of these different\u000adevices, perhaps multiple of each in some cases:</p>\u000a\u000a<p><img src="many-smart-things.png" alt="Many smart things" /></p>\u000a\u000a<p>It pains me to imagine an app for each of these different services -\u000athere are simply too many things to juggle. I would not want to install\u000aapps for each of these devices for reasons similar to why I would never\u000ainstall the United app: there is significant overhead to managing apps\u000aon your smart phone. They take up space on your home screens, you need\u000ato keep them up to date. <a href="http://jenson.org/">Scott Jenson</a> writes eloquently about\u000athis problem on his blog.</p>\u000a\u000a<p>If each hardware manufacturer releases their device with a central\u000aregistry and public API, however, third parties might be able to build\u000aon top of the services that these devices provide, and certainly some\u000ainteresting and pleasant user experiences will arise in this nascent\u000aarea.</p>\u000a\u000a<h2>A minimum viable physical web architecture</h2>\u000a\u000a<p>There are many discovery protocols (eg. UPnP, zeroconf, ...), and many\u000amessaging protocols (eg. TCP, HTTP, ...), and I won't argue for one over\u000aanother. The technical tradeoffs of each of these is outside of the\u000ascope for this discussion. My approach was to devise the simplest\u000aproof-of-concept that would work anywhere without fancy protocols, only\u000ausing widely available features available on the web, to enable a\u000aplatform for prototyping, the goal being to establish useful and\u000ainteresting interactions first, and implement the underlying technology\u000alater.</p>\u000a\u000a<p>In general, we want to be able to interact with devices that are nearby\u000aand also with devices that are far away. With this constraint, even the\u000asimplest approach requires a server component. Here's a sketch:</p>\u000a\u000a<p><img src="lamp-arch.png" alt="A simple WiFi lamp" /></p>\u000a\u000a<p>In this setup, the controller listens to the server for what it should\u000ado. At the same time, the remote sends commands to the server based on\u000auser interaction. The server keeps track of the state of all of the\u000adevices it manages.</p>\u000a\u000a<h2>Example: cloud lamp</h2>\u000a\u000a<p>The above approach is to simply have a RESTful server. The remote can\u000asend commands to via POST requests, and the controller can poll for its\u000anew state periodically with GET requests. A simple API for a lamp might\u000alook like this:</p>\u000a\u000a<ul>\u000a<li><code>GET /</code>: Returns 1 if the lamp is on. Otherwise returns 0.</li>\u000a<li><code>POST /on</code>: Turns the lamp on.</li>\u000a<li><code>POST /off</code>: Turns the lamp off.</li>\u000a</ul>\u000a\u000a<p>This API and the three components (server, client and device) above are\u000aenough to get a prototype off the ground. A simple implementation of all\u000athree pieces can be found in the <a href="http://github.com/borismus/hello-lamp">Hello Lamp on github</a>.</p>\u000a\u000a<p>The server-side is written on AppEngine, and simply tracks state. The\u000ahandlers all use <a href="http://en.wikipedia.org/wiki/Cross-origin_resource_sharing">CORS</a>, which lifts the cross-domain security policy\u000aof the web, making it possible to POST to the lamp from any web page. </p>\u000a\u000a<p>The client is a really simple web page with two ways of controlling the\u000alamp: the toggle switch and <a href="https://dvcs.w3.org/hg/speech-api/raw-file/tip/speechapi.html">speech recognition</a> (available\u000abehind a flag in Chrome), which enables simple "lamp, turn off" and "on"\u000acommands.</p>\u000a\u000a<p><img src="client.png" alt="Simple client" /></p>\u000a\u000a<p>Finally, the device itself is an Arduino Uno with a <a href="http://arduino.cc/blog/2012/08/16/the-arduino-wifi-shield-is-now-available/">WiFi\u000ashield</a>. One of the pins is connected to a relay which controls\u000aa power socket with the plugged in lamp. The program itself is nearly\u000aidentical to the existing sample apps available for the WiFi shield.</p>\u000a\u000a<p><img src="arduino.png" alt="Arduino WiFi shield" /></p>\u000a\u000a<h2>Practical considerations</h2>\u000a\u000a<p>The solution described here is definitely not one to use in production\u000afor many reasons, including robustness, performance and security.</p>\u000a\u000a<p>In practice, we'd probably want a server to control multiple devices, so\u000aeach request should specify an ID representing the device. Also,\u000acontinuous polling is a very inefficient approach, since every HTTP\u000arequest has a lot of overhead. It's better to maintain open connection\u000aserver-controller and server-remote connections to enable pushing of\u000adata directly. This connection should be bidirectional to let the device\u000afeed back to the remote as well as enabling control.</p>\u000a\u000a<p>A promising startup working in the direction of making vision more real\u000ais <a href="http://electricimp.com">electricimp</a>. These guys provide\u000aconnectivity through a SD card form factor they call an "imp". Though\u000athey have a proprietary communication protocol between their imps and\u000atheir cloud (mainly for performance), they ultimately provide a RESTful\u000aweb API to control each imp. Definitely looking forward to playing with\u000atheir offerings.</p>\u000a\u000a<h2>Mash it up</h2>\u000a\u000a<p>Now we have a device registering with a public web server. The server\u000aprovides an API for developers to access that device. Developers can\u000athen build applications around that API which interact with the device.\u000aThis is no different from a regular web API, and at this point, all we\u000ahave built is a glorified remote control that uses the web instead of IR\u000afor transmitting messages.</p>\u000a\u000a<p>However! One of the great things about web APIs is that they can easily\u000abe mixed with one another to create something greater than the sum of\u000athe parts. This is known on the internet as a "mashup". The first\u000amashups emerged about decades years ago, but are still an important part\u000aof the web landscape. I recently used a really nice one called\u000a<a href="http://livelovely.com/">lovely</a>, which is a house hunting mashup combining Google Maps and\u000acraigslist. Mashup making services like <a href="http://pipes.yahoo.com/pipes/">Yahoo! Pipes</a> and\u000a<a href="https://ifttt.com/">IFTTT</a> have existed for a while. Notably, IFTTT includes support\u000afor some of the "smart" physical devices I mentioned in the beginning of\u000athis post.</p>\u000a\u000a<p>To me, the truly interesting question around this physical web is the\u000auser's experience. Each of the various promises of\u000adevice/appliance/thing control or automation is not terribly compelling.\u000aWhy would I unlock my phone, find and run the lamp app, and find the\u000aright fixture? The alternative of walking up to the light switch and\u000aflipping it seems much more lucrative. That said, I'm optimistic that\u000athere are compelling user interfaces to be discovered, but they will\u000aonly emerge on top of this physical web framework.</p>\u000a\u000a<h2>A plea for physical APIs</h2>\u000a\u000a<p>As we move into a world connected devices, we need to have a solid,\u000awidely available platform for communication between these devices. This\u000ainfrastructure can and should ultimately be the web.</p>\u000a\u000a<p>This opinion of mine is not zealotry, but completely practical: I do not\u000awant every device in my home to be stuck in an networked ecosystem\u000aprovided by some single company. Whether it's an Android app, an iOS\u000aapp, a web app, a universal dashboard on a smart phone, a voice\u000acontrolled system in the living room, or (more likely) something\u000acompletely different, any of these systems should be able to easily\u000ainterface with any device in the world.</p>\u000a\u000a<p>The recent stink around <a href="http://www.technologyreview.com/view/508666/twitter-instagram-and-the-internet-of-disconnected-things/">twitter no longer embedding instagram\u000aphotos</a> is just one example of important chunks of the\u000ainternet become segregated from one another. This wall-building trend is\u000aalarming and dangerous to the integrity of the web platform, which is\u000abased on interoperability: cross-linking, cross-embedding, and API\u000amashups. While this new web of things is still in its infancy, it's\u000aespecially important that we nurture the same principles that made the\u000aearly web so great.</p>\u000a
p2017
tp2018
Rp2019
sg11
V/mashup-of-things
p2020
sg13
Nsg14
I01
sg15
VInternet mashup of things
p2021
sg18
VMany new devices come with unexpected connectivity - often a WiFi\u000aconnection that enables them to connect to a hotspot and the larger\u000ainternet.
p2022
sg4
V<p>Many new devices come with unexpected connectivity - often a WiFi\u000aconnection that enables them to connect to a hotspot and the larger\u000ainternet. <a href="http://www.nest.com/">Nest</a>, a smart thermostat, was one of the first\u000acommercial products to do this. Many more indie projects are following\u000asuit, with an explosion of kickstarters like this <a href="http://www.kickstarter.com/projects/limemouse/lifx-the-light-bulb-reinvented">teleoperated\u000alight</a>, <a href="http://www.withings.com/">connected scale</a> or this <a href="http://supermechanical.com/twine/">general purpose\u000aconnected sensor</a>. The idea of an Internet of Things, in which\u000aevery appliance and object is somehow connected, has long been popular\u000ain academic circles, and this time around it feels like we're actually\u000aclose.</p>\u000a\u000a<p>If we think of these physical devices/appliances as web services with\u000aAPIs, we can mash them up just like we did in the early days of the web,\u000acreating applications that are more useful than the sum of their parts.\u000aIn this post I argue for using the web as the medium to tie everything\u000atogether, describe a simple architecture for building networked physical\u000adevices and build a web lamp controlled by an arduino.</p>\u000a\u000a
p2023
sg23
g147
sg31
g2020
sg148
(dp2024
g150
S'Jan'
p2025
sg152
S'January 11, 2013'
p2026
sg154
I1
sg155
S'2013-01-11T09:00:00-00:00'
p2027
sg157
I1357923600
sg158
I2013
sg159
I11
ssg61
g160
sg29
S'mashup-of-things'
p2028
sS'posted'
p2029
g166
(S'\x07\xdd\x01\x0b'
p2030
tp2031
Rp2032
ssg32
S'content/posts/2013/mashup-of-things/index.md'
p2033
sg34
F1433825522.0
sa(dp2034
g2
(dp2035
g26
g5
(g6
g7
V<p>All good things must come to an end. VPS hosting paid for by my former\u000auniversity is no exception! Ever since the University of\u000aMadeira-provided credit card paying for the account expired, I began\u000awondering whether it's worth paying for a VPS that I hardly use.\u000aCombined with two consecutive 10-minute stretches of downtime last week,\u000aI had my answer.</p>\u000a\u000a<p>I run this blog, my mother's site and a handful of mini-sites, all of\u000awhich are inherentily static content. Today, I moved them all away from\u000amy VPS completely. I migrated the relatively complex sites to the\u000a<a href="https://github.com/borismus/lightning">lightning</a> engine, and updated the engine with a couple of nice\u000afeatures: fixed content links in list pages and feeds, and support for\u000apublishing to S3.</p>\u000a\u000a<!--more-->\u000a\u000a<h2>System administration</h2>\u000a\u000a<p>In my early Linux days, I ran an AMD Athlon server off my parents'\u000ainternet connection. I took pride in configuration, maintenance,\u000aadministration, endlessly recompiling updates and dealing with broken\u000adependencies. I enjoyed the challenge and got very good at it. By\u000asinking enough time into any problem, I was confident that I would\u000aultimately solve it. Sometimes I contributed an ebuild or two to\u000aportage. I learned a lot, and eventually my web server outgrew my\u000aparents' internet connection.</p>\u000a\u000a<p>So I turned to managed hosting. Several years later, sick of the crappy\u000amanagement UI, and yearning to flex some sysadmin muscle, I jumped on\u000aVPS opportunity for performance reasons. While clearly overkill for\u000astatic sites, it was appealing from a "what if?" perspective: what if\u000asuddenly I wanted to run a complex webapp? No problem, VPS was ready!</p>\u000a\u000a<h2>Except system administration sucks</h2>\u000a\u000a<p>My VPS slice was running Ubuntu 8. Since Ubuntu 12 was released, I was\u000agreeted with a "48 packages are out of date" message upon logging into\u000athe machine. </p>\u000a\u000a<p>Long ago, this message would have sent me down a rabbit hole of emerging\u000aall of the outdated packages, resolving dependencies and rewriting\u000aconfig files. It was gratifying to be on the bleeding edge, to have a\u000aclean system with all of the daemons dancing to your tune in perfect\u000aharmony.</p>\u000a\u000a<p>These days, I could care less about being up-to-date. In fact, I\u000aactively dislike upgrading. An upgrade is a risk, likely to lead to\u000asomething breaking, likely without me noticing at first. So rather than\u000athe "ooh, new shiny" feeling I used to have when Apache needed an\u000aupdate, I actively dread needing to update anything. I don't want to\u000aneed to tweak configurations, especially because I've forgotten a lot of\u000athe domain-specific config languages. </p>\u000a\u000a<h2>S3 for static hosting, PaaS for everything else</h2>\u000a\u000a<p>Happily, all of my sites are currently static. Blogs and mini-sites all\u000alend themselves very well to being hosted on S3.</p>\u000a\u000a<p>For the hypothetical case that I require a dynamic web server on\u000athe internet, I'll turn to a Platform-as-a-service solution like\u000a<a href="http://nodejitsu.com/">Nodejitsu</a> or <a href="https://developers.google.com/appengine/">AppEngine</a> to avoid doing rote configuration\u000atasks.</p>\u000a\u000a<p>Being a sysadmin is not a part time job.</p>\u000a
p2036
tp2037
Rp2038
sg11
V/moved-to-s3
p2039
sg13
Nsg14
I01
sg15
VFrom VPS to static hosting
p2040
sg18
VAll good things must come to an end.
p2041
sg4
V<p>All good things must come to an end. VPS hosting paid for by my former\u000auniversity is no exception! Ever since the University of\u000aMadeira-provided credit card paying for the account expired, I began\u000awondering whether it's worth paying for a VPS that I hardly use.\u000aCombined with two consecutive 10-minute stretches of downtime last week,\u000aI had my answer.</p>\u000a\u000a<p>I run this blog, my mother's site and a handful of mini-sites, all of\u000awhich are inherentily static content. Today, I moved them all away from\u000amy VPS completely. I migrated the relatively complex sites to the\u000a<a href="https://github.com/borismus/lightning">lightning</a> engine, and updated the engine with a couple of nice\u000afeatures: fixed content links in list pages and feeds, and support for\u000apublishing to S3.</p>\u000a\u000a
p2042
sg23
g147
sg31
g2039
sg148
(dp2043
g150
S'Jan'
p2044
sg152
S'January 16, 2013'
p2045
sg154
I1
sg155
S'2013-01-16T09:00:00-00:00'
p2046
sg157
I1358355600
sg158
I2013
sg159
I16
ssg61
g160
sg29
S'moved-to-s3'
p2047
sS'posted'
p2048
g166
(S'\x07\xdd\x01\x10'
p2049
tp2050
Rp2051
ssg32
S'content/posts/2013/moved-to-s3/index.md'
p2052
sg34
F1433825517.0
sa(dp2053
g2
(dp2054
g26
g5
(g6
g7
V<p>I've been thinking about this for a while, but the recent <a href="http://googlereader.blogspot.ca/2013/03/powering-down-google-reader.html">sunset\u000aannouncement of Google Reader</a> made me revisit this topic.\u000aGoogle Reader isn't the only thing that's dead. RSS (aka. Really Simple\u000aSyndication) has long been proclaimed dead as well. In fact most people\u000anever even knew what RSS was. That said, it was a very useful tool for\u000ame and many others that like to stay up-to-date in their areas of\u000ainterest. Increasingly, I've been getting my dose of news through social\u000anetworks. However, social networks contain a lot of noise that I care\u000alittle for. I want to rebuild the RSS spirit using modern social\u000anetworks. This post describes one possible approach, which I refer to as\u000a<strong>Really Simple Social Syndication (RSSS)</strong>.</p>\u000a\u000a<!--more-->\u000a\u000a<h2>Really simple syndication: the good old days</h2>\u000a\u000a<p>Here's what the content flow used to be with blogs and RSS:</p>\u000a\u000a<p><img src="rss-flow.png" alt="RSS-based content syndication" /></p>\u000a\u000a<p>It was simple. Really simple, actually!</p>\u000a\u000a<h2>Social syndication: today</h2>\u000a\u000a<p>I don't care much for social networks. I mostly see them as a two-way\u000autility for ultimately connecting content creators to content consumers.</p>\u000a\u000a<p>One way, social networks like Facebook, Twitter, G+, etc are just\u000avehicles for finding out what to read based on your interests. I spend\u000atoo much time checking them individually for my news, and this is\u000aunfortunate. </p>\u000a\u000a<p>The other way, social networks make it easier to have your content read\u000aby a bunch of the right people. In my case, the vast majority of\u000anon-organic search traffic comes from social sources (mostly twitter). I\u000awaste a bit of time tweeting, and G+ing new posts on my blog as they\u000acome out, but sometimes they are picked up by others and I don't\u000aactually need to do that.</p>\u000a\u000a<p>As long as the content itself stays outside of the walled gardens of the\u000asocial networks, I think we can come to a syndication solution that\u000amight rival the old RSS-based one, but enjoy the benefits of having some\u000aextra signals from all of the social network junk. Here's the model I'm\u000athinking of:</p>\u000a\u000a<p><img src="social-flow.png" alt="Social-based content syndication" /></p>\u000a\u000a<p>Now for a little bit more about Pub and Sub.</p>\u000a\u000a<h2>Sub: Requirements for consumption</h2>\u000a\u000a<ul>\u000a<li><p><strong>Reuse existing feeds</strong>. I don't want to re-create my sources. Use\u000aexisting people you follow on Twitter, from G+ circles, Facebook\u000afriends.</p></li>\u000a<li><p><strong>De-duplicated content</strong>. Some people post the same link on multiple\u000anetworks. Some popular posts are reshared by everyone. I only want to\u000asee a post once.</p></li>\u000a<li><p><strong>Content centric</strong>. Show me the content in some standard, readable\u000away. Hide the social stuff unless I explicitly ask. Most of the time I\u000adon't care where it came from, don't care how many people liked it, or\u000awhat they wrote in the comments. Sometimes I'm curious and want a way\u000ato trace it back.</p></li>\u000a<li><p><strong>Social signals as a metric</strong>. If many of my sources share something,\u000aI probably should at least take a skim.</p></li>\u000a<li><p><strong>Web based service</strong> so that I can use it anywhere.</p></li>\u000a<li><p>Set a <strong>volume-based daily quota</strong>. If I'm busy today I'd like to only\u000asee the top N articles, sorted by some transparent metric of my\u000achoosing.</p></li>\u000a</ul>\u000a\u000a<p>Flipboard, Pulse, Feedly all sort of fit into this class of readers.\u000aIdeally this would be an API that just lets me connect a few social\u000aaccounts, and get back a filtered feed of content. This could be the API\u000afor the product - a Really Simple Social Syndication (RSSS) feed.</p>\u000a\u000a<p>Anyone could then build a UI on top of it for their favorite platform\u000a(Google Reader-like).</p>\u000a\u000a<h2>Pub: Requirements for content production</h2>\u000a\u000a<ul>\u000a<li><p>Automatically post content to a bunch of social services. </p></li>\u000a<li><p>Intelligent shortening of links and content (eg. for twitter, to fit\u000ain 140 chars).</p></li>\u000a</ul>\u000a\u000a<p>Wordpress plugins, Tumblr and others provide ways to automatically tweet\u000aand otherwise post new updates to your content. There needs to be some\u000aother way that works in general for any type of content. Such a service\u000acould be similar to feedburner, in that it would take an existing RSS\u000afeed and socialize it.</p>\u000a\u000a<h2>Social networks, let's be friends!</h2>\u000a\u000a<p>I'm not religious about Google Reader or RSS. It was a good solution for\u000acontent syndication at the time, but I'm ready to accept that perhaps\u000ait's time to move on. Hopefully with tools like the above, we can have\u000asomething that comes close to the utility of RSS feeds.</p>\u000a\u000a<p>Social networks could do some evil stuff which would preclude RSSS from\u000ahappening. Economically, they are incentivized own content, create\u000awalled gardens, insert advertisements, and prevent access to their\u000afeeds. I'm hoping that some human element will prevent that from\u000ahappening.</p>\u000a\u000a<p>Here is a short list of what we need from the social networks:</p>\u000a\u000a<ul>\u000a<li><p>The content itself must be free from walled gardens (eg. paywalls,\u000alogin walls, etc)</p></li>\u000a<li><p>Social network feeds are available to read in full, as is, without\u000amagical suggestions, collaborative filtering, etc.</p></li>\u000a<li><p>Social networks provide some programmatic way to post content.</p></li>\u000a</ul>\u000a\u000a<p>Once we have a good flow for the production and consumption of content,\u000ausing social networks as a delivery mechanism, I will be very happy to\u000aminimize the amount of time spent on social networks directly, and focus\u000aon consuming and producing interesting things. Also, happy &pi; day!</p>\u000a
p2055
tp2056
Rp2057
sg11
V/really-simple-social-syndication
p2058
sg13
Nsg14
I01
sg15
VReally simple social syndication
p2059
sg18
VI've been thinking about this for a while, but the recent [sunset\u000aannouncement of Google Reader][sunset] made me revisit this topic.
p2060
sg4
V<p>I've been thinking about this for a while, but the recent <a href="http://googlereader.blogspot.ca/2013/03/powering-down-google-reader.html">sunset\u000aannouncement of Google Reader</a> made me revisit this topic.\u000aGoogle Reader isn't the only thing that's dead. RSS (aka. Really Simple\u000aSyndication) has long been proclaimed dead as well. In fact most people\u000anever even knew what RSS was. That said, it was a very useful tool for\u000ame and many others that like to stay up-to-date in their areas of\u000ainterest. Increasingly, I've been getting my dose of news through social\u000anetworks. However, social networks contain a lot of noise that I care\u000alittle for. I want to rebuild the RSS spirit using modern social\u000anetworks. This post describes one possible approach, which I refer to as\u000a<strong>Really Simple Social Syndication (RSSS)</strong>.</p>\u000a\u000a
p2061
sg23
g147
sg31
g2058
sg148
(dp2062
g150
S'Mar'
p2063
sg152
S'March 14, 2013'
p2064
sg154
I3
sg155
S'2013-03-14T09:00:00-00:00'
p2065
sg157
I1363276800
sg158
I2013
sg159
I14
ssg61
g160
sg29
S'really-simple-social-syndication'
p2066
sS'posted'
p2067
g166
(S'\x07\xdd\x03\x0e'
p2068
tp2069
Rp2070
ssg32
S'content/posts/2013/really-simple-social-syndication/index.md'
p2071
sg34
F1433825504.0
sa(dp2072
g2
(dp2073
g26
g5
(g6
g7
V<p>Largely because of the plummeting price and thickness of touch screens,\u000athese devices are increasingly ubiquitous. One of the latest trends is\u000atouch screen laptops, spearheaded by <a href="http://www.microsoft.com/Surface/en-US">Surface</a> devices and the\u000arecently announced <a href="http://chrome.blogspot.com/2013/02/the-chromebook-pixel-for-whats-next.html">Chromebook Pixel</a>. In this post I'll dive into\u000asome experiements around this new form factor. The main goal is to try\u000ato convince myself that this form factor makes sense for reasons other\u000athan economic ones.</p>\u000a\u000a<p>In exploring the interaction design angle of these new devices, I came\u000aacross a couple of what I think are a couple of interesting ideas that\u000aI'd like to share with you: <strong>responsive input</strong> and <strong>simultaneous\u000ainteractions</strong> using both mouse/trackpad and touchscreen. I wrote some\u000ademos that illustrate these ideas. <em>A touchscreen laptop is required for\u000athese demos to work properly</em>.</p>\u000a\u000a<ul>\u000a<li><a href="http://borismus.github.com/touch-laptop-experiments/responsive">Auto scaling in response to input type</a>.</li>\u000a<li><a href="http://borismus.github.com/touch-laptop-experiments/map">Mouse-to-map and touch-to-mark</a>.</li>\u000a<li><a href="http://borismus.github.com/touch-laptop-experiments/transform">Multimodal transform demo</a>.</li>\u000a</ul>\u000a\u000a<!--more-->\u000a\u000a<p>Because you probably don't have a touch screen laptop, I recorded a\u000arough video showing some of these interactions:</p>\u000a\u000a<iframe width="640" height="360" src="http://www.youtube.com/embed/rcE2z9tudGw" frameborder="0" allowfullscreen></iframe>\u000a\u000a<p>Hopefully this gives you a better sense of what I mean by responsive\u000ainput and simultaneous touch and mouse interactions.</p>\u000a\u000a<h2>Responsive input</h2>\u000a\u000a<p>The touch laptop class of device has a two main interaction modes:</p>\u000a\u000a<ol>\u000a<li>As a regular laptop with trackpad (or external mouse) and keyboard.</li>\u000a<li>As a touch tablet with a keyboard.</li>\u000a</ol>\u000a\u000a<p>These two interaction modes differ fundamentally in many ways. The\u000afollowing are some examples of these differences:</p>\u000a\u000a<ul>\u000a<li>Touch has no hover state.</li>\u000a<li>Touch is less precise than mouse and requires bigger targets.</li>\u000a<li>Touch requires that you are closer to the screen.</li>\u000a</ul>\u000a\u000a<p><img src="touch-laptop.png" alt="Chrome Pixel" /></p>\u000a\u000a<p>Ideally, you want to provide optimal experiences for both cases. For the\u000amouse case, this means taking advantage of hover states and a finer\u000apointer. For the touch case, this means ensuring that touch targets are\u000abig enough to be tapped, not relying on hover at all.</p>\u000a\u000a<p>So I explored a user interface concept that adapts touch laptop\u000ainterfaces to the user's current input mode. The tricky bit is detecting\u000athe user's current input mode. Several adaptation options are possible:</p>\u000a\u000a<ol>\u000a<li>Immediately transform to the mouse-style UI as soon as the input mode\u000achanges (simplest, but can cause transitions to fire too rapidly\u000abetween the two modes, which may be jarring).</li>\u000a<li>Transform only after some period of not using the other input mode\u000a(eg. go to touch mode only if the user is actively using touch, and\u000anot touching the mouse at all).</li>\u000a<li>Transform based on some external criteria, like whether or not the\u000ascreen is docked to a mouse, or based on input from sensors other\u000athan mouse/touchscreen.</li>\u000a</ol>\u000a\u000a<p>The first approach is problematic in that your first touch transforms\u000athe page. If this transformation causes your target to move away from\u000ait's initial position, you will miss it entirely. This can be mitigated\u000aby having intelligent resizing which does not affect anything directly\u000aunder the touch point, but may result in a lopsidedly zoomed interface.</p>\u000a\u000a<p>The second approach is problematic since the mode switching will happen\u000aautomatically after some period of inactivity, which may be jarring. The\u000alast approach is either obvious (eg. mouse removed), or an area of\u000aresearch (eg. predicting when the user will touch based on camera).</p>\u000a\u000a<p>I wrote a <a href="http://borismus.github.com/touch-laptop-experiments/responsive">demo of auto scaling in response to input type</a>.\u000aIf you use the mouse, click targets will decrease in size. If you use\u000ayour finger, touch targets increase in size. (Of course, this will only\u000awork on a touchscreen laptop).</p>\u000a\u000a<h2>Simultaneous touchscreen + mouse/trackpad interactions</h2>\u000a\u000a<p>In the above section, I described an automatic way to switch between\u000atouch and mouse mode However, there is a middle ground between the two:\u000amultimodal interactions that involve both touchscreen and\u000amouse/trackpad. Simultaneous bimodal interaction is already common. For\u000aexample, using mouse and keyboard simultaneously makes a very efficient\u000ainterface for FPS games, with the movement via the WASD keys, and\u000amouse-look.</p>\u000a\u000a<p>One experiment involves using the mouse or trackpad as a navigation\u000adevice and using the touch screen as a way to input positional data.\u000aThis is illustrated through Google maps. You pan and zoom the map using\u000amouse events, and place markers on the map using the touch screen. Try\u000aout this demonstration of <a href="http://borismus.github.com/touch-laptop-experiments/map">mouse-to-map and touch-to-mark</a> (again,\u000athis requires a touchscreen laptop).</p>\u000a\u000a<p>Another experiment involves manipulating geometric objects on the\u000ascreen. The idea here was to use the touch screen to select objects, and\u000ause the trackpad/mouse as way of manipulating the selected object. In\u000athis demo, you can manipulate the object in a number of ways:</p>\u000a\u000a<ol>\u000a<li>Move it by simply dragging it around on the screen with touch.</li>\u000a<li>Rotate by selecting the object on the touchscreen, and then\u000aperforming a mousemove (either by moving a mouse or dragging one\u000afinger on a trackpad). The rotation happens around the point where\u000ayou touched the object, which acts as a fulcrum. </li>\u000a<li>Scale it in the same fashion as rotation (selecting object and\u000atransformation origin with the touchscreen), except with a two-finger\u000adrag on the trackpad, or using the mousewheel if a mouse is attached.</li>\u000a</ol>\u000a\u000a<p>With no selection, the canvas itself can be zoomed and panned with the\u000amouse/trackpad directly. Try out this <a href="http://borismus.github.com/touch-laptop-experiments/transform">multimodal transform\u000ademo</a> (requires touchscreen laptop).</p>\u000a\u000a<h2>Missing pieces</h2>\u000a\u000a<p>Like any brave new world, the one of multimodal input has its own set of\u000achallenges.</p>\u000a\u000a<p>It's currently impossible to distinguish a touch laptop from any other\u000atouch screen. Notably, this means that you should never assume that\u000atouch support implies no mouse support. In practice, make sure that you\u000aalways bind to mouse events. If you also have touch event handlers, just\u000ause <code>event.preventDefault()</code> there to ensure that you aren't handling\u000aone event in multiple handlers. If you're interested in this, follow the\u000adiscussion at <a href="http://crbug.com/174553">http://crbug.com/174553</a>.</p>\u000a\u000a<p>As a generalization of the above, there is currently no way to determine\u000awhich kinds of input are available in the browser. A fully fledged Input\u000aAvailability API might seem like overkill, but there are already some\u000acases beyond touch laptops that are relevant. For example, detecting the\u000apresence of a physical keyboard would be very useful. Further, detecting\u000ahardware features like an attached camera and microphone could fall into\u000athe same bucket rather than relying on exception handling from APIs like\u000a<code>getUserMedia</code>. Lastly, having such an API would allow websites to react\u000adynamically to changes in input (eg. a tablet gets docked to a physical\u000akeyboard, or a mouse is attached).</p>\u000a\u000a<p>The final missing piece is that dealing with two different event models\u000a(mouse and touch) is definitely clunky. I have already <a href="http://smus.com/mouse-touch-pointer/">written\u000aextensively about pointer events</a> and a <a href="https://github.com/borismus/pointer.js">pointer event\u000apolyfill</a>. In this particular case, pointer events would be\u000agreat, because although they provide a consolidated model for input,\u000ait's very easy and natural to distinguish between the two modalities.</p>\u000a\u000a<p>These experiments are all available <a href="https://github.com/borismus/touch-laptop-experiments">on github</a>. </p>\u000a\u000a<h2>Your turn!</h2>\u000a\u000a<p>Do you have thoughts or demos around new types of interactions using\u000atouch laptops? Please share them below.</p>\u000a
p2074
tp2075
Rp2076
sg11
V/touch-laptop-experiments
p2077
sg13
Nsg14
I01
sg15
VInteractive touch laptop experiments
p2078
sg18
V\u000aLargely because of the plummeting price and thickness of touch screens,\u000athese devices are increasingly ubiquitous.
p2079
sg4
V<p>Largely because of the plummeting price and thickness of touch screens,\u000athese devices are increasingly ubiquitous. One of the latest trends is\u000atouch screen laptops, spearheaded by <a href="http://www.microsoft.com/Surface/en-US">Surface</a> devices and the\u000arecently announced <a href="http://chrome.blogspot.com/2013/02/the-chromebook-pixel-for-whats-next.html">Chromebook Pixel</a>. In this post I'll dive into\u000asome experiements around this new form factor. The main goal is to try\u000ato convince myself that this form factor makes sense for reasons other\u000athan economic ones.</p>\u000a\u000a<p>In exploring the interaction design angle of these new devices, I came\u000aacross a couple of what I think are a couple of interesting ideas that\u000aI'd like to share with you: <strong>responsive input</strong> and <strong>simultaneous\u000ainteractions</strong> using both mouse/trackpad and touchscreen. I wrote some\u000ademos that illustrate these ideas. <em>A touchscreen laptop is required for\u000athese demos to work properly</em>.</p>\u000a\u000a<ul>\u000a<li><a href="http://borismus.github.com/touch-laptop-experiments/responsive">Auto scaling in response to input type</a>.</li>\u000a<li><a href="http://borismus.github.com/touch-laptop-experiments/map">Mouse-to-map and touch-to-mark</a>.</li>\u000a<li><a href="http://borismus.github.com/touch-laptop-experiments/transform">Multimodal transform demo</a>.</li>\u000a</ul>\u000a\u000a
p2080
sg23
g147
sg31
g2077
sg148
(dp2081
g150
S'Feb'
p2082
sg152
S'February 21, 2013'
p2083
sg154
I2
sg155
S'2013-02-21T09:00:00-00:00'
p2084
sg157
I1361466000
sg158
I2013
sg159
I21
ssg61
g160
sg29
S'touch-laptop-experiments'
p2085
sS'posted'
p2086
g166
(S'\x07\xdd\x02\x15'
p2087
tp2088
Rp2089
ssg32
S'content/posts/2013/touch-laptop-experiments/index.md'
p2090
sg34
F1433825466.0
sa(dp2091
g2
(dp2092
g26
g5
(g6
g7
V<p>The phone in your pocket is an amazing, fluid, multi-functional tool.\u000aWhen it comes to talking to other devices, such as your TV or laptop,\u000athe user experience drops off sharply. Bill Buxton <a href="http://www.youtube.com/watch?v=ZQJIwjlaPCQ&amp;feature=youtu.be&amp;t=21m00">speaks\u000aeloquently</a> on the subject, describing three stages of\u000ahigh tech evolution:</p>\u000a\u000a<ol>\u000a<li>Device works: feature completeness and stability</li>\u000a<li>Device flows: good user experience</li>\u000a<li>Many devices work together</li>\u000a</ol>\u000a\u000a<p>But connecting devices is a pain and we have been squarely at stage 2\u000asince the release of the iPhone. There are many competing approaches to\u000ado this: Bluetooth, Bluetooth LE, WiFi direct, discovery over the same\u000alocal WiFi network, and many many others. This post is dedicated to\u000aattacking this problem from an unexpected angle: using ultrasound to\u000abroadcast and receive data between nearby devices. Best of all, the\u000aapproach uses the Web Audio API, making it viable for pure web\u000aapplications:</p>\u000a\u000a<iframe width="640" height="360" src="//www.youtube.com/embed/w6lRq5spQmc" frameborder="0" allowfullscreen="true"></iframe>\u000a\u000a<!--more-->\u000a\u000a<h2>A device tower of babel</h2>\u000a\u000a<p><a href="http://www.apple.com/airplay/">Airplay</a> and <a href="http://www.google.com/chromecast">Chromecast</a> are great approaches to a subset of the\u000aproblem for devices within the same ecosystem (eg. Apple, or Google),\u000abut the general problem remains hard to solve.</p>\u000a\u000a<p>Because there are so many possible technical approaches, chances are\u000athat the pair of devices that you happen to be using don't have a common\u000alanguage to speak. Even if both devices have Bluetooth, one of them may\u000arequire a profile the other doesn't support, or support a different\u000aversion of the standard. This is especially common with Bluetooth today,\u000awhere many devices have the hardware to support Bluetooth 4.0 (aka\u000aBTLE), but many devices don't currently support the new protocol for\u000avarious reasons.</p>\u000a\u000a<p>On the web, the problem is even worse, since low level device connection\u000aAPIs aren't exposed for <a href="http://smus.com/installable-webapps/">security sandbox reasons</a>. Because of\u000ahow slowly the web evolves, it's hard to imagine this changing any time\u000asoon.</p>\u000a\u000a<h2>Transmitting data in interesting ways</h2>\u000a\u000a<p><a href="http://www.youtube.com/watch?v=sVWlQNzU4Ak">Blinkup from Electric Imp</a> is an interesting approach to\u000across-device communication. It uses a series of blinks to transfer\u000aconfiguration data between a smart phone and an Imp, a small SD-card\u000ashaped device with a light sensor.</p>\u000a\u000a<p>Dial-up modems did a similar thing. They encoded and decoded digital\u000adata onto an analog phone line. Remember those annoying connection\u000anoises? Dial-up modems would turn on their speaker to give the user an\u000aidea of how the handshake is progressing. If you don't remember, here's\u000aa <a href="http://www.windytan.com/2012/11/the-sound-of-dialup-pictured.html">refresher</a>. Even today on analog phones, the sounds you hear\u000awhen pressing numbers on a dialer correspond to the frequencies the\u000aphone system uses for analog-to-digital conversion. The conversion\u000ahappens using <a href="http://en.wikipedia.org/wiki/Dual-tone_multi-frequency_signaling">Dual-tone multi-frequency signaling (DTMF)</a>.</p>\u000a\u000a<p>Your phone and a lot of other devices around you has a speaker and a\u000amicrophone. These two pieces of hardware can be used for sending and\u000areceiving data using sounds, similar to how modems did it over phone\u000alines. Better yet, if the OS supports high enough frequency sending and\u000areceiving, we can create an inaudible data channel.</p>\u000a\u000a<h2>Transmitting data using sound</h2>\u000a\u000a<p>I should note that encoding data in sound is not new. The idea of <a href="http://en.wikipedia.org/wiki/Audio_watermark">audio\u000awatermarking</a> is to encode a signature into music that is not\u000adiscernable by the listener (due to the way humans hear), but can be\u000apicked up by a machine. This is used as a clever piracy detection\u000ascheme. </p>\u000a\u000a<p>Most commodity speakers are capable of producing sound with a 44.1KHz\u000asample rate (resulting in a maximum frequency of about 22KHz by the\u000a<a href="http://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem">Nyquist-Shannon sampling theorem</a>). This lets us encode data\u000anot just as sound, but as sound that adults can't hear. Children and\u000anon-human animals are still susceptible, though :)</p>\u000a\u000a<p>One technical caveat is that microphones are sometimes not as capable as\u000aspeakers, especially in phones, since they are often optimized for human\u000aspeech, which sounds fine with a lower sample rate. In other cases,\u000aeven though the hardware is capable, the firmware runs at a lower sample\u000arate for energy efficiency. If this is the case, one of the devices will\u000anot be able to receive the wave and the sound-based connection will be\u000aone-way only.</p>\u000a\u000a<h2>Sonicnet.js, a web audio implementation</h2>\u000a\u000a<p>To illustrate these concepts, I built a <a href="https://github.com/borismus/sonicnet.js">JavaScript library</a>\u000athat can send and receive data as sounds. My approach is and not nearly\u000aas sophisticated as the audio watermarking technique, and even simpler\u000athan the DTMF approach. Basically, you can specify a range of\u000afrequencies to use, and an alphabet of characters that can be\u000atransmitted. The frequency spectrum is split into ranges corresponding\u000ato the specified alphabet and start/end codes, with each character/code\u000acorresponding to a part of the full frequency range.</p>\u000a\u000a<p>The sending side converts each character of the word to be sent into the\u000acenter of the corresponding frequency range, and transmits that\u000afrequency for a certain duration. The receiving side does a continuous\u000afourier transform of the signal and looks for peaks in the specified\u000afrequency range. Upon finding a peak for a significant duration, it does\u000athe conversion back from frequency to character. This is essentially\u000aa <a href="http://en.wikipedia.org/wiki/Selective_calling#Tone_burst_or_single_tone">single-tone multi-frequency signaling (STMF)</a> scheme.</p>\u000a\u000a<p>There is a timing issue: on the sending side, how long should each\u000acharacter be transmitted for, and on the receiving side, how long should\u000athe listened for? An easy workaround for this is to disallow adjacently\u000arepeated characters.</p>\u000a\u000a<p>I built a socket-like API for sonic networking. Client code\u000alooks like this:</p>\u000a\u000a<pre><code>ssocket = new SonicSocket({alphabet: '0123456789'});\u000afunction onButton() {\u000a  ssocket.send('31415');\u000a}\u000a</code></pre>\u000a\u000a<p>And the server can look like this:</p>\u000a\u000a<pre><code>sserver = new SonicServer({alphabet: '0123456789'});\u000asserver.on('message', function(message) {\u000a  // Expect message to be '31415'.\u000a  console.log(message);\u000a});\u000asserver.start();\u000a</code></pre>\u000a\u000a<p>The library is available for use on <a href="https://github.com/borismus/sonicnet.js/tree/master/lib">github</a>.</p>\u000a\u000a<p>Of course, using it requires a Web Audio implementation (mostly\u000a<code>OscillatorNode</code> on the sending side, and <code>AnalyserNode</code> on the\u000areceiving side) and good enough hardware. I have experimented with\u000aChrome-to-Chrome transmission on Mac Books, as well as between Chrome\u000afor Android (beta) and Chrome for Mac.</p>\u000a\u000a<p>I wrote a couple of demos to illustrate the idea. These appear in the\u000a<a href="http://www.youtube.com/watch?v=w6lRq5spQmc">video I embedded at the top of the post</a>. The first demo lets\u000ayou <a href="http://borismus.github.io/sonicnet.js/emoticons">send emoticons</a> from one device to the other. It uses a\u000asmall alphabet of just 6 characters - one for each emoticon. You pick\u000aone of 6 emoticons, and the corresponding character is sent over the\u000asonic network, received and shown prominently on the other end.</p>\u000a\u000a<p>A more realistic use for sonicnet.js is this <a href="http://borismus.github.io/sonicnet.js/chat-pair">chat\u000aapplication</a>, which generates a non-repeating 5-digit token\u000aand uses it to create connections between two devices. This is done with\u000athe help of a pairing server, which helps establish a proxied connection\u000abetween the two devices, over a websocket. Once the connection is\u000aestablished, the chats themselves are sent through the websocket. The\u000a<a href="https://github.com/borismus/sonicnet.js/tree/master/server">server code</a> is hosted on <a href="https://www.nodejitsu.com/">nodejitsu</a>.</p>\u000a\u000a<h2>Conclusions and a request</h2>\u000a\u000a<p>It's great to see that the Web Audio API has come far enough that\u000aapplications like these are possible. I'm fascinated by the implications\u000aof sonicnet.js for the Web of Things. It is a pure web technology that\u000acan be used to pair devices together. Because of the ubiquity of web\u000abrowsers and audio hardware, the combination can be a huge win, even\u000aamong commodity hardware, without having to wait for Bluetooth and other\u000aclose-range connectivity technology to become available to the web\u000aplatform.</p>\u000a\u000a<p>If this post has piqued your interest and you are interested in helping,\u000atry writing an app using sonicnet.js. As I mentioned earlier, receiving\u000ahigh frequency sounds does not work on all devices because of\u000afirmware/hardware limitations so I'd love to know which devices it does\u000aand does not work on. My expectation is that most phones should be able\u000ato send only, and that most laptops should be able to both send and\u000areceive. Please fill out <a href="https://docs.google.com/forms/d/1dAgNdVdhss-QR-Owm556RZch-MV_ntnAMP8_ZJi5XLA/viewform">this form</a> once you try the <a href="http://borismus.github.io/sonicnet.js/emoticons">emoticons\u000ademo</a> on your own hardware. At the time of writing, <a href="http://crbug.com/242894">live\u000ainput is not supported</a> in Chrome for Android Beta, so sending\u000adata from mobile device to laptop is the only possible configuration.</p>\u000a
p2093
tp2094
Rp2095
sg11
V/ultrasonic-networking
p2096
sg13
Nsg14
I01
sg15
VUltrasonic networking on the web
p2097
sg18
VThe phone in your pocket is an amazing, fluid, multi-functional tool.
p2098
sg4
V<p>The phone in your pocket is an amazing, fluid, multi-functional tool.\u000aWhen it comes to talking to other devices, such as your TV or laptop,\u000athe user experience drops off sharply. Bill Buxton <a href="http://www.youtube.com/watch?v=ZQJIwjlaPCQ&amp;feature=youtu.be&amp;t=21m00">speaks\u000aeloquently</a> on the subject, describing three stages of\u000ahigh tech evolution:</p>\u000a\u000a<ol>\u000a<li>Device works: feature completeness and stability</li>\u000a<li>Device flows: good user experience</li>\u000a<li>Many devices work together</li>\u000a</ol>\u000a\u000a<p>But connecting devices is a pain and we have been squarely at stage 2\u000asince the release of the iPhone. There are many competing approaches to\u000ado this: Bluetooth, Bluetooth LE, WiFi direct, discovery over the same\u000alocal WiFi network, and many many others. This post is dedicated to\u000aattacking this problem from an unexpected angle: using ultrasound to\u000abroadcast and receive data between nearby devices. Best of all, the\u000aapproach uses the Web Audio API, making it viable for pure web\u000aapplications:</p>\u000a\u000a<iframe width="640" height="360" src="//www.youtube.com/embed/w6lRq5spQmc" frameborder="0" allowfullscreen="true"></iframe>\u000a\u000a
p2099
sg23
g147
sg31
g2096
sg148
(dp2100
g150
S'Aug'
p2101
sg152
S'August 8, 2013'
p2102
sg154
I8
sg155
S'2013-08-08T09:00:00-00:00'
p2103
sg157
I1375977600
sg158
I2013
sg159
I8
ssg61
g160
sg29
S'ultrasonic-networking'
p2104
sS'posted'
p2105
g166
(S'\x07\xdd\x08\x08'
p2106
tp2107
Rp2108
ssg32
S'content/posts/2013/ultrasonic-networking/index.md'
p2109
sg34
F1433825460.0
sa(dp2110
g2
(dp2111
g26
g5
(g6
g7
V<p>I wrote a short book about the Web Audio API. The book is meant as an\u000aintroduction to the web audio API, as well as some audio basics for web\u000adevelopers with little audio experience. It is <a href="http://chimera.labs.oreilly.com/books/1234000001552/">available for free on\u000aChimera</a>, a web-based book viewer, which presents a nicely laid\u000aout page and lets you leave per-paragraph comments. The online version\u000aalso includes inline samples from <a href="http://webaudioapi.com">webaudioapi.com</a>. If you don't\u000alike reading on the web, you can also <a href="http://shop.oreilly.com/product/0636920025948.do">buy a physical copy or an\u000aebook</a> from O'Reilly.</p>\u000a\u000a<!--more-->\u000a\u000a<p>So I got this cool bat on the cover. I am told that technically\u000aspeaking, it is a brown long-eared bat (<a href="http://en.wikipedia.org/wiki/Brown_long-eared_bat">Plecotus auritus</a>). Though\u000ait's no orca (an ideal O'Reilly cover, don't you think?), I'm very happy\u000athat it's a <a href="http://en.wikipedia.org/wiki/Animal_echolocation">sound related animal</a>.</p>\u000a\u000a<p><img src="cover.jpg" alt="Web Audio Book Cover" /></p>\u000a\u000a<p>The book was written on my laptop in a Google Doc. I hand-drew some\u000aillustrations in a notebook and brought them into the doc. Once I was\u000aready for feedback, I sent the doc around to my technical reviewers, who\u000aleft feedback in comments. After incorporating their feedback, I got\u000asome editorial feedback, still in the doc. Once the draft was more or\u000aless ready to go, I created an <a href="https://www.odesk.com/">oDesk</a> task to convert the Google\u000aDoc into docbook.xml format. The contractor did a great job and charged\u000ame about $100. This was my first time paying anyone to do work for me.</p>\u000a\u000a<p>Thanks to all of the reviewers, editors, illustrators and organizers for\u000ahelping. Also, thanks to <a href="http://www.kevincennis.com/">Kevin Ennis</a> who kindly donated the\u000a<a href="http://webaudioapi.com">webaudioapi.com</a> domain which I'm currently using to host the\u000asamples.</p>\u000a
p2112
tp2113
Rp2114
sg11
V/webaudio-book
p2115
sg13
Nsg14
I01
sg15
VWeb Audio book
p2116
sg18
VI wrote a short book about the Web Audio API.
p2117
sg4
V<p>I wrote a short book about the Web Audio API. The book is meant as an\u000aintroduction to the web audio API, as well as some audio basics for web\u000adevelopers with little audio experience. It is <a href="http://chimera.labs.oreilly.com/books/1234000001552/">available for free on\u000aChimera</a>, a web-based book viewer, which presents a nicely laid\u000aout page and lets you leave per-paragraph comments. The online version\u000aalso includes inline samples from <a href="http://webaudioapi.com">webaudioapi.com</a>. If you don't\u000alike reading on the web, you can also <a href="http://shop.oreilly.com/product/0636920025948.do">buy a physical copy or an\u000aebook</a> from O'Reilly.</p>\u000a\u000a
p2118
sg23
g147
sg31
g2115
sg148
(dp2119
g150
S'Mar'
p2120
sg152
S'March 18, 2013'
p2121
sg154
I3
sg155
S'2013-03-18T09:00:00-00:00'
p2122
sg157
I1363622400
sg158
I2013
sg159
I18
ssg61
g160
sg29
S'webaudio-book'
p2123
sS'posted'
p2124
g166
(S'\x07\xdd\x03\x12'
p2125
tp2126
Rp2127
ssg32
S'content/posts/2013/webaudio-book/index.md'
p2128
sg34
F1433825453.0
sa(dp2129
g2
(dp2130
g26
g5
(g6
g7
V<p>It is human nature to create taxonomies for everything: people, places,\u000aand things.  Without such a system of reference, we become lost and\u000adisoriented.  Imagine your city with street names and addresses blanked\u000aout. Finding your favorite cafe, meeting up with your friend on the\u000aweekend, even locating your own parked car would become incredibly\u000adifficult. Travel outside your city would become far more\u000achallenging.</p>\u000a\u000a<p>The web's defining property is addressability. URLs on the web are like\u000astreet names and addresses in the physical world. This makes sharing\u000aand cross-linking easy. Non-web platforms are a little bit like our\u000acity with blanked out street names and addresses. There's no good\u000away of talking about where you currently are, or how to get somewhere\u000aelse. These platforms typically give users a crutch to help with the\u000aissue, such as a share button or dialog. But these create an\u000ainherently inferior experience, since addressability is no longer\u000abuilt-in. Addressability becomes a burden on the app developer, and\u000aas a result, the platform is no longer navigable.</p>\u000a\u000a<p>In light of the success of Android and iOS, and given a potential\u000aexplosion in new types of lower power computing (wearables, IoT, etc),\u000ait's unclear if <a href="http://smus.com/ebb-of-the-web/">browsers will be as ubiquitous</a> as they are\u000atoday (at least in the near term). I'm very interested in seeing if and\u000ahow non-web platforms can embrace URLs.  How closely coupled are URLs to\u000aHTML, and do they make sense without a presentation layer?</p>\u000a\u000a<!--more-->\u000a\u000a<h2>Not all URLs are created equal</h2>\u000a\u000a<p>The modern URL can host several very different kinds of entities:</p>\u000a\u000a<ol>\u000a<li><strong>Data</strong>: text files, images, audio, movies, JSON, etc.</li>\u000a<li><strong>Hypertext</strong> (content-program hybrid): HTML that can reference\u000acontent.</li>\u000a<li><strong>Program</strong>: webapps designed to deliver a bundle of JavaScript that\u000athen constructs the HTML dynamically from other URLs.</li>\u000a</ol>\u000a\u000a<p><img src="data-vs-program.png" alt="Program vs. data: evolution of web" /></p>\u000a\u000a<p>Without an HTML renderer, hypertext and program URLs cannot be\u000ainterpreted. Only one of these types of entities makes sense: data. Data\u000aURLs are seen everywhere on the web: whenever you include an <code>&lt;img&gt;</code> tag\u000aon your page, or embed a <code>&lt;video&gt;</code>, reference some CSS, or make an XHR\u000ato fetch some JSON, you are using a data URL.</p>\u000a\u000a<p>Apps on other platforms use data URLs too, though not as much. Images\u000aare typically included as part of the app itself, but all API access is\u000adone in exactly the same fashion as on the web: using HTTP requests to\u000atext or binary data.</p>\u000a\u000a<p>The similarity isn't entirely superficial. Any sort of web-connected app\u000acan be seen as just a view on top of a series of data URLs (APIs).\u000aHowever, data URLs are typically hidden from the user. The only types of\u000aURLs that users see are hypertext and program URLs. These are the ones\u000athat are being shared around. But both of these types of URLs ultimately\u000amap to HTML, sometimes via JavaScript. The underlying data URLs  are\u000aconcealed inside the page, and aren't exposed to the user.</p>\u000a\u000a<h2>The "URL in, URL out" principle</h2>\u000a\u000a<p>A user need not understand schemes, domain names, DNS, HTTP or GET\u000arequests. They don't need to think about conceptual distinctions\u000abetween URL types to know that a URL is an address that gets you to the\u000asame thing you're looking at right now. Whether it's Android/Java,\u000aPolymer/JS or <em>InsertPlatform/InsertLanguage</em> underneath, the only thing\u000athey want to be able to do is to continue reading their book on whatever\u000adevice they happen to be transitioning to. They want to share it with\u000atheir friend too, and have them enjoy a good read.</p>\u000a\u000a<p>To make a platform URL-friendly, it should satisfy two simple\u000arequirements:</p>\u000a\u000a<ol>\u000a<li>The platform should provide a way for apps to reveal the underlying\u000aURL for the view.</li>\u000a<li>Given a URL, the platform should open it in a way that yields the\u000abest available user experience.</li>\u000a</ol>\u000a\u000a<p><img src="url-in-url-out.png" alt="Platforms handle content URLs and provide them on demand." /></p>\u000a\u000a<p>However, to bring URL friendliness to a platform retroactively takes a\u000alot of effort. Taking a quick look at today's trending web-alternatives,\u000ait's plain to see that Android has some form of URL in (via intent\u000afilters), but no URL out. iOS has neither in, nor out (you're stuck). To\u000aaddress this lack of URL out in Android, you can imagine all Android\u000aactivities having to implement a <code>URLReporter</code> interface like this:</p>\u000a\u000a<pre><code>class TwitterProfileViewer extends Activity implements URLReporter {\u000a  @Override\u000a  String reportURL() {\u000a    return String.format("http://twitter.com/%s", username);\u000a  }\u000a}\u000a</code></pre>\u000a\u000a<p>Of course, there is the not-unimportant UX question of how to then\u000areveal the URL and transfer it to other devices and people. But this\u000aquestion will for now be left unanswered. With this API, a very\u000atasty carrot, and a very painful stick to force developers to implement\u000ait (and a bit more UX thinking), we can make Android URL-friendly. </p>\u000a\u000a<h2>But URLs aren't just identifiers</h2>\u000a\u000a<p>You can look at URLs in one of two ways:</p>\u000a\u000a<ol>\u000a<li>As a <strong>universal identifier</strong>. The same URL is also the universal and\u000acanonical way of getting to content that you are reading now.</li>\u000a<li>As a <strong>web address</strong>. A URL like <code>http://smus.com/addressable-apps</code>\u000acan be viewed as instructions for getting to a certiain place:\u000aresolve the <code>smus.com</code> to <code>205.251.243.108</code>, connect to port 80 over\u000aTCP, perform a <code>GET /addressable-apps</code> request.</li>\u000a</ol>\u000a\u000a<p>The real power of URLs is in both aspects combined. When only one facet\u000ais used, the system is broken. I can't quite put my finger on it, but\u000asomething feels wrong in the cases, where the addressability aspect (2)\u000aof URLs are not taken into account:</p>\u000a\u000a<ul>\u000a<li>AppLinks: these aren't universal identifiers, but fragile shortcuts to\u000athe platform-specific apps. (violates 1)</li>\u000a<li>Android intent filters: when you register an URL indent filter for\u000ayour activity, you aren't actually hosting anything at that URL.\u000a(violates 2)</li>\u000a<li>History API: a hack allowing developers to set the path of the URL to\u000aanything they want. (violates 2)</li>\u000a</ul>\u000a\u000a<p>The History API emerged from a trend on the web: highly imperative\u000aapplications. These apps have grown so far from being a collection of\u000ahyperlinked markup that they no longer have a natural URL-to-HTML-page\u000amapping. Because they are so script heavy, they need to be able to\u000apretend to respond to URLs. The History API is the webapp's hack for\u000aURL-out.</p>\u000a\u000a<h2>Mixed feelings</h2>\u000a\u000a<p>This is my umpteenth attempt at finishing a post on the complicated\u000asubject of URLs in non-web apps. And you have had the pleasure of\u000areading it not because the ideas in my head have crystallized into a\u000asomething coherent, but because I feel that the topic is difficult and\u000afundamentally unresolvable. In light of that, this post contains more\u000aquestions than answers. Sorry to disappoint :)</p>\u000a\u000a<p>I'm still torn between maintaining the ideological and benefits of the\u000a<a href="http://www.polymer-project.org/">mostly-declarative web</a> and the practicality of <a href="http://smus.com/installable-webapps/">jumping out\u000aof the web's sandbox</a>. While I would be happy to see more\u000anative platforms embrace URL in URL out, I don't think that the solution\u000ais a clean one, nor do I think that first-class URLs are likely to\u000aemerge in any platform after-the-fact. Unfortunately I don't think that\u000athere is a clean solution.</p>\u000a\u000a<p>However, as an optimist, I must believe in the long-term victory of the\u000aweb, though not in the sense that the web will rule over all other\u000auser-facing platforms unopposed. Instead, platforms will continue to\u000arise and fall; the web's influence will ebb and flow as well. But the\u000aweb must be the one that wins out the most, keeping the idea of\u000aaddressability alive.</p>\u000a\u000a<p>That URLs become a universal address that works across all platforms,\u000aand not just the web, is a proposition worth considering.</p>\u000a
p2131
tp2132
Rp2133
sg11
V/addressable-apps
p2134
sg13
Nsg14
I01
sg15
VAddressable apps
p2135
sg18
VIt is human nature to create taxonomies for everything: people, places,\u000aand things.
p2136
sg4
V<p>It is human nature to create taxonomies for everything: people, places,\u000aand things.  Without such a system of reference, we become lost and\u000adisoriented.  Imagine your city with street names and addresses blanked\u000aout. Finding your favorite cafe, meeting up with your friend on the\u000aweekend, even locating your own parked car would become incredibly\u000adifficult. Travel outside your city would become far more\u000achallenging.</p>\u000a\u000a<p>The web's defining property is addressability. URLs on the web are like\u000astreet names and addresses in the physical world. This makes sharing\u000aand cross-linking easy. Non-web platforms are a little bit like our\u000acity with blanked out street names and addresses. There's no good\u000away of talking about where you currently are, or how to get somewhere\u000aelse. These platforms typically give users a crutch to help with the\u000aissue, such as a share button or dialog. But these create an\u000ainherently inferior experience, since addressability is no longer\u000abuilt-in. Addressability becomes a burden on the app developer, and\u000aas a result, the platform is no longer navigable.</p>\u000a\u000a<p>In light of the success of Android and iOS, and given a potential\u000aexplosion in new types of lower power computing (wearables, IoT, etc),\u000ait's unclear if <a href="http://smus.com/ebb-of-the-web/">browsers will be as ubiquitous</a> as they are\u000atoday (at least in the near term). I'm very interested in seeing if and\u000ahow non-web platforms can embrace URLs.  How closely coupled are URLs to\u000aHTML, and do they make sense without a presentation layer?</p>\u000a\u000a
p2137
sg23
g147
sg31
g2134
sg148
(dp2138
g150
S'May'
p2139
sg152
S'May 21, 2014'
p2140
sg154
I5
sg155
S'2014-05-21T09:00:00-00:00'
p2141
sg157
I1400688000
sg158
I2014
sg159
I21
ssg61
g160
sg29
S'addressable-apps'
p2142
sS'posted'
p2143
g166
(S'\x07\xde\x05\x15'
p2144
tp2145
Rp2146
ssg32
S'content/posts/2014/addressable-apps/index.md'
p2147
sg34
F1402287753.0
sa(dp2148
g2
(dp2149
g26
g5
(g6
g7
V<p>Tech pundits like to lament that the web has <a href="http://cdixon.org/2014/04/07/the-decline-of-the-mobile-web/">no viable\u000afuture</a>, while web idealists hold that in fact the web is\u000atotally fine, with a "too big to fail" <a href="http://schepers.cc/the-recline-of-the-mobile-web">sort of attitude</a>.</p>\u000a\u000a<p>At the root of this disagreement are poorly defined terms. The web can\u000amean many different things to different people. Though it started from a\u000apretty abstract notion of a series of interlinked documents, it has now\u000aevolved to refer to a very specific technology stack of hyperlinked HTML\u000adocuments styled with CSS, enhanced with JavaScript, all served on top\u000aof HTTP. In light of an increasing movement away from desktop-style\u000acomputing, we've seen a big shift away from the web in mobile platforms. </p>\u000a\u000a<p>Let's take apart this gob of web technology in light of the increasingly\u000acomplex landscape of computing and try to make sense of what the web is\u000aand where it's going.</p>\u000a\u000a<p><img src="webiness.png" alt="A framework for webiness" /></p>\u000a\u000a<!--more-->\u000a\u000a<h2>Webiness: how far down the rabbit hole?</h2>\u000a\u000a<p>I want to introduce the concept of webiness, a framework for evaluating\u000ahow deeply an application embraces "the web":</p>\u000a\u000a<p>Games are typical examples of apps that are <strong>pure native</strong> and not\u000awebby at all (barring high score servers, etc). Because many are\u000aplayable offline, with no added benefit of having an internet\u000aconnection, there is obviously no room for web. They are built entirely\u000aon native APIs, benefiting from being as close to the hardware as\u000apossible for performance reasons.</p>\u000a\u000a<p>Another class of native apps are Twitter, Facebook and the like, which\u000aessentially act as <strong>specialized browsers</strong>. They largely use HTTP to\u000aaccess RESTful endpoints that serve up JSON, which is rendered by the\u000anative clients. Because a specialized browser is designed for a specific\u000ause case in mind (eg. interacting with Twitter), it can be streamlined\u000afor that purpose and doesn't need to deal with processing the web's\u000apresentation layer (HTML, CSS, JavaScript). Therefore it presents a\u000amore focused experience and benefits from being faster at start-up and\u000aduring interaction, offline support, and a generally better experience.</p>\u000a\u000a<p><strong>Embedded browsers</strong> (also called hybrid apps) embed a webview into the\u000auser interface of the page to use the web's presentation layer for part\u000aof the user interface. How much of the user interface is created using\u000aweb technologies varies widely. This approach is beneficial because it\u000aallows web developers to be featured in app stores, and also gives them\u000aaccess to native APIs that are not available on the open web.</p>\u000a\u000a<p>Lastly, <strong>browser-based</strong> pages and apps are ones where even the code\u000aitself is fetched over HTTP. The app is written in a cross-browser way\u000ato ensure that regardless of which browser loads the page, the user is\u000apresented with a reasonable experience. The idea of this is to be able\u000ato write once, deploy everywhere. Like the other types of apps on our\u000aspectrum, these also use HTTP to interact with the server. All of the\u000acontent is rendered using HTML/CSS/JavaScript. Additionally, the HTML is\u000aoften hyperlinked.</p>\u000a\u000a<h2>The web as greatest common divisor</h2>\u000a\u000a<p>In mathematics, the greatest common divisor (GCD) between multiple\u000adigits is the largest positive integer that divides the numbers without\u000aa remainder. Observe that the GCD of a set of numbers (S) is by\u000adefinition less than or equal to each number. Furthermore, if the\u000anumbers in S are not multiples of one another, the inequality\u000abecomes strict. So <code>foreach n in S, gcd(S) &lt; n</code>.</p>\u000a\u000a<p>In the webbiest case above, where we write once for all browsers, the\u000aweb user interface becomes the GCD for all existing interfaces. It is\u000aguaranteed to be slower, less featureful, etc, than each individual\u000anative platform, but when the web was conceived, the benefits outweighed\u000athe costs. As the web evolved in the 90s, native platforms evolved\u000aalongside it. Computing at the time was very desktop-centric, requiring\u000aa physical keyboard, mouse, and relatively large display at, let's say\u000a1024x768 pixels. At best, variance was between operating systems. The\u000acomputer geeks were on the Linux fringe. The art and music geeks used\u000aMacs. But the hardware was pretty much the same.</p>\u000a\u000a<p>Because of this uniformity, it was easy to standardize on a set of input\u000aand outputs that would work reasonably well across a bunch of existing\u000acomputer configurations and operating systems. Computer hardware was all\u000avery similar, just off by some factor. And the GCD of this orderly set\u000awas pretty large: <code>gcd(200, 300, 400) = 100</code>.</p>\u000a\u000a<h2>We're not in Kansas anymore</h2>\u000a\u000a<p>Contrast this uniformity to today, when our mots du jour are "mobile\u000afirst", or even "mobile only". But even these notions are becoming\u000apass, as our day-to-day tech encounters start including wearable\u000asensors for health, chips embedded in your appliances, shoes, and\u000acomputers on your face. Even with the most conservative notion of what\u000amobile means - small screens and touch input - we've really thrown a\u000awrench into the big-screen, mouse-and-keyboard web. Scott Jenson is\u000aabsolutely right in remarking in an <a href="https://www.youtube.com/watch?v=6u03xYkwMVI">Edge conf panel</a>, that the\u000aweb hasn't even recovered from the fact that screens have gotten\u000asmaller. With today's extended notion of computing, the GCD of all\u000aof the devices and use cases the web is trying to support becomes very\u000asmall: <code>gcd(100, 200, 300, 50, 99, 198, 33) = 1</code>.</p>\u000a\u000a<p>At the same Edge conf panel on the future of the web, somebody\u000aasked the question of how the web would work on hardware without a\u000adisplay. Answers from the panel were incoherent, but it's unclear how\u000athis would be built into today's frankenweb, which is already a snowball\u000aof many, often redundant technologies. And this is largely because of the\u000anotion of <strong>THE WEB</strong> as a single platform. This is both its greatest\u000astrength, and ultimately its tragic flaw, as the legacy of the early 90s\u000acauses the singular web to cave in on itself, as we are experiencing\u000atoday.</p>\u000a\u000a<p>We can no longer have a one-web-for-all approach. We need to focus on\u000ahaving many different webs, each specializing on a particular subset of\u000aour universe of devices. Taking our set above, we can split it in two\u000asubsets, <code>S1 = {100, 200, 300, 50}</code>, and <code>S2 = {99, 198, 33}</code>.  Imagine\u000aS1 are the desktop-like devices, and S2 are the phone-like devices. Now,\u000awe have pretty okay GCDs: <code>gcd(S1) = 50</code>, and <code>gcd(S2) = 33</code>. Our worst\u000acase GCD is now 33, which is a lot better than 1!</p>\u000a\u000a<h2>The web is dead, long live the web!</h2>\u000a\u000a<p>I'm pretty sure that HTTP is here to stay. Our desire for content is\u000auniversal, and that content needs to live somewhere. Regardless of how\u000athat content is presented to us, it is likely to be served to us through\u000athe cloud, over HTTP in the forseeable future.</p>\u000a\u000a<p>What is indisputably being downplayed in the vibrant and variant near\u000afuture of computing, is the web's presentation layer - HTML, JavaScript\u000aand CSS. These comprise the lingua franca of the web, and have no real\u000acompetition. However, this browser-served presentation layer of the web\u000awill become less relevant as fewer things are done through the browser,\u000aespecially on mobile platforms.</p>\u000a\u000a<p>To me, the critical thing is that content be addressable by URL, and\u000across-linkable in some reasonable way. This is conventionally achieved\u000awith HTML's <code>&lt;a&gt;</code> elements, but can also be done with JavaScript (eg. a\u000abutton that runs <code>javascript:window.location.href = myUrl;</code>, or a\u000a<code>&lt;form&gt;</code> that creates a POST request to some other resource. This can\u000aeven be done without HTML at all. As long as we continue using HTTP, we\u000aare guaranteed to have content that is available at a given URL. And as\u000along as we can guarantee that there's some handler for that content, the\u000aspirit of the web lives on.</p>\u000a\u000a<h2>Specialized browsers are a good solution</h2>\u000a\u000a<p>RSS readers are good examples of specialized browsers focused on\u000apresenting a good reading experience to the user. They are a single\u000aentry point for all of the interesting things on the internet for the\u000auser to read.</p>\u000a\u000a<p>The Twitter app on your mobile device is an example of a specialized\u000abrowser designed for reading and sending tweets. This specialized\u000abrowser relies on the web's ability to link to various kinds of content\u000aaddressable by URLs. Tweets typically include an article or an image\u000awhich are served up using HTTP, and sometimes require HTML to render.\u000aTwitter is all about hyperlinked content, without ever using an <code>&lt;a&gt;</code>\u000atag.</p>\u000a\u000a<p>Apple also has several projects that are specialized browsers in spirit,\u000athough they rarely link out to the wild west of the world wide\u000aweb. Generally, these specialized browsers focus on giving a great\u000aexperience for the user aiming to do something specific. Similar in\u000afunction to an RSS reader, Newsstand is an entry point to the magazines\u000aand newspapers you read. Passbook is a specialized browser for tracking\u000aevent tickets, boarding passes and coupons. From a developer\u000aperspective, both of these browsers require you to setup a server and\u000awrite some iOS code. (Unfortunately you're then stuck in Apple's\u000aecosystem forever.)</p>\u000a\u000a<p>By identifying common patterns of functionality, Apple is able to\u000asuccessfully introduce a specialized browser notion that spans across\u000amultiple services, unifying it with a consistent user experience. This\u000abegins to address the concern that there are <a href="http://designmind.frogdesign.com/blog/mobile-apps-must-die.html">too many apps for\u000aeverything</a>. Of course you don't want separate apps for\u000aNYTimes, WSJ, USA Today, LA Times, etc. And of course you don't want\u000aseparate apps for each event booking service, airline and coupon\u000acompany.</p>\u000a\u000a<p>The problem is endemic to the web community. The standards process\u000abehemoth is slow, heavy and change-averse. Its focus is on a monolothic\u000aweb, <strong>THE WEB</strong>. Imagine if we focused on use cases in the same way\u000athat Apple does, and created specialized sub-webs for various related\u000athings. The next big thing is wearable health. Imagine a sub-web for\u000athat, where we get to define the way all of these devices can talk to\u000aone another. There would be a browser for that sub-web, providing a good\u000aexperience to see historical data, analyze trends, and plot data over\u000atime. This is not the stuff of a web browser, but a completely different\u000abeast.</p>\u000a\u000a<p>Android's intent filters are a step in the direction of a specialized\u000abrowser. Intent filters let apps register to handle specific URL\u000apatterns. If the URL pattern is opened by the user, she is presented\u000awith a dialog of all of the possible handlers for that resource, which\u000amay include Chrome, other web browsers, and specialized browsers that\u000asubscribe to that URL. Once you're in an Android app, however, you're\u000astuck. Unless the app provides the ability to share content, there's no\u000away to send your state to someone else. In contrast, with a web browser,\u000ayou just take the URL and send it to your friend and if they have access\u000ato that content, they get to see it.</p>\u000a\u000a<h2>The web, the good parts</h2>\u000a\u000a<p>When tech pundits hail the ebb of the web, they mean that mobile native\u000aapps are eating away at the presentation layer on the mobile web and\u000abeyond. After some contemplation, I have made peace with this\u000apossible future. HTML is not the best possible way of creating content,\u000anor is CSS a reasonable way of laying out content. JavaScript may be\u000acommonly used, but that does not make it a very good language. The\u000apresentation layer is optimized for content consumption using pointers\u000aand keyboard input, or if you're really adventurous, a smaller touch\u000ascreen.</p>\u000a\u000a<p>Apps are another story. Frameworks like iOS and Android provide a much\u000amore modern, cogent way of developing apps for mobile platforms. But\u000aunfortunately they aren't linkable or portable across platforms. As I\u000amentioned earlier, there's no need for <code>&lt;a&gt;</code> elements, or anything from\u000athe presentation layer to preserve benefits of linkability. But what\u000adoes the web look like once we've gutted the presentation layer?  Here\u000aare some characteristics that we should preserve:</p>\u000a\u000a<ul>\u000a<li>The ability to take any content that is currently being shown and\u000aserialize it into a URL.</li>\u000a<li>The ability to open a URL with the right specialized browser of the\u000auser's choosing.</li>\u000a<li>If no specialized browser is installed, some way of presenting the\u000auser with a list of good browsers.</li>\u000a</ul>\u000a\u000a<p>By dropping the notion of <strong>THE WEB</strong> (singular), and ushering an era of\u000aspecialized browsers, we can split our universe of devices into subsets\u000aand increase the baseline greatest common denominator. Trying to extend\u000athe web to work for every possible case will lead to even more feature\u000acreep in a web platform that is already keeling over.</p>\u000a\u000a<p>The web community should take a look at verticals and spec and build\u000aspecialized browsers for them. How would a web for boarding passes,\u000aconcert tickets and coupons look like? How about a web for personal\u000ahealth tracking data? A web for content that should be consumed on an\u000aaudio-only device?</p>\u000a\u000a<p>This has been my slightly edited brain dump on the future of the web.\u000aThanks for reading, I eagerly await your thoughts :)</p>\u000a
p2150
tp2151
Rp2152
sg11
V/ebb-of-the-web
p2153
sg13
Nsg14
I01
sg15
VThe ebb of the web
p2154
sg18
VTech pundits like to lament that the web has [no viable\u000afuture][no-viable], while web idealists hold that in fact the web is\u000atotally fine, with a "too big to fail" [sort of attitude][recline].
p2155
sg4
V<p>Tech pundits like to lament that the web has <a href="http://cdixon.org/2014/04/07/the-decline-of-the-mobile-web/">no viable\u000afuture</a>, while web idealists hold that in fact the web is\u000atotally fine, with a "too big to fail" <a href="http://schepers.cc/the-recline-of-the-mobile-web">sort of attitude</a>.</p>\u000a\u000a<p>At the root of this disagreement are poorly defined terms. The web can\u000amean many different things to different people. Though it started from a\u000apretty abstract notion of a series of interlinked documents, it has now\u000aevolved to refer to a very specific technology stack of hyperlinked HTML\u000adocuments styled with CSS, enhanced with JavaScript, all served on top\u000aof HTTP. In light of an increasing movement away from desktop-style\u000acomputing, we've seen a big shift away from the web in mobile platforms. </p>\u000a\u000a<p>Let's take apart this gob of web technology in light of the increasingly\u000acomplex landscape of computing and try to make sense of what the web is\u000aand where it's going.</p>\u000a\u000a<p><img src="webiness.png" alt="A framework for webiness" /></p>\u000a\u000a
p2156
sg23
g147
sg31
g2153
sg148
(dp2157
g150
S'Apr'
p2158
sg152
S'April 15, 2014'
p2159
sg154
I4
sg155
S'2014-04-15T09:00:00-00:00'
p2160
sg157
I1397577600
sg158
I2014
sg159
I15
ssg61
g160
sg29
S'ebb-of-the-web'
p2161
sS'posted'
p2162
g166
(S'\x07\xde\x04\x0f'
p2163
tp2164
Rp2165
ssg32
S'content/posts/2014/ebb-of-the-web/index.md'
p2166
sg34
F1433826756.0
sa(dp2167
g2
(dp2168
g26
g5
(g6
g7
V<p>When the world wide web was first conceived, it was as a collection of\u000ainterlinked textual documents. Today's web is full of rich media.\u000aYouTube and other video sites alone consume an enormous 53% of all\u000ainternet traffic. Web denizens often have an open audio player in one of\u000atheir tabs. Web-based photo sharing services such as Flickr are the most\u000acommon way of enjoying photos on our computers. The remote control,\u000afoundations of which are attributed to everyone's favorite inventor\u000aNikola Tesla in <a href="https://www.google.com/patents/US613809">patent US613809</a>, has been the preferred way of\u000acontrolling media for over half a century.</p>\u000a\u000a<p>Yet the only way we can control all of this web media is via the\u000aon-screen user interfaces that the websites provide. The web has no\u000aremote control, and this is a big usability problem. Many use the\u000adesktop versions of streaming services like Spotify and Rdio rather than\u000atheir web player, exclusively because of mac media key support. For\u000ascenarios where you're far from the screen, like showing friends a\u000aslideshow of photos on a TV, the lack of remote controllability is a\u000anon-starter.</p>\u000a\u000a<p>This post is a concrete proposal for what a remote controls for the web\u000ashould be like. To get a sense for how it might feel, try a <a href="http://borismus.github.io/media-control-prototype/">rough\u000aprototype</a>.</p>\u000a\u000a<!--more-->\u000a\u000a<p><img src="inputs.png" alt="Ways of controlling media: dedicated keyboard buttons,headphone\u000aremotes, hardware remote controls, second-screen remote controls,\u000acamera-based gestures, voice commands" /></p>\u000a\u000a<h2>Related attempts to solve this problem</h2>\u000a\u000a<p>Many platforms, especially Android, Mac and iOS, do a pretty good job of\u000asupporting some of the inputs from the above image. The web, one of the\u000amost common platforms for consuming media, supports none of them. The\u000aonly exception, of course, is the mouse and keyboard, but only when the\u000aplayer tab is in the foreground.</p>\u000a\u000a<p>On the web, there have been a number of proposals and half-solutions to\u000aaddress this problem. Back in 2011, I shared <a href="http://smus.com/chrome-media-keys-revisited/">KeySocket</a>, a\u000aMenu Bar app for OS X that handles media keys on the mac keyboard and\u000asends them to a companion Chrome extension that injects content scripts\u000ainto web-based media players. A similar project, <a href="http://sway.fm/">Sway.fm</a> built\u000asupport for media keys as an NPAPI plugin (a now <a href="http://blog.chromium.org/2013/09/saying-goodbye-to-our-old-friend-npapi.html">deprecated</a>\u000atechnology). The <a href="https://flutterapp.com/">Flutter app</a> takes a similar approach (native\u000aapp and companion extension), but enables webcam-based gestures for\u000acontrolling media.</p>\u000a\u000a<p>Recently, I contributed the Mac implementation to the new <a href="https://codereview.chromium.org/60353008/">global\u000akeyboard shortcuts API</a> for Chrome Apps and\u000aExtensions. This API lets developers bind to any global shortcut,\u000aincluding media keys. This is a good start since it brings the media key\u000ahandling infrastructure into Chrome, but does not address the problem\u000afor the web in general.</p>\u000a\u000a<h2>Starting with a good user experience</h2>\u000a\u000a<p>Since we have a blank slate when it comes to controlling media on the\u000aweb, how should media controls behave? Let's start with some\u000asub-optimal behaviors. Here's one: all media events to get routed to all\u000aopen tabs capable of handling them. Imagine the case with many YouTube\u000atabs open, and the ensuing cacophony! Another bad approach is to route\u000acommands only to the foreground tab, since a very common case for\u000aneeding media controls occurs when music is playing in the background.</p>\u000a\u000a<p><a href="http://borismus.github.io/media-control-prototype/">This rough prototype</a> illustrates what I think is a pretty\u000agood experience. It follows a focus-based model inspired by mobile\u000aoperating systems like iOS and Android. However, the web is messier than\u000athe app model and edge cases like multiple sources of media playing\u000asimultaneously (eg. music player and YouTube video) are likely to\u000ahappen, so we need to be careful.</p>\u000a\u000a<p>Here is what happens when a user issues a play/pause command. I'll\u000adefine the bold terms in a second.</p>\u000a\u000a<ol>\u000a<li>If any media is <strong>playing</strong> in a <strong>background tab</strong>, it should pause.</li>\u000a<li>Otherwise, if the <strong>foreground tab</strong> supports <strong>media events</strong>, it\u000ashould receive the media control and be pushed to the <strong>media focus\u000astack</strong>.</li>\u000a<li>Otherwise, if the <strong>media focus stack</strong> is non-empty, the event\u000ashould be routed to the tab at the top of the stack.</li>\u000a<li>Otherwise (if the stack is empty), find the first open tab\u000asupporting <strong>media events</strong>, relay the event to that page and push it\u000aon the <strong>media focus stack</strong>.</li>\u000a<li>If there are no open tabs supporting <strong>media events</strong>, do nothing.\u000aOptionally alert the user with a non-modal notification (eg. audible\u000achime).</li>\u000a</ol>\u000a\u000a<p>When a next or previous control is issued, the command should be routed\u000ato the tab with <strong>media focus</strong>. If there are no tabs with <strong>media\u000afocus</strong> and none capable of media control, we drop the event.</p>\u000a\u000a<p>If a tab closes, remove it from the <strong>media focus stack</strong> and ensure\u000athat <strong>media focus</strong> is granted to the tab at the top of the stack.</p>\u000a\u000a<p>To clarify the description above, here are a few terms:</p>\u000a\u000a<ul>\u000a<li><strong>Foreground</strong>: the active tab of the foreground browser window.</li>\u000a<li><strong>Background</strong>: every tab that is not in the foreground.</li>\u000a<li><strong>Media events</strong>: a new event type that a page can listen to,\u000aindicating how to interpret media controls (see the next section).</li>\u000a<li><strong>Playing tab</strong>: a tab that is currently playing audio or video\u000acontent.</li>\u000a<li><strong>Media focused tab</strong>: the tab which is the default receiver of media\u000acontrol events.</li>\u000a<li><strong>Media focus stack</strong>: a stack of tabs where the top-most tab\u000ais the one that currently has media focus. If that tab is\u000apopped off the stack, the next one gets media focus.</li>\u000a</ul>\u000a\u000a<p>The dry description above and <a href="http://borismus.github.io/media-control-prototype/">the prototype</a> should give a\u000asense of what tab should handle basic media controls, regardless of\u000atheir origin: keyboard, remote control hardware, gesture, etc.</p>\u000a\u000a<p>Now, when a command comes in, how does the page know how to interpret\u000ait? That's up to the web developer, and is done through <code>media</code> events,\u000adescribed in the next section.</p>\u000a\u000a<h2>Enabling media controls using media events</h2>\u000a\u000a<p>A fundamental missing piece so far is a way for a web page to indicate\u000athat it can receive media controls, and a way for it to specify how it\u000awants to handle them. The solution to this is to create a new type of\u000aevent, the <code>media</code> event, which are defined on a page-level, bound to\u000athe window object. This suggestion is not new, and first (as far as I\u000acan tell) came up in this <a href="http://paulrouget.com/e/mediaevents/">blog post by Paul Rouget</a>. Here's\u000ahow media events work:</p>\u000a\u000a<pre><code>// Subscribing to media events.\u000awindow.addEventListener('media', function(e) {\u000a  if (e.data == e.MEDIA_PLAY) {\u000a    myPlayer.play();\u000a  } else if (e.data == e.MEDIA_PAUSE) {\u000a    myPlayer.pause();\u000a  } else if (e.data == e.MEDIA_NEXT_TRACK) {\u000a    myPlayer.next();\u000a  } else if (e.data == e.MEDIA_PREVIOUS_TRACK) {\u000a    myPlayer.previous();\u000a  }\u000a});\u000a</code></pre>\u000a\u000a<p>This code tells the browser that this page can accept media controls,\u000aand what this page should do when a particular media control is\u000areceived.</p>\u000a\u000a<h2>Determining user-initiated media playback change</h2>\u000a\u000a<p>Another missing piece in the narrative so far is how to populate the\u000afocus stack. So far, we know that a closed tab should be popped from the\u000astack, and that play/pause sometimes causes a tab to be pushed onto the\u000astack. But this is not enough, since the user can still interact with\u000amedia using the UI of the player. For example, if I start listening to\u000amusic through a Spotify tab, and then switch tabs, media commands should\u000aobviously go to the Spotify tab, despite me never having issued any\u000amedia controls.</p>\u000a\u000a<p>One option is to, as the user navigates between tabs, push any tab with\u000asupporting media events onto the stack. This approach fails for the case\u000awhere you are listening to music in the background, and then change\u000atabs, passing an open YouTube video on the way. In this case, that\u000aYouTube video would become focused and there would be no way to control\u000athe music (until you close the YouTube tab). What we actually need is to\u000abe able to <strong>track when the user interacts with the page using the media\u000aplayer UI</strong>, in order to then push page to the media focus stack.</p>\u000a\u000a<p>A browser can distinguish user-initiated events (like clicks and\u000akeyboard presses) from programatic ones (like a timer firing, or a page\u000aloading). <a href="https://developer.apple.com/library/safari/documentation/AudioVideo/Conceptual/Using_HTML5_Audio_Video/Device-SpecificConsiderations/Device-SpecificConsiderations.html">iOS does this</a> to prevent annoying pages from\u000aautoplaying music (remember the 90s?). Using the same idea, browsers can\u000aalso track when a media player's playback state changes due to user\u000ainput.</p>\u000a\u000a<p>Even so, there may be special cases that aren't perfect. For example,\u000aimagine a music app with media controls and a video ad on the side. If\u000athe user then clicks the video ad, it doesn't mean that the page should\u000anow have media focus. There are other tricky cases such as a page full\u000aof videos. Here, if a user starts playing a particular video, and then\u000awants to stop it using media controls, the expectation is that the same\u000avideo pauses. If the web developer does not handle this case gracefully,\u000aanother video may start playing concurrently.</p>\u000a\u000a<h2>Response at FOMS was positive</h2>\u000a\u000a<p>Pitching this idea at <a href="http://www.foms-workshop.org/foms2013/pmwiki.php/Main/MediaFocus">FOMS 2013</a> a few months ago, folks seemed\u000areceptive. There was an understanding that a lack of media controls on\u000athe web is a genuine problem. Additionally, I got good feedback on the\u000asolution, which helped to iterate and get to this stage. This is all\u000avery encouraging, and I've written this post to keep the discussion\u000aalive and keep the momentum going. To make remote controls for the web a\u000areality, we need is a critical mass of interested browser implementers.</p>\u000a\u000a<p>As always, thanks for reading, and let me know if you have thoughts or\u000asuggestions on this topic, especially if you make browsers for a living\u000aand want to help standardize!</p>\u000a
p2169
tp2170
Rp2171
sg11
V/remote-controls-web-media
p2172
sg13
Nsg14
I01
sg15
VRemote controls for web media
p2173
sg18
VWhen the world wide web was first conceived, it was as a collection of\u000ainterlinked textual documents.
p2174
sg4
V<p>When the world wide web was first conceived, it was as a collection of\u000ainterlinked textual documents. Today's web is full of rich media.\u000aYouTube and other video sites alone consume an enormous 53% of all\u000ainternet traffic. Web denizens often have an open audio player in one of\u000atheir tabs. Web-based photo sharing services such as Flickr are the most\u000acommon way of enjoying photos on our computers. The remote control,\u000afoundations of which are attributed to everyone's favorite inventor\u000aNikola Tesla in <a href="https://www.google.com/patents/US613809">patent US613809</a>, has been the preferred way of\u000acontrolling media for over half a century.</p>\u000a\u000a<p>Yet the only way we can control all of this web media is via the\u000aon-screen user interfaces that the websites provide. The web has no\u000aremote control, and this is a big usability problem. Many use the\u000adesktop versions of streaming services like Spotify and Rdio rather than\u000atheir web player, exclusively because of mac media key support. For\u000ascenarios where you're far from the screen, like showing friends a\u000aslideshow of photos on a TV, the lack of remote controllability is a\u000anon-starter.</p>\u000a\u000a<p>This post is a concrete proposal for what a remote controls for the web\u000ashould be like. To get a sense for how it might feel, try a <a href="http://borismus.github.io/media-control-prototype/">rough\u000aprototype</a>.</p>\u000a\u000a
p2175
sg23
g147
sg31
g2172
sg148
(dp2176
g150
S'Jan'
p2177
sg152
S'January 27, 2014'
p2178
sg154
I1
sg155
S'2014-01-27T09:00:00-00:00'
p2179
sg157
I1390842000
sg158
I2014
sg159
I27
ssg61
g160
sg29
S'remote-controls-web-media'
p2180
sS'posted'
p2181
g166
(S'\x07\xde\x01\x1b'
p2182
tp2183
Rp2184
ssg32
S'content/posts/2014/remote-controls-web-media/index.md'
p2185
sg34
F1433825428.0
sa(dp2186
g2
(dp2187
g26
g5
(g6
g7
V<p>A live-input spectrogram written using <a href="http://polymer-project.org">Polymer</a> using the <a href="http://webaudioapi.com">Web\u000aAudio API</a>.</p>\u000a\u000a<p><img src="screenshot.png" alt="Screenshot of spectrogram" /></p>\u000a\u000a<p>If you're running Chrome or Firefox, <a href="http://borismus.github.io/spectrogram">see it in action</a>. Once the\u000aspectrogram is running, see if you can make a pattern with your speech\u000aor by whistling. You can also click anywhere on the page to turn on the\u000aoscillator. For a mind-blowing effect, <a href="https://www.youtube.com/watch?v=M9xMuPWAZW8&amp;t=5m30s">load this</a> in a parallel\u000atab.</p>\u000a\u000a<!--more-->\u000a\u000a<h2>Why?</h2>\u000a\u000a<p>Having a spectrogram is incredibly handy for a lot of the work I've been\u000adoing recently. So a while ago, I built one that satisfies my needs. It\u000aruns in a full-screen, using the microphone input as the source.</p>\u000a\u000a<p>It also includes an oscillator, which plays a sine wave at the frequency\u000aof your pointer. It also shows you the frequency that it plays back, and\u000aplots a short buffer of pointer positions. This is handy for measuring\u000ainternal latency:</p>\u000a\u000a<p><img src="latency.png" alt="Latency estimation" /></p>\u000a\u000a<p>Having the oscillator built-in is also pretty fun. You can <a href="sounds/morse.wav">send morse\u000acode</a> (short short short, long long, short short long, short\u000ashort short), <a href="sounds/radio.wav">scan for radio stations</a>, make 8-bit character\u000a<a href="sounds/sfx.wav">dying sound effects</a>, simulate <a href="sounds/ghosts.wav">aliens, ghosts and\u000atheremins</a>, and annoy <a href="sounds/dogs.wav">small, annoying dogs</a>.</p>\u000a\u000a<p>I use the tool mostly in Chrome, but it also works in Firefox.\u000aUnfortunately no other browser currently has both <code>getUserMedia</code> and Web\u000aAudio API support.</p>\u000a\u000a<h2>Configuration parameters</h2>\u000a\u000a<p>The following are HTML attributes of the <code>g-spectrogram</code> component. Many\u000aof them are also configurable via the spectrogram controls component,\u000awhich shows up if the <code>controls</code> attribute is set to true.</p>\u000a\u000a<ul>\u000a<li><code>controls</code> (boolean): shows a config UI component.</li>\u000a<li><code>log</code> (boolean): enables y-log scale (linear by default).</li>\u000a<li><code>speed</code> (number): how many pixels to move past for every frame.</li>\u000a<li><code>labels</code> (boolean): enables y-axis labels.</li>\u000a<li><code>ticks</code> (number): how many y labels to show.</li>\u000a<li><code>color</code> (boolean): turns on color mode (grayscale by default).</li>\u000a<li><code>oscillator</code> (boolean): enables an oscillator overlay component. When\u000ayou click anywhere in the spectrogram, a sine wave plays corresponding\u000ato the frequency you click on.</li>\u000a</ul>\u000a\u000a<h2>Using the Polymer component</h2>\u000a\u000a<p>If you are inclined to embed this component somewhere, you can,\u000asince it's implemented in Polymer, which, by the way, is an\u000aawesome framework. Once you've <a href="http://www.polymer-project.org/docs/start/getting-the-code.html">gotten started</a>, here's\u000athe simplest possible version:</p>\u000a\u000a<pre><code>&lt;g-spectrogram/&gt;\u000a</code></pre>\u000a\u000a<p>Enable controls:</p>\u000a\u000a<pre><code>&lt;g-spectrogram controls&gt;&lt;/g-spectrogram&gt;\u000a</code></pre>\u000a\u000a<p>Pass parameters to the component:</p>\u000a\u000a<pre><code>&lt;g-spectrogram log labels ticks="10"&gt;&lt;/g-spectrogram&gt;\u000a</code></pre>\u000a\u000a<h2>Future work ideas</h2>\u000a\u000a<p>It would be great to add a few things to this tool. If you're interested\u000ain helping, submit your changes as a pull request <a href="https://github.com/borismus/spectrogram">on github</a>.\u000aSome ideas for things that can be done:</p>\u000a\u000a<ul>\u000a<li>Improved axis labeling.</li>\u000a<li>Make it work in mobile browsers.</li>\u000a<li>Loading/saving of traces.</li>\u000a<li>Loading audio data from a file.</li>\u000a<li>Zoom support.</li>\u000a<li>Higher precision FFT results (would require writing a custom FFT\u000arather than using the one built into Web Audio API.)</li>\u000a</ul>\u000a
p2188
tp2189
Rp2190
sg11
V/spectrogram-and-oscillator
p2191
sg13
Nsg14
I01
sg15
VSpectrogram and oscillator
p2192
sg18
VA live-input spectrogram written using [Polymer][polymer] using the [Web\u000aAudio API][wapi].
p2193
sg4
V<p>A live-input spectrogram written using <a href="http://polymer-project.org">Polymer</a> using the <a href="http://webaudioapi.com">Web\u000aAudio API</a>.</p>\u000a\u000a<p><img src="screenshot.png" alt="Screenshot of spectrogram" /></p>\u000a\u000a<p>If you're running Chrome or Firefox, <a href="http://borismus.github.io/spectrogram">see it in action</a>. Once the\u000aspectrogram is running, see if you can make a pattern with your speech\u000aor by whistling. You can also click anywhere on the page to turn on the\u000aoscillator. For a mind-blowing effect, <a href="https://www.youtube.com/watch?v=M9xMuPWAZW8&amp;t=5m30s">load this</a> in a parallel\u000atab.</p>\u000a\u000a
p2194
sg23
g147
sg31
g2191
sg148
(dp2195
g150
S'Jun'
p2196
sg152
S'June 9, 2014'
p2197
sg154
I6
sg155
S'2014-06-09T09:00:00-00:00'
p2198
sg157
I1402329600
sg158
I2014
sg159
I9
ssg61
g160
sg29
S'spectrogram-and-oscillator'
p2199
sS'posted'
p2200
g166
(S'\x07\xde\x06\t'
p2201
tp2202
Rp2203
ssg32
S'content/posts/2014/spectrogram-and-oscillator/index.md'
p2204
sg34
F1433825421.0
sa(dp2205
g2
(dp2206
g26
g5
(g6
g7
V<p>This year's <a href="http://www.acm.org/uist/uist2014/">UIST</a> was held in Waikiki, Honolulu, the undisputed\u000atourist capital of Hawaii. I've stuck to my now three year old habit of\u000ataking notes and <a href="http://smus.com/uist-2013">posting my favorite work</a>. Since last year,\u000athe conference has grown an extra track. The split was generally OK for\u000ame, with my track mostly dedicated to user interface innovation\u000a(sensors, etc) and another more concerned with crowdsourcing,\u000avisualization, and more traditional UIs.</p>\u000a\u000a<p>My overall feeling was that the research was mostly interesting from a\u000atech perspective, but focused on solving the wrong problem. For example,\u000aat least 5 papers/posters/demos were focused on typing on smartwatches.\u000aThe keynotes were very thought provoking, especially when juxtaposed\u000awith one another.</p>\u000a\u000a<!--more-->\u000a\u000a<h2>Focused Ultrasonic Arrays</h2>\u000a\u000a<p>I got to play with a holographic display with touch-feedback. Sounds\u000acrazy, and it is. HaptoMime uses an array of ultrasonic transducers to\u000abeam-form focused ultrasound to a specific target. The touch feedback\u000afeels like a little electric shock, but it's incredible that it works.\u000aThe field of view of the holographic screen is a bit limited:</p>\u000a\u000a<iframe width="600" height="339" src="//www.youtube.com/embed/uARGRlpCWg8" frameborder="0" allowfullscreen></iframe>\u000a\u000a<p>As a kid, I loved drawing patterns on my grandma's rug with my finger.\u000aThis research team was clearly inspired by the same activity, and they\u000acreated several ways of automating the process: using a roller device, a\u000apen, and an focused ultrasonic array:</p>\u000a\u000a<iframe width="600" height="339" src="//www.youtube.com/embed/L0hrETGddLQ" frameborder="0" allowfullscreen></iframe>\u000a\u000a<h2>Multi-device interactions</h2>\u000a\u000a<p>GaussStones built on a bunch of other "Gauss"-prefixed previous work\u000afrom the same lab, showing an array of hall sensors used to sense a\u000avariety of shielded magnetic tokens, which can encode an ID using field\u000astrength. You could play physical chess, or even combine magnetic tokens\u000ato create more complex interactions, like a slider or button unit:</p>\u000a\u000a<iframe width="600" height="339" src="//www.youtube.com/embed/qlr-15Oto6s" frameborder="0" allowfullscreen></iframe>\u000a\u000a<p>Another nice example of multi-device interaction came from MIT, where\u000aa group used this extremely clever way of tracking the phone's position\u000arelative to a laptop, using a 2D gradient, where the color of each pixel\u000amaps to a position in space:</p>\u000a\u000a<iframe width="600" height="339" src="//www.youtube.com/embed/hFH6hJLDoLE" frameborder="0" allowfullscreen></iframe>\u000a\u000a<p>The awkwardly named Vibkinesis shows a smartphone case which is\u000aequipped with two vibrator motors which give a phone the ability to\u000atranslate and rotate on a flat surface. In one example,\u000anotifications caused the phone to rotate by 90 degrees, which had the added benefit of\u000anotifying the user of a notification even if the battery runs out of\u000ajuice. This is apparently funny from a Japanese culture perspective,\u000awhere characters often die under strange circumstances, leaving no clue\u000abut a "dying message" on or around their person. Another example\u000ainvolved a fish-eye lens on the front-facing camera to detect the\u000aposition of the user's hand (based on skin color), and then physically\u000anudging the user to get their attention:</p>\u000a\u000a<iframe width="600" height="339" src="//www.youtube.com/embed/UlFwVUHotrU" frameborder="0" allowfullscreen></iframe>\u000a\u000a<h2>Awesome fabrication techniques</h2>\u000a\u000a<p>I'm a huge fan of subtractive techniques (eg. laser cutting) rather than\u000aadditive ones (eg. 3D printing). FlatFitFab is a CAD tool for easily\u000acreating balsa dinosaur-style models, and evaluating their stability and\u000afeasibility. Super cool work:</p>\u000a\u000a<iframe width="600" height="339" src="//www.youtube.com/embed/HeFQw0chSJY" frameborder="0" allowfullscreen></iframe>\u000a\u000a<p>Rather than creating PCBs in something like Eagle, why not just sketch\u000athem with a conductive pen instead? ShrinkyCircuits does just this,\u000afollowing the principles of Shrinky Dinks, which shrinks when heated.\u000aBecause the whole board shrinks, it improves conductivity of the\u000aconductive ink, and the contact points with electronics components.</p>\u000a\u000a<iframe width="600" height="339" src="//www.youtube.com/embed/4p-l374rb8M" frameborder="0" allowfullscreen></iframe>\u000a\u000a<h2>Spatial AR</h2>\u000a\u000a<p>Research from Microsoft showed rooms instrumented with multiple\u000aProjector+Depth Camera rigs, which allowed for some interesting\u000amulti-user interactions:</p>\u000a\u000a<iframe width="600" height="339" src="//www.youtube.com/embed/ILb5ExBzHqw" frameborder="0" allowfullscreen></iframe>\u000a\u000a<p>Of course, the setup above doesn't allow perspective-corrected scenes.\u000aTo remedy this, they had a companion project which split the room in\u000atwo, creating head-tracked scenes for two participants. Pretty cool,\u000athough it does not generalize to more than two people, nor does it\u000asupport stereo:</p>\u000a\u000a<iframe width="600" height="339" src="//www.youtube.com/embed/Df7fZAYVAIE" frameborder="0" allowfullscreen></iframe>\u000a\u000a<p>Still, entering either of these VR rooms feels a lot less dorky than\u000ahaving to don a VR headset.</p>\u000a\u000a<h2>Keynotes</h2>\u000a\u000a<p>UIST was punctuated by three keynotes, from Ken Perlin, Mark\u000aBolas and Bret Victor, all of which were thought provoking and sometimes\u000afrightening, but unfortunately not recorded. When I used to be in\u000aDeveloper Relations, we would be hardpressed to show up at a conference\u000aif the talk was not recorded because so much of the engagement happened\u000aafter the fact online. <strong>Academia needs the same culture</strong>.</p>\u000a\u000a<p>Ken Perlin kicked off the co-located conference, SUI (Symposium for Spatial\u000aInteraction), off with a nice talk about making computer use more like\u000aperforming a music, and less like writing a musical composition. As he\u000agave the talk, he very effectively used a tool he created called Chalk\u000aTalk, which lets you sketch objects with behaviors in short hand - very\u000ameta. Unfortunately I've been unable to find anything published about\u000athe tool, as it would be interesting to play with. Ken envisioned a\u000aworld where you could do something conceptually similar to Chalk Talk\u000awithout a computer in the way. If this "virtual chalk" capability was\u000aavailable to all humans, it would transform the way we communicate. I\u000awasn't completely convinced. When I'm discussing something with\u000acolleagues, we only use a whiteboard for only very specialized things\u000alike drawing a diagram of multiple objects. So there are two things that\u000aneed to happen:</p>\u000a\u000a<ol>\u000a<li>This virtual chalk needs to be <strong>easier to access</strong> than a whiteboard\u000awhile in a meeting room.</li>\u000a<li><strong>Expand the set of concepts</strong> that can be expressed with virtual chalk.\u000aText and speech is <a href="http://graydon.livejournal.com/196162.html">pretty powerful</a>.</li>\u000a</ol>\u000a\u000a<p>Mark Bolas started UIST with a pretty terrifying keynote on virtual\u000areality. His premise was that "we are headed into a virtual future,\u000awhether we want it or not". Terrifyingly, Mark seemed to be okay with\u000athis inevitability, even going as far as discounting augmented reality,\u000asince by the time we've built VR, we'll just want to stay in our\u000ahelmets. The real world isn't that great anyway. One thing I liked was\u000ahis call for creating more <strong>surreal experiences</strong> in VR rather than\u000atrying to emulate the real world. These types of simulations are\u000aconspicuously missing from <a href="https://share.oculusvr.com/category/all">existing VR demos</a>.</p>\u000a\u000a<p>Bret Victor ended the conference with a much needed humanist\u000acounterpoint to Mark' vision. I cannot do the talk justice, and eagerly\u000aawait a recording of it to try to understand all of the nuances. The big\u000aidea of the talk was that "knowledge work" which started with the\u000aprinting press is tyrannical, reducing all of our senses and abilities\u000ato manipulating symbols on a sheet of paper. So many other things that\u000aevolution has designed for us, like hearing, smell, sense of space,\u000atouch, etc, are all thrown out of the window. This problem only gets\u000acompounded as we move to virtualize everything with touch screens. Bret\u000athinks we're poised to design the next great "dynamic" medium after the\u000aprinting press, something that is always interactive and multimodal, and\u000atakes advantage of a wide array of human capabilities.</p>\u000a\u000a<p>According to Mark Bolas, the real world is flawed, and we should build a\u000abetter virtual one. Bret Victor's vision is that humans are perfect,\u000ahaving evolved over thousands of years. Rather than changing what it\u000ameans to be human, we should build a new medium that adapts to our\u000ainherent strengths and weaknesses. Ken Perlin's "virtual chalk" is a\u000agreat example application for this dynamic medium.</p>\u000a\u000a<h2>Tracks I missed</h2>\u000a\u000a<p>Because UIST has become a multi-track conference, I inevitably missed\u000ainteresting parts. In particular, the collaboration track had some\u000a<a href="https://www.youtube.com/watch?v=QtyO-oFlzGg">awesome</a> <a href="https://www.youtube.com/watch?v=jMH_qQF0vKg">work</a>, and there was one <a href="https://www.youtube.com/watch?v=YMfzAstvij0">music-related paper</a>\u000apaper. It was great to have had a good excuse to go this year, showing\u000aCardboard to the academic community. Looking forward to next year,\u000aalthough it is to be held in a somewhat <a href="http://uist.acm.org/about">less glamorous location</a>.</p>\u000a\u000a<p><img src="sunset.jpg" alt="Hawaii sunset." /></p>\u000a
p2207
tp2208
Rp2209
sg11
V/uist-2014
p2210
sg13
Nsg14
I01
sg15
VUIST 2014 highlights
p2211
sg18
VThis year's [UIST][uist] was held in Waikiki, Honolulu, the undisputed\u000atourist capital of Hawaii.
p2212
sg4
V<p>This year's <a href="http://www.acm.org/uist/uist2014/">UIST</a> was held in Waikiki, Honolulu, the undisputed\u000atourist capital of Hawaii. I've stuck to my now three year old habit of\u000ataking notes and <a href="http://smus.com/uist-2013">posting my favorite work</a>. Since last year,\u000athe conference has grown an extra track. The split was generally OK for\u000ame, with my track mostly dedicated to user interface innovation\u000a(sensors, etc) and another more concerned with crowdsourcing,\u000avisualization, and more traditional UIs.</p>\u000a\u000a<p>My overall feeling was that the research was mostly interesting from a\u000atech perspective, but focused on solving the wrong problem. For example,\u000aat least 5 papers/posters/demos were focused on typing on smartwatches.\u000aThe keynotes were very thought provoking, especially when juxtaposed\u000awith one another.</p>\u000a\u000a
p2213
sg23
g147
sg31
g2210
sg148
(dp2214
g150
S'Oct'
p2215
sg152
S'October 14, 2014'
p2216
sg154
I10
sg155
S'2014-10-14T09:00:00-00:00'
p2217
sg157
I1413302400
sg158
I2014
sg159
I14
ssg61
g160
sg29
S'uist-2014'
p2218
sS'posted'
p2219
g166
(S'\x07\xde\n\x0e'
p2220
tp2221
Rp2222
ssg32
S'content/posts/2014/uist-2014/index.md'
p2223
sg34
F1433825417.0
sa(dp2224
g2
(dp2225
g26
g5
(g6
g7
V<p>Sensors found in smartphones define the mobile experience. GPS and the\u000amagnetometer enable the fluid experience of maps; motion sensing enables\u000aactivity recognition and games, and of course the camera and microphone\u000aallow whole categories of rich media applications. Beyond these now\u000aobvious examples, sensors can also enable clever inventions, such as\u000a<a href="http://www.cycloramic.com/">Cycloramic</a>, which used the vibrator motor in iPhones (4 and 5) to\u000arotate the phone and take a panorama, <a href="https://play.google.com/store/apps/details?id=com.runtastic.android.pushup.pro">pushup counters</a> which\u000ause the proximity sensor to count repetitions, and <a href="https://play.google.com/store/apps/details?id=com.carrotpop.www.smth">Send Me To\u000aHeaven</a>, which uses the accelerometer to determine flight time of\u000aa phone thrown vertically as high as possible. I've had some experience\u000ausing and abusing sensors too, most recently for the <a href="http://smus.com/talk/2014/io14/">Cardboard magnet\u000abutton</a>.</p>\u000a\u000a<iframe width="640" height="360" src="https://www.youtube.com/embed/l5775qwORk4" frameborder="0" allowfullscreen></iframe>\u000a\u000a<p>However, over the last couple of years, I've had to step away from the\u000aweb as a development platform, in part because of the poor state of\u000asensor APIs.  In this post, I will describe some of the problems, take a\u000alook at sensor APIs on iOS and Android, and suggest a solution in the\u000aspirit of the <a href="https://extensiblewebmanifesto.org/">extensible web manifesto</a>.</p>\u000a\u000a<!--more-->\u000a\u000a<h2>Existing sensor APIs are underspecified</h2>\u000a\u000a<p>One of the most popular sensor APIs on the web is the <a href="http://w3c.github.io/deviceorientation/spec-source-orientation.html#devicemotion">DeviceMotion event\u000aAPI</a>, which is basically always just an opaque abstraction around the\u000aaccelerometer. The web, as always, tries to solve the problem in the\u000amost general way possible:</p>\u000a\u000a<blockquote>\u000a  <p>This specification provides several new DOM events for obtaining\u000a  information about the physical orientation and movement of the hosting\u000a  device. The information provided by the events is not raw sensor data,\u000a  but rather high-level data which is agnostic to the underlying source\u000a  of information. Common sources of information include gyroscopes,\u000a  compasses and accelerometers.</p>\u000a</blockquote>\u000a\u000a<p>This could be fine in theory, except the specs end up being so vague in\u000atheir attempt to please everybody, that they under-specify the behavior\u000aof events such as <code>DeviceOrientation</code>. Throw in some rogue implementers,\u000aand you end up with huge discrepancies in browsers, as <a href="http://www.html5rocks.com/en/tutorials/device/orientation/">Pete found back\u000ain 2011</a>:</p>\u000a\u000a<blockquote>\u000a  <p>For most browsers, alpha returns the compass heading, so when the\u000a  device is pointed north, alpha is zero. With Mobile Safari, alpha is\u000a  based on the direction the device was pointing when device orientation\u000a  was first requested. The compass heading is available in the\u000a  webkitCompassHeading parameter.</p>\u000a</blockquote>\u000a\u000a<p>A useful sensor abstraction would be to build a compass on top of the\u000amagnetometer (and maybe gyro) sensors, and then expose that as a high\u000alevel Compass API. Unfortunately many web sensor APIs give us a\u000amid-level of abstraction. They don't map reliably to particular hardware\u000asensors, nor do they provide much use. Sensors allow many applications\u000athat were not originally envisioned by the spec writers. By choosing\u000apoorly specified ivory-tower abstractions, the web limits what can be\u000adone on the platform.</p>\u000a\u000a<h2>Low level sensor APIs don't exist</h2>\u000a\u000a<p>While you can work around the insanity of <code>Device*</code> style events on the\u000aweb with platform-specific shims, you cannot work around missing sensor\u000aAPIs. Magnetometers, pressure sensors, proximity, light, temperature,\u000abattery, etc. These are mostly missing, and the ones that are specified\u000aare specified in a very narrow way that does not generalize across to\u000aother types of sensors (eg. <a href="http://www.w3.org/TR/2013/CR-ambient-light-20131001/">DeviceLightEvent</a>).</p>\u000a\u000a<p>Unfortunately it seems that previous attempts to push for a general low\u000alevel sensor API <a href="http://lists.w3.org/Archives/Public/public-geolocation/2011Oct/0000.html">haven't really gotten much traction</a>. In\u000afact, it's a bit unclear whether or not the <a href="http://www.w3.org/2009/dap/">Device API working\u000agroup</a>, is even the right place for sensor APIs, since their\u000amandate is supposedly more about services than sensors:</p>\u000a\u000a<blockquote>\u000a  <p>[To] enable the development of Web Applications and Web Widgets that\u000a  interact with devices services such as Calendar, Contacts, Camera,\u000a  etc.</p>\u000a</blockquote>\u000a\u000a<p>Except <a href="https://dvcs.w3.org/hg/dap/raw-file/default/sensor-api/Overview.html">here's a sensor API</a> from the same group, which\u000aseems to be abandoned... I don't even</p>\u000a\u000a<p>There are more recent voices (circa 2014) that seem to be pushing in a\u000ageneric sensor API direction, from folks like <a href="https://github.com/rwaldron/sensors">Rick Waldron</a>\u000aand <a href="http://lists.w3.org/Archives/Public/public-device-apis/2014Sep/0024.html">Tim Volodine</a>. Many of these ideas are still working within\u000athe confines of a sensor API for each type of sensor. This does not\u000ascale well for the web, which tends to take a long time for any new web\u000astandard, but this renewed interest is very exciting and promising!</p>\u000a\u000a<h2>Sensors on other platforms</h2>\u000a\u000a<p>The web is woefully behind native platforms in almost every regard (with\u000apossibly the exception of audio). Sensors on iOS and Android have a rich\u000ahistory, and ended up in a pretty similar place as the two platforms\u000ahave scrambled to converge. Let's take a look.</p>\u000a\u000a<p>iOS started off with a <a href="https://developer.apple.com/LIBRARY/ios/documentation/UIKit/Reference/UIAccelerometer_Class/index.html">UIAccelerometer API</a>, which was\u000areplaced by <a href="https://developer.apple.com/LIBRARY/ios/documentation/CoreMotion/Reference/CoreMotion_Reference/index.html">CoreMotion</a> in iOS 5. Rather than providing a\u000aseries of specific APIs for each type of sensor API as it had before,\u000aCoreMotion provides a unified framework for sensor events. Each data\u000atype inherits from a common base class <code>CMLogItem</code>, and most of the API is\u000aencapsulated in <code>CMMotionManager</code>, which explicitly lists accelerometer,\u000agyroscope and magnetometer-related APIs. iOS went from specific to\u000ageneric, which makes it super easy to add new types of sensor data. That\u000asaid, the API is generic only for motion sensors, which excludes a bunch\u000aof sensors not directly related to motion like temperature, humidity,\u000aetc.</p>\u000a\u000a<p>Android started off right, and hasn't had to change much, providing a\u000a<a href="http://developer.android.com/reference/android/hardware/SensorEvent.html">generic API for sensors</a> since API level 3. Android's API\u000ais accessed through a SensorManager, which provides a somewhat overly\u000aabstract API, because of its support for multiple sensors of one type\u000a(eg. two accelerometers) in the same device. Still, the idea is good,\u000aand all of the low level sensor data are well specified (per sensor\u000atype)so the hardware/firmware vendor knows what data format their sensor\u000ashould stream. Of course there are still rogue implementations that\u000adon't follow the spec, but that is a perennial problem for any open-ish\u000aecosystem.</p>\u000a\u000a<p>Android also has a <a href="http://developer.android.com/guide/topics/sensors/sensors_overview.html">distinction</a> between software-based sensors and\u000ahardware-based ones. The idea is that the same framework can provide\u000aboth the low level data coming directly from the hardware, as well as\u000auseful higher level data obtained through <a href="http://en.wikipedia.org/wiki/Sensor_fusion">sensor fusion</a>. As of\u000aAPI level 19, Android also provides <a href="http://developer.android.com/reference/android/hardware/SensorManager.html#flush(android.hardware.SensorEventListener)">batch mode</a> for sensor data, which\u000ais very useful for conserving battery and CPU for applications where\u000asome delay is acceptable.</p>\u000a\u000a<p>One nice advantage of an iOS style API is that each sensor type has its\u000aown structure (rather than just an amorphous array of floats, as in\u000aAndroid), which is quite a bit easier to parse. The downside is that\u000aadding new sensor types introduces more overhead, since each one\u000arequires a new structure to be defined and agreed upon. Since we are\u000atalking about web standards, which evolve at a glacial pace, we should\u000aerr on a simple API that works well without spec modifications.</p>\u000a\u000a<h2>Great artists steal</h2>\u000a\u000a<p>There is no need for the web to reinvent the wheel. The wheel has\u000aalready been invented by iOS and Android. All we need to do is take the\u000agood parts from these successful sensor platforms, and integrate them\u000ainto the web in a way that makes sense. The web is not the place for\u000ainnovation, but for standardization.</p>\u000a\u000a<p>Conceptually, a sensor provides a stream of data. The developer should\u000abe able to configure the rate at which new data comes in, as well as\u000abatching the data in windows of sensor data (as is customarily done with\u000aaudio data, for example). In Android, because of a plurality of devices,\u000ait's important to be able to check if a particular sensor is available.\u000aThe same concept maps well to the web.</p>\u000a\u000a<h2>Toward A Web Sensor API</h2>\u000a\u000a<p>In general, here are the requirements for a Web Sensor API that works:</p>\u000a\u000a<ul>\u000a<li>A specification defining the format of the data, similar to\u000a<a href="http://developer.android.com/reference/android/hardware/SensorEvent.html">Android</a>.</li>\u000a<li>A way to feature detect for the existence of a particular sensor.</li>\u000a<li>A way to request (and revoke) a stream of sensor data.</li>\u000a<li>A way to specify how often to poll the sensor.</li>\u000a<li>Bonus: A way to request sensor data in batch form.</li>\u000a</ul>\u000a\u000a<p>While bringing an API like this to the web is a huge undertaking, there\u000ais a silver lining. The sensors we're talking about are all considered\u000a(at least for now) low-security, in the sense that on native platforms,\u000athere is no extra permission required to access them. This makes it\u000apossible to simply propose an API, convince everybody of it's worth, and\u000athen have it implemented across the web!</p>\u000a\u000a<p>I don't have a strong opinion about how the API itself looks like as\u000along as it fulfils the above requirements. Here's a simple strawman\u000awhich should satisfy them:</p>\u000a\u000a<pre><code>// Check for magnetometer support.\u000aif (sensors.Magnetometer === undefined) {\u000a  console.error('No magnetometer found');\u000a}\u000a\u000a// Start listening for changes to the sensor.\u000avar magnetometer = sensors.Magnetometer;\u000amagnetometer.addEventListener('changed', onMagnetometer, {\u000a  sample_rate: sensors.POLL_FAST, // In hertz, eg. POLL_FAST == 100\u000a  batch: 1 // Number of data points to provide in a single poll.\u000a});\u000a\u000a// Handle sensor events.\u000afunction onMagnetometer(event) {\u000a  var data = event.data[0];\u000a  // Get the timestamp (in millis).\u000a  var t = data.timestamp;\u000a  // Get the data (in this case T, as per spec).\u000a  var x = data.values[0];\u000a  var y = data.values[1];\u000a  var z = data.values[2];\u000a  // Process the data.\u000a  superAdvancedSensorFusionThing.addData(t, x, y, z);\u000a}\u000a\u000a// Stop listening.\u000amagnetometer.removeEventListener('changed', onMagnetometer);\u000a</code></pre>\u000a\u000a<h2>Conclusion</h2>\u000a\u000a<p>If you aren't yet convinced that we need access to low level sensors on\u000athe web, recall web developers scoffing at device pixel ratio (DPR),\u000areally questioning the need for to ever go above 2x. Now that <a href="https://developers.google.com/cardboard/">some\u000ascreens</a> are ending up 5cm from your face, the current\u000ageneration of 4x displays isn't enough. The same exact thing applies to\u000asensors. The need is there, but it is not seen as enough of a priority\u000aby the web community.</p>\u000a\u000a<p>By enabling low level sensor access, we can allow new experiences never\u000abefore possible on the web. <a href="https://play.google.com/store/apps/details?id=com.runtastic.android.pushup.pro">Pushup rep counters</a>, the <a href="http://smus.com/talk/2014/io14/">magnet\u000abutton</a> in Cardboard, and myriads more applications\u000ayet to be concieved could all be built on the web platform, eliminating\u000aa big reason why the web is increasingly losing its relevance on mobile\u000adevices. Providing low level sensor access is critical and aligns\u000aperfectly with the <a href="https://extensiblewebmanifesto.org/">extensible web vision</a>.</p>\u000a\u000a<p><em>Update (Nov 14, 2014): There was a <a href="http://lists.w3.org/Archives/Public/public-device-apis/2014Nov/0018.html">W3C call</a> about this very\u000atopic yesterday! Kicking off efforts in <a href="https://github.com/w3c/sensors">this github repo</a>.\u000aJoin us!</em></p>\u000a
p2226
tp2227
Rp2228
sg11
V/web-sensor-api
p2229
sg13
Nsg14
I01
sg15
VWeb Sensor API: raw and uncut
p2230
sg18
VSensors found in smartphones define the mobile experience.
p2231
sg4
V<p>Sensors found in smartphones define the mobile experience. GPS and the\u000amagnetometer enable the fluid experience of maps; motion sensing enables\u000aactivity recognition and games, and of course the camera and microphone\u000aallow whole categories of rich media applications. Beyond these now\u000aobvious examples, sensors can also enable clever inventions, such as\u000a<a href="http://www.cycloramic.com/">Cycloramic</a>, which used the vibrator motor in iPhones (4 and 5) to\u000arotate the phone and take a panorama, <a href="https://play.google.com/store/apps/details?id=com.runtastic.android.pushup.pro">pushup counters</a> which\u000ause the proximity sensor to count repetitions, and <a href="https://play.google.com/store/apps/details?id=com.carrotpop.www.smth">Send Me To\u000aHeaven</a>, which uses the accelerometer to determine flight time of\u000aa phone thrown vertically as high as possible. I've had some experience\u000ausing and abusing sensors too, most recently for the <a href="http://smus.com/talk/2014/io14/">Cardboard magnet\u000abutton</a>.</p>\u000a\u000a<iframe width="640" height="360" src="https://www.youtube.com/embed/l5775qwORk4" frameborder="0" allowfullscreen></iframe>\u000a\u000a<p>However, over the last couple of years, I've had to step away from the\u000aweb as a development platform, in part because of the poor state of\u000asensor APIs.  In this post, I will describe some of the problems, take a\u000alook at sensor APIs on iOS and Android, and suggest a solution in the\u000aspirit of the <a href="https://extensiblewebmanifesto.org/">extensible web manifesto</a>.</p>\u000a\u000a
p2232
sg23
g147
sg31
g2229
sg148
(dp2233
g150
S'Nov'
p2234
sg152
S'November 13, 2014'
p2235
sg154
I11
sg155
S'2014-11-13T09:00:00-00:00'
p2236
sg157
I1415898000
sg158
I2014
sg159
I13
ssg61
g160
sg29
S'web-sensor-api'
p2237
sS'posted'
p2238
g166
(S'\x07\xde\x0b\r'
p2239
tp2240
Rp2241
ssg32
S'content/posts/2014/web-sensor-api/index.md'
p2242
sg34
F1433868666.0
sa(dp2243
g2
(dp2244
g26
g5
(g6
g7
V<p>It's been over three years since the design of this site has been\u000aupdated. Time to change that!</p>\u000a\u000a<p><img src="0days.jpg" class="floatright"/></p>\u000a\u000a<p>This is the fifth revision of this site's design. Looking over <a href="/redesign-2012">previous\u000adesigns</a>, I've been happier with minimal designs,\u000aespecially <a href="/redesign-2012/v2.png">this one</a> from 2012. I was inspired\u000aby many excellent designs such as <a href="http://practicaltypography.com/typography-in-ten-minutes.html">Butterick's Practical\u000aTypography</a>,\u000a<a href="http://www.teehanlax.com/">Teehan+Lax</a>, <a href="http://www.erikjohanssonphoto.com/">Erik\u000aJohansson</a>,\u000a<a href="https://medium.com/designing-medium/death-to-typewriters-9b7712847639">Medium</a>\u000aand <a href="http://frankchimero.com/writing/other-halves">Frank Chimero</a>.</p>\u000a\u000a<p>The new design is visually cleaner. I <a href="http://caniuse.com/#feat=flexbox">use\u000aflexbox</a> in many places, which makes\u000athe CSS far more intuitive. The responsive parts are very simple,\u000aconsisting of just ten CSS declarations.</p>\u000a\u000a<!--more-->\u000a\u000a<p>Rather than subjecting readers to my face on every page, I have a simple\u000astipple background on the <a href="/about">about page</a>, which I created using the\u000acomplex but functional <a href="http://www.evilmadscientist.com/2012/stipplegen2/">StippleGen</a>.</p>\u000a\u000a<p>Also, I've started working on a self-hosted visual link blog that you\u000acan check out in under <a href="/inspiration">inspiring clippings</a>. I've\u000aimplemented a companion Chrome extension that makes it super easy to\u000aclip inspiring content from anywhere on the web and bring it to that\u000apage.</p>\u000a\u000a<p><strong>Typography:</strong> I'm continuing to use Google fonts, which seems to be so\u000amuch simpler to use than various competitors. I have not completely\u000aoptimized my selection of fonts, but this is satisfactory given my\u000abelief that no design is ever finished. Furthermore, as Scott rightly\u000anotes, <a href="">performance is UX</a> too, and aesthetic decisions need to be\u000acounterbalanced by mundane considerations like page load time.\u000aUnfortunately <a href="https://www.google.com/fonts/specimen/Dosis">Dosis</a> didn't make the cut.</p>\u000a\u000a<p><strong>Infrastructure:</strong> This site is still built using the <a href="https://github.com/borismus/lightning">lightning static\u000ablog</a> engine, which I'm continuing to improve. On that front,\u000aI've dropped the ambitious goal of being able to edit content from any\u000adevice using dropbox, since in practice I always author on my laptop.\u000aInstead, the focus has been on optimizing the edit flow for the local\u000aoffline case, and I have built <a href="https://github.com/lepture/python-livereload">livereload</a> into the local preview\u000aserver. As far as hosting, I have conceded to GitHub Pages, and have\u000amigrated away from using S3 directly.</p>\u000a\u000a<p><strong>Thanks:</strong> to <a href="https://twitter.com/mikemartin604">Mike</a>,\u000a<a href="https://twitter.com/paul_irish">Paul</a>,\u000a<a href="https://twitter.com/smattyang">Seungho</a>,\u000a<a href="https://twitter.com/scottjenson">Scott</a>,\u000a<a href="https://twitter.com/mahemoff">Michael</a>, and other awesome friends that\u000agave me excellent design suggestions and found bugs!</p>\u000a\u000a<p>While I appreciate companies like <a href="http://medium.com">Medium</a> and\u000a<a href="http://svbtle.com/">Svbtle</a> advancing the aesthetics of the web, I\u000acompletely <a href="http://practicaltypography.com/billionaires-typewriter.html">agree with Matthew Butterick</a>'s view, and will\u000acontinue self-hosting my writings for as long as possible. Long live the\u000aplurality of the web!</p>\u000a
p2245
tp2246
Rp2247
sg11
V/design-v5
p2248
sg13
Nsg14
I01
sg15
VSite redesign, version five
p2249
sg18
VIt's been over three years since the design of this site has been\u000aupdated.
p2250
sg4
V<p>It's been over three years since the design of this site has been\u000aupdated. Time to change that!</p>\u000a\u000a<p><img src="0days.jpg" class="floatright"/></p>\u000a\u000a<p>This is the fifth revision of this site's design. Looking over <a href="/redesign-2012">previous\u000adesigns</a>, I've been happier with minimal designs,\u000aespecially <a href="/redesign-2012/v2.png">this one</a> from 2012. I was inspired\u000aby many excellent designs such as <a href="http://practicaltypography.com/typography-in-ten-minutes.html">Butterick's Practical\u000aTypography</a>,\u000a<a href="http://www.teehanlax.com/">Teehan+Lax</a>, <a href="http://www.erikjohanssonphoto.com/">Erik\u000aJohansson</a>,\u000a<a href="https://medium.com/designing-medium/death-to-typewriters-9b7712847639">Medium</a>\u000aand <a href="http://frankchimero.com/writing/other-halves">Frank Chimero</a>.</p>\u000a\u000a<p>The new design is visually cleaner. I <a href="http://caniuse.com/#feat=flexbox">use\u000aflexbox</a> in many places, which makes\u000athe CSS far more intuitive. The responsive parts are very simple,\u000aconsisting of just ten CSS declarations.</p>\u000a\u000a
p2251
sg23
g147
sg31
g2248
sg148
(dp2252
g150
S'Jun'
p2253
sg152
S'June 10, 2015'
p2254
sg154
I6
sg155
S'2015-06-10T09:00:00-00:00'
p2255
sg157
I1433952000
sg158
I2015
sg159
I10
ssg61
g160
sg29
S'design-v5'
p2256
sS'posted'
p2257
g166
(S'\x07\xdf\x06\n'
p2258
tp2259
Rp2260
ssg32
S'content/posts/2015/design-v5/index.md'
p2261
sg34
F1433977909.0
sa(dp2262
g2
(dp2263
g26
g5
(g6
g7
V<p>VR on the web threatens to cleave the web platform in twain, like mobile\u000adid before it. The solution then and the solution now is <a href="http://en.wikipedia.org/wiki/Responsive_web_design">Responsive Web\u000aDesign</a>, which websites to scale well for all form factors.\u000aSimilarly, for VR to succeed on the web, we need to figure out how to\u000amake VR experiences that work both in any VR headset, and also without a\u000aVR headset at all.</p>\u000a\u000a<p><img src="hmds.png" alt="Various head mounted displays." /></p>\u000a\u000a<p><a href="https://github.com/borismus/webvr-boilerplate">WebVR boilerplate</a> is a new starting point for building\u000aresponsive web VR experiences that work on popular VR headsets and\u000adegrace gracefully on other platforms. Check out a couple of demos, <a href="http://borismus.github.io/webvr-boilerplate/">a\u000asimple one</a> and one <a href="http://borismus.github.io/sechelt">ported from MozVR</a>.</p>\u000a\u000a<!--more-->\u000a\u000a<h2>Preview the VR experience for everyone</h2>\u000a\u000a<p>Say you visit a webpage, and it opens up in split-screen mode barrel\u000adistortion, chromatic aberration correction, personalized interpupillary\u000adistance, and provides 6 DOF tracking. <a href="https://www.youtube.com/watch?v=PkFyGNjaQ8k">Whoa</a>. You reach for your\u000aVR headset only to find that you forgot it at work! How disappointing!\u000aThe vast majority of normal people with no head mounted display lying\u000aaround will surely be even more disappointed.</p>\u000a\u000a<p>Responsive web design promises content which automatically adapts to\u000ayour viewing environment by using fluid layouts, flexible images,\u000aproportional grids; a cocktail of modern web technologies. Similarly,\u000aWebVR experiences need to work even without VR hardware. This has two\u000aobvious advantages:</p>\u000a\u000a<ol>\u000a<li>The vast majority of people that don't have VR hardware can still get\u000aa feeling for the experience.</li>\u000a<li>Even if you have VR gear, donning it is a pain. This preview lets you\u000aquickly evaluate whether or not wearing is worth the hassle.</li>\u000a</ol>\u000a\u000a<p>What are some reasonable fallbacks to the in-helmet VR experience? The\u000amain question boils down to emulating head tracking without wearing\u000aanything on your head. On mobile phones, the obvious answer is to use\u000athe gyroscope, for a <a href="http://googlespotlightstories.com/">Spotlight Stories</a>-type experience. On\u000adesktop, we use the mouse to free-look, and also support turning using\u000athe arrow keys. This covers enough of the 3DOF orientation that all HMDs\u000aprovide. Clearly missing are the three translational degrees of freedom,\u000abut these are provided only by some VR headsets, and we can imagine some\u000a<a href="http://topheman.github.io/parallax/">interesting fallbacks</a> for those too.</p>\u000a\u000a<h2>Write once, run in any VR headset</h2>\u000a\u000a<p>Remember the old "write once, run anywhere" promise? The web is the\u000aclosest thing we have to fulfilling it, but what it actually delivers is\u000aoften far from this ideal. The latest VR wave has barely begun and\u000aalready the web VR world is fragmented. Case in point,\u000a<a href="http://vr.chromeexperiments.com/">vr.chromeexperiments.com</a> don't work on Oculus, and\u000a<a href="http://mozvr.com/">mozvr.com</a> demos don't work in Cardboard.  The promise of WebVR\u000ais that once it lands, all will be well in the world. However, this\u000ameans that we need to wait for WebVR to become fully baked. In other\u000awords, we are blocked on the valiant efforts of our dear <a href="https://twitter.com/vvuk">WebVR</a>\u000a<a href="https://twitter.com/Tojiro">implementers</a> (as well as the heavyweight browser development\u000aprocess consisting of spec authors, security reviews, binary size, etc).</p>\u000a\u000a<p>To speed up the process, we need a polyfill for WebVR which uses web\u000aAPIs to provide functionality to the WebVR specification (currently, in\u000a<a href="https://github.com/vvuk/gecko-dev/blob/oculus/dom/webidl/VRDevice.webidl">IDL form</a>). In the absence of a WebVR implementation, the polyfill\u000akicks in and supports mobile VR headsets like Cardboard and Durovis\u000aDive, which are passive contraptions that just piggyback on the\u000asmartness found in your smartphone.</p>\u000a\u000a<h2>Introducing: WebVR Boilerplate</h2>\u000a\u000a<p>The <a href="https://github.com/borismus/webvr-boilerplate">WebVR boilerplate project</a> is on github, and consists of\u000atwo parts. Firstly, the <a href="https://github.com/borismus/webvr-polyfill">WebVR polyfill</a> provides WebVR\u000asupport for Cardboard-compatible devices, and orientation tracking\u000afallbacks where no headset is available. The WebVR polyfill can also be\u000ainstalled from npm (available via <code>npm install webvr-polyfill</code>).</p>\u000a\u000a<ol>\u000a<li>A <code>CardboardHMDDevice</code>, provides a reasonable default for\u000ainterpupillary distance and field of view for cardboard-like devices.</li>\u000a<li>On mobile devices, a <code>GyroPositionSensorVRDevice</code>, which provides\u000aorientation through the <code>DeviceOrientationEvent</code>.</li>\u000a<li>On PCs, a <code>MouseKeyboardPositionSensorVRDevice</code>, which provides\u000aorientation through keyboard and mouse events.</li>\u000a</ol>\u000a\u000a<p>It is designed to be used with <a href="http://threejs.org/">THREE.js</a> plugins that are built\u000afor the WebVR API: <a href="https://github.com/mrdoob/three.js/blob/master/examples/js/controls/VRControls.js">VRControls.js</a> and\u000a<a href="https://github.com/mrdoob/three.js/blob/master/examples/js/effects/VREffect.js">VREffect.js</a>, but of course any valid usage of the WebVR API\u000ashould work (modulo bugs).</p>\u000a\u000a<p>Secondly, the <a href="https://github.com/borismus/webvr-boilerplate/blob/master/js/webvr-manager.js">WebVR manager</a> surfaces VR compatibility\u000ausing consistent iconography and simplifies transitioning in and out of\u000afull VR mode. It also contains some of the best practices for making VR\u000awork on the web, for example, using orientation lock to keep the phone\u000ain landscape orientation, and a means of keeping the phone screen on. If\u000ayou're ready to dive in, the <a href="https://github.com/borismus/webvr-boilerplate">github has all of the technical\u000ainformation</a> available.</p>\u000a\u000a<p>WebVR boilerplate is meant to make it easy to develop immersive\u000aexperiences that run on all VR hardware, including Oculus and Cardboard,\u000aand also provide reasonable fallbacks when no specialized viewer is\u000aavailable.</p>\u000a\u000a<h2>WebVR boilerplate in action</h2>\u000a\u000a<p><img src="sechelt.png" alt="Screenshot of the mozvr.com Sechelt demo." /></p>\u000a\u000a<p>I really liked Mozilla's <a href="http://mozvr.com/projects/sechelt/">Sechelt demo</a>, inspired by the\u000aeponymous town on British Columbia's beautiful Sunshine Coast. I've\u000a<a href="http://borismus.github.io/sechelt">ported it</a> to WebVR Boilerplate. The result is the same\u000ademo, which works in Cardboard, as well as continuing to work on desktop\u000aand mobile devices, and on the Oculus Rift via <a href="https://nightly.mozilla.org/">Firefox Nightly</a> and\u000a<a href="https://drive.google.com/folderview?id=0BzudLt22BqGRbW9WTHMtOWMzNjQ&amp;usp=sharing#list">Brandon's WebVR Chrome builds</a>. It also yielded less confusing\u000aboilerplate code and a <a href="https://github.com/borismus/sechelt/commit/671cff9fc283b58d2ebce0ae6f0dfa3580050aab">simplified code base</a> since\u000athere is no longer need for an unweildy conditional to determine whether\u000ato use <code>DeviceOrientationControls</code>, <code>OrbitControls</code>, or <code>VRControls</code>,\u000aand decide between <code>VREffect</code> and <code>StereoEffect</code>.</p>\u000a\u000a<p>As always, <a href="https://twitter.com/borismus">let me know what you think</a> and feel free to point\u000aout (or fix!) my mistakes on <a href="https://github.com/borismus/webvr-boilerplate">the github</a>.</p>\u000a
p2264
tp2265
Rp2266
sg11
V/responsive-vr
p2267
sg13
Nsg14
I01
sg15
VResponsive WebVR, headset optional
p2268
sg18
VVR on the web threatens to cleave the web platform in twain, like mobile\u000adid before it.
p2269
sg4
V<p>VR on the web threatens to cleave the web platform in twain, like mobile\u000adid before it. The solution then and the solution now is <a href="http://en.wikipedia.org/wiki/Responsive_web_design">Responsive Web\u000aDesign</a>, which websites to scale well for all form factors.\u000aSimilarly, for VR to succeed on the web, we need to figure out how to\u000amake VR experiences that work both in any VR headset, and also without a\u000aVR headset at all.</p>\u000a\u000a<p><img src="hmds.png" alt="Various head mounted displays." /></p>\u000a\u000a<p><a href="https://github.com/borismus/webvr-boilerplate">WebVR boilerplate</a> is a new starting point for building\u000aresponsive web VR experiences that work on popular VR headsets and\u000adegrace gracefully on other platforms. Check out a couple of demos, <a href="http://borismus.github.io/webvr-boilerplate/">a\u000asimple one</a> and one <a href="http://borismus.github.io/sechelt">ported from MozVR</a>.</p>\u000a\u000a
p2270
sg23
g147
sg31
g2267
sg148
(dp2271
g150
S'Feb'
p2272
sg152
S'February 2, 2015'
p2273
sg154
I2
sg155
S'2015-02-02T09:00:00-00:00'
p2274
sg157
I1422896400
sg158
I2015
sg159
I2
ssg61
g160
sg29
S'responsive-vr'
p2275
sS'posted'
p2276
g166
(S'\x07\xdf\x02\x02'
p2277
tp2278
Rp2279
ssg32
S'content/posts/2015/responsive-vr/index.md'
p2280
sg34
F1433825386.0
sa(dp2281
g2
(dp2282
g26
g5
(g6
g7
V<p>Last summer I visited Austria, the capital of classical music. I had the\u000apleasure of hearing the <a href="http://en.wikipedia.org/wiki/Vespro_della_Beata_Vergine">Vespers of 1610</a> in the great\u000a<a href="http://goo.gl/e3WBB2">Salzburger Dom (photosphere)</a>. The most memorable part of\u000athe piece was that the soloists movedbetween movements, so their voices\u000aand instruments emanated from surprising parts of the great hall.\u000aInspired, I returned to the west coast and eventually came around to\u000abuilding a spatial audio prototypes like this one:</p>\u000a\u000a<p><a href="http://borismus.github.io/moving-music"><img src="collage_small.jpg" alt="Screenshot of a demo" /></a></p>\u000a\u000a<p>Spatial audio is an important part of any good VR experience, since the\u000amore senses we simulate, the more compelling it feels to our sense\u000afusing mind. WebVR, WebGL, and WebAudio all act as complementary specs\u000ato enable this necessary experience. As you would expect, because it\u000auses the <a href="https://github.com/borismus/webvr-boilerplate">WebVR boilerplate</a>, this demo can be viewed on\u000amobile, desktop, in Cardboard or an Oculus Rift. In all cases, you will\u000aneed headphones :)</p>\u000a\u000a<!--more-->\u000a\u000a<h2>Early spatial music</h2>\u000a\u000a<p>One of the things that made my acoustic experience in the Salzburg Dom\u000aso memorable was the beauty of the space in which it was performed. The\u000apotential for awesome sound was staggering, with one massive organ at\u000athe back, and four smaller organs surrounding the nave. During the\u000aperformance of the vespers, the thing that struck me the most was that\u000aas the piece transitioned from movement to movement, choreographed\u000asoloists also moved around the cathedral, resulting in haunting acoustic\u000aeffects. Sometimes, a voice would appear quietly from the far end of the\u000acloister, sounding distant and muffled. Other times, it would come from\u000athe balcony behind the audience, full of unexpected reverb. It was a\u000atruly unique acoustic experience that I will never forget, and it made\u000ame wonder about the role of space in music.</p>\u000a\u000a<p>As it turns out, there is a rich history on the <a href="http://en.wikipedia.org/wiki/Spatial_music">topic of spatialization\u000ain music</a> going back to the 16th century. For the\u000apurposes of this blog, I am more interested in the present day. In\u000aparticular, given the <a href="http://www.w3.org/TR/webaudio/">excellent state of audio APIs</a> on the\u000aweb, what follows is a foray into spatial audio with WebVR.</p>\u000a\u000a<h2>Experiments in spatial audio</h2>\u000a\u000a<p>How does music sound if in addition to pitch, rhythm and timbre, we\u000acould tweak position and velocity as additional expressive dimension?\u000aMy demo places you into a virtual listening space, that you look\u000aaround into (using whatever means you have available: mouse and\u000akeyboard, mobile phone gyroscope, or cardboard-like device) -- thanks to\u000a<a href="https://github.com/borismus/webvr-boilerplate">WebVR boilerplate</a>. Each track is visualized as a blob of\u000aparticles. These animate according to the instantaneous amplitude of the\u000atrack, serving as a per-track visualizer and indicating where the track\u000ais in space.</p>\u000a\u000a<p>There is a surprising amount of multi-track music out there, such as\u000a<a href="http://www.cambridge-mt.com/ms-mtk.htm">Cambridge Music Technology's raw recordings archive</a> for aspiring\u000aaudio engineers and <a href="https://soundcloud.com/search/sets?q=stems">soundcloud stems</a> which are typically\u000arecorded separately in a studio and then released publicly for <a href="http://www.remixcomps.com/">remix\u000acontests</a>. In the end, I went with a few different sets just to\u000aget a feeling for spatializing a variety of tracks:</p>\u000a\u000a<ul>\u000a<li>Multiple people <a href="http://borismus.github.io/moving-music/?set=speech">speaking (demo)</a> simultaneously (the <a href="http://en.wikipedia.org/wiki/Cocktail_party_effect">cocktail party effect</a>).</li>\u000a<li>Multi-track <a href="http://borismus.github.io/moving-music/?set=jazz">live (demo)</a> recording with many mic'ed instruments.</li>\u000a<li>Multi-track <a href="http://borismus.github.io/moving-music/?set=phoenix">studio recording (demo)</a> with cleanly recorded tracks (A <a href="http://en.wikipedia.org/wiki/Phoenix_%28band%29">Phoenix</a> track).</li>\u000a</ul>\u000a\u000a<p>In addition to selecting the sounds to spatialize, the demo supports\u000alaying out the tracks in various formations. To cycle between these\u000amodes, hit space on desktop, or tap the screen on mobile:</p>\u000a\u000a<ul>\u000a<li>Sources are <a href="http://borismus.github.io/moving-music/?mode=0">clumped together (demo)</a> in one place.</li>\u000a<li>Sources are <a href="http://borismus.github.io/moving-music/?mode=1">positioned around the observer (demo)</a>.</li>\u000a<li>Sources are <a href="http://borismus.github.io/moving-music/?mode=2">moving around the observer (demo)</a>.</li>\u000a</ul>\u000a\u000a<p>Given that the code is <a href="https://github.com/borismus/moving-music">open sourced on github</a>, it's pretty\u000aeasy to try your own tracks, implement new trajectories or change the\u000avisualizer. Please fork away!</p>\u000a\u000a<h2>Implementation details</h2>\u000a\u000a<p>In an attempt to eat my own dogfood, this project partly serves as a way\u000ato test the <a href="https://github.com/borismus/webvr-boilerplate">WebVR boilerplate project</a> to make sure that\u000ait is usable, and provides the functionality that it purports to. I've\u000amade a bunch of changes to the boilerplate in parallel, fixing browser\u000acompatibility issues and resolving bugs. Notable improvements since\u000ainception include the ability to mouselook via <a href="http://www.html5rocks.com/en/tutorials/pointerlock/intro/">pointer\u000alock</a> in regular desktop mode and improved support for iOS\u000aand Firefox nightly. Thanks to <a href="http://www.antoniocosta.eu/">Antonio</a>, my awesome designer\u000acolleague, the WebVR boilerplate has a new icon!</p>\u000a\u000a<p>This project relies heavily on audio, but requires the page to be\u000arunning in the foreground for you to enjoy the immersive nature of the\u000aexperience. Browsers, especially on mobile devices, can have some weird\u000abehaviors when it comes to backgrounded tabs. It's a safe bet to just\u000aprevent this from happening altogether, so I've been using the <a href="http://www.smashingmagazine.com/2015/01/20/creating-sites-with-the-page-visibility-api/">page\u000avisibility API</a> to mute the music when the tab goes out of\u000afocus, and then resume it when it's back in focus. This works super well\u000aacross browsers I've tested in and prevents the page-hunt where you're\u000atrying to find which annoying tab/activity/app is playing!</p>\u000a\u000a<p>I toyed a little bit with the doppler effect, but found it to be\u000aterrible for music. Because in the moving case, each track moves with\u000aits own velocity relative to the viewer, frequency shifts are\u000anon-uniform, leading to a cacophany of out-of-tune instruments. For\u000aspoken word, it worked quite well, though. The caveat to all this is that the\u000acurrent <a href="http://crbug.com/439644">doppler API is deprecated</a>, so I\u000adidn't delve too deeply into doppler until we have a new implementation.</p>\u000a\u000a<h2>Pitfalls and workarounds</h2>\u000a\u000a<p><strong>Set your listener's up vector properly.</strong> Something you should beware\u000aof is to always set the up vector correctly in the\u000a<code>listener.setOrientation(...)</code> call. Initially, I was only setting the\u000adirection vector, keeping up fixed at <code>(0, 1, 0)</code>, but this yielded\u000aunpredictable results and took a long time to track down.</p>\u000a\u000a<p><strong>Streaming is broken in mobile implementations.</strong> A couple of issues\u000arelated to loading audio bit me as I was developing, proving to be\u000anearly show stoppers (please star if you feel strongly):</p>\u000a\u000a<ul>\u000a<li>Streaming audio doesn't work on Android (or iOS). This means that\u000aevery track we play needs to be first loaded, and then decoded:\u000a<a href="http://crbug.com/419446">http://crbug.com/419446</a></li>\u000a<li>Decoding mp3 on Android takes a very very long time (same in Firefox):\u000a<a href="http://crbug.com/232973">http://crbug.com/232973</a></li>\u000a<li>Though it doesn't directly affect my spatial sound experiments, the\u000ainability to bring in remote WebRTC audio streams into the audio graph\u000ais blocking other ideas: <a href="https://crbug.com/121673">https://crbug.com/121673</a></li>\u000a</ul>\u000a\u000a<p>I tried to work around the streaming issue by doing my own chunking\u000alocally and then writing a <code>ChunkedAudioPlayer</code>, but this is harder than\u000ait seems, especially when you want to synchronize multiple chunked\u000atracks.</p>\u000a\u000a<p><strong>Beware of implementation differences.</strong> It's also worth noting that\u000adifferent browsers have slightly different behaviors when it comes to\u000aPannerNodes. In particular, Firefox spatialization can appear to sound\u000abetter, but this is simply because it's louder (the same effect can be\u000areplicated in Chrome by just increasing gain). Also, on iOS, it seems\u000athat the spatialization effect is weaker -- potentially because they are\u000ausing a different HRTF, or maybe they are just panning.</p>\u000a\u000a<p><strong>Spatialization effect can be subtle.</strong> I found that there wasn't\u000aenough oomph to the effect provided by WebAudio's HRTF. Perhaps it is\u000aacoustically correct, but it just wasn't obvious or compelling enough as\u000ais. I had to fudge the situation slightly, and implement a sound cone\u000afor the observer, so that sources that are within the field of view got\u000aa slight gain boost.</p>\u000a\u000a<h2>Parting words and links</h2>\u000a\u000a<p><a href="http://www.ee.usyd.edu.au/people/philip.leong/UserFiles/File/papers/errors_hr97.pdf">The nature and distribution of errors in sound localization</a>\u000ais a seminal paper from 1997, giving a thorough psychoacoustic analysis\u000aon our hearing limits. In this web audio context, however, it is unclear\u000ahow much of this perceptual accuracy is lost due to variations in\u000aheadphone style and quality, and software implementation details.  To\u000atruly bring my Austrian cathedral experience to the web, we would\u000aprobably need a personalized HRTF, and also a more sophisticated room\u000amodel that could simulate reflections from the walls of the building.\u000aThis is concievable on the web in the near future, especially with the\u000aprospect of the <a href="https://plus.sandbox.google.com/+ChrisWilson/posts/QapzKucPp6Y">highly anticipated</a> <a href="http://webaudio.github.io/web-audio-api/#audio-worker-examples">AudioWorker</a>.</p>\u000a\u000a<p>Let me conclude by linking you to a couple more spatial audio demos:</p>\u000a\u000a<ul>\u000a<li>Ilmari wrote an <a href="http://www.html5rocks.com/en/tutorials/webaudio/positional_audio/">html5rocks post</a> a while back about three.js\u000aand the Web Audio API.</li>\u000a<li>Mozilla built the <a href="https://hacks.mozilla.org/2013/10/songs-of-diridum-pushing-the-web-audio-api-to-its-limits/">Songs of Diridum demo</a>, showing a cute\u000aspatialized jazz band.</li>\u000a<li>Arturo recently emailed me, showing me a <a href="http://inspirit.unboring.net/">pretty compelling WebVR +\u000aAudio project</a>, in the spirit of WebVR.</li>\u000a<li>Not web-based, but a painstakingly recorded and tweaked <a href="https://www.youtube.com/watch?v=IUDTlvagjJA">binaural\u000ahaircut simulation</a> just to illustrate the potential.</li>\u000a</ul>\u000a
p2283
tp2284
Rp2285
sg11
V/spatial-audio-web-vr
p2286
sg13
Nsg14
I01
sg15
VSpatial audio and web VR
p2287
sg18
VLast summer I visited Austria, the capital of classical music.
p2288
sg4
V<p>Last summer I visited Austria, the capital of classical music. I had the\u000apleasure of hearing the <a href="http://en.wikipedia.org/wiki/Vespro_della_Beata_Vergine">Vespers of 1610</a> in the great\u000a<a href="http://goo.gl/e3WBB2">Salzburger Dom (photosphere)</a>. The most memorable part of\u000athe piece was that the soloists movedbetween movements, so their voices\u000aand instruments emanated from surprising parts of the great hall.\u000aInspired, I returned to the west coast and eventually came around to\u000abuilding a spatial audio prototypes like this one:</p>\u000a\u000a<p><a href="http://borismus.github.io/moving-music"><img src="collage_small.jpg" alt="Screenshot of a demo" /></a></p>\u000a\u000a<p>Spatial audio is an important part of any good VR experience, since the\u000amore senses we simulate, the more compelling it feels to our sense\u000afusing mind. WebVR, WebGL, and WebAudio all act as complementary specs\u000ato enable this necessary experience. As you would expect, because it\u000auses the <a href="https://github.com/borismus/webvr-boilerplate">WebVR boilerplate</a>, this demo can be viewed on\u000amobile, desktop, in Cardboard or an Oculus Rift. In all cases, you will\u000aneed headphones :)</p>\u000a\u000a
p2289
sg23
g147
sg31
g2286
sg148
(dp2290
g150
S'Mar'
p2291
sg152
S'March 19, 2015'
p2292
sg154
I3
sg155
S'2015-03-19T09:00:00-00:00'
p2293
sg157
I1426780800
sg158
I2015
sg159
I19
ssg61
g160
sg29
S'spatial-audio-web-vr'
p2294
sS'posted'
p2295
g166
(S'\x07\xdf\x03\x13'
p2296
tp2297
Rp2298
ssg32
S'content/posts/2015/spatial-audio-web-vr/index.md'
p2299
sg34
F1433825402.0
sa.