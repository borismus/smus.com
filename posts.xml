<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"
      xml:lang="en"
      xml:base="https://smus.com">
  <title>Boris Smus (Posts)</title>
  <link href="https://smus.com/atom.xml" rel="self"/>
  <link href="https://smus.com"/>
  <updated>2020-12-21T17:06:37-00:00</updated>
  <id>https://smus.com/atom.xml</id>
  
  <entry>
    <title>ESUP Builders</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/esup-builders"/>
    
    <updated>2020-07-30T09:00:00-00:00</updated>
    
    <id>https://smus.com/esup-builders</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>Since May, I’ve been tinkering with brushless electrical motors, trying to make floating objects move more quickly. The proximate result is this electric motorized paddleboard:</p>

<p><a href="https://photos.app.goo.gl/9BbHWMzhPdxekJZf7"><img src="/esup-builders/esup.gif" alt="Riding my DIY eSUP" /></a></p>

<p>This project is by no means finished. Traveling at 5 mph is fun, but not nearly fast enough. As we speak, a motor ten times the power of this one is sitting in a shipping container, en route to my basement. While I wait, let me pause and reflect on the project so far.</p>

<!--more-->

<p>This summer I’ve been spending more time on the water. Sailing and stand-up paddling are great escapes from a sometimes monotonous routine. As a result, I’m slowly forgetting luxuries I once took for granted: work trips, visits to Vancouver, vacations to faraway lands.</p>

<p>After my forced sailing hiatus, I needed to find another way to get out on the water. SUPing was great fun, but a bit repetitive, and frustrating in even light headwind. Driven by the need for speed, I found <a href="http://efoil.builders/">efoil.builders</a>. They are a community of crazy European kids building electric hydrofoils that scream above the surface of the water at breakneck speed.</p>

<p>I’m no stranger to strange electric vehicles, but I certainly was when it came to high voltage and high current. When my Solowheel broke down last year and I was unable to fix it on my own, I donated it to an acquaintance who promptly replaced the battery and brought it into tip-top shape. It took him no time at all, but I had no idea where to begin.</p>

<p>Happily, the last few months have presented many opportunities for electrical work around the house. I installed a ceiling fan in our bedroom and a window AC in my daughter’s. I wired up and installed new outlets in the basement. These are not glamorous achievements, just small victories that slightly reduced my fear of electricity.</p>

<h1>Building for …</h1>

<p>There is something very satisfying about creating physical objects with your own hands. In these strange times, I find this to be one of my most comforting escapes.</p>

<p>If the resulting object is of value to you or others, so much the better. <a href="/toddler-music-box">Building for my daughter</a> takes the cake, especially when she appreciates the results. <a href="/little-free-library">Building for the community</a> is also gratifying and surprising. Sometimes you just build to learn. This time, I’m building for myself. And in this case, a big part of the challenge will be learning to ride the damn thing. I think I’m up for the challenge!</p>

<p>Realizing that I’m dealing with a completely new domain, and my chance of failure is high, I decided to start small. The first milestone was to create some sort of vessel from a plastic IKEA bin (max 30A, 11V, 5000 RPM, 3cm prop). Having proved the concept, I moved on to something that would support my weight, and substantially larger components (60A, 22V, 1000 RPM, 15cm prop).</p>

<p>Let me conclude with a parts list for both builds.</p>

<h1>Tropfast components</h1>

<p>As you can see, this is not a seaworthy vessel.</p>

<p><a href="https://photos.app.goo.gl/eUSU6HqHvKrR3dzi8"><img src="/esup-builders/tropfast.jpg" alt="HMS Tropfast, IKEA RC boat shelf" /></a></p>

<p>Electronics:</p>

<ul>
<li>ESC: <a href="https://www.amazon.com/gp/product/B00PR8XT7G">Hobbywing Seaking 30A</a></li>
<li>Motor: <a href="https://www.amazon.com/gp/product/B00PRAME7U">Hobbywing Seaking 4800KV-2040SL</a></li>
<li>Battery: <a href="https://www.amazon.com/gp/product/B07TS2GVS3">POVWAY 5200mAh 3S</a></li>
</ul>

<p>Mechanics:</p>

<ul>
<li>Hull: IKEA <a href="https://www.ikea.com/us/en/p/trofast-storage-box-white-80089239/">TROFAST bin</a> with <a href="https://www.ikea.com/us/en/p/trofast-lid-white-57454500/">TROFAST lid</a></li>
<li>Propeller: <a href="https://www.amazon.com/gp/product/B07Q32KHDW">uxcell D=36mm</a></li>
<li>Motor mount: <a href="https://www.amazon.com/gp/product/B07NWKGTZT">L base for RC motors</a></li>
<li>Rudder: <a href="https://www.amazon.com/gp/product/B0773D9L9X">Basic RC boat rudder with water-cooling intake</a></li>
</ul>

<h1>SUPSMUS v1 components</h1>

<p><a href="https://photos.app.goo.gl/eUSU6HqHvKrR3dzi8"><img src="/esup-builders/esup.jpg" alt="ESUP assembly" /></a>
<a href="https://photos.app.goo.gl/eUSU6HqHvKrR3dzi8"><img src="/esup-builders/esup-drive.jpg" alt="ESUP drive closeup" /></a></p>

<p>Electronics:</p>

<ul>
<li>ESC: <a href="https://flipsky.net/products/torque-esc-vesc-%C2%AE-bldc-electronic-speed-controller">Flipsky 50A VESC based on V4.12</a></li>
<li>Motor: <a href="https://flipsky.net/products/f5085-140kv-brushless-motor-for-direct-drive-propeller">Flipsky 5085 outrunner motor</a></li>
<li>Battery: <a href="https://www.amazon.com/Gens-ace-4000mAh-Battery-Goblin/dp/B0721BJT3J">Gens Ace 6S 4 mAh LiPo battery</a></li>
</ul>

<p>Mechanics:</p>

<ul>
<li>Hull: Foiling board with Tuttle box. Bought it from a guy on craigslist.</li>
<li>Propeller: <a href="https://cad.onshape.com/documents/07ebbbfe9074e0b9b543ca4c/v/837c590e6d8c15772aa5432e/e/81de0414d9560e59eb06bc57">3D printed prop design</a></li>
</ul>

<p><a href="https://photos.app.goo.gl/eUSU6HqHvKrR3dzi8"><img src="/esup-builders/3d-prop.png" alt="3D print of propeller" /></a></p>

<ul>
<li>Motor mount: <a href="/esup-builders/TODO">Naish 65cm mast</a> and <a href="https://cad.onshape.com/documents/9008f491eb3b37b138306395/w/151f8733bac1d09cb1fbce5b/e/dc088e9e03eb76f671d5bc26">3D printed mast clamp</a></li>
</ul>

<p><a href="https://photos.app.goo.gl/eUSU6HqHvKrR3dzi8"><img src="/esup-builders/3d-mast-clamp.png" alt="3D printed mast clamp" /></a></p>

<h1>Shared components</h1>

<p>As you can see, both vessels have a lot of component types in common. So much so that I simply reused some of the same parts in both projects:</p>

<ul>
<li>Remote: <a href="https://www.amazon.com/gp/product/B00VE3QC7C">Flysky FS-GT2E</a></li>
<li>Charger: <a href="https://www.amazon.com/gp/product/B07QRQT3LC">Haisito 80W 6A Balance Charger</a></li>
</ul>

<p>I plan to write up a technical summary of what I learned in the process. Stay tuned.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>DIY Community Library</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/little-free-library"/>
    
    <updated>2020-04-25T09:00:00-00:00</updated>
    
    <id>https://smus.com/little-free-library</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>When my sailing class was canceled back in early March, I realized it was time for a more individualistic hobby. And so, I furled the sails and tied up the boat and picked up a circular saw from the hardware store. After I got over my fears of loud, terrifyingly quickly spinning metal blades, a whole bunch of cutting and screwing, then digging and painting, I have a Little Free Library standing outside my house, and a little bit more confidence for the next woodworking project!</p>

<p><img src="/little-free-library/little-free-library.jpg" alt="Little free library photo" /></p>

<!--more-->

<p>I drafted some <a href="https://cad.onshape.com/documents/d57c2f5444558be437e513c4/w/9e862248a8541fe7ce7a38a6/e/3efc51d554390f104d869fa6">initial plans in OnShape</a>, and deviated from them only slightly, the main difference being a much smaller front-right panel to make book access easier. As I became familiar with woodworking tools, I became a little bit less rigid and precise with measurements. </p>

<h2>Materials</h2>

<ul>
<li>A very long plank of scrap wood found in my basement, 20mm deep was enough for structural materials.</li>
<li>30 or so brass screws</li>
<li>6 long screws</li>
<li>A fence post</li>
<li>Self-closing hinges and a button</li>
<li>Exterior-grade paint</li>
<li>A sheet of thin scrap plywood for shingles</li>
<li>Plexiglass sheet</li>
<li>Circular saw</li>
<li>Electric drill</li>
<li>Silicone mat</li>
<li>Caulk</li>
<li>Hammer</li>
<li>Shovel</li>
<li>Wood hand saw</li>
<li>Probably lots more</li>
</ul>

<h2>Mistakes and learnings:</h2>

<ul>
<li>Don’t use brass screws, they are fragile. I broke two of them as I was screwing them. Instead, use steel. It’s cheaper and far stronger.</li>
<li>Don’t rely on glue for shingles. They warp, so use nails to hammer them on. I had to do this after it rained and they warped, despite a copious amount of glue.</li>
<li>Don’t rely on the guide of the circular saw. Instead, look at what the blade is actually doing at the cut point. I had a few rough cuts early on.</li>
<li>Use a protractor in order to measure 45 degree angles more accurately, which I found tricky to do through thick 4x4 fence posts (circular saw radius was too small, had to use a hand saw)</li>
</ul>

<p>Here are some <a href="https://photos.app.goo.gl/K3xP2XjPmUQ2bX1N7">more photos</a> that document the process. The results are passable but honestly, a little bit sloppy. Build quality aside, I’m thrilled to see actual use: the roster of books changing quickly. My initial dump of philosophy books was partially picked off, and then swiftly amended. I had predicted very little movement during the pandemic because of decreased foot traffic and increased paranoia about touching public things, but was sorely mistaken.</p>

<p>Many thanks to Matthew for the pro-tips and help!</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Visual Chronology of Science &amp; Discovery</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/visual-chronology-science-discovery"/>
    
    <updated>2020-02-11T09:00:00-00:00</updated>
    
    <id>https://smus.com/visual-chronology-science-discovery</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>As Newton wrote, “If I have seen further it is by standing on the shoulders of giants”. But whose giant shoulders did Newton stand on? And did those giants stand on the shoulders of other giants? And how about Newton’s successors, or people working in other fields? As far as I can tell, it’s giants all the way down.</p>

<p>Last year, I got my hands on a remarkable book, <a href="/books/asimovs-chronology-of-science-and-discovery/">Asimov’s Chronology of Science and Discovery</a>. It inspired me to produce a visual summary of human ingenuity, to see what one giant saw from the shoulders of another. After some experimentation, I turned it into an interactive visualization. You can play with it <a href="https://borismus.github.io/asimov/web/cross-shape#steel">here</a>:</p>

<p><a href="https://borismus.github.io/asimov/web/cross-shape/#steel"><img src="/visual-chronology-science-discovery/screenshot-steel.jpg" alt="Screenshot of the visual chronology centered at Steel." /></a></p>

<!--more-->

<p>One of the unnerving things about Isaac Asimov’s book was the visual table of contents for the first edition:</p>

<p><img src="/visual-chronology-science-discovery/original-visualization.jpg" alt="Winding visualization of Asimov’s Chronology in the first edition." /></p>

<p>Following this winding path is tricky to say the least. With this in mind, I carefully read the first quarter of the book, spanning from the beginning of time until 1700 CE, which amounts to about 300 entries, or about a quarter of those contained in the book. I took notes in a spreadsheet, providing a 1-2 sentence summary of each entry and manually extracting some key metadata. For each of Asimov’s entries, I captured the title, a couple sentences of description, the associated person’s name, and where the invention was created or discovery found, usually the country or empire.</p>

<p>In addition, I provided two extra fields which are more subjective and frankly, made up by yours truly, to bring the visualization to life: </p>

<ol>
<li><strong>Field</strong>: which domain was this discovery made. (How granular should this be? Should Science be split up into Chemistry, Physics, and Biology?)</li>
<li><strong>Dependencies</strong>: what older inventions and discoveries enabled this one. (This can be really tricky.)</li>
</ol>

<p>With this in place, Asimov’s linear chronology becomes a directed graph, and since we don’t know how to time travel, there are no time cycles. So what we have is a <a href="https://en.wikipedia.org/wiki/Directed_acyclic_graph">directed acyclic graph (DAG)</a>. It is not a tree since I allow each node to have multiple parents. For instance, the field of Geometry is built up logically from axioms. At the same time, the founding text of the field, Euclid’s Elements, is mostly derived work from many mathematicians working before Euclid in the Academy and Lyceum. So the DAG shows <strong>Geometry</strong> rooted both in <strong>Advanced Schools</strong> and <strong>Logic</strong>.</p>

<p><a href="https://borismus.github.io/asimov/web/cross-shape/#geometry"><img src="/visual-chronology-science-discovery/screenshot-geometry.jpg" alt="Screenshot of the visual chronology centered at Geometry." /></a></p>

<h2>Manual entry is tiring</h2>

<p>Sadly, the Chronology is not available as a text-based ebook, which complicates matters. As it turns out, optical character recognition (OCR) is not easy if your content is laid out like a newspaper:</p>

<p><img src="/visual-chronology-science-discovery/book-flow-ocr.jpg" alt="Excerpt of the book showing tricky text flow" /></p>

<p>Google Cloud Vision and <a href="https://github.com/tesseract-ocr/tesseract">tesseract</a> don’t do well on complex text flows, often failing to recognize section boundaries. For the image above, tesseract wrongly assumes that that the text flows in three full height columns.</p>

<p>I also tried to do automatic entity extraction using python’s <code>nltk</code> and <code>ne_chunk</code> to automatically generate inventors and locations but a quick experiment yielded too many entities for each entry, meaning that I’d have to make a manual pass regardless, and this step would provide minimal time savings.</p>

<h2>Playing cards and tech trees</h2>

<p>Each invention and discovery needs its own view. I gravitated to a card metaphor, which is often used in historical strategy games like Civ, role playing games like Diablo, and in many playing card games like Magic the Gathering. Here are a <a href="https://www.are.na/boris-smus/tech-tree-cards">few examples</a> that inspired me.</p>

<p>One challenge that I found ultimately insurmountable was to find good images for each entry. While you can find reams of royalty free images, and even download them automatically using tools like <a href="https://github.com/hardikvasa/google-images-download">googleimagesdownload</a>, finding a set that is visually consistent was tough. I did this manually for the first hundred inventions. Theoretically a style transfer model might be able to convert them all to a consistent look. But ultimately, they didn’t add much value to the visualization as a whole, so I ditched the idea.</p>

<p><img src="/visual-chronology-science-discovery/card-images.jpg" alt="Attempt to use images in the cards" /></p>

<h2>Chronological visualization</h2>

<p>To visualize the entries, I started with a naive approach: render all of the entries at once in a giant horizontal scrolling view, kind of like how tech trees work in Civilization-like games. Doing such a thing linearly makes no sense at all, since the first entry in the book is Bipedalis, dating to 4 million years ago (ma), followed by Stone Tools at 2 ma, Fire at 0.5 ma, then 8 entries later, Agriculture at 0.01 ma (10,000 years ago). This sort of timeline is best represented on a log-scale, which makes entries more reasonably spaced out, although still not perfect. Actually there’s quite a lot of variation in density. For example, this is the chronology of the first millennium CE (1000 years):</p>

<p><img src="/visual-chronology-science-discovery/chrono-first-millennium.jpg" alt="Chronology of first millennium" /></p>

<p>While this is just the 16th century (100 years, an order of magnitude shorter than above):</p>

<p><img src="/visual-chronology-science-discovery/chrono-sixteenth-century.jpg" alt="Chronology of 16th century" /></p>

<p>Even with the log-scale, inventions are by no means well distributed in time, with the 16th century far more visually dense than the first millennium. In both examples, links are especially obscure, since they often stretch out for many screens, and are basically impossible to trace from source to destination. </p>

<p>I tried variants of this view as well, involving collapsing entries into more compact default representations, and allowing them to be expanded for more detail. One promising variant involved expanding a selected entry and all transitively linked ones, but the predecessors and successors are still typically positioned far off-screen, so a lot of scrolling is required.</p>

<h2>Entry-centric visualization</h2>

<p>One of the shortcomings of a purely chronological view is that connections between entries are lost. Yet this is the most important part of this whole project, so I kept searching.</p>

<p>I opted for a more structured approach, one that lets you focus on a particular entry and at a glance see what technology led to it, and what technology it enabled. At the same time, I wanted to show the invention in its chronological context, in the spirit of Asimov’s book. So the current design takes a Cartesian approach, with one card centered at the origin, serving as the focus. Technologies that enabled the focused card are shown to the left and technologies that the focus enabled are shown to the right, along the x axis. Chronologically previous and next entries are shown along the y axis. </p>

<h2>Technology transcending fields</h2>

<p>Once the dependency graph is in place and each entry is associated with a field, one starts seeing interesting patterns in the data. </p>

<p>The discovery of magnetism lead to the invention of the compass, giving navigators confidence to traverse the ocean, in turn leading to the discovery of the new world:</p>

<p><a href="https://borismus.github.io/asimov/web/cross-shape#compass"><img src="/visual-chronology-science-discovery/screenshot-compass.jpg" alt="Screenshot of the visual chronology centered at Compass." /></a></p>

<p>Melting glass beads and glass blowing enabled lens crafting and microscopes which in turn let careful observers see microorganisms in pond water, ultimately leading to great advances in medicine. </p>

<p><a href="https://borismus.github.io/asimov/web/cross-shape#microscope"><img src="/visual-chronology-science-discovery/screenshot-microscope.jpg" alt="Screenshot of the visual chronology centered at Microscope." /></a></p>

<p>Cross field dependencies like the ones I described above are visualized as dashed red arrows.</p>

<h2>Please help complete this project</h2>

<p>This visualization is backed by a spreadsheet which is <a href="https://docs.google.com/spreadsheets/d/1hDNXas7DzwglB95HV2_2u1utWAwBZR2hQHlMPz-fj5A/edit#gid=0">publicly viewable</a>, the result of a bunch of my own reading and summarizing. I’ve compiled a quarter of Asimov’s impressive tome by hand, but I’d love your help finishing the project.</p>

<p>If you’re game to help, here are the necessary pieces:</p>

<ol>
<li>A <a href="https://drive.google.com/file/d/1qQDnNQr6L-NGRTyID8kWGMnnVVPeCC6o/view?usp=sharing">PDF of the second edition of Asimov’s Chronology</a> I scanned in hopes of automating the whole process (see above).</li>
<li>A <a href="https://docs.google.com/spreadsheets/d/1uDeBCfcaVUfZFEK-0WJIb43dT6cqHHq9o6Uxn6PihLY/edit#gid=158368026">publicly editable spreadsheet</a> containing stubs for all of the inventions listed in the chronology from 1700 to 1993.</li>
</ol>

<p>Please read a chunk of the latter quarter of the book (starting 1700) and contribute it to the public spreadsheet. I hope you will be as excited as I was to learn a whole lot about the history of science and compile it into this format. Once the project is complete, it will be glorious! </p>

<p>I’ll update this post as more of the spreadsheet is filled out. Meanwhile, thanks for reading and stay tuned – I have a few follow up posts in mind already.</p>

<p><strong>Update Feb 23</strong>: People seem to be interested in the project; excellent! I’ve <a href="https://github.com/borismus/asimov">open sourced</a> the visualization code.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>WebUSB, Arduino, and Nunchucks!</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/webusb-arduino"/>
    
    <updated>2019-11-25T09:00:00-00:00</updated>
    
    <id>https://smus.com/webusb-arduino</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>WebUSB bridges two amazing universes: the open web and the maker movement. Web pages can now talk directly to external hardware over USB, and it works on both mobile and desktop (at least in Chrome). There are a <a href="https://github.com/webusb/arduino">few basic samples</a> out there, but for my own edification, I wanted to get my hands dirty. I hooked up a Wii Nunchuck to an Arduino, and built a webpage to plot sensor readings in real-time. Here’s the resulting <a href="https://www.youtube.com/watch?v=mqmtltjk66w">video</a> and <a href="/webusb-arduino/#" title="https://github.com/borismus/sensor-streamer">code</a>.</p>

<!--more-->

<p>I used this handy <a href="https://www.adafruit.com/product/345">breakout board</a> to hook up the Nunchuck to an Arduino Leonardo without having to cut cables and solder. The Arduino runs a sketch which reads sensor values over I2C and sends them to the host webpage over WebUSB. In this case, the host plots the sensor data as it streams in.</p>

<h1>The UX is a mixed bag</h1>

<p>Conveniently, any WebUSB device can be configured to broadcast a specific URL. As soon as you plug it in, Chrome displays a notification telling you that a new device was detected. Clicking the notification will take you directly to the advertised URL. Great!</p>

<p><img src="/webusb-arduino/notification.png" alt="Notification to go to a URL" /></p>

<p>To actually connect to the device, you need a user gesture (button press) to open up a native “Connect to USB device” dialog. You then pick the device from a list, and press the Connect button. Far from frictionless, but it makes sense given the web’s security model.</p>

<p><img src="/webusb-arduino/dialog.png" alt="Webpage wants to connect to device" /></p>

<p>Despite the inconvenience, this is still super interesting, especially for doing one-time setup for a new hardware device. Certainly preferable to requiring the user to install a junky app on their phone!</p>

<h1>Some minor caveats</h1>

<p>WebUSB won’t work with all Arduinos, only those that support a low level USB profile (eg. Emulating a mouse or keyboard). Specifically, the Leonardo is supported, but the Uno is not.</p>

<p>Arduino setup requires installing the <a href="https://github.com/webusb/arduino">WebUSB library</a> manually, and even changing some Arduino header files. This could definitely be streamlined. Also, beware the large number of Wii Nunchuck-related Arduino libraries, all of which seem subtly broken.</p>

<p>Looking forward to using WebUSB in upcoming hardware projects! Stay tuned, and hopefully I’ll have something to share soon.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Toddler’s First Music Box</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/toddler-music-box"/>
    
    <updated>2019-09-26T09:00:00-00:00</updated>
    
    <id>https://smus.com/toddler-music-box</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>Toddler toys are stacked with blinking lights, loud attention seeking noises, and earworm songs. They are often made of plastic and sadly, feel cheap. My daughter deserves better! </p>

<p>So I set out to design her a perfect music box: an old concept infused with modern technology, without subjecting her to the hazards of screens. I wanted the box to play her favorite songs, be durable and portable, have long battery life, all while being a beautiful object. This is the result:</p>

<p><a href="https://www.youtube.com/watch?v=nS879aGP6O0"><img src="/toddler-music-box/final-box.jpg" alt="My daughter’s music box" /></a></p>

<!--more-->

<h1>Building a portable music player</h1>

<p>My first instinct was to try to build this using a Raspberry Pi,  but turns out running Linux has a few major downsides:</p>

<ol>
<li>Eeking out reasonable battery life is really difficult.</li>
<li>Boot times are very slow, in the 10s of seconds.</li>
</ol>

<p>An alternative to running embedded Linux is programming a prototyping board directly. So I dusted off an old Arduino Uno and discovered the <a href="https://www.adafruit.com/product/94">Adafruit Wave Shield</a>, which does exactly what I needed. It reads audio from an SD card, and plays it back through a speaker. When the Wave Shield kit arrived, I was somewhat shocked to see a bare PCB and all of the components in a little baggie. </p>

<p>Luckily, there’s an incredible Makerspace at work, so I dropped by after my actual work was done, picked up a soldering iron, and got to work. I’ve soldered before, but not much since my favorite class of all time: Making Things Interact at CMU, taught by Mark Gross. Initially daunting, I knocked it out in no time thanks to some really detailed instructions. The results were amazing. Fuse a bunch of metal and silicon together, attach a battery and 1.5 seconds later (that’s the boot time), a song is playing through the speaker! Any sufficiently advanced technology is indistinguishable from magic.</p>

<p><img src="/toddler-music-box/solder.jpg" alt="Soldering the Adafruit Wave Hat" /></p>

<h1>Randomly playing songs</h1>

<p>The <a href="https://learn.adafruit.com/adafruit-wave-shield-audio-shield-for-arduino/play6-hc">sample software</a> for the Wave Shield plays all of the .wav files on the SD card in lexical order. I wanted something a bit more delightful than playing the same playlist in the same order. So I wrote a program that plays a random song instead. Easy peasy, here we go.</p>

<p>I enumerated all of the wavs on the card and stored all of their filenames in a <code>char**</code> dynamically allocated on the heap. Oops! There’s barely enough space there to allocate a dozen file names, and I’d selected 60 songs. It appears that years of front-end UI engineering have dulled my low level embedded software development instincts. After fighting the C++ compiler about static 2D array allocation, I took a simpler and more memory efficient approach, first counting all of the songs, then picking a random song number to play. </p>

<p>Even generating random numbers is non-trivial, since there’s no reliable absolute clock to use as a seed - the device cold-starts every time. Instead, I’m using a technique which reads in analog inputs for a pseudo random input. This may or may not be a good idea, but seems to provide some variation. Anyway, the <a href="https://github.com/borismus/toddler-music-box">Arduino sketch</a> is in the GitHub repo.</p>

<h1>Prototyping in plastic</h1>

<p>One of the great perks of my work’s Makerspace is access to all sorts of awesome prototype manufacturing equipment, including a <a href="https://www.inventables.com/technologies/carvey">Carvey</a>. So I went to Rockler and bought a 5”x5” maple block, thinking I’d hollow it out into a box with the CNC machine. Not so fast! It’d take a mere 15 hours of drilling. </p>

<p>Rather than wait, I opted for a faster route: prototyping with 3D printing. So I began designing music boxes on paper, then in OnShape, then printing them using the <a href="https://www.prusa3d.com/">Prusa</a> printers. Seeing a design evolve from figment of imagination to tangible physical object is incredibly satisfying. However, once that satisfaction wore off, I can honestly say that the results were functional, but not at all aesthetically pleasing:</p>

<p><img src="/toddler-music-box/plastic-box.jpg" alt="Early 3D printed plastic box" /></p>

<h1>That old time wooden aesthetic</h1>

<p>So I went for a different strategy: buy a nice off-the-shelf box to house everything and just design its insides. This way, the 3D print is mostly hidden, and can be capped off with a laser cut or CNC milled wooden lid. I can learn about CNC joinery later.</p>

<p>I found a <a href="https://www.amazon.com/gp/product/B071WFSRBD/ref=ppx_yo_dt_b_asin_title_o06_s00?ie=UTF8&amp;psc=1">nice hexagonal box</a> on Amazon and bought two, discarding both lids. The bottom half of the clam would house the Arduino, Shield and battery, while the top would house the speaker. They’d be connected with a speaker wire and joined by wood hinges.</p>

<p>Designing the innards was a delight. <a href="https://www.onshape.com">OnShape’s</a> UI is excellent and responsive. The constraint system makes a lot of sense, sketching on arbitrary surfaces and then extruding them is amazingly powerful. The Assembly View and Edit-in-Context feature made aligning elements between upper and lower clamshells a cinch. This included the speaker wire port and holes for the microswitch. I experimented with a variety of designs for fastening lid to container. Initially, I opted for ambitious <a href="https://markforged.com/blog/embedding-nuts-3d-printing/">embedded nut</a> designs, but ultimately went with a self-tapping (into plastic) approach using <a href="https://www.homedepot.com/b/Hardware-Fasteners-Screws/Internal-Hex/Flat/N-5yc1vZc2b0Z1z0sfp4Z1z0sgtn">flat head screws</a>. For fastening electronics to the plastic, I printed offsets and used nuts.</p>

<p>After a few iterations of fastener tweaking, design adjustments, and measurement corrections, I had a top insert for housing the speaker, and a speaker grille to protect the speaker’s membrane, a bottom insert for housing the electronic core, and a cover to hide them. The <a href="https://github.com/borismus/toddler-music-box">STL files</a> are all in the GitHub repo. Here’s the result:</p>

<p><img src="/toddler-music-box/final-disassembly.jpg" alt="Finished wooden musical box" /></p>

<p>If you’re curious, the <a href="https://cad.onshape.com/documents/786c5f0b153cdd48f9b0f2f8/w/b4275650dc7aac134e30275f/e/4a850077e5ce208659a14aab">OnShape project is public</a>, you just need to make an OnShape account.</p>

<h1>Toddler user testing</h1>

<p>Once everything was in place, it was time for toddler testing! Luckily I have a very cute and curious user on retainer for the next 18 years. I asked her to open the box, and when she did, she began wiggling along to Cat Stevens’ “la-la-la”s. I’ve since corrected the poor initial music choice – the music box now plays a healthy milieu of classics from Soviet cartoons.</p>

<p>A few other things became immediately clear as a result of user testing:</p>

<ol>
<li>My daughter was completely fascinated by the small speaker wire running between the top and bottom lid. She kept pulling at it, and eventually the wire came out enough to prevent the lid from closing fully.</li>
<li>Predictably, she loves abusing the box in creative ways. At one point she was dancing on top of the box. Then she used it as a step stool to climb onto the couch. Later, she smashed the box so hard the microswitch toggled and restarted the music.</li>
</ol>

<p>In the current version, I’ve concealed the speaker wire in nylon casing and affixed it on both ends using zipties. This solves the first issue where the wire would prevent the box from closing. I’ve also padded the battery pack so that the contents of the box rattles less, hopefully making it less satisfying to shake and smash.</p>

<p>I’m not sure how much battery life I’ll get from this construction, but it’s been running off the same triple AAA pack for about a week of sporadic play with maybe a couple of hours of being actually on. The fact that the device is fully off when the lid is closed (and not secretly draining batteries) makes me happy.</p>

<p>Finally, massive thanks to Matthew Wilson and Jon Ward for their sage advice and friendly encouragement with this project. How might we imbue other beautiful everyday objects with magical abilities?</p>

<p>Until next time.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Link&#39;s Awakening LEGO Mosaic</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/zelda-lego"/>
    
    <updated>2019-09-13T09:00:00-00:00</updated>
    
    <id>https://smus.com/zelda-lego</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>A parismonious use of pixels (160 x 114) and color (four-color greenscale) lends
itself super well to reproduction as a LEGO mosaic! LEGO has this amazing
service called
<a href="https://www.lego.com/page/static/pick-a-brick#shopxlink">Pick-a-Brick</a> where
you can buy spare parts that accidentally ended up in your vacuum cleaner's
dustbin. Or you can buy tiles and flats and make an awesome mosaic based on your
favorite 2-bit sprite. But how many tiles of each color do you need? Not to
worry, I've got you covered with this <a href="https://github.com/borismus/lego-mosaic/blob/master/count_colors.py">color counting
script</a>.
Fully assembled, two-bit LEGO Link makes an excellent coaster for the office
mug.</p>

<p><img src="/zelda-lego/link-mosaic.jpg" alt="Link coaster assembled" /></p>

<!--more-->

<p>I'm not much of a gamer, but Zelda is the one series that continues to occupy a
special place in my heart. Link's Awakening was my first, and remains my
favorite. Twenty five years later, the most memorable part of the game was of
course <a href="https://www.youtube.com/watch?v=NccSaUwoibM">the music</a>. But the
graphics were amazing in their own right. This to me is concentrated nostalgia:</p>

<p><img src="/zelda-lego/links-awakening.png" alt="Screenshot Link's awakening" /></p>

<p>The mosaic is based directly on the most iconic sprite from the game: Link and
his shield strolling sideways through Koholint Island. Here's the colorized
version from the DX version:</p>

<p><img src="/zelda-lego/awakening-link-side-16x16-scaled.png" alt="Link sprite" /></p>

<p>I tried making a Link mosaic from my second favorite, Zelda: Link to the Past.
Unfortunately, a 16-bit color palette is way harder to replicate with LEGO's
limited set of colors, and the result isn't worth sharing.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Comparing classical music interpretations</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/classical-interpreters"/>
    
    <updated>2018-09-20T09:00:00-00:00</updated>
    
    <id>https://smus.com/classical-interpreters</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>I built an audio player to easily compare multiple interpretations of the same
piece. Here's an interactive <a href="https://borismus.github.io/classical-interpreter/">demo</a>, and a video to give you a sense of how it
works:</p>

<p><video src="/classical-interpreters/two-goulds.mp4" style="display: block; margin: 0 auto;" controls /></p>

<!--more-->

<h2>What does it mean to interpret classical music?</h2>

<p>At first glance, sheet music is prescriptive: the composer has provided all of
the notes, the dynamics (forte, piano), tempo (lento, presto) and changes in
tempo (de/accelerando).</p>

<p>In practice, however, the interpreter has a lot of leeway. In some extreme
cases, such as the <a href="https://en.wikipedia.org/wiki/Cadenza">Cadenza</a> in solo concertos, the performer gets to
improvize a melody based on a chord progression. Some pieces include
ornamentation (eg. trills, etc) which are largely left up to the performer to
interpret.</p>

<p>That said, cadenzas and ornaments are somewhat rare. In general, every piece is
under-specified by the composer. This gives the performer a lot of leeway to
express themselves through the performance, selecting tempo, phrasing,
articulation and tone.</p>

<h2>Example: Bach's Goldberg Variations</h2>

<p>The Goldberg Variations were composed by Johann Sebastian Bach in 1741, and then
popularized by Glenn Gould in his debut album in 1955, transforming a work once
considered esoteric into one of the most iconic piano recordings.</p>

<p>In 1981, a year before his death, Gould recorded the pieces again. After a long
period of reclusion, he was able to revisit the variations and produce a
completely different take. In an interview, he said:</p>

<blockquote>
  <p>...since I stopped playing concerts, about 20 years, having not played it in
  all that time, maybe I wasn't savaged by any over-exposure to it...</p>
</blockquote>

<h2>Compare Gould's 1955 and 1981 recordings</h2>

<p>Both the <a href="https://youtu.be/Cwas_7H5KUs?t=1m55s">1955</a> and <a href="https://www.youtube.com/watch?v=zpsfhTxo5yw&amp;t=173s">1981</a> recordings are available on YouTube, of course.
I found that listening to two distinct performances is not the same as having one
integrated player. So I built one: a player specifically for comparing multiple
interpretations of the same piece.</p>

<p>Here is a demo that lets you compare the first variation from the Goldberg
Variations. <a href="https://borismus.github.io/classical-interpreter/">Try it out here</a>. You can use your keyboard to skip between
interpretations (↑, ↓) just as easily as you can seek within a track (←, →).
The mouse works as well. Note that I haven't tested at all on mobile. Sorry,
it's just a prototype and I'm on paternity leave 😇</p>

<h2>I also tried it on Mozart's Requiem</h2>

<p>I am a huge fan of Mozart's Requiem, and once came across an <a href="https://www.reddit.com/r/classicalmusic/comments/1xpqyh/what_is_the_best_recorded_performance_of_mozarts/">online thread
debating</a> which conductor's performance was the best. I soon
found myself listening to a dozen or so different versions of the same piece.
When I was a younger music appreciator, I would often wonder what the point of
a conductor <em>really</em> was. I no longer have this question.</p>

<p>Just to give you a taste for how different the interpretations are, here's an
example of three conductors performing the Introitus, the first movement in the
Requiem. <a href="https://borismus.github.io/classical-interpreter/?json=https://splendid-society.surge.sh/index.json">Check it out here</a>, but be patient as it may take a
minute to load and decode the audio. Böhm's brooding tempo and lumbering chorus
(ugh) contrasts especially well with Levin's crisp and minimalist take.</p>

<p><video src="/classical-interpreters/three-requiems.mp4" style="display: block; margin: 0 auto;" controls /></p>

<h2>Technical details</h2>

<p>For this prototype, I focused on creating a reasonable UI to play back and
interact with multiple time-aligned performances of the same piece. An <a href="https://borismus.github.io/classical-interpreter/goldberg/index.json">index
file</a> specifies metadata for each track, most importantly the URL to
the label file and the URL to the audio file. Each <a href="https://borismus.github.io/classical-interpreter/goldberg/gould-1955.txt">label file</a> is a
text file with lines in the format <code>START_TIME  END_TIME BAR_NUMBER</code>. </p>

<p>To create the label files, I manually annotated the waveform. Even with
Audacity's extremely useful <a href="https://manual.audacityteam.org/man/label_tracks.html">label track</a> feature, it was a lot
of manual work to go through the score, and find each bar's time
range in each recording. At the end of the day, I had start and end times for
each bar. For times that don't fall exactly on bar lines, I linearly interpolate
between the bar boundaries, which works reasonably well, but is sometimes a bit
off. More granular timing references would address this better, but that
currently means doing more manual labor. No thanks!</p>

<h3>Science, help me automate this, please</h3>

<p>An obvious question is how to automate the labor of synchronizing a recording to
a score. In general, I think this is an unsolved problem, especially for complex
tracks containing hundreds of instruments and varying levels of background
noise.</p>

<p>An promising approach that could work for solo piano music might be to use
something like <a href="https://magenta.tensorflow.org/onsets-frames">Onsets and Frames</a> to extract piano rolls and
then apply something like a Dynamic Time Warp (DTW) in piano roll space.  A more
general approach might be to synthesize each bar into raw audio (from MIDI), and
then align recordings to synthesized audio using something like DTW based on a
Constant-Q transform (CQT).</p>

<p>My brief and ill-guided attempts to <a href="https://musicinformationretrieval.com/dtw_example.html">do something like this</a> on real-world
examples didn't yield good enough results. Any ML/DSP experts want to take this
on?</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Web-based voice command recognition</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/web-voice-command-recognition"/>
    
    <updated>2018-01-04T09:00:00-00:00</updated>
    
    <id>https://smus.com/web-voice-command-recognition</id>
    <content type="html">
      <![CDATA[
      <div>
        <p><a href="/web-audio-ml-features">Last time</a> we converted audio buffers into images. This time
we'll take these images and train a neural network using
<a href="https://deeplearnjs.org">deeplearn.js</a>. The result is <a href="https://google.github.io/web-audio-recognition/inference-demo/?model=yesno">a browser-based demo</a> that 
lets you speak a command ("yes" or "no"), and see the output of the classifier
in real-time, like this:</p>

<p><video src="/web-voice-command-recognition/inference-demo.mp4" autoplay muted loop type="video/mp4"
style="width: 100%"></p>

<p>Curious to play with it, see whether or not it recognizes <em>yay</em> or <em>nay</em> in
addition to <em>yes</em> and <em>no</em>? <a href="https://google.github.io/web-audio-recognition/inference-demo/?model=yesno">Try it out live</a>. You will quickly see
that the performance is far from perfect. But that's ok with me: this example is
intended to be a reasonable starting point for doing all sorts of audio
recognition on the web. Now, let's dive into how this works.</p>

<!--more-->

<h1>Quick start: training and testing a command recognizer</h1>

<p>Here's how you can train your own yes/no classifier:</p>

<ol>
<li>Go to the <a href="https://google.github.io/web-audio-recognition/train-model/?data_url=https://storage.googleapis.com/audio-recognition-data&amp;data_extension=mp3">model training page</a>. It will take a bit of time to download
the <a href="https://storage.googleapis.com/audio-recognition-data/yes.mp3">training</a> <a href="https://storage.googleapis.com/audio-recognition-data/no.mp3">data</a> from the server.</li>
<li><p>Click the train button, and you'll see a graph showing training progress.
Once you are ready (this will take a while, perhaps 500 iterations or 2
minutes, depending on your hardware), stop training, and press the save
weights (file) button.  This will download a JSON file.</p>

<p><img src="/web-voice-command-recognition//web-voice-command-recognition/2-training-graph.png" alt="Training graph" /></p></li>
<li><p>Then go to <a href="https://google.github.io/web-audio-recognition/inference-demo/?model=yesno">the inference demo page</a>, press the load weights (file) button
and select the downloaded JSON file to load the trained model.</p></li>
<li><p>Flip the switch, grant access to the microphone and try saying "yes" or "no".
You'll see microphone and confidence levels indicated at the bottom of the
page.</p>

<p><img src="/web-voice-command-recognition/3-inference.png" alt="Inference screenshot" /></p></li>
</ol>

<p>The above is a mechanistic account of how the training example works.  If you
are interested in learning about the gory (and interesting) details, read on.</p>

<h1>Data pre-processing and loading</h1>

<p>Training a neural net requires a lot of training data. In practice, millions of
examples may be required, but <a href="https://storage.cloud.google.com/download.tensorflow.org/data/speech_commands_v0.01.tar.gz">the dataset</a> we'll be using is small by
modern standards, with <em>just</em> 65,000 labeled examples. Each example is a
separate wav file, with the label in the filename.</p>

<p>Loading each training wav as a separate request turned out to be quite slow. The
overhead from each request is small, but when compounded over a few thousand
times, really starts to be felt. An easy optimization to load data more quickly
is to put all examples with the same label into one long audio file. Decoding
audio files is pretty fast, and so is splitting them into one second long
buffers. A further optimization is to use a compressed audio format, such as
mp3. <a href="https://github.com/google/web-audio-recognition/blob/master/train-model/scripts/preprocess.py"><code>scripts/preprocess.py</code></a> will do this concatenation for you,
producing this <a href="https://storage.googleapis.com/audio-recognition-data/yes.mp3">mesmerising result</a>.</p>

<p>After we "rehydrate" our raw audio examples, we process buffers of raw data into
features. We do this using the <a href="/web-audio-ml-features">Audio Feature extractor</a> I
mentioned in the <a href="/web-audio-ml-features">last post</a>, which takes in raw audio, and
produces a log-mel spectrogram. This is relatively slow, and accounts for most
of the time spent loading the dataset.</p>

<h1>Model training considerations</h1>

<p>For the yes/no recognizer, we have only two commands that we care about: "yes",
and "no". But we also want to detect the lack of any such utterances, as well as
silence. We include a set of <a href="https://storage.googleapis.com/audio-recognition-data/other.mp3">random utterances</a> as the "other" category
(none of which are yes or no). This example is also generated by the
<a href="https://github.com/google/web-audio-recognition/blob/master/train-model/scripts/preprocess.py">preprocessing script</a>.</p>

<p>Since we're dealing with real microphones, we never expect to hear pure silence.
Instead, "silence" is some level of ambient noise compounded by crappy
microphone quality. Luckily, the training data also includes background noise
which we mix with our training examples at various volumes. We also generate a
set of silence examples, which includes only the background audio.  Once we've
prepared our samples, we generate our final spectrograms as our input.</p>

<p>To generate these final spectrograms, we need to decide on buffer and hop
length. A reasonable buffer length is 1024, and a hop length of 512. Since we
are dealing with sample rate of 16000 Hz, it works out to a window duration of
about 60ms, sampled every 30ms.</p>

<p>Once we have labeled spectrograms, we need to convert inputs and labels into
deeplearn arrays. Label strings "yes", "no", "other", and "silence" will be
<a href="https://en.wikipedia.org/wiki/One-hot">one-hot</a> encoded as a <code>Array1D</code>s of four integers, meaning that "yes"
corresponds to <code>[1, 0, 0, 0]</code>, and "no" to <code>[0, 1, 0, 0]</code>. Spectrograms from the
feature extractor need to be converted into an <code>Array3D</code>, which can be fed as
input for the model.</p>

<p>The model we are training consists of two convolution layers, and one fully
connected layer. I took this architecture directly from the MNIST example
of deeplearn.js, and hasn't been customized for dealing with spectrograms at
all. As a result, performance is a far cry from state of the art speech
recognition. To see even more mis-classifications, try out <a href="https://google.github.io/web-audio-recognition/inference-demo/?model=number">MNIST for
audio</a> which recognizes spoken digits (eg. "zero" through "ten").
I am confident that we could do better by following <a href="http://www.isca-speech.org/archive/interspeech_2015/papers/i15_1478.pdf">this paper</a>. A
real-world speech recognizer might not use convolution at all, instead opting
for an <a href="https://en.wikipedia.org/wiki/Long_short-term_memory">LSTM</a>, which is better suited to process time-series data.</p>

<p>Lastly, we want to tell the machine learning framework how to train the model.
In ML parlance, we need to set the <a href="https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)">hyperparameters</a>, which includes
setting the learning rate (how much to follow the gradient at each step) and
batch size (how many examples to ingest at a time). And we're off to the races:</p>

<p><img src="/web-voice-command-recognition//web-voice-command-recognition/2-training-graph.png" alt="Training graph" /></p>

<p>During training, the gradient descent algorithm tries to minimize cost, which
you can see in blue. We also plot accuracy in orange, which is occasionally
calculated by running inference on a test set. We use a random subset of the
test set because inference takes time, and we'd like to train as quickly as
possible.</p>

<p>Once we are happy with the test accuracy, we can save the model weights and use
them to infer results.</p>

<h1>Saving and loading model weights</h1>

<p>A model is defined by its architecture and the weights of its weight-bearing
nodes. Weights are the values that are learned during the process of model
training, and not all nodes have weights. ReLUs and flatten nodes don't. But
convolution and fully connected nodes have both weights and biases. These
weights are tensors of arbitrary shapes. To save and load models, we need to be
able to save both graphs <strong>and</strong> their weights.</p>

<p>Saving &amp; loading models is important for a few reasons:</p>

<ol>
<li>Model training takes time, so you might want to train a bit, save weights,
 take a break, and then resume from where you left off. This is called
 checkpointing.</li>
<li>For inference, it's useful to have a self-contained model that you can just
 load and run.</li>
</ol>

<p>At the time of writing, deeplearn.js didn't have facilities to serialize models
and model weights. For this example, I've implemented a way to load and save
weights, assuming that the model architecture itself is hard-coded. The
<a href="https://github.com/google/web-audio-recognition/blob/master/train-model/src/GraphSaverLoader.ts"><code>GraphSaverLoader</code></a> class can save &amp; load from a local store (IndexedDB),
or from a file. Ultimately, we will need a non-hacky way of saving and loading
models and their corresponding weights, and I'm excited for the near future of
improved ML developer ergonomics.</p>

<h1>Wrapping up</h1>

<p>Many thanks to <a href="https://twitter.com/nsthorat">Nikhil</a> and
<a href="https://twitter.com/dsmilkov">Daniel</a> for their hard work on deeplearn.js, and
willingness to answer my barrages of stupid little questions. Also, to
<a href="https://twitter.com/petewarden">Pete</a>, who is responsible for creating and
releasing the <a href="https://storage.cloud.google.com/download.tensorflow.org/data/speech_commands_v0.01.tar.gz">dataset</a> I used in this post. And thank you dear reader,
for reading this far.</p>

<p>I'm stoked to see how this kind of browser based audio recognition tech can be
applied to exciting, educational ML projects like <a href="https://teachablemachine.withgoogle.com/">Teachable
Machine</a>. How cool would it be if you could make a
self-improving system, which trains on every additional spoken utterance? The
ability to train these kinds of models in the browser allows us to entertain
such possibilities in a privacy preserving way, without sending anything to any
server.</p>

<p>So there you have it! This has been an explanation of voice command recognition
on the web. We covered feature extraction in the <a href="/web-audio-ml-features">previous
post</a>, and this time, dug a little bit into <a href="https://google.github.io/web-audio-recognition/train-model/?data_url=https://storage.googleapis.com/audio-recognition-data&amp;data_extension=mp3">model
training</a> and <a href="https://google.github.io/web-audio-recognition/inference-demo/?model=yesno">real-time inference</a> entirely in the browser.</p>

<p>If you build on this example, please drop me a note on
<a href="https://twitter.com/borismus">twitter</a>.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Audio features for web-based ML</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/web-audio-ml-features"/>
    
    <updated>2017-12-15T09:00:00-00:00</updated>
    
    <id>https://smus.com/web-audio-ml-features</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>One of the first problems presented to students of deep learning is to classify
handwritten digits in the <a href="https://www.tensorflow.org/get_started/mnist/beginners">MNIST dataset</a>. This was recently <a href="https://deeplearnjs.org/demos/model-builder/">ported to
the web</a> thanks to <a href="https://deeplearnjs.org">deeplearn.js</a>. The web version has
distinct educational advantages over the relatively dry TensorFlow tutorial.
You can immediately get a feeling for the model, and start building intuition
for what works and what doesn't. Let's preserve this interactivity, but change
domains to audio. This post sets the scene for the auditory equivalent of MNIST.
Rather than recognize handwritten digits, we will focus on recognizing spoken
commands. We'll do this by converting sounds like this:</p>

<p><audio src="/web-audio-ml-features//web-audio-ml-features/left.wav" controls></audio></p>

<p>Into images like this, called log-mel spectrograms, and in the <a href="/web-voice-command-recognition/">next post</a>,
feed these images into the same types of models that do handwriting recognition
so well:</p>

<p><img src="/web-audio-ml-features/final-log-mel-spectrogram.png" alt="Final log-mel spectrogram." /></p>

<p>The audio feature extraction technique I discuss here is generic enough to work
for all sorts of audio, not just human speech. The rest of the post explains
how. If you don't care and just want to <a href="https://github.com/google/web-audio-recognition/tree/master/audio-features">see the code</a>, or <a href="https://google.github.io/web-audio-recognition/audio-features/">play with some
live demos</a>, be my guest!</p>

<!--more-->

<h1>Why?</h1>

<p>Neural networks are having quite a resurgence, and for good reason. Computers
are beating humans at many challenging tasks, from identifying faces and images,
to playing Go. The basic principles of neural nets is relatively simple, but the
details can get quite complex. Luckily non-AI experts can get a feeling for what
can be done because a lot of <a href="http://www.cs.ubc.ca/~van/papers/2017-TOG-deepLoco/">output</a> is <a href="https://www.youtube.com/watch?v=5h4R959O0cY">quite</a> <a href="http://prostheticknowledge.tumblr.com/">engaging</a>.
Unfortunately these demos are mostly visual in nature, either examples of
computer vision, or generate images or video as their main output. And
few of these examples are interactive.</p>

<h1>Pre-processing audio sounds hard, do we have to?</h1>

<p>Raw audio is a pressure wave sampled at tens of thousands times per second and
stored as an array of numbers. It's quite a bit of data, but there are neural
networks that can ingest it directly.  Wavenet does <a href="https://github.com/buriburisuri/speech-to-text-wavenet">speech to
text</a> and <a href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/">text to speech</a> using raw audio sequences,
without any explicit feature extraction. Unfortunately it's slow: running speech
recognition on a 2s example took 30s on my laptop. Doing this in real-time, in
a web browser isn't quite ready yet.</p>

<p>Convolutional Neural Networks (CNNs) are a big reason why there has been so much
interesting work done in computer vision recently. These networks are designed
to work on matrices representing 2D images, so a natural idea is to take our raw
audio and generate an image from it. Generating these images from audio is
sometimes called a frontend in <a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43960.pdf">speech recognition papers</a>. Just to
hammer the point home, here's a diagram explaining why we need to do this step:</p>

<p><img src="/web-audio-ml-features/front-end-diagram.png" alt="Audio processing vs. image processing" /></p>

<p>The standard way of generating images from audio is by looking at the audio
chunk-by-chunk, and analyzing it in the frequency domain, and then applying
various techniques to massage that data into a form that is well suited to
machine learning. This is a common technique in sound and speech processing, and
there are great implementations in <a href="https://github.com/librosa/librosa">Python</a>. TensorFlow even has a
<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/wav_to_spectrogram/wav_to_spectrogram.cc">custom op</a> for extracting spectrograms from audio.</p>

<p>On the web, these tools are lacking. The Web Audio API can almost do
this, using the <code>AnalyserNode</code>, as I've shown <a href="https://borismus.github.io/spectrogram/">in the past</a>, but
there is an important limitation in the context of data processing:
<code>AnalyserNode</code> (nee <code>RealtimeAnalyser</code>) is <a href="https://stackoverflow.com/questions/45697898/web-audio-api-getfloatfrequencydata-function-setting-float32array-argument-data">only for real-time</a> analysis.
You can setup an <code>OfflineAudioContext</code> and run your audio through the analyser,
but you will get unreliable results. </p>

<p>The alternative is to do this without the Web Audio API, and there are
<a href="https://github.com/vail-systems/node-mfcc">many</a> <a href="https://github.com/oramics/dsp-kit">signal processing</a> <a href="https://github.com/corbanbrook/dsp.js/">JavaScript libraries</a> that might
help. None of them are quite adequate, for reasons of incompleteness or
abandonment. But here's an illustrated take on extracting Mel features from raw
audio.</p>

<h1>Audio feature extraction</h1>

<p>I found an <a href="http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/">audio feature extraction tutorial</a>, which I followed closely
when implementing this feature extractor in TypeScript. What follows can be a
useful companion to that tutorial.</p>

<p>Let's begin with an audio example (a man saying the word "left"):</p>

<p><audio src="/web-audio-ml-features//web-audio-ml-features/left.wav" controls></audio></p>

<p>Here's that raw waveform plotted as pressure as a function of time:</p>

<p><img src="/web-audio-ml-features/1-raw-audio.png" alt="Raw audio" /></p>

<p>We could take the FFT over the whole signal, but it changes a lot over time.
In our example above, the "left" utterance only takes about 200 ms, and most of
the signal is silence. Instead, we break up the raw audio signal into
overlapping buffers, spaced a hop length apart. Having our buffers overlap
ensures that we don't miss out on any interesting details happening at the
buffer boundaries. There is an art to picking the right
buffer and hop lengths:</p>

<ul>
<li>Pick too small a buffer, and you end up with an overly detailed image, and
risk your neural net training on some irrelevant minutia, missing the forest
for the trees. </li>
<li>Pick too large a buffer, and you end up with an image too coarse to be useful.</li>
</ul>

<p>In the illustration below, you can see five full buffers that overlap one
another by 50%. For illustration purposes only, the buffer and hop durations are
large (400 ms and 200ms respectively). In practice, we tend to use much shorter
buffers (eg. 20-40 ms), and often even shorter hop lengths to capture minute
changes in audio signal.</p>

<p><img src="/web-audio-ml-features/2-buffer-hop.png" alt="Break-up audio" /></p>

<p>Then, we consider each buffer in the frequency domain. We can do this using an
Fast Fourier Transform (FFT) algorithm. This algorithm gives us complex values
from which we can extract magnitudes or energies. For example, here are the FFT
energies of one of the buffers, approximately the second one in the above image,
where the speaker begins saying the "le" syllable of "left":</p>

<p><img src="/web-audio-ml-features/3-fft-buffer-linear.png" alt="Frequency of buffer" /></p>

<p>Now imagine we do this for every buffer we generated in the previous step, take
each FFT arrays and instead of showing energy as a function of frequency, stack
the array vertically so that y-axis represents frequency and color represents
energy. We end up with a spectrogram:</p>

<p><img src="/web-audio-ml-features/4-fft-spectrogram.png" alt="STFT spectrogram" /></p>

<p>We could feed this image into our neural network, but you'll agree that it looks
pretty sparse. We have wasted so much space, and there's not much signal there
for a neural network to train on.</p>

<p>Let's jump back to the FFT plot to zoom our image into our area of interest. The
frequencies in this plot are bunched up below 5 KHz since the speaker isn't
producing particularily high frequency sound. Human audition tends to be
logarithmic, so we can view the same range on a log-plot:</p>

<p><img src="/web-audio-ml-features/5-fft-buffer-log.png" alt="Frequency of buffer" /></p>

<p>Let's generate new spectrograms as we did in an earlier step, but rather than
using a linear plot of energies, use can a log-plot of FFT energies:</p>

<p><img src="/web-audio-ml-features/9-log-spectrogram.png" alt="STFT log spectrogram" /></p>

<p>Looks a bit better, but there is room for improvement. Humans are much better at
discerning small changes in pitch at low frequencies than at high frequencies.
The Mel scale relates pitch of a pure tone to its actual measured frequency. To
go from frequencies to Mels, we create a triangular filter bank:</p>

<p><img src="/web-audio-ml-features/6-mel-filterbank.png" alt="Mel filter bank" /></p>

<p>Each colorful triangle above is a window that we can apply to the frequency
representation of the sound. Applying each window to the FFT energies we
generated earlier will give us the Mel spectrum, in this case an array of 20
values:</p>

<p><img src="/web-audio-ml-features/7-mel-spectrum.png" alt="Mel spectrum" /></p>

<p>Plotting this as a spectrogram, we get our feature, the log-mel spectrogram:</p>

<p><img src="/web-audio-ml-features/10-mel-spectrogram.png" alt="Mel spectrogram" /></p>

<p>The 1s images above are generated using audio feature extraction software
written in TypeScript, which I've released publicly. Here's a <a href="https://google.github.io/web-audio-recognition/audio-features/">demo</a> that
lets you run the feature extractor on your own audio, and <a href="https://github.com/google/web-audio-recognition/tree/master/audio-features">the code on
github</a>.</p>

<h1>Handling real-time audio input</h1>

<p>By default the feature extractor frontend takes a fixed buffer of audio as
input.  But to make an interactive audio demo, we need to process a continuous
stream of audio data. So we will need to generate new images as new audio comes
in. Luckily we don't need to recompute the whole log-mel spectrogram every time,
just the new parts of the image. We can then add the new parts of spectrogram on
the right, and remove the old parts, resulting in a movie that feeds from the
right to the left. The <a href="https://github.com/google/web-audio-recognition/blob/master/audio-features/src/StreamingFeatureExtractor.ts"><code>StreamingFeatureExtractor</code></a> class implements this
important optimization.</p>

<p>But there is one caveat: it currently relies on <code>ScriptProcessorNode</code>, which is
notorious for dropping samples. I've tried to mitigate this as much as possible
by using a large input buffer size, but the real solution will be to use
<a href="https://drafts.css-houdini.org/worklets/#worklet-section">AudioWorklets</a> when they are available.</p>

<h1>Wrapping up</h1>

<p>An implementation note: here is a <a href="https://thebreakfastpost.com/2015/10/18/ffts-in-javascript/">comparison of JS FFT libraries</a> which
suggests the Emscripten-compiled KissFFT is the fastest (but still 2-5x slower
than native), and the one I used.</p>

<p>Here is a sanity check comparing the output of my web-based feature extractor to
that of other libraries, most notably <a href="https://github.com/librosa/librosa">librosa</a> and from <a href="https://github.com/tensorflow/models/blob/master/research/audioset/mel_features.py">AudioSet</a>:</p>

<p><img src="/web-audio-ml-features/mel-comparison.png" alt="Log mel feature comparison" /></p>

<p>The images resulting from the three implementations are similar, which is a good
sanity check, but they are not identical. I haven't found the time yet, but it
would be very worthwhile to have a consistent cross platform audio feature
extractor, so that models trained in Python/C++ could run directly on the web,
and vice versa.</p>

<p>I should also mention that although log-mel features are commonly used by
serious audio researchers, this is an active area of research. Another audio
feature extraction technique called <a href="https://arxiv.org/pdf/1607.05666.pdf">Per-Channel Energy Normalization
(PCEN)</a> appears to perform better at least in some cases, like processing
far field audio. I haven't had time to delve into the details yet, but
understanding it and porting it to the web also seems like a worthy task.</p>

<p>Major thanks to <a href="http://www.dicklyon.com/">Dick Lyon</a> for pointing out a few bugs in my feature
extraction code. Pick up his <a href="https://www.amazon.com/Human-Machine-Hearing-Extracting-Meaning/dp/1107007534">"Human and Machine Hearing"</a> if you're ready
to delve deeper into sound understanding.</p>

<p>Ok, so to recap, we've generated log-mel spectrogram images from streaming audio
that are ready to feed into a neural network. Oh yeah, the actual machine
learning part? That's the <a href="/web-voice-command-recognition/">next post</a>.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>UIST 2017 highlights</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/uist-2017"/>
    
    <updated>2017-10-28T09:00:00-00:00</updated>
    
    <id>https://smus.com/uist-2017</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>Picking up where I left off <a href="/uist-2014">3 years ago</a> with this year's UIST
highlight reel. As expected, the research creatively applied interesting
principles, but many applications were adorably contrived. Also, I miss academia!</p>

<!--more-->

<p><style>
.container {
    position: relative;
    width: 100%;
    height: 0;
    padding-bottom: 56.25%;
}
.video {
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
}
</style></p>

<h2>WhichFingers / characterizing end-to-end latency</h2>

<p>Both papers used a finger-mounted vibration sensor made from a piezoelectric
polymer, and implemented simple finger tap detection.  One showed how this
method could be used for measuring latency using a blinking screen. They did a
great job of measuring latency at every stage.</p>

<div class="container">
<iframe src="//www.youtube.com/embed/s2iUJsm7JcI" 
frameborder="0" allowfullscreen class="video"></iframe>
</div>

<p>The other paper had vibration sensors for each finger, and did time-based
correlation to determine which finger tapped a screen. They had a 30ms window
within which you had to fall to have unambiguous classification.  Interesting:
they also moved their vibration sensors up to the ring position (third phalanx),
and seemed to also get pretty good results. Also nice is that they <a href="http://cristal.univ-lille.fr/~casiez/whichfingers/">open sourced
everything</a>, and that the whole system is cheaper than $35.</p>

<h2>Carpacio: unobtrusive passenger &amp; driver ID</h2>

<p>The cheesily named project served to repurpose capacitative screens to
differentiate between drivers and passengers. Motivated by driver distraction,
cars are now shipping with on-screen UIs that are locked until you answer an
onscreen question: are you a driver or passenger?  This is counter-productive.
Edward Jay Wang from UW showed how the car system can identify who is
interacting with it automatically. Both seats are instrumented with signal
transmitters, transmitting a unique signal through each of the sitting people,
and the screen is instrumented with a receiver. The rest is signal correlation.
Then they did validation and got really good results (for research): &gt; 99%
accuracy.</p>

<p>Zooming out for a second, I really wish that this wasn't a problem that needed
to be solved. Physical controls work better in a car, and allow the driver to
interact with them more easily.</p>

<h2>Grabity: ungrounded haptics for grasping</h2>

<p>This project is an end-to-end look at an ungrounded device that provided haptic
feedback for grasping an object with your hand. They detailed each stage:
grabbing it, picking it up, feeling its weight, then moving it around and
feeling its inertia.</p>

<p><img src="/uist-2017/grabity.jpg" alt="Grabity steps in action" /></p>

<p>Weight is perceived by multiple senses in the human body: muscle spindles (eg.
in biceps), golgi tendons (eg. in elbow), mechanoreceptors sensing shear (eg. in
palm and fingers). Their idea: only use mechanoreceptors in hand, which lets you
simulate the equivalent of about 15g of weight. It was cool that they compared
the sensation to an actual weight with real users. I've seen prior work which
relies on asymmetric vibration to create virtual forces through skin
displacement. A few limitations: it looks pretty wacky, and users thought that
the constant vibration was annoying, but pretty cool tech demo.</p>

<h2>Dodecapen: 6DOF Pose Tracking System</h2>

<p>This project puts a dodecahedron-shaped AR marker on the end of a regular pen. A
cube didn’t work because of pose ambiguity, and other Platonic solids have
triangular faces that are relatively small in area.</p>

<div class="container">
<iframe src="//www.youtube.com/embed/7Xczpq4VkHM" 
frameborder="0" allowfullscreen class="video"></iframe>
</div>

<p>3D printing a perfect dodecahedron is hard, so they needed a calibration step
for it. The pen-tip also needs to be calibrated, since pens vary in size and
shape. Also the paper surface itself had to be calibrated. That's a lot of
calibrations! They used a mocap system with 8 markers to create ground truth.
Looked at the mean shortest distance and saw mm-level precision. They claim that
precision is comparable to a mocap system with 10 active cameras.</p>

<p>I also tried the demo, which worked very well. One caveat is that it relies on a
global shutter camera, so not sure how well it generalizes to smartphones yet.
Another big limitation here is that the dodecahedron needs to be in the camera’s
FOV.</p>

<h2>Inviso: 3D design environment to create sonic VR</h2>

<p>Anil Camci showed a web-based GUI to define point sources as well as regions for
playing back ambisonic soundscapes. Point sources could have multiple cones that
you could attach with a graphical tool, and also adjust their direction, angles,
and amplitude. Each point source, and the observer could also be animated over a
trajectory. You could then play it back through the Web Audio API. Most of the
design is done from a top-down view, since this is a good match to human
auditory perception, which is much better at lateral than vertical
source differentiation.</p>

<p><img src="/uist-2017/inviso.jpg" alt="Inviso screenshot" /></p>

<p>In the end he showed multiple examples of people using the system and creating
complex auditory scenes, including overlapping zones, and animated, multi-coned
point sources. Seems to be quite efficient for building up immersive auditory
environments.</p>

<p>I encouraged them to recreate classic spatial audio demos, like my favorite
barbershop example. The editor is <a href="http://inviso.cc/">available online</a>, and
just recently <a href="https://github.com/CreativeCodingLab/Inviso">open sourced</a>.</p>

<h2>SoundCraft: smart watch interaction using hand generated acoustics</h2>

<p>This project features a microphone array that fits onto a watch form factor.
They can localize sounds made by the other hand against itself (what is the
sound of one hand clapping?) or against another surface. Localization gives you
angles relative to the watch (angular resolution ~15 deg).</p>

<div class="container">
<iframe src="//www.youtube.com/embed/O1G-j1EBQh0" 
frameborder="0" allowfullscreen class="video"></iframe>
</div>

<p>The applications were kind of weak, but having a small aperture (1/2 inch
between microphones) and wearable form factor was an interesting idea. They also
showed tracking the lateral position of a finger on the same hand as the watch,
and using the watch as an audio marker for an AR application.</p>

<h2>Data storage &amp; interaction using magnetized fabric</h2>

<p>Justin Chan from UW showed what can be achieved when conductive fabric (woven
out of conductive thread) is magnetized using a permanent magnet. First they
explored various properties of the medium, like how long it stays magnetized
(28% decrease per week), how far the field can be detected from (up to 1cm), how
its properties vary with weave pattern (better for dense patterns), and whether
magnetization decreases from washing (not very much). The basic idea is that you
can encode data onto a conductive thread by having conductive segments that can
be magnetized interleaved with insulated segments. A binary string is then
encoded by mapping 0 to one polarity and 1 to the other.</p>

<p><img src="/uist-2017/magnetized-fabric.jpg" alt="Magnetized fabric" /></p>

<p>They also showed use cases, such as writing and reading data from the clothing,
or even engraving data images (smallest pixel using a regular loom was 1.8cm^2)
onto clothing. They also showed some interactive examples where the clothing can
be read by a magnetometer on a phone or watch. This is interesting especially
because it doesn't require any batteries, and can be achieved using
off-the-shelf embroidery machines. </p>

<h2>iSoft: soft touch and stretch sensor</h2>

<p>This project uses electrical impedance tomography (EIT, which I know nothing
about, and wasn't explained) to detect touch and stretch in (1D) in a special
elastomer which is designed to be affordable. Discrete contact accuracy was
really good: 96%, using a small dataset. Distance error was ~10% of sensor size.</p>

<div class="container">
<iframe src="//www.youtube.com/embed/JVaYEl9nbME" 
frameborder="0" allowfullscreen class="video"></iframe>
</div>

<p>Applications (as usual, weak): a bunch of instrumented objects (lamp, cup,
clothing, travel pillow, etc). Limitations: hard to make it big, hard to make it
arbitrarily shaped, doesn’t support multidimensional stretching.</p>

<h2>Mutual human actuation</h2>

<p>I missed most of the VR sessions because parallel tracks were more interesting,
but caught this one because it was super clever. Previously, Haptic Turk used
support people to lift and lower the player immersed in VR. TurkDeck was an
environment for players that is constructed by support people outside VR.
However, previous research found that players enjoyed the experience more than
the support people, so Mutual Turk (presented here) made all participants into
players. Two people are in VR, but they are playing different games that are
complimentary in force (eg. A is fishing, B is flying a kite.)</p>

<div class="container">
<iframe src="//www.youtube.com/embed/JKGdQjx-_BI" 
frameborder="0" allowfullscreen class="video"></iframe>
</div>

<p>This generalizes to two things: 1. shared props through which users exchange
forces, and 2. synchronization of state between the people involved. They had a
few other examples, like having one player fan a campfire, which created wind
for the other player in the middle of a storm. As far as practicality, obviously
this requires multiple people to be co-present, and also requires multiple
players to synchronize up to the action sequence.</p>

<h2>Demos, Posters, Contests</h2>

<p>Apparently you can print piezoelectric elements. I tried a set that emits low
frequency vibrations for haptic feedback. But you need a special print process
(won’t work on an inkjet).</p>

<p>Artem Dementyev built a robot that walks along human skin using pneumatics.
Supposed applications in health care.</p>

<p>Pepper’s cone was a great illusion from Steve Seitz’s student. Nice improvement
over the usual ghost setup, which has four discrete viewing points, where here
you can pick any point in a circle.</p>

<p>Gierad showed a poster about a multimodal sensing platform that plugs right into
the wall and which can track all sorts of activity in the house, using high
frequency accelerometers, microphones, and other sensors.</p>

<p>Hanchuan Li from UW showed a poster summarizing some of his PhD projects,
including passive sensing, IR based user localization, and various power
harvesting techniques.</p>

<p>Jihyeon Janel Lee showed her poster which automatically created accompanying
visualizations for travel podcasts via geographic entity extraction.</p>

<p>The good folks at i.am showed a way of making TTS sound better by extracting
contours from speech spoken by a person onto a TTS-synthesized example.</p>

<p>The student design contest featured creative ideas implemented with the help of
an Arduino Braccio robot arm. One project required multiple people to operate
one arm. Each person had to do a gesture to operate one degree of freedom of the
arm. Another project involved two front body-mounted robot arms that fed the
other person. Yet another featured an improv show where an actor operated a sock
puppet, and a robot arm spoke lines that were crowdsourced by the audience.</p>

<h2>Keynotes...</h2>

<p><strong>Gabriella Coleman</strong> kicked off UIST 2017 with a slightly off-topic, but
nevertheless interesting (and timely) keynote, summarizing hacker culture from a
historical and anthropological perspective. Much of it was review for me, but I
also learned a fair amount:</p>

<ul>
<li>The origin of "hacker" comes from the Tech Model Railroad Club from MIT. Now
James' fascination with model railroads is crystallizing.</li>
<li>Some of the earlier phreakers were blind, and phreaking was empowering. Some
were (sometimes additionally) gifted with perfect pitch, and could whistle the
correct frequency to emulate phone network signals.</li>
<li>The speaker's personal beliefs were very anti-US, and she was very aligned
with the hackers. Perhaps it is not coincidental that she is at McGill now.</li>
</ul>

<p><strong>Upon receiving a lasting impact award</strong>, Ivan Popyrev gave a short summary of his
storied career. I liked his frankness on the topic of VR: "With every VR fad,
the next fad is the AR fad". At one point he also suggested that we need to
break away from “heavy reliance on rich visual feedback and undivided attention
from the user" that are common in today’s UIs. I completely agree. </p>

<p>I liked his succinct vision: "How can we make the whole world augmented and
interactive?", which he then elaborates on as having two parts: </p>

<ol>
<li>Augment physical, tactile &amp; proprioceptive channels of human augmentation &amp; actuation.</li>
<li>Deliver novel UI at scale by integrating them into everyday physical objects.</li>
</ol>

<p>I should come up with one for my HCI work. </p>

<p><strong>Niki Kittur</strong> closed off the conference with a summary of his crowdsourcing work,
and I was quite glad to see CrowdForge prominently mentioned in the beginning.
He did a good job weaving a coherent narrative throughout the last five years of
his career, which covered disparate topics such as peer production,
crowdsourcing and web browsing. I often wonder how much of that storytelling is
premeditated, and how much of it is done post-hoc. He showed off some neat
projects, including Alloy (name perhaps forge-related?) and KA, as well as
<a href="https://iotabrowser.com">Iota, a task-oriented browser</a> inspired by V. Bush's vision of people
leaving trails of knowledge for one another.</p>

<p>I skipped the last few UISTs, and it was nice to say hi to some familiar faces,
especially from CMU. I also met some great people, including Danny Fan from an
<a href="http://i.am">audio startup</a> in LA, some talented grad students and fortuitously
re-connected with old acquaintances (Hi Trevor) who are apparently also based in
Seattle. </p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Memento Mori</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/memento-mori"/>
    
    <updated>2017-10-19T09:00:00-00:00</updated>
    
    <id>https://smus.com/memento-mori</id>
    <content type="html">
      <![CDATA[
      <div>
        <blockquote>
  <p>The association of sundials with time has inspired their designers over the
  centuries to display mottoes as part of the design. Often these cast the
  device in the role of memento mori, inviting the observer to reflect on the
  transience of the world and the inevitability of death. – <a href="https://en.wikipedia.org/wiki/Sundial#Sundial_mottoes">Wikipedia</a></p>
</blockquote>

<p><img src="/memento-mori/watch-screenshot.png" alt="WatchKit screenshot" 
  style="margin: 0 auto; display: block; width: 200px" /></p>

<p>This rich tradition is now available on <a href="https://itunes.apple.com/us/app/memento-mori-apple-watch/id1294913922">Apple Watch</a>.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Filter playground</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/filter-playground"/>
    
    <updated>2017-08-10T09:00:00-00:00</updated>
    
    <id>https://smus.com/filter-playground</id>
    <content type="html">
      <![CDATA[
      <div>
        <script src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=AM_HTMLorMML"></script>

<p><style>
iframe {
  width: 640px;
  height: 480px;
  border: 0;
}
</style></p>

<blockquote>
  <p>"You don't understand anything until you learn it more than one way." – Marvin Minsky</p>
</blockquote>

<p>In my <a href="http://chimera.labs.oreilly.com/books/1234000001552/">short Web Audio book</a>, I covered the <code>BiquadFilterNode</code>, but didn't
have any sense for how it worked. As I sat down to read <a href="https://www.amazon.com/Human-Machine-Hearing-Extracting-Meaning/dp/1107007534">Human and Machine
Hearing</a>, it became clear that I needed to catch up on some digital
filtering fundamentals.</p>

<p>What follows is an introduction to digital filters via <a href="http://explorabl.es/">explorable
explanation</a> I built to help myself better understand some DSP concepts.
The approach I took was to try to present the concept as visually and aurally as
possible, maximizing opportunities to build intuition. I learned a lot in the
process. Read on for a introduction, jump ahead to the <a href="https://borismus.github.io/filter-playground/">Filter Playground</a>,
or check out this video:</p>

<iframe width="600" height="338" src="//www.youtube.com/embed/6OIOTpQYsts?rel=0&amp;controls=0&amp;showinfo=0" frameborder="0" allowfullscreen></iframe>

<!--more-->

<h1>Cycles are everywhere</h1>

<p>The world is full of examples of cyclical phenomena. Large cycles include
planetary motion, seasons, tides, and ocean waves. Societies are governed by
cycles: empires rise and fall, economies boom and bust, and fashion keeps
repeating itself. On an individual scale, the human lifecycle, eating and
sleeping, heartbeats and breathing, and locomotion are all periodic.  And so are
sound and light, the very nature of the world we percieve.</p>

<p>Wouldn't it be nice to understand and manipulate cyclical phenomena? A couple
hundred years ago, <a href="https://en.wikipedia.org/wiki/Fourier_transform">Fourier showed</a> that any repeating periodic signal,
regardless of its complexity, can be represented as a sum of sine functions,
paving the way for much deeper signal understanding. More recently, <a href="https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem">Shannon
showed</a> that you can take analog signals and represent them as numbers,
which brings us to digital signal processing. For modifying digital signals, we
need to look to digital filters.</p>

<h1>The surprising link between periodic signals and difference equations</h1>

<p>Bear with me on a brief mathematical tangent. Let's start with a simple
difference equation that gives you an output sequence of numbers y[n] given
an input sequence x[n]:</p>

<div>
`y[n] = bx[n] + ay[n - 1]`
</div>

<p>Given `x = [1, 2, 3], a = b = 0.5`, we can calculate y[n] with simple
arithmetic, assuming that `y[-k] = 0`:</p>

<div>
`y[n] = 0.5 x[n] + 0.5 y[n - 1]`
`y[0] = 0.5 x[0] + 0.5 y[-1] = 0.5 * 1 + 0.5 * 0 = 0.5`
`y[1] = 0.5 x[1] + 0.5 y[0] = 0.5 * 2 + 0.5 * 0.5 = 1.25`
`y[2] = 0.5 x[2] + 0.5 y[1] = 0.5 * 3 + 0.5 * 1.25 = 2.125`
</div>

<p>And so, given `x[n] = [1, 2, 3]` and the above difference equation, we get
`y[n] = [0.5, 1.25, 2.125]`. Computers can do this sort of arithmetic really
really quickly. But so what? And what does this have to do with our goal of
manipulating periodic signals in general? Let's explore a bit more generally.</p>

<p>We can take any function x(t) and sample it numerically to get a sequence x[n].
For example, if we take `x(t) = t^2` and sample every integer from 1 to 10, we
would get `x[n] = [x(1), x(2), ..., x(10)]`, in blue. Next, calculate y[n] and
color it green:</p>

<p><img src="/filter-playground/sampling.png" alt="x(t) = t^2 sampled for the first 10 integers" /></p>

<p>Let's take several x(t), sample them into x[n], and see what the above
difference equation gives us for y[n]. When we plot x[n] in blue and y[n] in
green, we get the following graphs:</p>

<p><img src="/filter-playground/math-filter-plot.png" alt="Several discretized mathematical functions with x in blue, and y in
green" /></p>

<p>The first row shows the result of our difference equation on some mathematical
functions: `t^2`, `2^t`, `tan(0.1 t)`, and very little relationship
between x and y. The second row shows some sinusoidal functions with varying
period and phase in blue, sampled and run through the same difference equation
and shown in green. The pattern starts to become pretty clear: sine functions
are special in the same way. The generalized result is surprising and
awesome: if you take any sine function, and feed it into any difference
equation, you end up with another sine function  with the same frequency, but a
different amplitude (<a href="https://en.wikipedia.org/wiki/Linear_time-invariant_theory#Exponentials_as_eigenfunctions_2">gory details here</a>).</p>

<p>We've looked at just one specific difference equation: `y[n] = 0.5 x[n] + 0.5
y[n-1]`. Let's look at difference equations in general, and see how they
affect the relationship between x[n] and y[n]. </p>

<h1>Transfer functions describe the behavior of filters</h1>

<p>We just saw that for an input sine function with a frequency and amplitude, a
difference equation will produce an output sine function with another amplitude.
In the `sin(0.1 t)` graph above, we can see that the amplitude of the green
graph is 0.5. If we look across all frequencies `x(t) = sin(omega t)` and see
how difference equations change the amplitude for all `omega`. This will give
us the frequency response for this specific difference equation. But what
happens in general? </p>

<p>If we bring our difference equation into a canonical form, we can apply the
<a href="https://en.wikipedia.org/wiki/Z-transform">Z-transform</a> and get a corresponding transfer function. The math to
derive this is complicated, but the bottom line is that for any difference
equation in the general form:</p>

<div>
`y[n] = 1/a_0 (b_0 x[n] + b_1 x[n - 1] + ...) - (a_1 y[n - 1] + a_2 y[n - 2] + ...)`
</div>

<p>The corresponding transfer function is of this form:</p>

<div>
`H(z) = (b_0 + b_1 z^(-1) + b_2 z^(-2) + ...) / (a_0 + a_1 z^(-1) + a_2 z^(-2) + ...)`
</div>

<p>Let's go back to our example, `y[n] = ax[n] + by[n − 1]`. We can
see that this fits the general form, with `a_0 = 1`, `b_0 = b`, `a_1 =
a`, with all of the other `a_i = b_i = 0`. Plugging in these values, the
transfer function for this example is:</p>

<div>
`H(z) = (b)/(1 + (-a)z^(-1)) = (bz)/(z - a)`
</div>

<p>This transfer function can tell us a lot about the behavior of the difference
equation, and ultimately its frequency response.</p>

<h1>Visual intuition around transfer functions</h1>

<p>But what the heck does this transfer function tell us?! Let's try to build some
visual intuition. Firstly you see that our H(z) is a rational polynomial.
This means the numerator and denominator both have roots.  Numerator roots are
called zeros and denominator zeros are called poles. From high school math,
recall that we can plot roots on a number line. Except in this case, we're also
interested in imaginary roots, and we can plot them on the complex plane. O's
represent zeros, X's represent poles. For our H(z), the numerator is `0.5 z`,
which has a root (zero) at zero, and the denominator is `z - 0.5` with a root
(pole) at 0.5, and so plotting poles and zeros, we get:</p>

<iframe src="https://borismus.github.io/filter-playground/?view=pole-zero&equation=(0.5z)/(z-0.5)&mutable=false"></iframe>

<p>Here's how you can think about zeros and poles: if z is really close to a zero,
|H(z)| will be close to zero. If it's really close to a pole, |H(z)| will blow
up, approaching infinity. Now, imagine you took a rubber sheet and draped it
over the complex plane. Now put stones where the O's are, and telescoping tent
poles where the X's are, and then extended the tent poles to be really tall. You
would end up with a circus tent. This is the surface that we get by plotting
complex values of z (along the x-y plane) and using |H(z)| as the height
corresponding to each z.</p>

<iframe src="https://borismus.github.io/filter-playground/?view=transfer-function&equation=(0.5z)/(z-0.5)&tfmode=COMPLEX&mutable=false"></iframe>

<p>We can recover the pole-zero diagram by taking a birds-eye view at the same
circus tent, and plotting the poles and zeros on the xy-plane:</p>

<iframe src="https://borismus.github.io/filter-playground/?view=transfer-function&equation=(0.5z)/(z-0.5)&tfmode=POLEZERO&mutable=false"></iframe>

<p>The neat thing about this circus tent is that it tells you the frequency
response of the filter. To do that, we look at the unit circle as it sits on top
of the circus tent diagram:</p>

<iframe src="https://borismus.github.io/filter-playground/?view=transfer-function&equation=(0.5z)/(z-0.5)&tfmode=BODE&mutable=false"></iframe>

<p>Which we can unwrap into a frequency response plot by looking at `|H(e^(i
omega))|` with `omega in [0, pi]`:</p>

<iframe src="https://borismus.github.io/filter-playground/?view=bode-plot&equation=(0.5z)/(z-0.5)"></iframe>

<p>Now let's see to what our example filter does to a white noise sample. Here is
the frequency response of the sample played through our filter according to a
Web Audio <code>AnalyserNode</code> (you can also <a href="https://borismus.github.io/filter-playground/?equation=(0.5z)/(z-0.5)">hear the filter in the Filter
Playground</a>):</p>

<iframe src="https://borismus.github.io/filter-playground/?view=audio-player&view=audio-visualizer&equation=(0.1z)/(z-0.9)&muted=true"></iframe>

<p>Since noise has equal power across the frequency range, it is a good end-to-end
test. We expect the <code>AnalyserNode</code>'s frequency response to line up closely with
the bode plot in the previous figure, and they do.</p>

<p>The filter we created above is a low pass filter, meaning that it allows low
frequencies to pass, but attenuates high frequencies. Let's look at other kinds
of filters.</p>

<h1>More complex filters</h1>

<p>Our first filter had one pole and one zero. What happens if we make a filter
with two poles and two zeros? Here's an example:</p>

<p>`H(z) = (z^2-1)/(z^2-1.975z+0.99) `</p>

<iframe src="https://borismus.github.io/filter-playground/?view=pole-zero&equation=(z^2-1)/(z^2-1.975z+0.99)&mutable=false"></iframe>

<p>This example has two poles and two zeros, which is also called a Biquad filter.
The Web Audio <code>BiquadFilterNode</code> has <a href="https://www.w3.org/TR/webaudio/#idl-def-BiquadFilterType">8 different filter types</a>, all
of which are implemented with two poles and two zeros.</p>

<p>The filter in question is called bandpass filter, because it allows a band of
frequencies through, and attenuates the rest. Go ahead and open this in <a href="https://borismus.github.io/filter-playground/?equation=(z^2-1)/(z^2-1.975z+0.99)">the
filter playground</a>, and you'll see a variety of views of this
bandpass filter.</p>

<p>The point of a playground isn't just to look at other people playing, it's to
play with them!  So I invite you to try it out. You can generate a biquad filter
by selecting parameters with the filter wizard, or input any H(z) manually,
or move around poles and zeros visually. Using the pole-zero view, you can add
poles and zeros with buttons, or remove them by dragging them far enough out of
the unit circle. Check out <a href="https://youtu.be/6OIOTpQYsts">my YouTube video</a> for more examples of playing
around on the playground.</p>

<h1>Implementation notes and thanks</h1>

<p>I'd like to thank a handful of people for their help on this side project.
Firstly, to Raymond Toy for continuing to update the Web Audio API spec with
useful goodies. This side project wouldn't be possible without the recently
added <code>IIRFilterNode</code>.  Raymond has a few filter-related projects on the web,
including <a href="http://rtoy.github.io/webaudio-hacks/more/filter-design/filter-design.html">Digital Filter Design</a>, which lets you create more complex
digital filters using a cascade of second order filters.</p>

<p>I built the 3D complex function plots with <a href="https://github.com/unconed/mathbox">Mathbox</a>, a very powerful
WebGL based visualization toolbox. If you haven't seen it yet, check out <a href="http://acko.net/blog/how-to-fold-a-julia-fractal/">How to
Fold a Julia Fractal</a>, which is awesome in its own right, but also
illustrates the power of mathbox. It's also a great introduction to complex
numbers, which I glossed over in this here post. Huge credit to Steven Wittens
for both mathbox and the inspiring blog post.</p>

<p>Finally, my thanks to Dick Lyon for writing an <a href="https://www.amazon.com/Human-Machine-Hearing-Extracting-Meaning/dp/1107007534">interesting and challenging
book</a> and responding to my email (squee!), which ultimately inspired this
project.</p>

<h1>A mathematical appendage</h1>

<p>I tried to make the post understandable as possible by reducing analytical math
and leaning heavily on interactive illustrations. Inevitably I have waved my
hands and collected massive mathematical debt along the way, most notably
everything to do with deeply understanding the <a href="https://en.wikipedia.org/wiki/Z-transform">Z-transform</a>, but
also:</p>

<ul>
<li>The filter playground requires poles to be inside the unit circle. If a
pole is outside of the unit circle, the filter <a href="https://www.dsprelated.com/freebooks/filters/Stability_Revisited.html">will become
unstable</a>, meaning that it will tend to blow up a signal that is
fed into it.</li>
<li>High order IIR filters <a href="https://en.wikipedia.org/wiki/Wilkinson%27s_polynomial">tend to become numerically unstable</a>. The
solution is to break up high order rational functions into products of lower
order rational functions, but the filter playground doesn't currently do this.</li>
<li>In most real life applications, complex filters are implemented as a cascade
of first or second order filters chained together. This <a href="http://rtoy.github.io/webaudio-hacks/more/filter-design/filter-design.html">Digital Filter Design
example</a> illustrates it well.</li>
<li>You might have noticed that points on the pole zero plot of the filter
playground are never found floating alone on the complex plane. All of the
coefficients of the numerator and denominator polynomials are real since they
were taken from the original difference equation y[n], so by the <a href="https://en.wikipedia.org/wiki/Complex_conjugate_root_theorem">complex
conjugate root theorem</a>, their roots (poles and zeros) must be either
purely real, or appear in complex conjugate pairs.</li>
</ul>

<h1>Over and out</h1>

<p>Thank you for getting this far, especially because I suspect this post may fall
into a sort of uncanny valley: too technical for a casual reader, and too
trivial for a DSP expert. At the very least, building the filter playground
helped me wrap my head around digital filters. Ultimately I hope the filter
playground can serve as a useful teaching tool for DSP novices.</p>

<p>I'd love to hear whether reading the post, watching the video and playing with
the filter playground helped you better understand digital filters. Please let
me know <a href="https://twitter.com/borismus">via twitter</a> or <a href="http://smus.com/about/">by email</a>.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Climate metaquiz results</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/climate-metaquiz-results"/>
    
    <updated>2017-06-14T09:00:00-00:00</updated>
    
    <id>https://smus.com/climate-metaquiz-results</id>
    <content type="html">
      <![CDATA[
      <div>
        <p><a href="/viewpoint-tolerance-through-curiosity/">Last week</a> I ran a <a href="https://docs.google.com/a/google.com/forms/d/e/1FAIpQLSf98rBi2CgU4b62kBFmAeX989ZGZrLW74cQitDdPT7wG906Xw/viewform">Climate metaquiz</a>, and 123 people responded.
The point of a metaquiz is to test how well political groups know the other
side, while questions on personal beliefs and knowledge about the climate are
secondary. Both the small sample size and potential sampling biases are
important caveats to keep in mind here. All that said, Republicans outperformed
Democrats on the factual part of the quiz, despite their low self-reported
self-confidence.  However, Democrats outperformed Republicans on the <em>metaquiz</em>
part, with Republicans tending to exaggerate levels of climate change-related
handwringing amongst Democrats, as well as their eagerness to exaggerate the
facts in the name of behavior change.</p>

<!--more-->

<p>Sanity checking, the self-reported data matches stereotypes. 60% of Republican
test takers believe climate change is less serious than the scientific
consensus, compared to 6% of Democrats. Democrat test takers were far more
concerned (2.8 on a 5-point likert scale) about climate change affecting them
personally and future generations than Republicans (1.8). Republicans were very
skeptical about water level increases by 2100, predicting an average of less
than 2 feet rise, while democrats were more concerned, predicting on average
more than 3 feet.</p>

<h1>Actual vs. predicted results</h1>

<p>Since this was a metaquiz, the focus was not on the participants' own beliefs
and knowledge of the climate, but their predictions about the other side. The
reason for the first sections was only to establish a baseline. Here are the
aggregated results. I calculate percentage error using this simple formula: <code>%
error = (actual - predicted) / actual</code>:</p>

<iframe src="https://docs.google.com/spreadsheets/d/1vvg3bjG841zwJJWIYhGLvBp40kBBAZUA_mL_tALDlj8/pubhtml
?gid=573016866&single=false&widget=true&headers=false" style="height: 400px"></iframe>

<h2>Interesting findings</h2>

<p>In aggregate, participants underestimated how well the other side did on the
quiz (total 10 points). Republicans predicted 4.84 (actual 5.9) for Democrats,
while Democrats predicted 4.95 (actual 6.39) for Republicans. About 20% of all
respondents thought they did better than the other side, as many thought
that they did worse, while the rest weren't sure. One exception here are
Republicans, who exhibited false modesty: 40% of them thought that the Democrats
would perform better.</p>

<p><strong>Exaggerating facts</strong>: Republicans thought Democrats would be much more
comfortable exaggerating scientific facts to convince others of environmentally
beneficial behavior change (predicted 2.9) more so than Democrats declared
(reported 1.6). On the flip side, democrats didn't think the Republicans would
be comfortable exaggerating scientific facts for the environment (predicted
1.4), and they were right (reported: 1.2). It was pointed out to me that the way
I phrased the question was quite leading: "It's okay to exaggerate scientific
facts in order to convince others to behave in a more environmentally friendly
way." but I'm not sure which way this would bias quiz and meta-quiz takers.</p>

<p><strong>Impact of climate change</strong>: Democrats accurately predicted Republican
lack of concern about climate change affecting them personally (reported: 1.8,
predicted: 1.7) and future generations (reported: 2.7, predicted: 2.3).
Republicans predicted Democrats would be slightly more concerned about the
climate than they were both personally (reported: 2.8, predicted: 3.5) and for
future generations (reported: 4.2, predicted: 4.5). The same trend was repeated
when asked about water level rise by 2100. Democrats correctly predicted that
Republicans would be conservative about future water level rises (reported: 1.8
ft, predicted: 1.8 ft), while Republicans were way off about what Democrats
would think (reported: 3.1 ft, predicted: 4.7 ft). This asymetry is intriguing.</p>

<p>I must again caveat all of this with the fact that 123 quiz takers does not
constitute a statistically significant sample, not to mention selection biases
that come from posting the quiz on twitter and some rationalist forums. Still,
it's interesting to see the consistency with which Republicans tended to
exaggerate Democratic positions more than vice-versa. One explanation is that
Paul Krugman was right <a href="http://cafehayek.com/2011/06/open-letter-to-paul-krugman-2.html">when he said</a> "A liberal can talk coherently
about what the conservative view is because people like me actually do listen."
Another explanation is that this disparity is due to the unique dynamics of
climate change, which is a much more important issue for Democrats than for
Republicans. It would be interesting to do more metaquizzes on other topics that
are more balanced in perceived importantness. Perhaps the metaquiz could be
framed as a "competition in understanding" between the two sides.</p>

<p>I find the metaquiz format to be an interesting one, serving a purpose similar
to the <a href="http://econlog.econlib.org/archives/2011/06/the_ideological.html">Ideological Turing Test</a>, but requiring less effort of everyone
involved (ie. essay reading or writing). Thanks again to the survey takers.  If
you left your email and are curious about how well you did on the metaquiz part,
let me know and I'll send you your personal results.</p>

<p>As always, please send me feedback on the metaquiz concept via twitter or email.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Viewpoint tolerance through curiosity</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/viewpoint-tolerance-through-curiosity"/>
    
    <updated>2017-06-07T09:00:00-00:00</updated>
    
    <id>https://smus.com/viewpoint-tolerance-through-curiosity</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>Polarization isn't necessarily problematic. Strictly defined, it refers to the
divergence of political extremes. In fact, a wider variety of opinions may
actually be a <a href="https://ratedzed.wordpress.com/2017/05/20/the-biased-media/">good situation</a>. Things start to go south when a tribal
us-verus-them mentality takes over, giving rise to an uncharitable view of the
other side. This thinking is especially common among the <a href="http://noahpinionblog.blogspot.com/2017/06/the-shouting-class.html">shouting
classes</a>:</p>

<blockquote>
  <p>Those that disagree with me must be stupid, evil, or both.</p>
</blockquote>

<p>Not only is this incorrect, but adhering to this position is actively bad for
society. It prevents finding common ground and encourages wild policy swings as
power is transfered from one uncompromising faction to the next. The same facts
can generate different viewpoints, each deserving of a spot in the marketplace
of ideas, even if we personally disagree with them.</p>

<p>With <a href="/debaters-friendly-disagreement/">Debaters</a>, Antonio and I tried to bring people that disagree
together. Sadly most people don't want to converse with the other side whom
they perceive to be their mortal enemies. The problem must be approached more
obliquely, taking into account human nature. This post is about using <a href="https://docs.google.com/a/google.com/forms/d/e/1FAIpQLSf98rBi2CgU4b62kBFmAeX989ZGZrLW74cQitDdPT7wG906Xw/viewform">quizzes
like this one</a> to lure people into learning more about the other side
by appealing to a powerful emotion: curiosity.</p>

<!--more-->

<h1>Metaquiz format</h1>

<p>I'm calling it a metaquiz, since it asks both about the test taker's own
knowledge and derived viewpoints, as well as their guesses about how well the
other side did on the same quiz. Interestingly, the only fuzzy thing here are
your own beliefs. Knowledge about other people's beliefs is factually
verifiable.</p>

<pre>
1. What do you believe about the issue?
2. What do you know about the issue?
3. What do you know about the other side?
     Evil: what does the other side believe about the issue?
     Stupid: what does the other side know about the issue?
</pre>

<h1>Why would anyone do this?</h1>

<p>In one word: curiosity.</p>

<p>According to the research in <a href="/books/psychology-of-curiosity/">this survey paper</a>, being a curious
person ("trait curiosity") doesn't correlate strongly to other traits like IQ,
age, and sex. Instead, it is situational context that can pique curiousity
("state curiosity"). This is good news for me, since anyone can become curious
about anything.</p>

<p>On the flip side, curiosity about something requires that you be interested in
the pertinent topic, and usually increases with knowledge.  Loewenstein explains
curiosity in terms of an information gap. The more you know about a subject, the
more you know what you don't know. The novice is proud of what he knows even if
it is small in the absolute sense, relating closely to the <a href="https://en.wikipedia.org/wiki/Dunning%E2%80%93Kruger_effect">Dunning–Kruger
effect</a>. Gaining expertise, the novice learns more about the scope of
the domain, and shifts to thinking in terms of what they don't know yet. This
reveals a gap that they strive to fill. This news is not so good, but
Loewenstein suggests curiosity inducing stimuli, which I hope to take into
account in designing the quiz:</p>

<ul>
<li>Directly questioning is itself curiosity inducing, especially with a guess and
feedback cycle found in quizzes. A study showed that the more immediate the
feedback, the higher the curiosity.</li>
<li>The nature of a good quiz is that you will sometimes be wrong. If you are
wrong about a fact related to the topic, you may learn about an information
gap which will increase curiosity about the topic. If your expectation about
the other group is violated (ie. violation of expectations), curiosity about
that group increases.</li>
<li>If you see yourself as knowledgeable about a topic, you may be curious about
how much you really know about it. This can be viewed in terms of
what Loewenstein describes as the "competence motive", the desire to master
your own environment. </li>
<li>Loewenstein brings up the example of someone laughing aloud as they read a
newspaper article. This posession of information by someone else is also
curiosity inducing. What does the other side know?</li>
<li>The <a href="https://en.wikipedia.org/wiki/Zeigarnik_effect">Zeigarnik effect</a> states that people remember uncompleted or
interrupted tasks better than completed tasks. The last part of the quiz (what
does the other side think) requires a critical mass of responders, which means
that results will take some time to produce. This may serve to increase
curiosity.</li>
</ul>

<h1>A metaquiz about climate change</h1>

<p>To test this format, I decided to pick a specific topic to reduce the scope of
questions. Climate change is a good topic for several reasons. </p>

<ul>
<li>Pro: the topic is increasingly polarizing along party lines, most recently
after the US withdrawal from the Paris Agreement.</li>
<li>Pro: it is well grounded in science, which means there are plenty of hard
facts that can be verified and serve as a baseline of truth.</li>
<li>Con: climate change consistently shows up last in terms of topics that the US
public cares about.</li>
</ul>

<p>I put together a quiz following the template above, which first surveys your
climate change-related beliefs, then quizzes you on climate knowledge, and
finally asks you to guess what the other side believes and how they did on the
quiz. </p>

<p>If participants do poorly on the "other side" section, they may begin to wonder:
"maybe they're not all stupid?". Then, if it turns out that they have
incorrectly stereotyped beliefs of the other side, they might wonder "maybe
they're not all evil?". If participants do poorly on the quiz itself, they may
learn something about climate change, which isn't such a bad thing either.</p>

<p>So if you have a few minutes, <a href="https://docs.google.com/a/google.com/forms/d/e/1FAIpQLSf98rBi2CgU4b62kBFmAeX989ZGZrLW74cQitDdPT7wG906Xw/viewform">please try the climate metaquiz</a> and send it to
everyone you know, especially friends (or enemies!) on the opposite side.
Dziękuję!</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Debaters: friendly disagreement</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/debaters-friendly-disagreement"/>
    
    <updated>2017-03-05T09:00:00-00:00</updated>
    
    <id>https://smus.com/debaters-friendly-disagreement</id>
    <content type="html">
      <![CDATA[
      <div>
        <blockquote>
  <p>We have a choice. We have two options as human beings. We have a choice
  between conversation and violence. That's it. </p>
  
  <p>– Sam Harris</p>
</blockquote>

<p>As technological progress plows forward, human nature is unchanged. We each look
at the world through our own lens. In a previous post, I found that <a href="/hot-bread-delicious-deadly/">translating
a query between English and Russian greatly determines search results</a>.
In the same way that language matters, so do religious views, culture, political
leanings, and much more. Here's a recent example highlighting a news
source-based lens on the same topic (Nancy Pelosi and Russia):</p>

<p><img src="/debaters-friendly-disagreement/pelosi.png" alt="Nancy pelosi russia on nytimes vs. breitbart" /></p>

<p>Humanity has always been divided, and in hindsight, the unifying promise of the
internet was a techno-utopian dream. By shrinking the world into a "global
village" (famously coined by communication theorist <a href="http://www.marshallmcluhan.com/biography/">Marshall McLuhan</a>)
we have balkanized into increasingly specialized sub-cultures and increased
cross-cultural conflicts. More recently, personalized search results, curated
social network feeds only serve to deepen the divide.</p>

<p><a href="https://catma-847d6.firebaseapp.com/topics">Debaters</a> is a new side project which aims to bring you and someone
with an opposing view into a private, friendly, anonymous conversation. It's
still in development, but I want to share it with you both as a milestone and
to get early feedback.</p>

<!--more-->

<h2>The problem</h2>

<p>We are social animals. Rather than starting with a blank slate and using our
brilliant brains to arrive at independent conclusions, we prefer to jump to
our conclusions first through social means, and then rationalize why we are
right. Once we <em>know</em> the answer, it's unlikely that we will change our minds.
Because of my-side bias (aka confirmation bias), arguments in favor will stick,
while arguments against will be easily swatted. Entrenched in our socially
defined beliefs, our social circles and personalized information sources quell
potential for dissent, while strengthening our worldview. On a macro scale, this
leads to a polarized society. We can tolerate anything <a href="https://slatestarcodex.com/2014/09/30/i-can-tolerate-anything-except-the-outgroup/">but the outgroup</a>.</p>

<p>In light of the above, we are unlikely change our minds. But if you are one of
those rare people that are open to changing their mind, you may have read
articles like <a href="https://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind">this one</a>. However, it's far easier to be understand
the theory of mind changing than it is to actually change your mind on a
specific issue. Many public intellectuals are actively involved in conversations
that test their limits, but normal folks like you and me don't often get the
chance.</p>

<p>I've only attended one <a href="https://www.meetup.com/Bay-Area-Conservatives/">conservative meetup</a>. I chose not to reveal my
identity as a crooked centrist, feeling that this would impede further
conversation. I think a similar thing happens in many cities: there must be
Trump supporters (dozens of them?) among us, but they seem to keep a low
profile. Sad!</p>

<p>The way we get better at anything is through practice, which in this case means
to actively test ourselves on new ideas and with new people. Projects like
<a href="http://www.livingroomconversations.org/">Living Room Conversations (LRC)</a> in the real world, or <a href="https://www.reddit.com/r/changemyview/">Change My View
(CMV)</a> online try to create an environment that enables conversations where
we can practice actual open mindedness.</p>

<h2>Some problems with existing mind changing tools</h2>

<p><a href="http://www.livingroomconversations.org/">LRC</a> requires getting a group of people together physically, and have a
structured conversation about a controversial topic. This is difficult to do
since you must find a group of friendly but disagreeing people in-person. I'd
love to try it, but haven't been able to find a more right-leaning
co-facilitator yet. It is also a social risk, since you are likely pulling in
people from your social circle. Presumably you have briefed them on the plan and
they have consented, but conversations may still escalate and feelings can
easily be hurt. In addition, the prospect of a serious, structured conversation
with close friends sounds quite awkward to me.</p>

<p>Online, <a href="https://www.reddit.com/r/changemyview/">CMV</a> is great but has its own problems, despite the efforts of
well meaning and intelligent moderators. Some users that start threads seem to
use CMV as a way of pressure testing their own view. They get all of the counter
arguments, learn how to counter them, and get even better at rationalizing away
any future doubts. Some respondents may make arguments whether or not they
actually think that way just for the sake of deltas. As a subreddit, CMV
users tend to fall into Reddit's skewed demographics. This means less potential
for viewpoint diversity. Lastly, CMV is public and not truly anonymous. This
encourages people to be clever rather than honest, although there is no shortage
of <a href="https://www.reddit.com/r/The_Donald/">subreddits</a> whose members prefer honesty over cleverness.</p>

<h2>What is Debaters?</h2>

<p>So, to address some of the shortcomings of existing approaches like CMV and LRC,
I've been working on a side project provisionally called <a href="https://catma-847d6.firebaseapp.com/topics">Debaters</a>.</p>

<blockquote>
  <p>Debaters enables one-on-one conversations on a controversial topic, with
  someone of the opposing view. You may not be convinced by their arguments,
  but your conversation may lead to a better understanding on both sides.
  Interaction with someone with a different viewpoint will lead to reduced
  animosity toward their whole group.</p>
</blockquote>

<p><img src="/debaters-friendly-disagreement/debaters.png" alt="Screenshots of Debaters" /></p>

<p>I took a bit of time off recently and got carried away on the implementation,
with <a href="https://twitter.com/abmcosta">Antonio's help</a> on the UX front, and now it's live on
<a href="https://catma-847d6.firebaseapp.com/topics">https://catma-847d6.firebaseapp.com/topics</a>. If you visit, you will be
presented with a list of issues and asked to opine on each. After you provide
your opinion you are matched to someone with the opposite view, then you engage
in a conversation about the issue.</p>

<p>Let me address one common question up-front: trolling. I do not think trolling
is much of an issue for Debaters. Trolls want to make an impact. In other words,
they want to either reach a lot of people, or affect some people in a
significant way. In a 1:1 conversation, their reach is limited. In an anonymous
context, the amount of personal harm a troll can inflict is limited too.</p>

<h3>Crowdsourced beta testing</h3>

<p>To work out bugs and test out the platform, I took to Mechanical Turk. That's
right, I paid people to have an argument in the spirit of Monty Python's
Argument Clinic:</p>

<iframe width="600" height="337" src="//www.youtube.com/embed/kQFKtI6gn9Y" frameborder="0" allowfullscreen></iframe>

<p>Kidding aside, Turkers effectively became poorly paid ($0.25 per session) QA
testers. I asked them to try out Debaters and answer a question or two.
Meanwhile, I would assume the position of devil's advocate (as needed) and we
would have 5-10 minute long conversations. This helped iron out the bugs and
prioritize features.</p>

<p>Getting people to use the service without bribes was hard, mainly because
Debaters is a marketplace. Two people are required to answer the same question
differently to get matched. So inevitably, the first respondent needs to wait
while a match is found. Attention spans are short, and Debaters users are few.
Debaters attempts to address this by taking advantage of <a href="https://developers.google.com/web/fundamentals/engage-and-retain/push-notifications/">web
notifications</a>. Once a match is made, you are notified through a
browser notification. By this point though, you may be less likely to be up for
a conversation.</p>

<p>One of my milestones for the first version of Debaters was to facilitate a
conversation between two people I didn't know. I managed to do this by actively
promoting it on Twitter while also paying users on Mechanical Turk, creating a
critical mass so that people would get matched without too much waiting. This
worked out, and finally I had a half organic conversation. This one was about
a federal minimum wage. Dustin answered "Not sure", Lawrence answered "No". In
case you are wondering, Debaters assigns names and avatars randomly.</p>

<pre><code>Dustin Collier:    hi Lawrence
Lawrence Castillo: hi dustin
                   guessing these names are not real
Dustin Collier:    hehe. mine isn't, dunno about yours :)
Lawrence Castillo: i was scared for a second and thought they were real but
                   thats good
Dustin Collier:    is yours really lawrence!!!!!
                   or did you forget your name for a sec
Lawrence Castillo: no no
                   i saw your name and was like "oh shit people can
                   see names"
                   glad they're fake
Dustin Collier:    ah yea
                   anonymous.
                   u dont like minimum wage?
Lawrence Castillo: i think federal minimum wage, at least how we've been
                   talking about it is pretty flawed
Dustin Collier:    how so?
Lawrence Castillo: like, the minimum wage in nebraska should be very
                   different from the minimum wage in nyc
                   if we want a minimum wage it needs to be a percent of
                   cost of living
Dustin Collier:    ah yeah, cost of living adjusted
Lawrence Castillo: the idea of 15 dollars is kind of crazy
                   people in ny are still poor, and business can't pay it
                   in rural areas
                   i feel that way about most federal laws though
Dustin Collier:    yea i agree, but that's not even on the table
                   bernie was all like "$15"
Lawrence Castillo: yeah i loved the energy but...
</code></pre>

<p>The next milestone is to have a fully organic conversation, where both sides
arrive at Debaters without monetary incentives, but out of legimiate interest.</p>

<h2>Problems with Debaters</h2>

<p>Now that the first version of Debaters is released, the technical problems have
been addressed, and the UX is in an early but usable state. The fundamental
problem is <strong>how to attract users</strong>. </p>

<p>I think that the name "Debaters" connotes exactly the wrong thing. Debates are
something you win, and invoke a high school debate club. The name is also
suggestive of conflict, which people generally tend to avoid. Unfortunately I
was unable to come up with a catchy alternative.</p>

<p>That said, the name is not the limiting factor on user acquisition; there are
more fundamental forces at play. In today's political climate, people want to be
upset and angry. We are constantly outraged, and <a href="https://www.nytimes.com/2017/02/27/opinion/the-uses-of-outrage.html">some view it as a good
thing</a> that builds social cohesion. We don't want to change our minds,
that would be like colluding with the enemy. After all, <a href="https://wiki.lesswrong.com/wiki/Arguments_as_soldiers">arguments are
soldiers</a>. I disagree.</p>

<p>Conversations with people that hold different views is like getting kids to eat
their vegetables. It's good for them, but they aren't necessarily going to like
it.</p>

<h2>Tricking people into friendly debate</h2>

<p>A common tactic for getting kids to each their vegetables is to disguise them as
something else. Could a similar approach be taken with Debaters?</p>

<p>One avenue might be to target people that want to proselytize their ideas.
They might come to Debaters to sway others about one issue they are passionate
about, and then become engaged in another conversation on another issue, where
they are more likely to listen. This is pure theory. Maybe proselytizers are
certain about everything.</p>

<p>Another avenue might be to target neurotic people. This has sort of been tried
in the form of <a href="http://asteroidsclub.org/">The Asteroids Club</a>. This project is framed as a
"non-debate on America's biggest problems, which are hurtling toward us through
space and time at an alarming rate of speed". Unfortunately it hasn't taken off
yet.</p>

<p>People are inherently curious. Projects like <a href="http://wolfmanproductions.com/haider-hamza/">Talk to an Iraqi</a> and <a href="https://www.theswedishnumber.com/">The
Swedish Number</a> have been effective at attracting an audience. Haidar
Hamza's public booth seems to have also been effective at bringing up difficult
political issues. Could we take advantage of this curiosity by surfacing
something unusual about your future interlocutor?</p>

<p>And yet resorting to trickery may not work. Even a more oblique form of it,
<a href="https://en.wikipedia.org/wiki/Nudge_theory">nudging</a>, has <a href="https://www.theguardian.com/commentisfree/2014/apr/24/nudge-backlash-free-society-dignity-coercion">had significant opposition</a>. But, as Sam
Harris starkly puts it, the only tools we have for changing minds are
conversation and violence. My opinion? I'd like to avoid the latter, so intend
to continue thinking about and building in this difficult but incredibly
important problem space.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Headlines, meet sparklines: news in context</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/headlines-meet-sparklines-news-in-context"/>
    
    <updated>2017-02-17T09:00:00-00:00</updated>
    
    <id>https://smus.com/headlines-meet-sparklines-news-in-context</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>News reporting suffers from two major issues I'd like to tackle. The first is a
bias towards negative, emotionally laden events. The second is the difficulty of
capturing information about gradual changes.</p>

<p>These two deficiencies distort our perception. They make it easy for demagogues
to claim that the world has gone to shit. The data tells a different story, as
the late <a href="https://www.ted.com/talks/hans_rosling_shows_the_best_stats_you_ve_ever_seen">Hans Rosling</a> was fond of reminding us. My hypothesis is that
if base rates were provided in a compelling way alongside news stories (or even
headlines), the public would be better informed. The challenges are many: first,
getting and analyzing the data, but even more important, presenting it in a
reasonable way.</p>

<p>In this post, let's explore what that would entail, from data collection, to
analysis, to visualization. We'll go through a couple of examples.</p>

<!--more-->

<h2>The problems with news</h2>

<p>I've already complained about the news in a <a href="/front-page-blues">previous blog post</a>, but
this time around, I'd like to hone in on two specific issues: negativity and
gradual changes:</p>

<ul>
<li><p>News is generally biased toward negative, emotionally laden events. A
terrorist rampage that claims five victims is practically guaranteed to make the
front page, while a cure that saves five hundred certainly wouldn't.</p></li>
<li><p>News does not inform about gradual changes. Many important
processes, such as climate change, are gradual. Like boiling a frog, there are
no specific events to report on, so they get no coverage in the news (until the
frog dies).</p></li>
</ul>

<p>The goal here is for perception to approach reality. I will assume that you
agree with me that this is a worthy goal to pursue. Otherwise, we now return you
to your regularly <a href="https://www.socialistalternative.org/">scheduled</a>
<a href="http://www.breitbart.com/">program</a>.</p>

<h2>Headlines invite questions</h2>

<p>I went through some recent news stories (on <a href="https://en.wikipedia.org/wiki/Portal:Current_events">Wikipedia</a>), asking some simple
questions. For example:</p>

<p><style>
table#headline-question {
  font-size: 70%;
}
table#headline-question td {
  padding: 1em;
  text-align: left;
}
</style></p>

<table id="headline-question">
<tr>
<th>Headline</th><th>Questions</th>
</tr>
<tr>
<td>The death toll from the Rigopiano avalanche rises to 29.</td><td>How frequent are avalanche deaths? What about just in
Italy? What are some big recent avalanches?</td>
</tr>
<tr>
<td>Ken Wyatt is sworn in as the first Indigenous Australian to serve in
Australia's cabinet.</td><td>What is the population of Indigenous Australians?
What is the racial breakdown in Australia's cabinet? What about other countries?
What about historically?</td>
</tr>
<tr>
<td>The Kremlin arrests four people, one from Kaspersky Lab and three from the
Federal Security Service, reportedly on treason charges for passing information
to America's CIA.</td><td>How many arrests does the Kremlin typically make? How
many for treason? How about the US government?</td>
</tr>
</table>

<p>Firstly, to even ask the question requires a skeptical mindset. Secondly,
finding the data requires time and research. Lastly, presenting the data in a
compelling way takes some thought and creativity. Keeping in mind that I make no
claims to any of the above, let's give it a shot.</p>

<h2>Why are base rates important?</h2>

<p>The questions above attempt to get at the <a href="https://en.wikipedia.org/wiki/Base_rate">base rates</a> relevant to the
news stories, which is important context to get a better understanding:</p>

<blockquote>
  <p>It may at first seem impressive that 1000 people beat their winter cold while
  using 'Treatment X', until we look at the entire 'Treatment X' population and
  find that the base rate of success is actually only 1/100.</p>
</blockquote>

<p>It is also well known from a large number of psych studies that people are
<a href="https://en.wikipedia.org/wiki/Base_rate_fallacy">really bad at integrating base rates</a> into their thinking. Maybe
this is why they are so rarely featured in the news? My hope is that by
pairing each headline with a bit of base rate information, we can become better
informed and address both negativity and get a better sense for trends over time.</p>

<h2>Exhibit A: avalanche deaths (time series data)</h2>

<p>Let's start with a simple quantitative (if morbid) example: Avalanche deaths.
We can better understand just how extreme the Rigopiano avalanche was if we put
it into context. But what sort of context makes sense? If we consider geography,
we can imagine concentric circles around Rigopiano.</p>

<p><img src="/headlines-meet-sparklines-news-in-context/rigopiano.png" alt="Possible geographic context for the Rigopiano avalanche" /></p>

<p>On one extreme, we could consider other avalanches at Rigopiano specifically.
But for most people, especially outside of Italy, this is too specific.
Expanding our search, we could consider all of the Apennines (the mountain range
containing Rigopiano), but I found that getting data for avalanche fatalities in
this region was challenging. The outermost circle of the map above represents
the European Alps, which does not include the Apennines. But it is the
geographically closest region with readily available data.</p>

<p><a href="https://docs.google.com/spreadsheets/d/1PyX0vav_NPziiaL9LWKhPOhTQLY-mcMYqYBl_VjUSmg/edit#gid=1197783313">This spreadsheet</a> contains data that I extracted from <a href="http://www.geogr-helv.net/71/147/2016/gh-71-147-2016.pdf">Avalanche
fatalities in the European Alps: long-term trends and statistics</a>,
which includes contiguous coverage from 1970 to 2015. Naturally, the paper
didn't link to a data set, so I had to create the spreadsheet by visually
inspecting the graph (ouch).</p>

<p>The paper contains some interesting findings. For example, the number of
avalanche deaths in controlled terrain (eg. ski resorts, where ski patrol
engages in <a href="https://en.wikipedia.org/wiki/Avalanche_control">avalanche control tactics</a>) has decreased
significantly, but that the number of avalanche deaths in uncontrolled terrain
remains significant (in the Alps, 100 yearly) and stable. Note that the numbers are
not adjusted for the increasing global population, or for the increasing numbers
of back country tourists.</p>

<p><img src="/headlines-meet-sparklines-news-in-context/alps-graph.png" alt="Avalanche deaths in the European Alps between 1970 and 2015" /></p>

<p>One of the things that becomes clear is the important distinction between
controlled and uncontrolled accidents. We now have context for better
understanding the tragedy at Rigopiano: it was a controlled accident that will
send the statistics for 2017 through the roof. Let's see it in the context of
other significant avalanches (controlled and not) over the years. The following
claimed more than 20 people since 1970, according to Wikipedia:</p>

<p><img src="/headlines-meet-sparklines-news-in-context/avalanches-since-1970.png" alt="Significant global avalanches since 1970" /></p>

<p>Now we are armed to the teeth with data, but how do we present inline in the
news? There are <a href="http://flowingdata.com/2017/01/24/one-dataset-visualized-25-ways/">tons of ways</a> of visualizing data in a compelling
way, but in this case we want it to appear in-situ in a digital newspaper. Why
not start with <a href="https://www.edwardtufte.com/bboard/q-and-a-fetch-msg?msg_id=0001OR">Tufte</a>-inspired sparklines, since they are compact and
can be placed adjacent to a headline.</p>

<p><style>
iframe#avalanche {
  border: 0;
  height: 200px;
}
</style></p>

<iframe id="avalanche" src="avalanche-example.html"></iframe>

<p>From the first graph, we can immediately see that significant avalanches are
rare, so this event is definitely newsworthy, but didn't claim as many lives as
some of the most fatal ones, even recently. The second sparkline shows that
avalanches on controlled terrain (in the Alps) claim fewer lives, which makes
Rigopiano even more significant. Then, to satisfy our curiosity, the third
sparkline shows annual avalanche fatalities on uncontrolled terrain is
persistently high (c. 100 yearly). We now have some context to better understand
this story.</p>

<p>A quick note on technology. The above is a slightly modified version of <a href="https://github.com/phuu/sparksvg.git">Spark
SVG</a>. I added a few things to the basic <code>bar.svg</code>:</p>

<ul>
<li>Set y-axis scale for fair comparisons across different graphs.</li>
<li>Labels (x, y) values on hover.</li>
<li>Ability to transpose the graph.</li>
</ul>

<p>As an aside, I was amused to discover the <a href="http://caaml.org/">Canadian Avalanche Association Markup
Language (CAAML)</a>, which is a "standard for the electronic representation
of information pertinent to avalanche safety operations". I had naively hoped to
one day escape XML by becoming a ski bum. Not so fast!</p>

<h2>Exhibit B: cabinet composition</h2>

<p>Let us now turn our attention to Ken Wyatt, the newly appointed member of the
Australian cabinet. How ethnically diverse is the Australian cabinet? At
minimum, we can look at base rates for ethnicity in the Australian cabinet. In
1997, the cabinet was 100% white, but now with Wyatt's joining, the cabinet is
96% white (he is the only non-white member). Not much to visualize yet, so let's
expand our scope.</p>

<p>Consider three metrics: % female, % non-white and % non-christian for each
cabinet, and compare them across three cabinets: US, Australian and Canadian,
between two years: 1997 and 2017. I've collected this data <a href="https://docs.google.com/spreadsheets/d/1r6e92Xf4h8e7T83lrex-BghQilswGh_Hj-xmOaZCasc/edit?usp=sharing">in a
spreadsheet</a>. It was a fair amount of work to skim Wikipedia
pages for six sets of cabinet members to try to gleam gender (easy), ethnicity
(tricky) and religion (hard). While there are surely mistakes in the
spreadsheet (please <a href="/about">email me</a> if you find one), it should be good
enough for broad strokes. My own position would favor a qualified cabinet over a
diverse one, but all things being equal, a cabinet that is representative of the
general population is a good thing. Here's the headline with data alongside:</p>

<p><style>
iframe#cabinet {
  border: 0;
  height: 200px;
}
</style></p>

<iframe id="cabinet" src="cabinet-example.html"></iframe>

<p>To summarize, <a href="/headlines-meet-sparklines-news-in-context/cabinet-sign.jpg">I've seen stronger cabinets at IKEA</a>!</p>

<p>One thing that is clear from the above is that indeed, Australia has a very
white cabinet. Of course, in the spirit of representation, we should be
comparing ethnicity numbers to the general population, but I'll leave that out
for now (for reference, 3% is indigenous, and ~10% is non-white).</p>

<p>Another thing the chart above shows are trends over the 20 year period.
Australia's cabinet is becoming more female, while staying roughly as white and
as Christian. Canada's cabinet has become vastly more representative in gender,
ethnicity and religion. In stark contrast to Canada, the US has actually
regressed in diversity on all fronts. Over the last 20 years, its has become
more male dominated, more Christian, and more white. The last is especially
disappointing since the US is far more ethnically diverse than Canada and
Australia put together (at "just" 72% white).</p>

<p>I should mention a couple caveats. First, I really fudged the % Christian
calculations, since it was so difficult to accurately determine religion for
many cabinet members. Also, this analysis would greatly benefit from more data
points. For example, Clinton's cabinet in 1997 was quite diverse but probably
became even more diverse under Obama, but that data point is missing. Getting
additional data points for Canadian and Australian cabinets is more challenging,
since there are no term limits, and cabinet members flow more freely in and out,
change roles inside them, and sometimes even hold multiple offices. Lastly
thanks to the <a href="https://twitter.com/borismus/status/831641415604064256">good people on Twitter</a>, who sent me many constructive
suggestions for improvement. I still think it's a bit too information dense,
but it has come a long way.</p>

<h2>Summing up</h2>

<p>We looked at two headlines: one clearly well suited for contextualizing through
data visualization (longitudinal time series), and another somewhat less so,
regarding the composition of the Australian cabinet. In both cases, my
understanding of the world has been enriched by the context that data
surrounding it provided.</p>

<p>Of course, there are many ways to <a href="https://en.wikipedia.org/wiki/Misleading_graph">mislead with graphs</a>, and sparklines
can succumb to some of them. The axes are unlabeled, so the time scale is
unknown unless specified. Nor is it clear whether or not the y-axis has been
deliberately truncated. As a result, it can also be unclear whether or not
graphs can be cross-compared. In the cabinet example, I had to explicitly
specify that all of the cabinet sparklines have a maximum value of 50% to
facilitate this visual comparison, and exposed raw data on mouse hover.</p>

<p>Imagine headlines from <a href="https://en.wikipedia.org/wiki/Portal:Current_events">your favorite news source</a> enhanced with a bit of
longitudinal base rate for context. This would bring more clarity to the news,
giving readers a better sense for general trends, as well as putting the event
in a broader context. In many cases, the broader context is actually pretty
positive: avalanche deaths in controlled areas have gone down drastically,
cabinets in many developed nations are becoming more representative.</p>

<p>Some headlines may not fit the mold I'm proposing. Many of them are
anecdotal in nature, like gossip stories, where You Won't Believe What Happened,
because it's such a unique situation. A certain president doing certain crazy
shit comes to mind.  For other stories it can be very challenging to acquire the
data required, like the Kremlin FSB arrest story. (I may or may not be privy to
that sort of information. If I told you, I'd have to kill you.)</p>

<p>One downside to this whole thing is that it requires a journalist to do more
work: data sleuthing, careful thought about presentation, possibly even
implementing a new visualization. This work has intrinsic value, since it forces
the author to broaden their understanding of the subject, and then whittle it
down to the substantive kernel for public consumption. But ultimately, just like
I really enjoy <a href="http://www.pewsocialtrends.org/interactives/what-do-police-think/">Pew Research's</a> approach to visualizing polls, headlines
with visualizations of relevant base rates would make for a much more
informative and interesting read, and ultimately make us better informed
citizens. What do you think?</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Tools for making better decisions</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/making-better-decisions"/>
    
    <updated>2017-02-01T09:00:00-00:00</updated>
    
    <id>https://smus.com/making-better-decisions</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>In a famous letter dating back to 1772, Benjamin Franklin described how he made
decisions to a friend who was facing a dilemma. Franklin's method involved
enumerating pros and cons of an argument, and then attempting to weigh one
against the other to ultimately decide which of the two possibilities to pursue.
Franklin wrote:</p>

<blockquote>
  <p>My way is to divide half a sheet of paper by a line into two columns; writing
  over the one Pro, and over the other Con. Then, during three or four days
  consideration, I put down under the different heads short hints of the different
  motives, ... I endeavor to estimate their respective weights.</p>
</blockquote>

<p>This post attempts to modernize Franklin's method to attempt to overcome some of
its shortcomings. Once we have gathered our thoughts in one place using this
spreadsheet format, we can, with the help of others or using (aspirational) AI,
assist the decision maker to help them combat common mistakes.</p>

<!--more-->

<h2>Modern tools for decision making</h2>

<p>Franklin's method is explicitly qualitative: "...the weight of the reasons
cannot be taken with the precision of algebraic quantities". Of course, this has
not stopped many scientists and engineers from attempting to create quantitative
tools that assist in decision making, called <a href="https://en.wikipedia.org/wiki/Decision_support_system">decision support systems</a>.
However these are mostly targeted at companies and not individuals. I
<a href="https://1000minds.com">tried</a> <a href="https://meenymo.com/">a couple</a> and failed to find one that was simple enough
for my purposes.</p>

<p>As a result, it seems that the state of the art for individuals hasn't advanced
much beyond Franklin's method. Product comparisons are one notable exception:</p>

<p><img src="/making-better-decisions/product-comparison.png" alt="Product comparison chart example" /></p>

<p>What if we could take product comparison charts, but make them a bit more
quantitative, and then apply the technique to decision making?</p>

<h2>Decision support spreadsheets</h2>

<p>Simply stated, a topic is controversial (or a decision is difficult) if:</p>

<blockquote>
  <p>...there are good arguments on all sides. Good thinking involves balancing these
  arguments in a quantitative way, taking into account their relative strengths
  and weaknesses.</p>
</blockquote>

<p>Inspired by this and other parts of Jon Baron's <a href="https://www.amazon.com/Thinking-Deciding-4th-Jonathan-Baron/dp/0521680433">Thinking and
Deciding</a>, I made <a href="https://docs.google.com/spreadsheets/d/1HBjUBa1NaD2jGr4QzN1prIbQgCBFqcjtHnRkXuenIHs/edit#gid=0">a spreadsheet</a> attempting to codify what he
describes as the "search-inference" process. Here's an example of an imaginary
rocket scientist deciding between two job offers based on two goals, resulting
in a 2x2 sheet:</p>

<p><img src="/making-better-decisions/decision-spreadsheet.png" alt="Screenshot of Google Sheets decision spreadsheet" /></p>

<p>Structurally, it works like this:</p>

<ul>
<li>Columns are possible courses of action (eg. NASA vs SpaceX).</li>
<li>Rows are goals that you are trying to achieve (eg. improve the world, work
with great people).</li>
<li>Cells contain evidence pertaining to the associated possibility (row) and goal
(column). In this case, the NASA job would improve the world by enabling much
faster space travel.</li>
</ul>

<p>There are also numbers involved:</p>

<ul>
<li>Each goal (row) has a number between 1 and 5 under it, corresponding to how
important the goal is to you. The higher the number, the more important.</li>
<li>Each piece of evidence (sub-cell) has a weight to the right between -2 and 2.
Positive weights are pros, negative ones are cons.</li>
</ul>

<p>A <a href="https://docs.google.com/spreadsheets/d/1HBjUBa1NaD2jGr4QzN1prIbQgCBFqcjtHnRkXuenIHs/edit#gid=374695355">second sheet</a> does all of the calculations. Each cell reduces to a weight in a
decision matrix. Ultimately, each possibility (column) is given a score between
0 and 1. The recommended course of action is the possibility with the highest
score. So the above 2x2 spreadsheet is converted into this decision matrix.</p>

<p><img src="/making-better-decisions/decision-spreadsheet-calculations.png" alt="Second sheet of Google Sheets decision
spreadsheet" /></p>

<p>Then, given these weights, we do a simple calculation for each possibility
(column): a normalized, weighted sum. So for the first possibility, we calculate:</p>

<pre><code>(0.6 * 0.9 + 0.8 * 0.63) / (0.6 + 0.8) = 0.75
</code></pre>

<p>We do the same for each possibility, and the one with the highest resulting
score is the "best" course of action.</p>

<h3>Advantages of this method</h3>

<p>Even geniuses like Franklin have a limited capacity for holding multiple
thoughts in their heads at once. After laying out all of the arguments, Franklin
wrote, "the whole lies before me, I think I can judge better". With all of the
possibilities, goals and evidence in one place, you too can be like Franklin.</p>

<p>As for the specifics of my spreadsheet above, I can't claim that this method is
optimal or even particularily good (though feedback on this would be
appreciated). I created it as a placeholder, loosely inspired by Baron,
Franklin, and other less rigorous approaches I've tried in the past. As it
turns out, this method is essentially an example of <a href="https://en.wikipedia.org/wiki/Analytic_hierarchy_process">Analytic Hierarchy Process
(AHP)</a>.</p>

<p>Rather than sticking to Franklin's two column split, this spreadsheet is
somewhat more complex, but there are some advantages:</p>

<ol>
<li>Most decisions <a href="http://lesswrong.com/lw/hu/the_third_alternative/">aren't actually binary</a>, and this is captured by
having multiple columns.</li>
<li>The method makes the notion of your goals and their relative importance
explicit.</li>
<li>Rather than pros and cons, we collect evidence that helps you decide about a
goal and a possibility, which can then be graded numerically.</li>
</ol>

<p>Despite the mechanistic appearance of this approach, Baron emphasizes the
nonlinearity of the thinking process. As you collect evidence, you may uncover
new possibilities and goals. With all of the evidence laid out, you can begin
asking better questions, attempting to fight known failure modes in human
thinking.</p>

<h3>Reducing and increasing complexity</h3>

<p>One significant challenge with the above approach is that of assigning weights.
At the moment, my method involves coming up with two kinds of weights: goal
weights (eg. how important is it for you improve the world, really?), and
evidence weights (eg. is space travel really such a world improving thing?).
This method is flexible enough to be easily simplified. For example:</p>

<ol>
<li>Evidence weights can be simplified by scoring pros as +1, and cons as -1.</li>
<li>Goal weights can be simplified by binary ranking (eg. 1 is critical, 0 is
nice-to-have).</li>
</ol>

<p>A potentially better approach is known as <a href="https://en.wikipedia.org/wiki/Potentially_all_pairwise_rankings_of_all_possible_alternatives">PAPRIKA</a>, which establishes
weights based on a bunch of pair-wise comparisons. This might work well, and
could actually be useful for capturing additional points of evidence. To get a
feeling for it, there's a consumer-oriented decision support system <a href="https://meenymo.com/">called
MeenyMo</a> that does this. The process is quite tedious though, involving
tens of comparisons like this:</p>

<p><img src="/making-better-decisions/meenymo.png" alt="MeenyMo's PAPRIKA style comparisons" /></p>

<p>The other downside of PAPRIKA is that it requires discrete categories (eg. cost
of living: cheap, moderate, expensive).</p>

<h2>Thinkos: inevitable irrationality</h2>

<p>People aren't perfect, and neither is our thinking. Biases are sort of like
thinking bugs that make our thoughts less rational. Irrational thinking leads to
conclusions that are further from the actual objective truth. This is, as I
hope you'll agree, undesirable.</p>

<p>Now that we're on the same page, Baron suggests that certain tactics that can
help us make better decisions by improve thinking and reducing bias. These he
broadly describes as "active open mindedness":</p>

<ul>
<li>Seek alternative possibilities. Anchoring bias tends to favor the first
possibilities you generate, but it is entirely possible that you haven't
searched enough.</li>
<li>Formulate goals better. What are you actually trying to achieve? (eg. "protect
walls from child's scribbling" vs. "prevent child from scribbling on walls").</li>
<li>Look for counterevidence (eg. if there are strong pros, see if there are some
cons too).</li>
<li>Avoid belief overkill, which happens when there is a strong correlation
between different goals (eg. most people are against capital punishment because
it is both ineffective and immoral, whereas those <em>for</em> capital punishment are in
favor because it's effective and moral. But why do both go together? They
should be unrelated.)</li>
<li>Allocate time that is proportionate to the importance of the decision.
Franklin's method suggests to take "three or four days consideration" to
capture evidence, and then "a day or two of further consideration" to let it
all settle.</li>
</ul>

<p>There are <a href="https://en.wikipedia.org/wiki/List_of_cognitive_biases">many other biases</a> that can lead to bad decisions. The above
serves as an example of some thinkos that can be reduced with external help:
other people or software.</p>

<h2>Summing up...</h2>

<p>In some facets of life, it is impossible to apply this level of rigor.
Quantifying your love for a person, for example, feels cold hearted and
calculating, and I try to avoid it. Ironically, one of the most famous uses of
Franklin's method was used by Charles Darwin in deciding whether or not to marry
Emma Wedgwood. For what it's worth, the method appears to have worked, with
Darwin emphatically scribbling "Marry, Marry, Marry, QED" after his
calculations.</p>

<p>It is hard to fully discount the role of feeling. The quality of the
rational decision making process depends heavily on your ability to formulate
your true goals and possibilities, and collect all of the evidence and score it
correctly. Gut feeling, (or as Kahneman says, System 1 thinking), can actually
incorporate many arguments that one might not even be able to formulate, and yet
those intangibles may end up being incredibly important.</p>

<p>And lastly, there is the question of practicality. Life is dynamic and
circumstances can change quickly. For the spreadsheet-powered decision maker,
this means constant revision, which can be complicated and time consuming. I
experienced this first hand, attempting to use this method to help make a career
move. Just when I thought I had established the teams that would have me,
another one emerged, and I had to re-enter additional evidence, remove options
that seemed appealing, but in retrospect were duds, and re-calibrate weights.</p>

<p>Thinking cannot be reduced to a spreadsheet, but when used in moderation, I hope
that this method can be useful for some.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Front page blues</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/front-page-blues"/>
    
    <updated>2016-12-07T09:00:00-00:00</updated>
    
    <id>https://smus.com/front-page-blues</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>According to the <a href="https://www.americanpressinstitute.org/journalism-essentials/what-is-journalism/purpose-journalism/">American Press Institute</a>,</p>

<blockquote>
  <p>News is that part of communication that keeps us informed of the changing
  events, issues, and characters in the world outside.</p>
</blockquote>

<p>There are many ways for news to be uninformative or even outright misleading.
Two trends in particular have received a lot of attention recently. The first is
social recommendation systems and selective unfollowing, which creates a
reality-distorting echo chamber. The second is fake news, which sure is in vogue
these days, and is obviously a problem that we should tackle.</p>

<p>This post is about a different trend: <a href="https://twitter.com/hamandcheese/status/801893793540796416">real news presented with misleading
frequency</a>. The issue at stake is the media's ability to inform its readers
and serve the public interest.</p>

<!--more-->

<h1>Real news, dubious frequency</h1>

<p>If you are a New York Times reader, you may have noticed a certain individual
prominently mentioned in the newspaper over the last several months. I wanted to
know just how much, and started daily screenshots on August 3rd, 2016. To my
great surprise, this was exactly one hundred days before the announcement of the
45th president-elect. My analysis was very simple.</p>

<ol>
<li>Does the lead story (that is, top-left corner article) include "Trump" in the
title? <strong>(50 / 100 days)</strong></li>
<li>Does the "Trump" appear in any headline above the fold? <strong>(86 / 100 days)</strong>.</li>
</ol>

<p>I should note some limitations of this approach. This process itself was more
manual than I would have liked. My script took screenshots only, so I did not
have access to the markup of the page. I tried auto-reconstructing the images using
<code>tesseract</code>, but the results were not perfect, <code>pdfgrep</code> had only limited use,
and I had to visually inspect the page to get my data analyzed. The data are <a href="https://docs.google.com/spreadsheets/d/e/2PACX-1vT-7Cz1s9YpvlXDz9ejuJa_0JP9pV6coeAMA2j_R0KxEZZpnZ4daMsNOdI86qWgDIwUyZhy8rUAs-2Y/pubhtml">in
this spreadsheet</a>, and I've made the <a href="https://drive.google.com/open?id=0B4Nj-yDXjBs_S3kwZXVqanZNdEU">raw screenshots</a>
available too.  There are non-technical problems too. The web version of the New
York Times is updated on a more frequent than daily basis, so my screenshots
missed some versions. Also, notions of "lead story" and "above the fold" don't
really make sense online. I arbitrarily defined "the fold" to be 1280 px, as in
this example:</p>

<p><img src="/front-page-blues/nytimes.png" alt="Example NYT screenshot from September 16th" /></p>

<p>As for the results? Half of the NYT's headlines had "Trump" in the name, and the
frequency matched my intuition. It sure did feel like this dude was getting a
whole lot of coverage. In fact, he got far more coverage than my method reveals,
since many lead articles without explicit mention of his name in the title were
still mostly about him.</p>

<p>I was amused to see long runs of adjacent days of headlines about the
president-elect-to-be followed by many days of respite. For example, from August
9th to 17th, "Trump" was featured consecutively, only to be broken by the
news of Zika having spread to Florida.  The longest respite took place between
September 19th and 27th, when the nation's focus switched to the Manhattan bomb
scare. Things heated back up on October 8th, and stayed hot until the 18th, when
his boasts about groping women were released to the public.</p>

<p>Just for fun, I looked through some <a href="https://en.wikipedia.org/wiki/Portal:Current_events">other news</a> that lost the
contest for most important story of the day. Here are three randomly chosen
examples:</p>

<ul>
<li><p>On August 14th, the NYT focused on G.O.P. politics rather than the Russian and
Syrian jets which conducted 26 airstrikes across the Idlib province, killing
122 civilians.</p></li>
<li><p>On September 8th, the NYT decided that the future potential president's vows
to bolster military capacity and raise spending were more important than Wells
Fargo's agreement to pay $190 million to settle a case involving deceptive
sales that pushed customers into fee-generating accounts they never requested.</p></li>
<li><p>On October 21st, the NYT featured a story about a presidential candidate
threatening to reject the election result rather than the Watts Bar Nuclear
Plant, which was the first U.S. nuclear reactor to enter commercial operation
in 20 years.</p></li>
</ul>

<p>I claim that the distribution of coverage in the days running up to the election
did a bad job of keeping us informed of significant events, and went against the
public interest.</p>

<h3>On not being well informed</h3>

<p>Only 15 reporters were present when Harry Truman announced the use of nuclear
weapons against Japan. <a href="https://www.amazon.com/Six-OClock-Presidency-Presidential-Television/dp/0275935981">By 1990</a>, nearly 2000 reporters held
passes to the White House pressroom. The presidency has become an increasingly
important focal point for the media. But as I write here in 2016, the coverage
of a mere presidential candidate has come to eclipse absolutely everything else.
Reasons why this is bad:</p>

<ul>
<li><p><strong>Opportunity cost</strong>: People have limited attention. By devoting half of the
headlines to one issue, we are inevitably less well informed about other
things.</p></li>
<li><p><strong>Reporting on outrage</strong>: The majority of the headlines were about his
Rump's outrageous statements: "twitter barage taunts Ryan as weak and ineffective",
"clung to birther lie for years, and still isn't apologetic", "assails his
accusers as liars and unattractive", "failing efforts to tame his tongue". Is
that truly newsworthy?</p></li>
</ul>

<h3>On not serving the public interest</h3>

<p>In one interpretation, "public interest" taken literally means providing the
public with what they are interested in. And if the object of interest is a
particular individual, so be it! That's how YouTube works: you watch a lot of
cat videos, YouTube learns you are into cat videos and gives you more cat
videos. This is problematic <a href="http://www.timewellspent.io/">in and of itself</a>. But capital-J journalism
that the New York Times is associated with is held to a different standard.</p>

<p>The literal definition leaves a lot to be desired. My view of public interest is
more paternalistic and centers on that which is beneficial for a <a href="http://reutersinstitute.politics.ox.ac.uk/publication/journalism-democracy-and-public-interest">well
functioning democracy</a>. In that view, here are a few reasons why focusing
so heavily on Cheeto Jesus is counterproductive:</p>

<ul>
<li><p><strong>General fairness</strong>: No individual is important enough to have half of
all stories in a newspaper be about them.</p></li>
<li><p><strong>Playing into his little hands</strong>: The NYT gave a person widely regarded as
"remarkably narcissistic" fifty percent of all headlines. Thanks guys!</p></li>
<li><p><strong>Conditioning effects</strong>: By reporting so heavily on a single topic,
people become morbidly obsessed with it. People wonder, "What's going to
happen next?" instead of moving on to more important stories.</p></li>
</ul>

<p>Events have a timeline, but if the object of obsession is an individual, it's
the gift that keeps on giving.</p>

<h1>Engagement, clicks, and the bottom line</h1>

<p>Why did the New York Times cover that despicable man to the extent that they
did? I venture a couple of guesses:</p>

<ol>
<li>Genuine fear of him being elected, and thus a desire to warn the people of
his evil ways.</li>
<li>A desire to increase views, catering in part to the morbid fascination
of their readers.</li>
</ol>

<p>All things considered, I lean more towards the second possibility. While I can't
rule out the first guess completely, it seems to be contradicted by the
<a href="http://www.nytimes.com/interactive/2016/upshot/presidential-polls-forecast.html">explicit confidence</a> the NYT had in a Democratic win. Back in March,
Vox <a href="http://www.vox.com/2016/3/3/11148296/donald-trump-media">asked the same question</a> and answered it simply:</p>

<blockquote>
  <p>The media covers him a lot because his campaign is fascinating and people are
  interested in it.</p>
</blockquote>

<p>Notably, at the time, 13 percent of Vox stories were about the short fingered
vulgarian, but generated 26 percent of their readership. With statistics like
that, guess what topic gets covered more? As for public interest, the author of
the piece doubted that his attention-getting tactics would continue to work in
the candidate's favor. Carte blanche justifying continued coverage? How
convenient!</p>

<p>The internet has changed the media in a fundamental way. Even great publications
like the New York Times haven't quite figured out how to balance their
(understandable) corporate need for profit with their journalistic
responsibility to the public's interests. Our culture of not paying for news
content leads to media companies to seek other sources of income, mostly in the
form of ads. But you get what you pay for! Rather than covering the world in a
balanced way, the world is covered in a way that more people will want to read.</p>

<p>The result is a killer combination of consumer driven demand (oh my god, what is
he going to say next?), and a desire for publications to maximize ad revenue.
The term is "engagement", and <a href="https://medium.com/@edelwax/is-anything-worth-maximizing-d11e648eb56f#.bt1ua0z6g">Joe Edelman</a> does a great job of explaining
why this is a dangerous thing to maximize.</p>

<p>If newspapers did not need to maximize engagement to be profitable, there would
be room to make decision that are aligned with actual public interest.</p>

<h1>Problems of novelty</h1>

<p>What makes a story newsworthy? Two ingredients at the very least,</p>

<ol>
<li>It must have happened recently, and</li>
<li>it must be of sufficient interest to the public.</li>
</ol>

<p>The thing is, it takes time and effort to decide whether something is of
sufficient interest to the public. So there is an inverse tradeoff between the
ability to deliver both. The faster you deliver news, the more shortcuts must be
taken to measure public interestingness.</p>

<p>The general trend in the media is one of spacetime compression. Before the
telegraph, it would take months for transatlantic news to travel. The fastest
way to get information across was a slow moving frigate. Before Gutenberg, it
would take weeks more for the newly arrived information to spread. Over the last
three centuries, the news cycle shrunk down to a daily basis. Today, since
everyone has a mini-printing press in their pocket, the news period is
arbitrarily small. According to <a href="http://www.tristanharris.com/2016/05/how-technology-hijacks-peoples-minds%E2%80%8A-%E2%80%8Afrom-a-magician-and-googles-design-ethicist/">Tristan Harris</a>, the average
person checks their phone 150 times a day.</p>

<p>With people checking the news every 10 minutes, there is a lot of pressure for
journalists to produce more content, more often. And indeed, there's a
lot of content online. But more does not mean better. Increasingly, news is
re-aggregated and re-published. Commentary is cheap and can be attached to a
brand or personality, which means more clicks. Investigative journalism takes
time. Unfortunately, by the time your investigation is finished... oh look a
squirrel!</p>

<p>It's tempting to blame the media for this, but the media is a reflection of our
collective psyche magnified by modern technology. We need to value novelty less
and learn how to delay gratification. Take a lesson from wine tasting and
meditation. Accept that news happens and let it breathe undisturbed for a period
of time. After all, hindsight is 20/20.</p>

<h1>Any publicity is good publicity</h1>

<p>Systematic flaws with today's media (see above) make it easy for cynical
operators to exploit. Fake news happens when the media-savvy operator runs a
news website. Stories are completely fictional, but negatively resonate with the
public, creating pure click bait. <em>Oh my god, <a href="https://www.buzzfeed.com/craigsilverman/viral-fake-election-news-outperformed-real-news-on-facebook?utm_term=.rwlPqdb6Po#.ds5Yo0mwYE">Hillary sold weapons to
ISIS</a>!</em></p>

<p>In another, even more cynical and dangerous variant, the media-savvy operator
mostly says and does outrageous things that may or not be true, purely for
attention. <em>Oh my god, he said he would date his daughter</em>! Minor celebrities
like <a href="https://swagbymilo.com/">M</a> have perfected this technique, but the Orange One is a true
master. Sometime in July, his wife gave a speech which was <a href="http://www.politico.com/magazine/story/2016/07/donald-trump-2016-convention-melania-trump-speech-dark-art-of-pr-214083">blatantly
plagiarized</a>. His response to the controversy?</p>

<blockquote>
  <p>“Good news is Melania’s speech got more publicity than any in the history of
  politics,” he said, “especially if you believe that all press is good press!”</p>
</blockquote>

<p>Straight from the horse's mouth. On the internet, we have an expression: <a href="http://rationalwiki.org/wiki/Don't_feed_the_Troll">"don't
feed the troll"</a>. I guess the New York Times didn't get the memo?
Whatever the case may be, the news media just helped feed the United States of
America to the biggest troll ever.</p>

<h1>This is bad, what do we do?</h1>

<p>Here are some possible things to try:</p>

<ol>
<li><p>Switch to a weekly news digest. At least this way events that happened early
in the week will have had time to settle. The Guardian has a <a href="https://www.theguardian.com/weekly">weekly
version</a>, but I struggled to find a subscribable <a href="http://www.nytimes.com/newsletters">weekly news
digest for the New York Times</a>.</p></li>
<li><p>Read a more balanced news source that is not driven by engagement. My new
favorite source for daily news is <a href="https://en.wikipedia.org/wiki/Portal:Current_events">Wikipedia's Current Events</a>. News
via Wikipedia has a nice side benefit: context for the story is readily
available in the form of other Wikipedia articles!</p></li>
<li><p>Focus on more international content. The US is a special snowflake, but it's
not <em>that</em> special. The world is increasingly global, but papers like the NYT
tend to weigh US politics very heavily. <a href="http://www.bbc.co.uk/worldserviceradio">BBC World Service</a> may be a
remedy.</p></li>
<li><p>Support your favorite news source through a subscription. This will reduce
their dependency on ads, which hopefully means less click-bait.</p></li>
<li><p>Don't give Demagogues a platform. In this post, I have taken the Voldemort
tactic: avoid mentioning their name.</p></li>
</ol>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>VR View 2.0: JavaScript API</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/vr-view-2.0"/>
    
    <updated>2016-11-14T09:00:00-00:00</updated>
    
    <id>https://smus.com/vr-view-2.0</id>
    <content type="html">
      <![CDATA[
      <div>
        <p><a href="https://github.com/googlevr/vrview">VR View</a> was just updated to version 2! This release includes some nice
new features, the main one of which is a JavaScript API. This allows VR Views to
be much more interactive. You can now load new content dynamically, play and
pause videos, and add hotspots that link from one piece of 360 imagery to
another. Here's a simple auto-advancing 360 slideshow showing some of my recent
escapes around Seattle...</p>

<!--more-->

<div id="vrview"></div>

<script src="//storage.googleapis.com/vrview/2.0/build/vrview.min.js"></script>

<script src="index.js"></script>

<p>The <a href="https://storage.googleapis.com/vrview/2.0/examples/index.html">official samples</a> show more complex and interesting examples.
The <a href="https://developers.google.com/vr/concepts/vrview-web">docs</a> are also updated to reflect VR View's new capabilities.</p>

<h1>Other new things</h1>

<p>Also added some other features:</p>

<ul>
<li>WebVR 1.1 support for compatibility with <a href="https://webvr.info/">Chrome WebVR</a> builds.</li>
<li>Programmatic playback controls and volume setting.</li>
<li>Support for handling clicks, taps, VR button presses.</li>
<li>Automatic panning mode for desktop.</li>
</ul>

<h1>Cardboard camera compatibility</h1>

<p>I captured the photos with the very handy <a href="https://itunes.apple.com/us/app/cardboard-camera/id1095487294?mt=8">Cardboard Camera</a>. But before
I could embed them into the VR View above, I had to do a conversion step.</p>

<p>VR View expects stereo images to be in ODS format, which is a square JPEG with
the left eye sphere stacked on top of the right eye sphere. Both spheres are
projected onto 2:1 rectangles using equirectangular projection.</p>

<p>The native Cardboard Camera format is different. Cardboard Camera produces an
image of the left eye only. The right JPEG is base64 encoded and embedded in an
XMP header, alongside other <a href="https://developers.google.com/streetview/spherical-metadata">Photo Sphere XMP metadata</a>. The images don't
need to be full photospheres, and may be cropped. Stopping a pano capture
mid-way, for example, will create a half-sphere. Also, the north and south poles
of the sphere are never captured, since the sweep is horizontal.</p>

<p>Now that you know more than you wanted about photosphere file formats, you can
forget it all. I've streamlined the conversion process through a web-based
<a href="https://storage.googleapis.com/cardboard-camera-converter/index.html">Cardboard Camera to ODS convertor</a>.</p>

<h1>Future work</h1>

<p>I was initially overjoyed by Safari 10's support for inline video texture
playback, which lets us finally play spherical video without <a href="http://stackoverflow.com/questions/29621199/three-js-video-textures-in-ios-play-back-in-a-separately-launched-player-ideas">gross
hacks</a>. Unfortunately, their current video texture rendering performance
is pretty abysmal. I'm getting about 10 FPS on a 2K (2048 x 2048) spherical
video in Cardboard mode, while Chrome, even on older Android hardware performs
substantially better.</p>

<p>Many thanks to <a href="https://twitter.com/lbayliss">Leon Bayliss</a> for writing the <a href="https://storage.googleapis.com/vrview/2.0/examples/index.html">official samples</a>
and test the API, and to <a href="https://twitter.com/aerotwist">Paul Lewis</a> for implementing tree shaking to
substantially reduce the size of the library.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Election 2016</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/election-2016"/>
    
    <updated>2016-11-13T09:00:00-00:00</updated>
    
    <id>https://smus.com/election-2016</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>Back in June, I gave Trump a coldhearted 55% chance of winning the 2016 US
election. You don't have to believe me, since I recorded it in my
<code>Predictions.md</code> file, and never on <a href="http://gjopen.com">gjopen.com</a>, where it
belongs. My assessment was mostly based on anecdotal observations that recent,
related polls have been terribly wrong. Brexit and then Trump's surprise
Republican nomination both came as a complete surprise to experts from all
sides.</p>

<p>But, despite my dire predictions, it somehow didn't <em>feel</em> that I could be right
on the eve of the election. When the final result was revealed, I was just as
disturbed as everyone else. In retrospect, I attribute my "successful"
prediction mostly to luck combined with my apparently contrarian tendencies,
rather than to skill. Nearly a week after the announcement of President Trump,
I'm still processing the verdict. Two big questions loom: 1) Why did he win?,
and 2) Why didn't we see it coming?</p>

<!--more-->

<h1>Who voted for him?</h1>

<p>Trump's electoral base was quite diverse. Working class whites were only a small
part of his base. I was surprised to learn that 37% of 18-29 year olds, and 29%
of hispanics voted for him, and that the average Trump voter had an annual
income of $71K.</p>

<p><strong>Single issue voters</strong>. Many people value only one issue, or weigh a single issue
so heavily that others pale in comparison. Fundamentalist Christians may really
want a pro-life president. If they care enough about that issue, they would be
willing to deal with a Buffoon, and forgive all the hate, racism and bigotry in
the world.</p>

<p><strong>People with nothing to lose</strong>. I suppose that the economic situation many people
are facing are worse than priveledged people like me can fully appreciate. If
you are low on Maslow's heirarchy, liberal values tend to fall by the wayside.
When offered a chance to burn it down and start fresh, people with nothing are
willing to oblige.</p>

<p><strong>Tired of walking on eggshells</strong>. The left won the culture war. In many
environments: at work, in universities, etc, expressing a dissenting opinion
puts you in dire straits. Loud and self-righteous activists have a trained ear,
and will be incredibly quick to label you a racist or a bigot for merely
entertaining certain notions, or bringing up controversial questions despite no
intended harm.</p>

<p>This atmosphere leads to reduced viewport diversity. Without the ability to have
an honest, civil conversation about difficult topics, people have fewer
opportunities to change their minds, and become deeper and deeper entrenched in
their current beliefs.</p>

<p><strong>Genuine xenophobes</strong>. Certainly some of Trump's electoral base are actually
racist immigrant haters. It's very hard to imagine a racist voting for Hilary.
But I continue to believe that the visibility of this small group is magnified
by media bias. Turns out it's really interesting to read about crazed people on
the fringe.</p>

<p>But as a side note, a group's support of a candidate doesn't imply that all or
most of the candidate's supporters are members of the that group. And this
statement holds even for deplorables: when the candidate is Trump and the group
is the KKK.</p>

<h1>Why did he win?</h1>

<p><strong>Attention economy</strong>. Goebbels supposedly said "If you repeat a lie often enough,
people will believe it, and you will even come to believe it yourself." Despite
his purported hatred of the media, I think Trump was really helped by it. The
amount of free publicity Trump's shenanigans received even from the most liberal
publications like NYTimes is staggering (and any publicity is good publicity).</p>

<p><strong>Terrible alternatives</strong>. It's hard to get excited about a candidate because they
won't burn the country down. Many people simply could not in good conscience
support Clinton because of legimiate grievances.  Yet Trump is clearly a
complete Buffoon. And principled voters found themselves stuck between a rock
and a hard place, hence the low turnout.</p>

<p><strong>Complacency due to expected outcomes</strong>. In the weeks leading up to the election,
many papers prominently featured polls leaning heavily in favor of Clinton. The
NYT ran an election forecast on their front page which depicted the race being
closest in July, giving Clinton a 30% lead over Trump. The day before the
election, Clinton had a whopping 70% lead. Why would you go out and vote if you
know that it's going to be Clinton anyway?</p>

<h1>Why didn't we see it coming?</h1>

<p>Experts and laymen love to tell you what will happen in the the next five years.
Yet with Trump and Brexit, we have collectively been unable to predict what will
happen the next day. Given this sad observation, whatever model we are using is
clearly broken. We must look inward, critically questioning many deeply held
assumptions about the world, or be prepared for a lot more surprises.</p>

<p><strong>Polling is broken</strong>. A lot of predictions rely on polls. 538 was based mainly on
aggregating existing poll data. Nate Silver's theory was that by including
enough polling companies, inacuraccies in each poll would be ironed out.
Unfortunately, if all of the polls are systematically skewed, this approach is
screwed. And I think it is! Most polling is done by phone, which is quite
different from a secret ballot. Imagine you are a disenfranchised voter and a
pollster from Gallup calls you, you would naturally tell them to go fuck
themselves.</p>

<p><strong>Filter bubbles distort reality</strong>. On election day, my twitter feed proudly
announced that "I'm With Her". The day after, when Trump won, people mourned,
observed that it was the anniversary of Kristallnacht, ushered in the antichrist
and took to the streets yelling "Not My President!". Yet of half the voting
population (not my twitter feed though) celebrated a 'uuuuuge victory.</p>

<h1>Parting thoughts</h1>

<p>The world is shrinking, and this is not always a good thing. Long ago, you would
be born into a village, hang out with the butcher and the baker and be forced,
by virtue of your birth, to listen to the candlestick maker's racist ramblings.
Affordable air travel allowed us to self-organize according to professions,
beliefs, and lifestyles. Social networks are hypersonic airplanes for the mind.
Did someone tweet something you didn't quite like? Relief is just one unfollow
away. And so we end up in an increasingly polarized world, a bimodal
distribution with increasing peak separation.</p>

<p>I am deeply concerned about the political future of the US. Now is the time for
supposedly open minded liberals like me to prove it. This means not running away
to Canada, but accepting the democratically elected president. It means turning
inward and trying to understand why the world behaves so differently from our
internal model, and starting by fixing the model. I recommend <a href="https://www.amazon.com/Righteous-Mind-Divided-Politics-Religion/dp/0307455777">Righteous Mind by Jon
Haidt</a>
as a relevant starting point.</p>

<p>Without conversation, there is no hope.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Ray Input: WebVR interaction patterns</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/ray-input-webvr-interaction-patterns"/>
    
    <updated>2016-10-11T09:00:00-00:00</updated>
    
    <id>https://smus.com/ray-input-webvr-interaction-patterns</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>What would the web look like if there were no scrollbars, no mouse cursors, and
no clickable links? That's what VR is like today. On one hand, this is great!
Developers are completely free to build however they want, leading to a lot of
interesting experiments. On the other hand, it takes a lot of engineering effort
to just get basic interactions up and running. Furthermore, it lacks
consistency. The alluring promise of being able to navigate from world to world
may be diluted by the frustration of having to rediscover new interaction
paradigms every time.</p>

<p>While sane interaction defaults are badly needed, baking them into the platform
violates principles of the <a href="https://github.com/extensibleweb/manifesto">Extensible Web</a>. With that in mind, I
implemented a basic Ray-based interaction library called <a href="https://github.com/borismus/ray-input">RayInput</a>, which
provides reasonable defaults for interacting with 3D objects in and outside of
VR. Here's what the interaction looks like on various platforms:</p>

<iframe width="600" height="340" src="//www.youtube.com/embed/gjj2XQYC998" frameborder="0" allowfullscreen></iframe>

<!--more-->

<h2>What does Ray Input actually do?</h2>

<p>Ray Input aims to provide reasonable interaction defaults, relying on the
hardware available for each platform:</p>

<ul>
<li>On desktop, look around by dragging, interact by clicking.</li>
<li>On mobile, look around via magic window or touch pan, interact by tapping.</li>
<li>In VR, interaction depends on a reticle or on a ray.
<ul>
<li>If there is no controller (eg. Cardboard), use a gaze based reticle to
interact with objects.</li>
<li>If there is a 3DOF controller (eg. Daydream), apply an arm model and
interact with objects using a ray emanating from the controller.</li>
<li>If there is a 6DOF controller, interact with objects using the ray.</li>
</ul></li>
</ul>

<p>Of course, you may want to customize your interactions on a per-platform basis.
For example, if you are developing an application primarily for the Vive, you
may want to take advantage of the specific richness that a Vive controller
provides. Ray Input is not meant to be prescriptive, merely to provide
reasonable defaults.</p>

<h2>API</h2>

<p>The library's API is documented on the <a href="https://github.com/borismus/ray-input">github page</a>, and I also provide a
<a href="https://borismus.github.io/ray-input">simple example that uses Ray Input</a> to pick items from a 2D menu.</p>

<h2>Arm models for orientation-only controllers</h2>

<p>If a VR controller is present, Ray Input defaults to using a ray-based input
method, which behaves much like a laser pointer.</p>

<p>The Daydream View controller is not position tracked. The only pose information
it provides is the orientation, which is in the same coordinate system as the
head. Where should we position such orientation-only (3DOF) controllers? In
particular, where should the ray come from? Having it emanate from the stomach
or head, like the arm of an exotic god, would be very unnatural. So we need to
be slightly more clever.</p>

<p>Enter the arm model, which, given a controller orientation, spits out a
plausible controller position. Obviously the position it provides is only a
reasonable guess, and may not correspond to the controller's actual position.
But it sure is a lot better than nothing. This sort of problem is common in
graphics and robotics, and can be solved with inverse kinematics.</p>

<p>In this case, we follow a simpler approach. Most of the <a href="https://github.com/borismus/ray-input/blob/master/src/orientation-arm-model.js">code to do
this</a> is lifted from a native implementation of Daydream arm model.
To debug it, I built a very rough simulator, which lets you specify the
orientation of a virtual head and hand, run it through the model, and visualize
the resulting pose of the controller:</p>

<p><a href="https://borismus.github.io/ray-input/daydream-simulator.html"><img src="/ray-input-webvr-interaction-patterns/arm-model.png" alt="Daydream arm model simulator" /></a></p>

<p>As always, very open to feedback, bug reports, and pull requests via
<a href="https://github.com/borismus/ray-input">github</a>.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Copresence in WebVR</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/copresence-webvr"/>
    
    <updated>2016-08-08T09:00:00-00:00</updated>
    
    <id>https://smus.com/copresence-webvr</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>The web platform is uniquely great for networked copresence. To demonstrate, I
built a multi-user chat prototype that uses peer-to-peer audio and data
connections to establish a virtual audio experience.  Voices are spatialized
based on the position and orientation of each participant (using Web Audio).
Also, you can shrink and grow, which, in addition to changing your avatar's
size, pitch shifts your voice. Large avatars have deep, god-like voices, while
smaller ones start to sound very mousey!</p>

<iframe width="560" height="395" src="//www.youtube.com/embed/FPJDNQJt2DQ" frameborder="0" allowfullscreen></iframe>

<p>Check out the <a href="https://borismus.github.io/copresence-vr/">demo for yourself</a>. It works on desktop (mouse look and
spacebar triggers movement), on mobile (magic window) and in VR (through the
<a href="https://webvr.info/">WebVR API</a>, via <a href="https://github.com/borismus/webvr-polyfill/">the polyfill</a>).</p>

<!--more-->

<h1>Better together: copresence is compelling</h1>

<p>The best things in life are enjoyed in good company. Virtual experiences are no
exception. My fondest gaming memories were from two decades ago with close
friends huddled around a CRT, whether it was Morris the Moose and Blombo the
Elephant <a href="https://3drealms.com/catalog/wacky-wheels_16/">racing around</a> the track, or co-strategizing in <a href="https://www.youtube.com/watch?v=hBrYtNTOTyE">Civ</a>. It
wasn't so much about the games, more about the people, and the experience of
being there together.</p>

<p>Putting a computer on your face greatly increases your odds of having an
isolating experience.  One of the biggest downsides of VR is that social
experiences are much harder to produce. While physically copresent VR is
possible, it presents logistical challenges. And since you are fully immersed in
a virtual world, the physical presence of your friends is nearly irrelevant.
Given the constraints, perhaps the best remedy to loneliness is to provide
networked friends. This can be fun too! <a href="https://en.wikipedia.org/wiki/Warcraft:_Orcs_%26_Humans">Orcs and Humans</a> over PBX, anyone?</p>

<h1>WebAudio + WebRTC + WebVR = ❤</h1>

<p>The web is the ideal platform for building copresent VR experiences. VR
copresence requires low latency connections between peers. It also requires a
real time audio channel, with a much smaller emphasis on remote video, since the
user is wearing a headset and their face is obscured. The powerful Web Audio
API has long been available on all modern browsers, and is well equipped for
processing audio of all sorts: spatialization, effects. WebRTC is widely
available too, with <a href="http://www.apple.com/safari/">one unfortunate exception</a>. And with the exception
of Service Workers and company, if you're on the web, you have connectivity. </p>

<p>Thanks to some <a href="http://crbug.com/121673">excellent bug squashing</a>, it's now possible to pipe
remote WebRTC streams into a Web Audio context. This enables devs to spatialize
and otherwise manipulate the remote stream to their heart's content.
Specifically, the prototype I'm launching today has a few fun audio features:</p>

<ul>
<li><p>Each remote stream is spatialized based on the pose of the peer using the
<code>PannerNode</code> (see <a href="/spatial-audio-web-vr/">my previous post</a> about this for more details).</p></li>
<li><p>Remote streams are analyse for voice activity, using an <code>AnalyserNode</code> to
inspect the frequency content between 300 Hz and 3400 Hz (the typical human
vocal range), and doing a simple thresholding. This is then used to animate
the Southpark-style avatar's mouth.</p></li>
<li><p>Changing the size of your avatar also changes how you hear your peer's voice.
I'm using the <a href="https://github.com/mmckegg/soundbank-pitch-shift">soundbank-pitch-shift</a> library to achieve this, courtesy of
<a href="https://twitter.com/cwilso">Chris Wilson</a> and <a href="http://twitter.com/MattMcKegg">Matt McKegg</a>.</p></li>
</ul>

<h1>Technical details: in the weeds with WebRTC</h1>

<p>Hoping to avoid learning the intricacies of WebRTC, which is a fairly low level
and intimidating API, I started exploring higher level abstractions around it.
The most popular wrapper I found was <a href="http://peerjs.com/">peer.js</a>, but unfortunately the
project doesn't seem to be actively maintained, and relies on a special Node.js
WebSocket server which, in my experience, often drops clients.</p>

<p>So I moved to Firebase which, in my implementation, performs the duty of
signaling server, and also maintains a roster of all connected users and their
current state. For each connected user, we store their display name (which
clients can set), and the room ID (if the user is currently in a room).</p>

<pre><code>{
  username: 'Your Name',
  roomId: 'A Random Room Identifier'
}
</code></pre>

<h2>Bird's eye view of WebRTC</h2>

<p>Having moved away from peer.js, I could no longer afford to let the intricacies
of WebRTC be handled by some third party, and had to get into the weeds. It was
especially important to understand how to handle multiple <code>RTCPeerConnections</code>
necessary for the case with more than peer-to-peer. Although I found the docs to
be quite obtuse, the core of the WebRTC API is fairly straight forward:</p>

<ol>
<li><p>The caller (A) gets its local stream and uses the signal server to send an
"offer" message to the callee (B), which includes information about A's local
stream.</p></li>
<li><p>The callee (B) gets A's "offer" and registers A's local stream as its remote
stream. It then gets its own local stream, and responds A's offer via the
signal server, sending an "answer" message to the caller (A), which contains
its own local information.</p></li>
<li><p>The caller (A) gets B's "answer" and registers B's local stream as its remote
stream. At this point, both A and B have basic information about one
another's local and remote streams.</p></li>
<li><p>At this point, A and B exchange ICE (Interactive Connectivity Establishment)
Candidates to work out the details of how to establish a peer-to-peer stream.
Eventually, when both sides are satisfied, we have contact.</p></li>
</ol>

<p>Hopefully the above serves as a useful summary. It certainly will be for me, as
I found the existing WebRTC documentation confusing. Many of the samples connect
to themselves, which does not give a great sense of what the protocol between
clients should actually be.</p>

<p>At the ICE stage, invoke more acronyms! STUN and TURN come into play in trickier
network topologies (ie. those involving NAT servers).  Google already provides a
STUN server by default, and I ended up using a <a href="http://xirsys.com/">free service</a> for TURN
server support. Each <code>RTCPeerConnection</code> is initialized with the specific STUN
and TURN servers that we use.</p>

<h1>Copresence is essential for VR</h1>

<p>Given the inherent isolation of virtual reality, copresence becomes an even more
compelling ingredient than ever before. Copresence is essential for VR, and the
web is a great place to make it happen. <a href="https://borismus.github.io/copresence-vr/">Try it out</a> with a friend, or
using two of your own devices. Oh, and if you find bugs, please let me know via
<a href="https://github.com/borismus/copresence-vr">github</a>.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Inspirata: for what inspires you</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/inspirata"/>
    
    <updated>2016-05-31T09:00:00-00:00</updated>
    
    <id>https://smus.com/inspirata</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>My site has a little section called <a href="http://smus.com/inspiration/">Clippings</a>. It's meant as a visual
record of some of the things I've found inspiring on the web. How do I add
new items to this visual record? Well, I'm glad you asked!</p>

<p>About a year ago, I cobbled together a Chrome extension for exactly this
purpose: screen grabs from any webpage. Releasing it on the webstore has been on
my backburner ever since. Over the last few weeks, I've spent a bit of time
improving it and today, I'm ready to release it for broader testing. I call it
<a href="https://inspirata.xyz">Inspirata</a>. Inspirata can be downloaded from the <a href="https://chrome.google.com/webstore/detail/oaddmiclfpjkcehhbhhojmphflhlompo">Web Store</a>, and it
works like this:</p>

<ol>
<li>Click the Inspirata icon button.</li>
<li>Select part of the page to save.</li>
<li>Enter an optional caption, et voila!</li>
</ol>

<p><img src="/inspirata/inspirata.gif" alt="How Inspirata works video" /></p>

<!--more-->

<h1>Bookmarks and breadcrumbs</h1>

<p>Bookmarks are like breadcrumbs. Hansel and Gretel left a trail of them to follow
home. But GPS made this application of breadcrumbs obsolete! In a similar
fashion, search engines killed bookmarking. Rather than browsing your curated
bookmarks to find your way to content, you can just search for it.</p>

<p>Bookmarks, like breadcrumbs, go stale quickly. When a website goes down, the
bookmark becomes useless, just like Hansel's breadcrumbs which got eaten by
birds. When a page does dark, your bookmark leaves no record of what used to be
there. And when your bookmarking service gets turned down, say bye bye to your
carefully curated archive!</p>

<p>Bookmarks, like breadcrumbs may be <a href="http://del.icio.us/">delicious</a> but aren't very appealing. A
URL has no appeal in itself, only the content at that URL does. And the bookmark
does not capture anything about that content: neither the content itself, nor
the presentation, nor a deep link into which part of that content spoke to you.
A pile of breadcrumbs, like a pile of bookmarks, is pretty nondescript.</p>

<h1>Why?</h1>

<p>I'm an avid user of little paper notebooks that I carry around in my pocket,
along with a trusty black pen. Sometimes while strolling down Valencia St on an
errand, I'll have an idea, stop and jot down it down using a wall or street
light for support. This way I don't get sucked into my phone, and capture
whatever's on my mind. Even if I don't revisit my note, the act of writing
itself has served a purpose. This notion is found explicitly in the <a href="https://fieldnotesbrand.com/">Field
Notes</a> tagline: "I'm not writing it down to remember it later, I'm
writing it down to remember it now". </p>

<p>Inspirata serves a similar purpose for content on the web. If something inspires
me, I want to capture it, not for the purpose of revisiting later or sharing
socially, but for the act itself. Perhaps a utilitarian argument can be made as
well: being on the lookout for inspiration helps to maintain a sharp and active
eye.</p>

<p>I am interested in being more creative in my consumption. Over the last year, I
have developed a habit of writing a summary after finishing each book, as if I
was going to share it with others. This forces me to gather my thoughts on the
subject, sometimes even taking notes while reading or listening to make my
summary more complete. I'm hoping this will increase retention and engagement.
Inspirata can perhaps serve a similar purpose in my web browsing endeavors.</p>

<h1>Ease of use is (always) key</h1>

<p>I continue using notebooks because of their amazing usability. The battery is
never dead, it's quick to turn on (open up, uncap pen, good to go), and writing
is a pleasant experience overall (perfect pen tip tracking, zero latency).
Technology at its finest!</p>

<p>I tried to make Inspirata as minimal and convenient as possible. One click on
the browser action button, select the area of interest, leave an optional
comment, and you're set.</p>

<p>There are a few entry points into capturing content:</p>

<ul>
<li>By clicking the Inspirata browser action (extension button), and clipping part
of the webpage you're currently on.</li>
<li>By right clicking an image and saving the inspiration.</li>
<li>By selecting text and right clicking it as above, or by clicking the extension
button.</li>
</ul>

<p>In all cases, Inspirata ultimately saves an image, even if the content in
question is text. An image of just the text can give interesting additional
context, such as layout and typography which is missing from the content
itself.</p>

<h1>Public by default and presentation agnostic</h1>

<p>Images captured with Inspirata are added to a public Firebase and hosted in the
Firebase bucket storage, which is part of the new <a href="https://firebase.google.com/docs/">Firebase 3.0
platform</a>. As an aside, the new Firebase platform is pretty amazing
once you work out some migration kinks. In terms of security, only you can
publish new Inspirata for yourself, but your Inspirata are publically available. </p>

<p>The Inspirata website <a href="https://inspirata.xyz">https://inspirata.xyz</a> provides a default public gallery
view. To give you a sense of what this looks like, here is <a href="https://inspirata.xyz/?uid=US3UvWWOBhhi21AZgKkyUK0QTHL2">my public
gallery</a>. Each users' feed is <a href="https://project-4121485576010625868.firebaseio.com/users/US3UvWWOBhhi21AZgKkyUK0QTHL2.json">served as JSON</a>, which is how its
stored in Firebase. This means your list of inspirata can be presented in any
way you like on any embedding website (as I have done <a href="http://smus.com/inspiration/">on mine</a>). This, as
far as I know, is not possible in other visual bookmarking services.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Browsing Wikipedia in VR</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/wikipedia-vr"/>
    
    <updated>2016-05-26T09:00:00-00:00</updated>
    
    <id>https://smus.com/wikipedia-vr</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>WebVR provides a solid technical foundation on which to build compelling VR
experiences. But it does not answer a critical question, which is the topic of
this post:</p>

<blockquote>
  <blockquote>
    <p>What could the web become in a Virtual Reality environment?</p>
  </blockquote>
</blockquote>

<p>Gear VR provides a simple and straightforward answer: same same. The fundamental
unit is still a page, but you use the immersion of VR to increase your effective
screen size. The input constraints result in a worse experience for the user.
Scrolling with your finger on your temple is tiring and head-based typing is a
massive pain. Given the input constraints, we need to beef up the output and
make it better matched to what VR excels at. A responsive design inspired
solution would involve deconstructing the page to better suit the nature of the
immersive environment.</p>

<p>Another approach is to make a clean break from legacy web content. What if
certain web pages had parallel content tailored for virtual reality? In this
post, I'll explore this idea with an example focused on Wikipedia.</p>

<p><a href="https://youtu.be/HcSvBAEXcWA"><img src="/wikipedia-vr/vr-forest.jpg" alt="Video of VR Wikipedia" /></a></p>

<!--more-->

<h1>Navigating the VR Forest</h1>

<p>The web in general and Wikipedia specifically covers a vast amount of
information &mdash; nearly everything. Everything is a daunting place to start,
so we will begin with something specific: a forest.</p>

<p>To begin, navigate to a (fake) <a href="http://borismus.github.io/wikipedia-vr/pages/moose">wikipedia article about a moose</a>, and hit
the VR button. This takes you to a forest meadow, with a life sized moose in front of
you. You are free to move inside the forest (focus on the grass and click) and
interact with other animals in it. Looking at an animal gives you some basic
Wikipedia-inspired information about it.  Clicking it focuses you in on
it and presents options. If you leave VR when focused on an animal, you end up
on the associated Wikipedia article. This closes the navigation loop: you can
start from one webpage, enter VR mode, navigate to another entity inside VR,
leave VR and end up on another webpage.</p>

<p>Try out this <a href="http://borismus.github.io/wikipedia-vr/pages/moose">Wikipedia VR sample</a> on your mobile phone in Cardboard. It
also works on desktop using the spacebar to simulate the Cardboard click.</p>

<p><img src="/wikipedia-vr/navigation.mp4.gif" alt="Video of navigating between pages" /></p>

<p>VR and education are naturally matched. As <a href="https://youtu.be/UuceLtGjDWY?t=1m40s">Ben explains eloquently in an I/O
talk</a>, "VR is a chance to scale experiential learning". Remembering what
you learned in class is much harder than remembering your favorite vacation. In
this Wikipedia example, you immediately get a sense of the animal's grandeur,
which is hard to convey in words and images. You can get a feeling for quickly
it runs, and what it sounds like. </p>

<h1>Closing thoughts</h1>

<p><strong>Tip of the iceberg</strong>. The entities in this Wikipedia demo (in green) represent
a tiny subgraph of Wikipedia:</p>

<p><img src="/wikipedia-vr/knowledge-graph.png" alt="Picture of the subgraph we implement" /></p>

<p>All of the above are positioned in a much bigger subgraph of Wikipedia which
might can be represented in VR. Of course, many Wikipedia pages are really
difficult to imagine in VR. Could <a href="https://en.wikipedia.org/wiki/Philosophy">Wikipedia's Philosophy</a> article have a
compelling VR version?</p>

<p><strong>Changing scale</strong>. The ability to change scale would make it possible to place
every entity from the above graph into VR Wikipedia. Imagine diving into the
hide of the moose, learning about symbiotic insects and hair folicles, then
going deeper to learn about the structure of hair on a molecular level. Or vice
versa, zooming out to look at planet Earth to see where moose live, or going
into an abstract view to explore Family Cervidae. It's easy to lose an hour or
two in Wikipedia's hyperlink maze. One day, it may be even easier to do this in
VR.</p>

<p><img src="/wikipedia-vr/alice-shrinking.jpg" alt="Alice in wonderland shrinking" /></p>

<p><strong>Content is king</strong>. The big open question is how to generate this content. Even a
scoped down project to VR-ify categories of Wikipedia pages (say, only forest
animals), is incredibly ambitious. Where do you get all of the models? How do
you animiate them to run, jump, stand around, sleep, play, eat and be eaten? How
do you place them in a the forest in a meaningful way? Doing this automatically
seems, at a glance, AI-hard.</p>

<p><strong>Limited knowledge graphs</strong>. Even if you imagine that we have a series of animated
models, how do we compose them together? Do moose and canaries live in the same
environment? Can you find fire ants in the bark of a Sequoia? How many? How big
are hyenas, and how quickly do they run? This information is missing from even
the best known knowledge graph.</p>

<p><strong>3D modeling is difficult</strong>. Wikipedia's giant corpus of quality content exists
because it's easy for many people to collaborate. Wikipedians need to be good
writers, well versed in their topic, and motivated to contribute. There is a
technical barrier - learning Wikipedia markup - but it is not incredibly
difficult. For a Wikipedia in VR, the technical barriers are much higher. Even
with a good collaborative editor, it seems inevitable that contributors would
need to have some sense of 3D modelling, and a far more specialized skillset.</p>

<p><strong>Artistic considerations</strong>. One of the challenges for a large community project
like Wikipedia is establishing a consistent style. Imagine if every Wikipedia
image was hand drawn. Artistic abilities vary wildly, and you can imagine a
funny and chaotic result. Aaron Koblin's now classic <a href="http://www.thesheepmarket.com/">Sheep Market</a>
experiment comes to mind:</p>

<p><img src="/wikipedia-vr/sheep-market.png" alt="Sheep market screenshot" /></p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Three approaches to VR lens distortion</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/vr-lens-distortion"/>
    
    <updated>2016-04-20T09:00:00-00:00</updated>
    
    <id>https://smus.com/vr-lens-distortion</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>Immersion requires a large field of view. This could be achieved by putting a
large curved spherical display on your face, but alas such technology is
prohibitively expensive. A more affordable solution to increasing the field of
view is to look at small ubiquitous rectangular displays through lenses:</p>

<p><img src="/vr-lens-distortion/how-lenses-increase-fov.png" alt="Why VR needs lenses" /></p>

<p>Lenses placed close to your eyes greatly increase your field of view, but there
is a cost: the image becomes spherically distorted. The larger the field of
view, the more distorted the image. This post is a quick summary of three
different approaches to undistorting the image, all of which have been
implemented in JavaScript for various WebVR-related projects.</p>

<!--more-->

<p>Here is a closer look at the lens distortion of a typical head mounted display.
The lenses cause a pincushion effect:</p>

<p><img src="/vr-lens-distortion/pincushion-distortion.png" alt="Pincushion distortion due to lenses" /></p>

<p>The solution is to apply barrel distortion to the image. When we look at it through
the distorting lenses, the image looks neutral:</p>

<p><img src="/vr-lens-distortion/barrel-predistortion.png" alt="Barrel pre-distortion" /></p>

<p>Lens distortion is well understood mathematically, governed by equations <a href="https://en.wikipedia.org/wiki/Distortion_(optics)#Software_correction">like
these</a>, with distortion coefficients corresponding to the particular lens.
To undo the distortion properly, we also need to calculate the centers of the
eyes, which requires knowing a bit about the geometry of the display and the
enclosure itself. This can all be done, even on the web! I summarize a few
implementation options below.</p>

<h1>1. Fragment based solution (bad)</h1>

<p>The simplest way to using two pass rendering. First, we render the left and
right eyes onto a texture, and then process that texture with a fragment (pixel)
shader, moving each pixel inward in relation to the centroid of the eye:</p>

<p><img src="/vr-lens-distortion/dense.png" alt="Per-pixel based distortion" /></p>

<p>This is the first and simplest method, which is also the least efficient, since
each pixel is processed separately. The <a href="https://github.com/borismus/webvr-boilerplate/blob/d91cc2866bd54e65d59022800f62c7e160dc9fee/src/cardboard-distorter.js">first version</a> of the
WebVR Boilerplate implemented this method.</p>

<h1>2. Mesh based solution (better)</h1>

<p>Rather than processing each pixel separately, we distort the vertices of a
relatively sparse mesh (40x20 works well). </p>

<p><img src="/vr-lens-distortion/sparse.png" alt="Mesh based distortion" /></p>

<p>This can save some direct computation and let the GPU do a fair amount of
interpolation. Rather than having to apply to every single pixel (<code>1920 * 1080 ~=
2e6</code>), we do the calculation for every vertex in the mesh (<code>40 * 20 = 800</code>). The
result is a significant reduction (3 magnitudes or so) of computation, and a
nice boost in performance. The <a href="https://github.com/borismus/webvr-polyfill/blob/master/src/cardboard-distorter.js">WebVR Polyfill</a> currently implements
this approach.</p>

<p>Applying distortion isn't the only expensive part in this rendering method. A
lot of time is wasted copying the whole scene to an intermediate texture.</p>

<h1>3. Vertex displacement based solution (best)</h1>

<p>This brings us to the most efficient method of the three, which eliminates the
need to render to an intermediate texture in the first place. In this approach,
the geometry itself is distorted using a custom vertex shader. The idea is that
knowing the position of the camera, we can displace vertices in such a way that
the resulting 2D render is already barrel distorted. In this case, no shader
pass is needed, and we save the expensive step of copying the rendering into a
texture. </p>

<p>This method does require a certain vertex density on every mesh that is being
deformed. Imagine the simple case of a large, 4-vertex rectangle being
rendered very close to the camera. Distorting these vertices would still yield a
4-vertex flat rectangle, and clearly there's no barreling effect. Because of
this, this is method does not generalize without extra work on the
developer's part.</p>

<p><img src="/vr-lens-distortion/cdl.png" alt="Cardboard Design Lab screenshot" /></p>

<p>This approach is used in the <a href="https://github.com/googlesamples/cardboard-unity/tree/master/Samples/CardboardDesignLab">Cardboard Design Lab</a> and in the open sourced
<a href="https://github.com/google/vrview/blob/master/src/vertex-distorter.js">VR View project</a>. Geometry-based distortion can also result in sharper
looking renderings, since the two pass approach can cause aliasing, especially
if the intermediate texture is small. You can read more about this distortion
method in <a href="http://www.gamasutra.com/blogs/BrianKehrer/20160125/264161/VR_Distortion_Correction_using_Vertex_Displacement.php">this helpful explainer</a>.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Embedding VR content on the web</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/vr-views"/>
    
    <updated>2016-03-31T09:00:00-00:00</updated>
    
    <id>https://smus.com/vr-views</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>During a two week trip to India, I took over 1000 shots, including photos,
videos and a few photospheres. A picture is worth one thousand words, but how
many pictures is a photosphere worth? We may never know, but I digress. My
favorite photosphere was of friends posing inside one of the turrets of the
Jaigarh Fort:</p>

<iframe class="vrview" width="100%" height="300px" allowfullscreen frameborder="0" src="//storage.googleapis.com/vrview/index.html?image=//smus.com/vr-views/india-photosphere-4096.jpg&preview=//smus.com/vr-views/india-photosphere-1024.jpg&is_stereo=false"></iframe>

<script>
function DeviceMotionSender(){if(!this.isIOS_()){return}window.addEventListener("devicemotion",this.onDeviceMotion_.bind(this),false);this.iframes=document.querySelectorAll("iframe.vrview")}DeviceMotionSender.prototype.onDeviceMotion_=function(e){var message={type:"DeviceMotion",deviceMotionEvent:this.cloneDeviceMotionEvent_(e)};for(var i=0;i<this.iframes.length;i++){var iframe=this.iframes[i];var iframeWindow=iframe.contentWindow;if(this.isCrossDomainIframe_(iframe)){iframeWindow.postMessage(message,"*")}}};DeviceMotionSender.prototype.cloneDeviceMotionEvent_=function(e){return{acceleration:{x:e.acceleration.x,y:e.acceleration.y,z:e.acceleration.z},accelerationIncludingGravity:{x:e.accelerationIncludingGravity.x,y:e.accelerationIncludingGravity.y,z:e.accelerationIncludingGravity.z},rotationRate:{alpha:e.rotationRate.alpha,beta:e.rotationRate.beta,gamma:e.rotationRate.gamma},interval:e.interval}};DeviceMotionSender.prototype.isIOS_=function(){return/iPad|iPhone|iPod/.test(navigator.userAgent)&&!window.MSStream};DeviceMotionSender.prototype.isCrossDomainIframe_=function(iframe){var html=null;try{var doc=iframe.contentDocument||iframe.contentWindow.document;html=doc.body.innerHTML}catch(err){}return html===null};var dms=new DeviceMotionSender;
</script>

<p>I captured this using the photosphere camera which ships with Android. It's
embedded into my blog using <a href="https://developers.google.com/cardboard/vrview">VR View</a>, which <a href="https://developers.googleblog.com/2016/03/introducing-vr-view-embed-immersive.html">launched
today</a>. The embed above lets you include an interactive photosphere
right in your website, which is especially fun on mobile, where the image reacts
directly to your phone's movements. You can view it in full screen mode, and
even in Cardboard mode (only on mobile).</p>

<p>But you know what's cooler than a photosphere? A stereo photosphere! And
luckily, you can capture stereo photospheres using <a href="https://play.google.com/store/apps/details?id=com.google.vr.cyclops&amp;hl=en">Cardboard Camera</a>, and
then use a VR View to <a href="https://storage.googleapis.com/vrview/examples/pano/index.html">embed them too</a>. You can even embed mono or
<a href="https://storage.googleapis.com/vrview/examples/video/index.html">stereo videos</a>. Check out <a href="https://developers.google.com/cardboard/vrview">the docs</a> for more info. Eager
to hear what you think!</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Simulating wealth inequality</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/simulating-wealth-inequality"/>
    
    <updated>2016-01-19T09:00:00-00:00</updated>
    
    <id>https://smus.com/simulating-wealth-inequality</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>Economic inequality is rising in the US. A viral video from several years ago
made this abundantly clear:</p>

<p><a href="https://www.youtube.com/watch?v=QPKKQnijnsM"><img src="/simulating-wealth-inequality/video_small.jpg" alt="Wealth inequality in U.S." /></a></p>

<p>The gap between desire, expectation and reality is truly shocking, and inspired
me to learn more. In particular, whether or not inequality is actually a big
problem, and then to better understand issues that the video above did not
address:</p>

<ol>
<li>How did the US become so economically unequal?</li>
<li>How can this inequality be reduced?</li>
</ol>

<p>My answers come in the form of simple simulations. For example, the following
simulation has two agents with different salaries, but the same spending habits.
You can <a href="http://borismus.github.io/inequality-simulator/?model=1-world-income-ineq-doesnt-lead-to-wealth-ineq.js">play with it</a> yourself!</p>

<p><a href="http://borismus.github.io/inequality-simulator/?model=1-world-income-ineq-doesnt-lead-to-wealth-ineq.js">
<video src="/simulating-wealth-inequality/simulator.mp4" autoplay loop style="width: 100%"></video>
</a></p>

<p>In the first part of this post, I try to provide some background on economic
inequality: how to measure it, various forms of it, and whether or not it's a
problem.  In the last part, I try to explain how we got to the status quo, and
how inequality can potentially be reduced. Rather than just making claims, I use
simulations like the one above to defend my claims. This way, you can see more
clearly where I'm coming from, and if you disagree, you can <a href="http://github.com/borismus/inequality-simulator">make your own
simulation</a> with better assumptions.</p>

<!--more-->

<h2>What is economic inequality?</h2>

<p>There are <a href="http://www.economist.com/blogs/freeexchange/2014/07/measuring-inequality">three main ways</a> to measure economic inequality: income,
consumption, and wealth. Income inequality in particular has become a huge
national issue, with <a href="https://berniesanders.com/issues/income-and-wealth-inequality/">some presidential candidates</a> focusing large
amounts of their time addressing it directly, with policies such as the $15
federal minimum wage.</p>

<p>Inequality can be measured using the Gini coefficient. The greater the Gini, the
more unequal a society is. A Gini of 0 means perfect equality: everybody has the
same. A Gini of 1 means perfect tyranny: the winner takes it all. This metric
can be applied to any distribution: wealth, income, or consumption.</p>

<p>All forms of inequality are unequal, but some are more unequal than others! In
general, wealth is the most unequally distributed of the three indicators,
consumption the least. But which measure of inequality is most important to
consider?</p>

<h3>Types of economic inequality</h3>

<p>Income inequality is difficult to measure. What constitutes income? Obviously a
salary is included, but how about investment income? Unsold stocks? Options?
The <a href="https://en.wikipedia.org/wiki/One-dollar_salary">list of $1 salary CEOs</a> is famously long, but what is their effective
income? Pew Research provides <a href="http://www.pewresearch.org/fact-tank/2015/09/22/the-many-ways-to-measure-economic-inequality/">many more reasons</a> why income is hard to
measure, and may not be a meaningful indicator:</p>

<blockquote>
  <p>Some economists say income data have too many flaws to be the primary measure
  of inequality. For one thing, many income inequality measures use income
  before accounting for the impact of taxes and transfer payments. [...]
  In addition, critics of the income-based approach note that an individual’s
  (or household’s) income can vary considerably over time, and may not reflect
  all available economic resources.</p>
</blockquote>

<p>Income also isn't a great indicator for quality of life. Indeed, many economists
agree that <a href="http://www.aei.org/wp-content/uploads/2012/06/-a-new-measure-of-consumption-inequality_142931647663.pdf">consumption inequality</a> is a better proxy for that.
<a href="http://www.economist.com/blogs/freeexchange/2014/07/measuring-inequality">The Economist</a> writes:</p>

<blockquote>
  <p>For the purpose of measuring how inequality affects a community [income
  inequality] is also probably the least interesting yardstick of the three.
  Consumption inequality, though harder to measure, provides a better proxy of
  social welfare. This is because people’s living standards depend on the amount
  of goods and services they consume, rather than the number of dollars in their
  wage packet.</p>
</blockquote>

<p>But consumption inequality has its limitations too. For one, it is difficult to
measure directly. More importantly, consumption is an indication of the current
state, but does not reflect ones ability to deal with the future. Wealth and
consumption are tightly linked. When times get tough, only the wealthy can
maintain their lifestyle by dipping into their savings. Having this reserve of
"potential energy" is especially important in inevitable periods of instability.</p>

<p>This leaves us with wealth inequality, which I will focus on from now on.</p>

<h3>Adverse effects of extreme wealth inequality</h3>

<p>There are plenty of arguments to be made for dangers of high wealth inequality.
The common sense reason is the diminishing marginal utility of wealth. For an
unemployed person, suddenly having a job that pays $40K is a game changer. But
for a top-1% income earner already making $500K, the additional $40K makes no
practical difference.</p>

<p>Another economic argument goes something like this. Low wealth causes reduced
purchasing power, which ultimately means less money going to corporations, fewer
jobs, and a slower economy. More people should have spending power, which will
keep our economy running smoothly. Robert Reich makes this point well in his
moving <a href="http://www.pbs.org/newshour/making-sense/why-robert-reich-cares-so-passionately-about-economic-inequality/">Inequality for All</a>. However, the link between low wealth and
reduced spending is somewhat tenuous, given the much less extreme consumption
inequality distribution.</p>

<p>Wealth as potential energy also has a psychological dimension. Wealth gives some
peace of mind that you have a buffer against unforseeable problems, increasing
well being. This is especially important in countries with weak social programs
and relatively small safety nets. In such scenarios, more people feel the need
to accumulate wealth as a personal buffer.</p>

<p>There is also something philosophically unfair about wealth concentration. What
makes a society fair is a matter of opinion, but the <a href="https://en.wikipedia.org/wiki/Original_position">Original Position</a>, a
thought experiment proposed by John Rawls provides an interesting starting
point. In the Original Position, you and your hypothetical countrymen select
principles that will determine the basic structure of the society you will live
in. This choice is made from behind what he calls a veil of ignorance, which
prevents you from knowing your economic status.</p>

<p>Being born into a society with high wealth inequality, you are subject to a
"lottery of birth". Quoth <a href="http://www.economist.com/blogs/freeexchange/2014/07/measuring-inequality">The Economist</a>:</p>

<blockquote>
  <p>Wealth is also an important metric since it can be inherited, unlike income.
  When wealth inequality increases, the lottery of birth becomes an increasingly
  important determinant of living standards. Consequently, a society which wants
  to ensure an equal level of opportunity, in which outcomes are not closely
  linked to surnames, will endeavour to keep wealth inequality at tolerably low
  levels.</p>
</blockquote>

<h3>Some wealth inequality is good</h3>

<p>Yet clearly we don't want complete economic equality. It's important that people
work and create value. The best way to do this is to incentivize them by
rewarding high performing individuals. History has shown socialist societies
like the Soviet Union fail in part because there was no incentive to work. In Soviet
Russia, wealth inequality was low: everybody except the ruling class had the
same amount of the sad little pie. A small piece of a much larger (eg. American)
pie is better than an average slice of a small (eg. Soviet) one. This <a href="https://www.khanacademy.org/economics-finance-domain/macroeconomics/gdp-topic/piketty-capital/v/inequality-good-or-bad">Khan
Academy video</a> makes this point well.</p>

<p>Wealth inequality alone is not a great indicator of prosperity either. Many
Scandinavian countries have very high wealth inequality, potentially because
life is already so good. According to <a href="http://www.businessinsider.com/why-socialist-scandinavia-has-some-of-the-highest-inequality-in-europe-2014-10">Credit Suisse</a>,</p>

<blockquote>
  <p>Strong social security programs, good public pensions, free higher education
  or generous student loans, unemployment and health insurance can greatly
  reduce the need for personal financial assets.</p>
</blockquote>

<p>Wealth acts as a personal safety net. In countries with significant public
safety nets for ailing citizens, accumulating wealth is less important. Compare
a society with high inequality but a solid public safety net, with one with
equality but no public safety net. From the perspective of Rawls' Original
Position, the solid safety net is preferred, since even if you are the poorest
in such a society, you are still guaranteed a standard of living.</p>

<h2>Modeling inequality</h2>

<p>Inspired by <a href="http://worrydream.com/">Bret Victor</a>, <a href="http://ncase.me/">Nicky Case</a>, and chats with <a href="http://mikejohnstn.com/">Mike
Johnston</a>, I've built a visualizer to give an <a href="http://worrydream.com/ExplorableExplanations/">explorable explanation</a>
of how wealth inequality arises, and how policy changes can reduce it.</p>

<p>The simulation itself is very simple, consisting of a set of rules which can be
defined in JSON, and then households that have property bags. Every year, each
rule is applied to each household in order. The result of each rule is some
change in the net worth of the household. Each simulation is defined by a
collection of rules and actors. In the GUI, you can inspect rules and actors by
clicking on them. The visualization itself is implemented in <a href="http://threejs.org/">three.js</a>.
For more information, check out the <a href="http://github.com/borismus/inequality-simulator">github</a> page.</p>

<p>Using these models, let's jump in and explore some factors contributing to our
current state of wealth inequality. Then, some policies that can change the
status quo.</p>

<h3>Cause 1: Income inequality</h3>

<p>In this first simulation, we consider two households: one with low income and
one with high income. They have the same spending habits, but the high income
household has twice the income.</p>

<p><a href="http://borismus.github.io/inequality-simulator/?model=1-world-income-ineq-doesnt-lead-to-wealth-ineq.js"><img src="/simulating-wealth-inequality/screenshots/1-world-income-ineq-doesnt-lead-to-wealth-ineq.png" alt="Income inequality not sole cause wealth inequality." /></a></p>

<p>As you can see above (or if you click the image and <a href="http://borismus.github.io/inequality-simulator/?model=1-world-income-ineq-doesnt-lead-to-wealth-ineq.js">run the simulation
yourself</a>), such a scenario does not yield huge differences in wealth.
Even a hundred years later, wealth remains proportional to income, so we look to
other factors to explain the wealth inequality we see today.</p>

<h3>Cause 2: Investors win over the long term</h3>

<p>In addition to salaries, households can also invest money. For simplicity,
assume that the net worth of each household is subject to some investment
return. Most Americans (52%) <a href="http://www.gallup.com/poll/182816/little-change-percentage-americans-invested-market.aspx">avoid the stock market</a> entirely, which
cuts them out from any investment income.</p>

<p>In the next simulation, one household does not invest at all, and another
household invests its whole net worth. We assume that the yield is the average
return of the market, which is about <a href="http://www.marketwatch.com/story/8-lessons-from-80-years-of-market-history-2014-11-19">10% from 1930 to 2013</a>.</p>

<p><a href="http://borismus.github.io/inequality-simulator/?model=2-investing-ability.js"><img src="/simulating-wealth-inequality/screenshots/2-investing-ability.png" alt="Simulation of investors vs. non-investors" /></a></p>

<p>This is effectively a demonstration of compound interest. Given the <a href="http://www.bloombergview.com/articles/2015-09-25/the-rich-are-different-they-re-better-investors-">correlation
between wealth and investment ability</a>, the impact of investing is huge on
wealth inequality.</p>

<h3>Cause 3: Entrepreneurship can have huge payoff</h3>

<p>Many of the wealthiest people in the world became so by creating new companies.
Most enterprises fail, but it only takes one incredible success to generate
massive amounts of wealth.</p>

<p>I found that modeling this accurately is very difficult, but for the purposes of
illustration, this next simulation includes three households: a
non-entrepreneur, a regular entrepreneur, and a lucky entrepreneur. An
enterprise failure (10% yearly chance) is modeled as a 5% reduction in wealth,
while a success (1% yearly chance, 2% if lucky) is modeled as a 50% increase in
wealth.</p>

<p>The expected wealth of the regular entrepreneur is the same as the
non-entrepreneur, but the lucky entrepreneur has a 2% chance of success, and
thus a higher expected wealth. Entrepreneurship introduces volatility and can
lead to more wealth inequality.</p>

<p><a href="http://borismus.github.io/inequality-simulator/?model=3-with-entrepreneurship.js"><img src="/simulating-wealth-inequality/screenshots/3-with-entrepreneurship.png" alt="Simulation of entrepreneurs vs. non-entrepreneurs" /></a></p>

<h3>All together: income, investments, and entrepreneurship</h3>

<p>Consider all of these factors together: varying salaries, investment abilities
and entrepreneurial inclinations/luck. Here we have 8 agents with varying
parameters along these dimensions.</p>

<p><a href="http://borismus.github.io/inequality-simulator/?model=4-income-invest-entrepreneur.js"><img src="/simulating-wealth-inequality/screenshots/4-income-invest-entrepreneur.png" alt="Simulation of entrepreneurs vs. non-entrepreneurs" /></a></p>

<p>We can see that after 50 years, we have a Gini of 0.44. In the real world, the
spread of incomes is much greater than here, the most successful entrepreneurs
make orders of magnitudes more than regular employees, and the best investors
are wildly successful. The <a href="https://docs.google.com/spreadsheets/d/1HZBF47q9DYgFj49Q2ZLBb_3gpsoayAuGF_kUdYIM9nQ/pubhtml?gid=0&amp;single=true">real world wealth Gini</a> of the US is
0.8.</p>

<h2>Reducing inequality</h2>

<p>Working within the system, inequality can be reduced through progressive
taxation of the wealthy. However, it's key to avoid becoming a <a href="https://wiki.lesswrong.com/wiki/Paperclip_maximizer">paperclip
maximizer</a> when it comes to the Gini coefficient. Making the
wealthiest slightly less wealthy will certainly reduce the Gini, but will do
little to improve life for actual poor people.</p>

<p>Through additional taxation, the wealthy end up being less wealthy, with the
difference going to the government. Implied is a hope that the government is
capable and sufficiently efficient to use this extra money for good. By
investing in public works, creating relevant jobs, and establishing a more solid
safety net, there is potential to improve lives of those that are less
fortunate.</p>

<h3>Solution 1: Tax capital gains like income</h3>

<p>Compound interest is a powerful force. Once an individual's wealth is large
enough, returns on investing their wealth will exceed even their salary.
However, the US currently imposes a very low capital gains tax, a long-term
capital gain tax rate of 15% for most normal annual incomes.</p>

<p>An easy solution is to increase capital gains taxes, or simply to treat capital
gains like regular income. This would effectively reduce the return rate on
investment and reduce inequality. The following simulation shows what happens
when investment income is taxed at a flat 40%. This is a crude estimate, since
it would actually be subject to a variable tax rate like the income tax, but
gets the point across.</p>

<p><a href="http://borismus.github.io/inequality-simulator/?model=5-higher-capital-gains-tax.js"><img src="/simulating-wealth-inequality/screenshots/5-higher-capital-gains-tax.png" alt="Simulation of higher capital gains." /></a></p>

<p>As you can see, the Gini at 100 years is much smaller than before.</p>

<h3>Solution 2: Estate taxes</h3>

<p>Estate tax is intended as an effective tool for preventing the concentration of
wealth in the hands of a relatively few powerful families. It also encourages
charitable giving, since the money that is to be bequeathed is subject to the
tax.</p>

<p>Estate tax is collected when the deceased transfers their wealth to the
recipient of their inheritance. The deceased's net worth exceeding a certain
threshold is subject to the estate tax rate. Both the threshold and the tax rate
have varied a surprising <a href="https://docs.google.com/spreadsheets/d/1lWzzz6RlMmxWTGYoU9kxxXqL9Q8Pto4cbzZRozVc8wU/pubhtml">amount over time</a>:</p>

<p><img src="/simulating-wealth-inequality/estate-tax-history.png" alt="Historical estate taxes" /></p>

<p>In a previous simulation, we saw what would have happened with no estate tax (as
was the case in 2010, a good year to die). The following simulation shows the
average tax rate since 2000, which is 41%, with a threshold of 100 units.</p>

<p><a href="http://borismus.github.io/inequality-simulator/?model=6-estate-tax.js"><img src="/simulating-wealth-inequality/screenshots/6-estate-tax.png" alt="Simulation of entrepreneurs vs. non-entrepreneurs" /></a></p>

<h3>Solution 3: Wealth taxes</h3>

<p><a href="http://www.cnbc.com/2015/03/10/why-we-need-a-global-wealth-tax-piketty.html">Piketty's solution</a> to inequality is a global wealth tax. The idea is
that individuals with over a certain amount of wealth (here, 100 units) be taxed
at some rate for just maintaining that level of wealth. This seems difficult to
enforce, especially since in a global economy, a single neutral country
(say, Switzerland?) that does not impose a wealth tax will end up being a
natural safe haven for the rich.</p>

<p><a href="http://borismus.github.io/inequality-simulator/?model=7-wealth-tax.js"><img src="/simulating-wealth-inequality/screenshots/7-wealth-tax.png" alt="Simulation of wealth taxes" /></a></p>

<h2>Conclusion</h2>

<p>Geez, you're still here?</p>

<h3>Practical limitations</h3>

<p>Theoretically, inequality is not an insurmountable issue by any stretch. As
shown, by introducing policies like increased capital gains tax, estate tax and
wealth tax, inequality can be reduced. The real question is how much inequality
is actually desirable, and how effective the above policies are in practice.</p>

<p><a href="http://borismus.github.io/inequality-simulator/?model=8-solution.js"><img src="/simulating-wealth-inequality/screenshots/8-solution.png" alt="Estate taxes, wealth taxes and " /></a></p>

<p>In practice, estate tax is often avoided or minimized, <a href="http://www.calculator.net/estate-tax-calculator.html">according to the
Urban-Brookings Tax Policy Center</a>,</p>

<blockquote>
  <p>Among the 3,780 estates that owe any tax, the "effective" tax rate — that is,
  the percentage of the estate's value that is paid in taxes — is 16.6 percent,
  on average. </p>
</blockquote>

<p>A wealth tax is even harder to enforce, since you can simply move your wealth to
a country that does not have a wealth tax.</p>

<h2>Conclusion</h2>

<p><strong>Roll your own</strong>. The good news is that the models above show how inequality
can arise and how inequality can be effectively reduced! The bad news is that I
just made these models up with only a minimal understanding of how the world
works. However, more good news! If you are wise in the ways of economics and/or
have a suggestion for a more accurate, or perhaps more provocative way of
modeling wealth inequality, get in touch! Or if you just want to DIY, simulation
and visualizer are <a href="http://github.com/borismus/inequality-simulator">on the githubs</a>.</p>

<p><strong>Eyes on the prize</strong>. <a href="https://docs.google.com/spreadsheets/d/1HZBF47q9DYgFj49Q2ZLBb_3gpsoayAuGF_kUdYIM9nQ/pubhtml?gid=0&amp;single=true">Zimbabwe and Denmark</a> both have high
wealth ginis (over 0.8), while <a href="https://docs.google.com/spreadsheets/d/1HZBF47q9DYgFj49Q2ZLBb_3gpsoayAuGF_kUdYIM9nQ/pubhtml?gid=0&amp;single=true">Yemen and Japan</a> both have low
wealth ginis (under 0.6), yet these pairs of countries couldn't be more
different. Wealth inequality in itself is not really the problem, just an
indicator. The Rawlsian goal is not to reduce it arbitrarily, but to make life
actually better for everybody.</p>

<p><strong>Micro to macro</strong>. The simple two household simulations I started with feel
like microeconomics. The more complex simulations we ended with started feeling
more like something from macroeconomics. Put another way, each household starts
with just a couple of bars of wealth, but as the simulation proceeds, the canvas
begins to resemble a bar chart. I found this quantity-to-quality transition
fascinating.</p>

<p><strong>On simulations</strong>. I'm intrigued by simulations as way of explaining
complicated things to non-experts. However, any simulation is inherently
inaccurate, as it approximates the real world in order to have explanatory
power. In other words, there is some continuum between accuracy and
insight. The simulations in this post are more simple than they are realistic,
however, I hope they are at least somewhat informative and interesting.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Sensor fusion and motion prediction</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/sensor-fusion-prediction-webvr"/>
    
    <updated>2015-11-05T09:00:00-00:00</updated>
    
    <id>https://smus.com/sensor-fusion-prediction-webvr</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>A major technical challenge for VR is to make head tracking as good as possible.
The metric that matters is called <strong>motion-to-photon latency</strong>. For mobile VR
purposes, this is the time that it takes for a user's head rotation to be fully
reflected in the rendered content.</p>

<p><img src="/sensor-fusion-prediction-webvr/latency-chain.jpg" alt="Motion to photon pipeline" /></p>

<p>The simplest way to get up-and-running with head tracking on the web today is
to use the <code>deviceorientation</code> events, which are generally well supported across
most browsers. However, this approach suffers from several drawbacks which can
be remedied by implementing our own sensor fusion. We can do even better by
predicting head orientation from the gyroscope.</p>

<p>I'll dig into these techniques and their open web implementations.  Everything
discussed in this post is implemented and available open source as part of the
<a href="https://github.com/borismus/webvr-polyfill">WebVR Polyfill</a> project. If you want to skip ahead, check out
the <a href="http://borismus.github.io/webvr-boilerplate/">latest head tracker</a> in action, and play around with this <a href="http://borismus.github.io/sensor-fusion/">motion
sensor visualizer</a>.</p>

<!--more-->

<h2>The trouble with device orientation</h2>

<p>The web provides an easy solution for head tracking through the
<code>deviceorientation</code> event, which gives Euler angles corresponding to your
phone's 3-DOF orientation in space. This orientation is calculated through an
undisclosed algorithm. Until very recently, <a href="http://w3c.github.io/deviceorientation/spec-source-orientation.html">the spec</a> didn't
even specify whether or not these events should give your phone's orientation in
relation to north or not. However, recently <a href="https://github.com/w3c/deviceorientation/pull/22">accepted spec
changes</a> make this behavior more standard across
browsers.</p>

<p>In Android, the JavaScript <code>deviceorientation</code> event was implemented using
<code>Sensor.TYPE_ORIENTATION</code> in Android, which fuses accelerometer, gyroscope and
magnetometer sensors together to give a North-aligned orientation. The trouble
is that the magnetometer's estimate of magnetic North is easily affected by
external metallic objects. On many devices, the North estimate continually
changes, even when you are not looking around. This breaks the correspondence
between motion and display, a recipe for disaster.</p>

<p>Another issue in some implementations is that the <code>deviceorientation</code> sensor
ramps up and down in firing rate depending on the speed of the phone's rotation.
Try opening up <a href="http://jsbin.com/device-inertial-sensor-diagnostics">this diagnostic page</a> on Android. This variation in
sensor update rate is not good for maintaining a reliable head track.</p>

<p>To top it off, a <a href="http://crbug.com/540629">recent regression in Android M</a> broke
<code>deviceorientation</code> for Nexus 5s. Why do bad bugs happen to good people?</p>

<h3>What is to be done?</h3>

<p>We implement our own sensor fusion with <code>devicemotion</code>, which provides lower
level accelerometer &amp; gyroscope events. These fire at a regular rate. When you
search for "sensor fusion", jumping into the rabbit hole will quickly take you
into the realm of Kalman Filters. This is a bit more firepower than we will need
for the moment, although I did finally get a better sense of the concept with
the help of a <a href="https://www.youtube.com/watch?v=18TKA-YWhX0">boring but understandable explanation</a>.</p>

<p>Luckily, there are simpler alternatives such as the Complementary Filter, which
is what we'll talk about next.</p>

<h2>Your sensing smartphone</h2>

<p>Let us start with the basics: sensors. There are three fundamental motion
tracking sensors in your smartphone. </p>

<p>Accelerometers measure any acceleration, returning a vector in the phone's
reference frame. Usually this vector points down, towards the center of the
earth, but other accelerations (eg. linear ones as you move your phone) are also
captured. The output from an accelerometer is quite noisy by virtue of how the
sensor works. Here's a plot of the rotation around the X-axis according to an
accelerometer:</p>

<p><img src="/sensor-fusion-prediction-webvr/accel.gif" alt="Animation of X-axis accelerometer output with a phone turning around the X axis" /></p>

<p>Gyroscopes measure rotations, returning an angular rotation vector also in the
phone's reference frame. Output from the gyro is quite smooth, and very
responsive to small rotations. The gyro can be used to estimate pose by keeping
track of the current pose and adjusting it every timestep, with every new gyro
reading. This integration works well, but suffers from drift. If you were to
place your phone flat and capture it's gyro-based position, then pick it up,
rotate it a bunch, and place it flat again, its integrated gyro position might
be quite different from what it was before due to the accumulation of errors
from the sensor. Rotation around the X-axis according to a gyroscope:</p>

<p><img src="/sensor-fusion-prediction-webvr/gyro.gif" alt="Animation of X-axis gyroscope output with a phone turning around the X axis" /></p>

<p>Magnetometers measure magnetic fields, returning a vector corresponding to the
cumulative magnetic field due to any nearby magnets (including the Earth). This
sensor acts like a compass, giving an orientation estimate of the phone. This is
incredibly useful combined with the accelerometer, which provides no information
about the phone's yaw. Magnetometers are affected not by the Earth, but by
anything with a magnetic field, including <a href="http://smus.com/magnetic-input-mobile-vr/">strategically placed permanent
magnets</a> and also ferromagnetic metals which are often found in substantial
quantities in certain environments.</p>

<h2>Intuition: why do we need sensor fusion?</h2>

<p>Each sensor has its own strengths and weaknesses. Gyroscopes have no idea where
they are in relation to the world, while accelerometers are very noisy and can
never provide a yaw estimate. The idea of sensor fusion is to take readings from
each sensor and provide a more useful result which combines the strengths of
each. The resulting fused stream is greater than the sum of its parts. </p>

<p>There are many ways of fusing sensors into one stream. Which sensors you fuse,
and which algorithmic approach you choose should depend on the usecase.
The accelerometer-gyroscope-magnetometer sensor fusion provided by the
system tries really hard to generate something useful. But as it turns out, it
is not great for VR head tracking. The selected sensors are the wrong ones, and
the output is not sensitive enough to small head movements.</p>

<p>In VR, drifting away from true north is often fine since you aren't looking at
the real world anyway. So there's no need to fuse with magnetometer. Reducing
absolute drift is, of course, still desirable in some cases. If you are sitting
in an armchair, maintaining alignment with the front of your chair is critical,
otherwise you will find yourself having to crank your neck too much just to
continue looking forward in the virtual world. For the time being, we ignore
this problem.</p>

<h2>Building a complementary filter</h2>

<p>The complementary filter takes advantage of the long term accuracy of the
accelerometer, while mitigating the noise in the sensor by relying on the
gyroscope in the short term. The filter is called complementary because
mathematically, it can be expressed as a weighted sum of the two sensor streams:</p>

<p><img src="/sensor-fusion-prediction-webvr/filter-equation.png" class="center" 
    title="Filter equation" /></p>

<p>This approach relies on the gyroscope for angular updates to head orientation,
but corrects for gyro drift by taking into account where measured gravity is
according to the accelerometer.</p>

<p>Initially inspired by <a href="http://www.pieter-jan.com/node/11">Pieter's explanation</a>, I built this filter by
calculating roll and pitch from the accelerometer and gyroscope, but quickly ran
into issues with <a href="https://en.wikipedia.org/wiki/Gimbal_lock">gimbal lock</a>. A better approach is to use quaternions
to represent orientation, which do not suffer from this problem, and are ideal
for thinking about rotations in 3D. Quaternions are complex (ha!) so I won't go
into much detail here beyond linking to a <a href="http://www.3dgep.com/understanding-quaternions/">decent primer</a> on the
topic. Happily, quaternions are a useful tool even without fully understanding
the theory, and many implementations exist. For this filter, I used <a href="http://threejs.org/docs/#Reference/Math/Quaternion">the
one</a> found in THREE.js.</p>

<p>The first task is to express the accelerometer vector as a quaternion rotation,
which we use to initialize the orientation estimate (see
<a href="https://github.com/borismus/webvr-polyfill/blob/master/src/complementary-filter.js"><code>ComplementaryFilter.accelToQuaternion_</code></a>).</p>

<pre><code>quat.setFromUnitVectors(new THREE.Vector3(0, 0, -1), normAccel);
</code></pre>

<p>Every time we get new sensor data, calculate the instantaneous change in
orientation from the gyroscope. Again, we convert to a quaternion, as follows
(see: <a href="https://github.com/borismus/webvr-polyfill/blob/master/src/complementary-filter.js"><code>ComplementaryFilter.gyroToQuaternionDelta_</code></a>):</p>

<pre><code>quat.setFromAxisAngle(gyroNorm, gyro.length() * dt);
</code></pre>

<p>Now we update the orientation estimate with the quaternion delta. This is a
quaternion multiplication:</p>

<pre><code>this.filterQ.copy(this.previousFilterQ);
this.filterQ.multiply(gyroDeltaQ);
</code></pre>

<p>Next, calculate the estimated gravity from the current orientation and compare
it to the gravity from the accelerometer, getting the quaternion delta.</p>

<p><img src="/sensor-fusion-prediction-webvr/complementary-filter.png" class="center" 
    title="Complementary filter visual illustration" /></p>

<pre><code>deltaQ.setFromUnitVectors(this.estimatedGravity, this.measuredGravity);
</code></pre>

<p>Now we can calculate the target orientation based on the measured gravity, and
then perform a <a href="https://en.wikipedia.org/wiki/Slerp">spherical linear interpolation (SLERP)</a>. How much to
slerp depends on that constant I mentioned before. If we don't slerp at all, we
will end up only using the gyroscope. If we slerp all the way to the target, we
will end up ignoring the gyroscope completely and only using the accelerometer.
In THREE parlance:</p>

<pre><code>this.filterQ.slerp(targetQ, 1 - this.kFilter);
</code></pre>

<p>Sanity checking the result, we expect the filter output to be roughly parallel
to the gyroscope readings, but to align with the accelerometer reading over the
long term. Below, you can see the accelerometer and gyroscope (green and blue)
and compare them to the fused output (orange):</p>

<p><img src="/sensor-fusion-prediction-webvr/fusion.gif" alt="Complementary filter output" /></p>

<h2>Predicting the future</h2>

<p>As your program draws each frame of rendered content, there is delay between
the time you move your head and the time the content actually appears on the
screen. It takes time for the sensors to fire, for firmware and software to
process sensor data, and for a scene to be generated based on that sensor data.</p>

<p>In Android, this latency is often on the order of 50-100 ms with sensors firing
on all cylinders (the technical term for 200 Hz) and some nice graphics
optimizations. The web suffers a strictly worse fate since sensors often fire
slower (60 Hz in Safari and Firefox), and there are more hoops of abstraction to
jump through. Reducing motion-to-photon latency can be done by actually reducing
each step in the process, with faster sensor processing, graphics optimizations,
and better algorithms. It can also be reduced by cheating!</p>

<p>We can rely on a <a href="https://en.wikipedia.org/wiki/Dead_reckoning#Directional_dead_reckoning">dead reckoning</a> inspired approach, but rather
than predicting position based on velocity, we predict in the angular domain.
Once we predict the orientation of the head in the (near) future, use that
orientation to render the scene. We predict based on angular velocity, assuming
that your head will keep rotating at the same rate. More complex schemes are
possible to imagine too, using acceleration (2nd order) or Nth order prediction,
but these are more complex, and so more expensive to calculate, and don't
necessarily yield better results.</p>

<pre><code>var deltaT = timestampS - this.previousTimestampS;
var predictAngle = angularSpeed * this.predictionTimeS;
</code></pre>

<p>The way this works is pretty straight forward, using angular speed from the
gyroscope, we can predict a little bit into the future to yield results like
this:</p>

<p><img src="/sensor-fusion-prediction-webvr/prediction.gif" alt="Predicted vs. sensor fusion." /></p>

<p>Notice that the predicted signal (in red) is somewhat ahead of the fused one (in
orange). This is what we'd expect based on the motion prediction approach taken.
The downside of this is that there is noticeable noise, since sometimes we
over-predict, and are forced to return back to the original heading.</p>

<h2>Plotting graphs</h2>

<p>Although still in very active development, <a href="https://gitgud.io/unconed/mathbox/">Mathbox2</a> is already a
formidable visualization toolkit. It is especially well suited to output in 3D,
which I used actively to debug and visualize the filter.</p>

<p>I also used Mathbox2 to generate plots featured earlier in this blog post. I
wrote a live-plotting tool that can compare gyroscope, accelerometer, fused and
predicted streams on each axis, and also let you tweak the filter coefficient
and how far into the future to predict.</p>

<p><img src="/sensor-fusion-prediction-webvr/plot-options.png" class="center"
    title="Preview of the options available in the plot"/></p>

<p>You too can <a href="http://borismus.github.io/sensor-fusion/">try the plots live on your phone</a>. After all, it's just a
mobile webpage! Many thanks to <a href="https://twitter.com/pierregeorgel">Pierre
Fite-Georgel</a> and <a href="https://github.com/jkammerl">Julius
Kammerl</a> for lending their incredible
filter-building skills to this project.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Hot bread: delicious or deadly?</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/hot-bread-delicious-deadly"/>
    
    <updated>2015-09-23T09:00:00-00:00</updated>
    
    <id>https://smus.com/hot-bread-delicious-deadly</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>Despite free access to information via the Internet and an increasingly global
world, people still seem to have all sorts of divergent ideas about how the world
works. For example, did you know that eating hot bread and pastries is
incredibly unhealthy? Indeed, it can often even lead to complete bowel
obstruction! I learned this fact as a kid, while growing up in the Soviet Union.
Understandably, I have been very careful to avoid eating hot baked goods.
That is, until recently, when my American girlfriend questioned the validity of my
belief and I began to harbor some doubts. I decided to check if it was actually
true, and asked Google. The results were very clear: I had fallen prey to an old
wives tale. My worldview, shattered.</p>

<p>Incredulous, I searched for the same thing in Russian and arrived at the
opposite conclusion. "What's up with that?" I thought, and wrote this post.</p>

<!--more-->

<h2>Asking in different languages</h2>

<p>I searched Google for "hot bread unhealthy", and tallied up the top 5 results:</p>

<table>
<tr><th>Domain</th><th>Unhealthy?</th></tr>
<tr><td>davidwalbert.com</td><td>No</td></tr>
<tr><td>chestofbooks.com</td><td>Maybe</td></tr>
<tr><td>gurumagazine.org</td><td>For some</td></tr>
<tr><td>lthforum.com</td><td>No</td></tr>
<tr><td>answers.yahoo.com</td><td>No</td></tr>
</table>

<p>I then compared it to an equivalent Russian search string: "горячий хлеб
вреден". The following are my results in English:</p>

<table>
<tr><th>Domain</th><th>Unhealthy?</th></tr>
<tr><td>useful-food.ru</td><td>Yes</td></tr>
<tr><td>foodblogger.ru</td><td>Yes</td></tr>
<tr><td>hlebopechka.ru</td><td>Yes</td></tr>
<tr><td>otvet.mail.ru</td><td>Maybe</td></tr>
<tr><td>otvet.mail.ru</td><td>Yes</td></tr>
</table>

<p>My <a href="https://goo.gl/ltPefm">working spreadsheet</a> contains more colorful details
if you are interested.</p>

<h2>Language shapes your... search results?</h2>

<p>No English language site suggested that eating hot bread was unhealthy. Three of
the top five results explicitly point it out as an old wives tale. The first hit,
<a href="http://goo.gl/Cj9jKS">the most skeptical of the bunch</a> even cites articles from
the 18th and 19th centuries which have since been refuted.</p>

<p>In stark contrast, no Russian language site suggested that eating fresh
bread was totally fine. Four of five of the top results explicitly said that it
was unhealthy, suggesting that fresh bread is difficult to digest, encourages
swallowing without chewing, and eating it leads to all sorts of gastrointestinal
trouble like stomach pain, inflammation, constipation and full on bowel
obstruction. Oh my!</p>

<p>One possibility is that the environments of the Russian and English speaker are
in fact completely different. The bread making processes in Russia could differ
from other places in the world. Many Russians favor rye bread, which takes some
effort to find in North America, for example. The main reason for unhealthiness
of fresh bread seems to be related to it being undercooked, with the yeast still
being active until it cools. Maybe rye better protects the yeast, or takes less
time or heat to cook? </p>

<p>This and other theories are possible, though not likely. My intuition suggests a
simpler explanation.</p>

<h2>Nothing Is True and Everything Is Possible</h2>

<p>There is an expression in Russian: "умом Россию не понять", which roughly
translates as "Russia cannot be understood with the mind". There is a certain 
mystery deeply ingrained in the national character which, fascinatingly, has
always been a point of pride.  The heading of this section is actually taken
from the title of a <a href="http://www.amazon.com/Nothing-Is-True-Everything-Possible/dp/1610394550">book about modern Russia</a>, subtitled "The Surreal
Heart of the New Russia". In Russia, rationalism and skepticism is on the
decline, in favor of traditionalism and magical thinking. Given a rich tradition
of <a href="https://en.wikipedia.org/wiki/Russian_traditions_and_superstitions">traditions, superstitions, and beliefs</a> in Russian culture, there is a
large pool of absurdity to pick from.</p>

<p>Given that, and my recent search history, you can imagine what I now believe
about the harmful effects of eating freshly baked bread. I don't much care
whether or not eating fresh bread is healthy, especially since as a card
carrying Celiac, I can't even enjoy the delicious kind. The fascinating
conclusion from my multilingual sojourn is this:</p>

<p><strong>Having searched for the same thing in their native languages, a Russian
speaker and an English speaker would have arrived at a completely different
world-view.</strong></p>

<p>The Russian language <a href="https://en.wikipedia.org/wiki/List_of_territorial_entities_where_Russian_is_an_official_language">maps closely</a> to Russia and Russian culture, certainly
more so than <a href="https://en.wikipedia.org/wiki/List_of_territorial_entities_where_English_is_an_official_language">English does</a> to any particular country and culture. The
result is that queries in Russian are suspect to a very natural echo chamber,
echoing and amplifying deeply held beliefs with the help of our supposedly
normalizing open Internet. </p>

<h2>Translated foreign pages</h2>

<p>There are hundreds of other examples of queries that when translated will yield
dramatically different results much like "hot bread unhealthy"/"горячий хлеб
вреден". There's a simple formula for finding more. Pick a language and write a
query string, translate it into another language, perform both searches and
analyze the top results.</p>

<p>This sounds a lot like something that can be automated. Indeed, Google used to
automatically translate queries, perform searches with translated queries, and
surface them to the user. Unfortunately this "Translated foreign pages" feature
was <a href="https://productforums.google.com/forum/#!topic/websearch/tYo0LpcVobI/discussion">removed several years ago</a>, due to lack of usage. Also, there are
difficulties with automating the process. The Google Translation of "hot bread
unhealthy" is "горячий хлеб нездоровый", which in Russian sounds like the bread
itself is ill, and yields less relevant search results.</p>

<p>It's surprising how clearly this cultural difference can be seen through the
simple example of warm bread and a search engine. The initial surprise can be
easily explained though, since the search engine crawls a naturally insular
corpus of articles in the same language. Many of the search results in English
cite the same sources. The same is true for search results in Russian. The key
point, though, is that there is very little shared linking between the English
and Russian sites, especially since <a href="https://en.wikipedia.org/wiki/List_of_countries_by_English-speaking_population">only 5% of Russians speak English</a>.
The language corpuses seem to be almost completely insulated from one another.
Inevitably, confirmation bias kicks in and you end up with the polarized world
we live in today.</p>

<p>I'd love to see what similar analyses on other search queries. For instance,
there is a Russian gadget called a <a href="http://www.amazon.com/Dark-Blue-Lamp-Minin-Reflector/dp/B00RPG6UTW">Minin Reflector</a>, which consists of a
lamp with a blue filter. You simply shine it onto the part of your body that
ails you, and presto, instant pain relief... sigh!</p>

<p>Wrapping up this blog, I am enjoying some delicious, fresh from the oven, hot
muffins. I'll keep you posted with the definitive truth!</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>UbiComp and ISWC 2015</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/ubicomp-iswc-2015"/>
    
    <updated>2015-09-16T09:00:00-00:00</updated>
    
    <id>https://smus.com/ubicomp-iswc-2015</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>I recently returned from ISWC 2015, where I presented the <a href="/magnetic-input-mobile-vr/">Cardboard
Magnet</a> paper. In addition to seeing old friends,
meeting new ones, and being inspired by some interesting research, it was an
excellent excuse to visit Osaka, Japan! This year, ISWC was co-located with
UbiComp, and the combined conference had four tracks. This post is by no means
exhaustive, just some of the more interesting work I got a chance to see.</p>

<!--more-->

<p><strong>Opening keynote: Visualizing and Manipulating Brain Dynamics</strong>. <a href="http://www.cns.atr.jp/~kawato/">Mitsuo
Kawato</a> showed some impressive <a href="https://ieeetv.ieee.org/conference-highlights/cb-exploring-neuroscience-withhumanoid-research-platform?">self-balancing
robots</a>.
It seems that we've <a href="http://tumblr.forgifs.com/post/111425301004/robot-soccer-kick-fail">come a long
way</a>. Most
of what he showed was around deep brain stimulation, artificial cochleas and
retinas, old work but mostly new to me. This is a recent, very impressive and
somewhat terrifying paper on reconstructing low-resolution grayscale <a href="http://neurosurgery.washington.edu/Lectures/science.1234330.full.pdf">imagery
from
dreams</a>.</p>

<h2>Novel input technology</h2>

<p><strong>SoQr: Sonically Quantifying the Content Level inside Containers</strong> is a
convoluted way of determining if you're out of milk. Inspired by acoustically
checking ripeness of watermellons, the idea is to use <a href="https://goo.gl/photos/BVhjSZXyn7MJV2sv7">contact speaker and mic
pair</a> to determine how full a container
is. The method's efficacy depends a lot on the placement of the sensor,
properties of the container, and other environmental factors, like whether or
not any other items are touching the container. Seems overly complex to me, you
could use another approach to reach higher fidelity (eg. a scale). That said,
maybe this can be done very inexpensively?</p>

<p><strong>MagnifiSense: Inferring Device Interaction Using Wrist-Worn Passive
Magneto-Inductive Sensors</strong> is about determining which electronic device is
being used. The idea is to use an inductor coil to detect nearby electromagnetic
radiation. They built their own hardware for the purpose which samples at a very
high frequency (44.1 KHz). They detect <a href="https://goo.gl/photos/RQXQ8fBJpJU4f8wn6">unique EM radiation
patterns</a> for each type of device.
Supposedly they can do the same using a regular smartphone magnetometer, but I'm
very skeptical. They also claim to be able to determine who is using the device,
but that part wasn't very clear from the talk.</p>

<p><strong>DoppleSleep: A Contactless Unobtrusive Sleep Sensing System Using Short-Range
Doppler Radar</strong> uses a 24 GHz doppler radar typically mounted near the bedside
to detect sleep patterns. The benefits are huge: you don't have to wear anything
or instrument the bed. <a href="https://en.wikipedia.org/wiki/Polysomnography">Medical sleep
trackers</a> require
7 electrodes, and <a href="https://en.wikipedia.org/wiki/Actigraphy">consumer ones</a>
don't work well.  They also had a demo where you just sit at your desk and the
doppler tracks your heart rate, breathing rate, as well as more macro
movements. It didn't work as well as I had hoped, but being a research demo, I
remain hopeful!</p>

<p><strong>Activity tracking and indoor positioning with a wearable magnet</strong> was a poster
showing a very cheap way of tracking just by placing magnetometers in strategic
locations and giving the user a magnet. More details <a href="https://goo.gl/photos/2un4nc5nrE7evC4D8">on the
poster</a>.</p>

<p><strong>IDyLL: Indoor Localization using Inertial and Light Sensors on Smartphones</strong>
attempts to solve indoor localization using the light sensor, and lamps as
features for tracking. They extract features from the lights using peak finding,
not absolute intensity. Then they wrote a kalman filter to fuse the IMU and
light-derived features. There's a lot of problems, like needing to have
structured light (eg. in a hallway with a low ceiling), and identify ambiguity
(ie. you're under a light, but which one?).</p>

<p><strong>Monitoring Building Door Events using Barometer Sensor in Smartphones</strong> used
the ubiquitous smartphone barometer, which is currently used to get faster GPS
lock and assist in weather forecasting, to determine if a door opens in a
building. This only works in buildings with HVAC systems, but it was pretty
clever, and they found that it can work reliably, even for multiple doors. Basic
idea <a href="https://goo.gl/photos/yKPbBYPv2RayMrcf6">described in this slide</a>.</p>

<p><strong>ProximityHat - A Head-Worn System for Subtle Sensory Augmentation with Tactile
Stimulation</strong> reminded me of various <a href="http://www.cc.gatech.edu/~acosgun3/papers/cosgun2014guidance.pdf">vibro-tactile belt
projects</a>, and
served a similar purpose: to exploit the sense of touch to give the wearer
another sense. This has many benefits like not blocking other senses. Anyway,
they built a hat and gave it ultrasonic sensors all around and inward-facing
linear actuators, not vibrator motors.  They studied sensitivity around the head
and found high variation around users, and that the forehead was generally less
sensitive. Main application appears to be navigation, and they did some blind
user studies.</p>

<p><strong>Controlling Stiffness with Jamming for Wearable Haptics</strong> makes it easier and
harder to move sliders with the help of a pneumatic bladder, and layered
material. As the bladder inflates, the additional force on the layered material
causes increased friction. Previous layer jamming had low fidelity (binary), so
this is a big improvement. They are currently using sandpaper, so it's unclear
how robust the effect would be over time.</p>

<p><strong>PneuHaptic: Delivering Haptic Cues with a Pneumatic Armband</strong> used a wearable
<a href="https://goo.gl/photos/XkFSKrbRmpiZgzCu9">pneumatic band with 2 pumps and 3
valves</a> to give haptic feedback. This
is a nice alternative to vibrating motors and linear actuators, but not sure how
miniaturizable in practice.</p>

<p><strong>Fast Blur Removal for Wearable QR Code Scanners</strong> is an image processing paper
for improving QR code detection on wearable devices. The proposed method uses
un-blurring techniques which involve predicting the blur direction and applying
de-convolutions. They also use an IMU to better guess the direction of movement.
However <a href="http://picturesofpeoplescanningqrcodes.tumblr.com/">QR codes are dead to
me</a>.</p>

<h2>Gadgets and fads</h2>

<p><strong>Why we use and abandon smart devices</strong> tried to answer the question of why
people abandon their various health and tracking devices so quickly. Basically,
people are motivated by curiosity and novelty, and these health trackers are too
gimmicky. Studies of Fitbit trackers saw majority of them abandoned (65%
abandoned in 2 weeks). Design implications are that encouraging routines
(changing behavior) and minimizing maintenance (charging) are the critical
things.This study had participants come up with a goal, and $1K to buy devices,
so quite contrived given that people didn't even choose to use the devices on
their own, but motivated by a study.</p>

<p>In a less contrived study about the same thing, <strong>No Longer Wearing:
Investigating the Abandonment of Personal Health-Tracking Technologies on
Craigslist</strong> scraped Craigslist for this data. They found that only 25% of
people sell their devices just for abandonment reasons. In many other cases,
they upgrade to something else, or reach their goals. That said, it's very
biased sample, since these people are selling (many just abandon, and don't sell
on CL).</p>

<h2>Machine learning</h2>

<p><strong>DeepEar: Robust Smartphone Audio Sensing in Unconstrained Acoustic
Environments Using Deep Learning</strong> used RNNs to learn whatever sound the user is
interested in. They did an interesting comparison to similar specialized systems
(eg. those that do speaker identification, stress detection, emotion, etc) and
claim to do better. Also, their RNN runs in hardware on a chip, which I thought
was super impressive.</p>

<p>In <strong>Sensor-based Stroke Detection and Stroke Type Classification in Table
Tennis</strong>, the authors instrumented paddles with IMUs and got people to perform
various strokes (in a somewhat controlled environment). They performed stroke
detection through peak recognition and thresholding, and then had a classifier
for stroke type determination. 97% detection and classification rates!
Impressive, but contrived. Wondering how it would do for a full game?</p>

<p><strong>Recognizing New Activities with Limited Training Data</strong> was an interesting
paper about recognizing new activities based on small amounts of labeled data.
Their idea was to leverage "semantic attributes" from core activities to learn
a new activity.  Example: biking is like sitting (body is not changing angle),
running (legs move up and down) and driving (hands are steering). They proposed
an <a href="https://goo.gl/photos/Rn2BbvQjhU3Lhv1K7">Activity-Attribute matrix</a>, and a
cascaded classifier. Problem is that multiple activities can share the same
attributes. So they combine this with a traditional approach.</p>

<p><strong>When Attention is not Scarce - Detecting Boredom from Mobile Phone Usage</strong>
predicted boredom with higher accuracy than I predicted. They collected a ground truth
of boredom data by polling users multiple times a day, asking if they were
bored, and collected activity traces (semantic location, demographics, network
usage, recent number of notifications, sensor data). They managed to detect
boredom with 73% accuracy. They then built an app which sent buzzfeed articles
when bored and compared engagement and click ratio to the random condition.
CTR was 8% for random, 20% when bored, and people were much more engaged.</p>

<h2>Virtual and augmented reality</h2>

<p><strong>Wearing Another Personality: A Human-Surrogate System with a Telepresence
Face</strong> was probably the most bizarre paper at the conference. This work
basically proposes to use a human surrogate instead of a telepresence robot. The
surrogate wears an HMD with pass-through camera feed and a tablet on their face.
The tablet shows the face of the director. The director gets audio and video
feed from the surrogate, and the surrogate gets audio instructions from the
director. They did creepy user studies like going to a city office to get a
public document (friend as surrogate), or meeting your grandmother (mother as
surrogate). Surprisingly, many participants liked being surrogates. The big
technical problem is camera pass through latency. If you go through the whole
Java stack it's something crazy like 300ms. Here's a <a href="https://goo.gl/photos/tZPoR4wvQiDekzg86">video from the
conference</a> to give you a better sense.</p>

<p><strong>Comparing Order Picking Assisted by Head-Up Display versus Pick-by-Light with
Explicit Pick Confirmation</strong> compared two order picking methods in warehouses.
The current method is via digital labels on each tray that count how many items
you're supposed to take from that tray. The new method is to show <a href="https://www.youtube.com/watch?v=yUZFaCP6rP4">which trays
to pick from using augmented
reality</a>. The benefit is that you
don't need an instrumented warehouse, so it's much cheaper. This was interesting
because it was a specific, potentially useful application for a Google
Glass-type device. At the same time, it may be an obsolete problem since aren't
robots supposed to automate that sort of thing pretty soon?</p>

<p><strong>ConductAR: An AR based tool for iterative design of conductive ink circuits</strong>
is a project that validates hand drawn circuits using augmented reality. You
sketch your circuit with a conductive pen, and then the tool takes a picture and
gives you the right voltage drops etc. The presenter showed resistance
calculation (the thicker the line, the more resistive), using <a href="https://goo.gl/photos/wLaMQZqoqf66ap477">a FEM
method</a>. But I wasn't convinced that
this is worthwhile. Sketching circuits should be exploratory and does not need
to be precise, that's sort of the point.</p>

<p><strong>An Approach to User Identification for Head-Mounted Displays</strong> uses blink and
head movements to identify users. They play a particular video and track your
patterns using Google Glass. They extract blinks using IR peaks, and head track
using the IMU. It takes about 30s to verify uniqueness, but not sure how large
their user base is. Results are good: 94% balanced accuracy, and blink features
are most important.</p>

<p><strong>Glass-Physics: Using Google Glass for Physics Experiments</strong> compared using
Google Glass to just a tablet for assisting students doing physics experiments.
The idea is to remove drudgery from data collection. The experiment was to
determine <a href="https://goo.gl/photos/kcxvxouYtDs6CfQv7">effect of fill level in a water glass on frequency of
sound</a> when the vessel was hit with a
fork. A Google Glass app did automatic collection of frequency and of water
level. People liked the wearable version more, but the tablet app involved
manual input. My theory is that a tablet app with AR features to auto-measure
fill level would do as well as an HMD.</p>

<p><strong>WISEglass: Multi-purpose Context-aware Smart Eyeglasses</strong> was like Google
Glass, except without the display. The main contribution was a light sensor on
the bridge of the nose, which could reliably determine when you are at the
computer (from the screen update frequency). Other than that, seems pretty much
the same as wearing an IMU anywhere else (eg. smartwatch).</p>

<h2>E-textiles are impressive</h2>

<p>I saw some nice demos of <a href="https://goo.gl/photos/cui8ucmGYjdXq2tj9">stretch-sensitive fabric
(video)</a>, and <a href="https://goo.gl/photos/HbXp9xK2GtBgcoEo8">pressure/capacitative
fabric (video)</a>. The real question is
where to embed the controller, and what to do about battery life (their stats
were pretty bad). E-textiles are interesting because everybody wears clothing,
which is not true for glasses or watches.</p>

<p><strong>Closing keynote: Behind the scenes</strong> delivered by <a href="http://www.daito.ws/en/">Daito
Manabe</a> was sequentially translated, which was
initially jarring, but the talk was so visually stimulating, it didn't really
matter. Daito walked through a lot of his data arts work, mind-blowingly
impressive art pieces involving drones, 3D graphics, depth cameras, etc. A nice,
if somewhat non-sequitur ending to the conference.</p>

<p>Signing off. Arigatou gozaimasu!</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Magnetic Input for Mobile VR</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/magnetic-input-mobile-vr"/>
    
    <updated>2015-09-07T09:00:00-00:00</updated>
    
    <id>https://smus.com/magnetic-input-mobile-vr</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>It's easy to do, just follow these steps:</p>

<ol>
<li>Cut two holes in a box</li>
<li>Put your phone in that box</li>
<li>Look inside the box</li>
</ol>

<p><a href="https://www.youtube.com/watch?v=ABrSYqiqvzc&amp;t=1m42s">And that's the way you do it</a>.</p>

<p>Your smartphone is now in a box, so how do you do input? Now that we have a
<a href="/magnetic-input-mobile-vr//magnetic-input-mobile-vr/mimvr-iswc-2015.pdf">paper</a> accepted to <a href="http://iswc.net/">ISWC 2015</a>, I can tell you!</p>

<!--more-->

<h2>It's not easy being in a box</h2>

<p>Let me remind you: your smartphone is still in a box. This means that your
fingers can't reach the touch screen or the volume buttons. Let's consider a
couple of input alternatives:</p>

<ul>
<li>Cameras and microphones require extra app permissions, are inefficient to keep
always on, and may face many false positives.</li>
<li>External electronic devices cost money. Plugging them in and out is a
usability nightmare.</li>
</ul>

<p>Ok, let's nix those. How about permanent magnets? They are inexpensive, robust,
require no power to operate, and do not degrade over time. The vast majority of
smartphones have a magnetometer, which is used for the compass. Intriguing...</p>

<h2>Fun with permanent magnets</h2>

<p>In 2009, Chris Harrison and Scott Hudson published <a href="http://www.chrisharrison.net/index.php/Research/Abracadabra">Abracadabra</a>, a
magnetic ring form factor for finger interactions with small devices:</p>

<p><img src="/magnetic-input-mobile-vr/abra.jpg" alt="Abracadabra" /></p>

<p>In 2010, Hamed Ketabdar and others published <a href="https://www.facebook.com/MagiTact">Magitact</a>, instead using
a magnetic rod for more varied interactions near smartphones:</p>

<p><img src="/magnetic-input-mobile-vr/magitact.jpg" alt="Magitact" /></p>

<p>In 2011, Daniel Ashbrook and others published <a href="http://dl.acm.org/citation.cfm?id=1979238">Nenya</a>, which is similar
to Abracadabra, but focused more on the eyes-free input aspects:</p>

<p><img src="/magnetic-input-mobile-vr/nenya.jpg" alt="Nenya" /></p>

<p>In 2013, Sungjae Hwang and others published <a href="https://www.youtube.com/watch?v=_sSgp0hD-jk">Maggetz</a>, which used
passive magnets to build all sorts of widgets around the device:</p>

<p><img src="/magnetic-input-mobile-vr/maggetz.jpg" alt="Maggetz" /></p>

<h2>Fucking magnets: how do they work?</h2>

<p>Magnets affect the magnetometer in an <a href="https://www.quora.com/Why-does-the-magnetic-field-obey-an-inverse-cube-law">inverse-cubic relationship</a>, so
distance between magnet and magnetometer really makes a dramatic difference in
signal strength. We empirically determined that in most phones, the sensor is
placed at the top of the device, near the earpiece:</p>

<table>
<tr><th>Smartphone Model</th><th>Sensor Location</th></tr>
<tr><td>Moto X</td><td>Top</td></tr>
<tr><td>Nexus 4</td><td>Top</td></tr>
<tr><td>Nexus 5</td><td>Top</td></tr>
<tr><td>Samsung S4</td><td>Top</td></tr>
<tr><td>Galaxy Nexus</td><td>Top</td></tr>
<tr><td>Samsung S3</td><td>Bottom</td></tr>
<tr><td>Moto G</td><td>Bottom</td></tr>
</table>

<p>Some magnetometers are really screwy, like the one found in the first revision
of the HTC M7, or broken, like in some models of the Galaxy Nexus we tested
with. There's little that we can do in these cases, but luckily they are quite
rare.</p>

<p><img src="/magnetic-input-mobile-vr/calibration.png" class="floatright" title="Plot of calibration events"/></p>

<p>The way you access the magnetometer on Android is via the sensor stack,
requesting the <code>TYPE_MAGNETIC_FIELD</code> sensor. This is a calibrated sensor, since
it's primarily used to determine the direction of magnetic north for the
compass. Calibration means that somewhere deep inside Android, software and
hardware periodically calibrates the output of the sensor. When calibration
occurs, magnetometer readings effectively reset to some new coordinate system.</p>

<p>Calibration can happen at any point, and the calibration pattern can look quite
different depending on the device. In some cases, it's a gradual calibration,
not a sudden spike as above. This limitation restricts what we can reliably
detect, which is why we chose a pull-and-release interaction. Android already
has provisions for an uncalibrated magnetometer via
<code>TYPE_MAGNETIC_FIELD_UNCALIBRATED</code>, but this sensor is not nearly as ubiquitous
as its calibrated cousin. Even so, we should be robust to phone insertions and
removals from Cardboard, which can also look like calibration events.</p>

<h2>Magnetic input for VR</h2>

<iframe width="853" height="480" src="//www.youtube.com/embed/a53a-9FLdL8" frameborder="0" allowfullscreen></iframe>

<p><img src="/magnetic-input-mobile-vr/mechanism.png" class="floatright" title="Interaction mechanism"/></p>

<p>As you can see, the interaction involves pulling the magnet downward, and
releasing it. The magnetic ring automatically returns to its rest position
because of the force exerted on it by an internal magnet. The external magnet is
also held in-place by the same force, and while it's possible to pull the magnet
off the cardboard side, it takes concerted effort to do so. The motion of the
magnet is constrained by a cardboard indentation, so it can only move downward.
The thing I find most elegant about this design is that both the digital signal
to the smartphone and the physical mechanism itself relies on the same
principle: magnetism.</p>

<p>We collected a bunch of data for this pull-and-release interaction from many
devices. We found that most devices behave predictably well. Here's a combined
plot of normalized, superimposed positives and negatives from all phones which
we collected data from, with each dimension of the magnetometer vector plotted
separately.</p>

<p><img src="/magnetic-input-mobile-vr/all_features.png" alt="Image of the true positives and negatives from all phones." /></p>

<p>The detector we built was not based on a template learned from all of the data
above, but a simpler state machine based on thresholding. The thresholds
themselves were learned empirically. Here's the simple state machine:</p>

<p><img src="/magnetic-input-mobile-vr/state_machine.png" alt="State machine of the detector" /></p>

<p>The basic idea is that we take a sliding window approach, normalizing all of the
data relative to the last value in the window. For each window, we calculate <code>min_1</code>,
which is the smallest value of the first half of the window, and <code>max_2</code>, the
largest value in the second half. Next, we compare to empirically determined
thresholds perform the appropriate transition in the state machine. I won't bore
you with details of normalizing the data, etc but you can find all of the
details in <a href="/magnetic-input-mobile-vr//magnetic-input-mobile-vr/mimvr-iswc-2015.pdf">the paper</a>. Oh, and all of the code is also available <a href="https://github.com/dodger487/MIST">on
github</a>.</p>

<h2>What's next?</h2>

<p><img src="/magnetic-input-mobile-vr/joystick.jpg" class="floatright" title="Hypothetical magnetic joystick"/></p>

<p>A lot more can be done using passive magnetic input. With uncalibrated
magnetometers, there is no fear of calibration events, so we could implement a
faster detector based on just the down motion of the magnet. We could reliably
detect long presses and double clicks. Alternatively, extensions to the existing
input can be implemented by simply changing the geometry of the physical
constraints, such as a joystick form factor.</p>

<p>I'm incredibly happy that Cardboard has been doing so well. Thanks to the great
team working so hard on it, there are now <a href="http://techcrunch.com/2015/05/28/google-has-shipped-over-1-million-cardboard-vr-units/">over 1 million units shipped</a>.
The press has been happy with it too, with kind reviews from many tech
publications.</p>

<p><a href="http://techcrunch.com/2014/06/25/hands-on-with-googles-incredibly-clever-cardboard-virtual-reality-headset/">Techcrunch</a> said:</p>

<blockquote>
  <p>This funny little cardboard faux-Rift has something even the original Rift
  itself does not: a built-in button. Your phone is able to sense the magnet’s
  movement, allowing it to act as a ridiculously clever little button. Yeesh.</p>
</blockquote>

<p><a href="http://www.techradar.com/news/phone-and-communications/mobile-phones/google-cardboard-everything-you-need-to-know-1277738">Techradar</a> said:</p>

<blockquote>
  <p>What's also somewhat amazing is the magnet on the side. […] The little magnet
  on the side is actually a quite ingenious design aspect of Google Cardboard.
  It's a button!</p>
</blockquote>

<p><a href="http://www.engadget.com/2014/12/10/google-cardboard/">Engadget</a> said:</p>

<blockquote>
  <p>One of the things I liked most was a switch located on the left temple, which
  consists of just a couple of magnets and a metal ring.</p>
</blockquote>

<p><a href="http://www.google.com/get/cardboard/downloads/wwgc_manufacturers_kit_v2.0.zip">Future versions of Cardboard</a> are switching to a different input method
using a conductive button which brings your body's capacitance to the screen,
similar to how a touch stylus works. It's cheaper without them, and the new
input works well, but I'll definitely miss the magnets!</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Site redesign, version five</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/design-v5"/>
    
    <updated>2015-06-10T09:00:00-00:00</updated>
    
    <id>https://smus.com/design-v5</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>It's been over three years since the design of this site has been
updated. Time to change that!</p>

<p><img src="/design-v5/0days.jpg" class="floatright"/></p>

<p>This is the fifth revision of this site's design. Looking over <a href="/redesign-2012">previous
designs</a>, I've been happier with minimal designs,
especially <a href="/redesign-2012/v2.png">this one</a> from 2012. I was inspired
by many excellent designs such as <a href="http://practicaltypography.com/typography-in-ten-minutes.html">Butterick's Practical
Typography</a>,
<a href="http://www.teehanlax.com/">Teehan+Lax</a>, <a href="http://www.erikjohanssonphoto.com/">Erik
Johansson</a>,
<a href="https://medium.com/designing-medium/death-to-typewriters-9b7712847639">Medium</a>
and <a href="http://frankchimero.com/writing/other-halves">Frank Chimero</a>.</p>

<p>The new design is visually cleaner. I <a href="http://caniuse.com/#feat=flexbox">use
flexbox</a> in many places, which makes
the CSS far more intuitive. The responsive parts are very simple,
consisting of just ten CSS declarations.</p>

<!--more-->

<p>Rather than subjecting readers to my face on every page, I have a simple
stipple background on the <a href="/about">about page</a>, which I created using the
complex but functional <a href="http://www.evilmadscientist.com/2012/stipplegen2/">StippleGen</a>.</p>

<p>Also, I've started working on a self-hosted visual link blog that you
can check out in under <a href="/inspiration">inspiring clippings</a>. I've
implemented a companion Chrome extension that makes it super easy to
clip inspiring content from anywhere on the web and bring it to that
page.</p>

<p><strong>Typography:</strong> I'm continuing to use Google fonts, which seems to be so
much simpler to use than various competitors. I have not completely
optimized my selection of fonts, but this is satisfactory given my
belief that no design is ever finished. <a href="http://alistapart.com/article/improving-ux-through-front-end-performance">Performance is UX</a> too,
and aesthetic decisions need to be counterbalanced by mundane
considerations like page load time.  Unfortunately <a href="https://www.google.com/fonts/specimen/Dosis">Dosis</a> didn't
make the cut.</p>

<p><strong>Infrastructure:</strong> This site is still built using the <a href="https://github.com/borismus/lightning">lightning static
blog</a> engine, which I'm continuing to improve. On that front,
I've dropped the ambitious goal of being able to edit content from any
device using dropbox, since in practice I always author on my laptop.
Instead, the focus has been on optimizing the edit flow for the local
offline case, and I have built <a href="https://github.com/lepture/python-livereload">livereload</a> into the local preview
server. As far as hosting, I have conceded to GitHub Pages, and have
migrated away from using S3 directly.</p>

<p><strong>Thanks:</strong> to <a href="https://twitter.com/mikemartin604">Mike</a>,
<a href="https://twitter.com/paul_irish">Paul</a>,
<a href="https://twitter.com/smattyang">Seungho</a>,
<a href="https://twitter.com/scottjenson">Scott</a>,
<a href="https://twitter.com/mahemoff">Michael</a>, and other awesome friends that
gave me excellent design suggestions and found bugs!</p>

<p>While I appreciate companies like <a href="http://medium.com">Medium</a> and
<a href="http://svbtle.com/">Svbtle</a> advancing the aesthetics of the web, I
completely <a href="http://practicaltypography.com/billionaires-typewriter.html">agree with Matthew Butterick</a>'s view, and will
continue self-hosting my writings for as long as possible. Long live the
plurality of the web!</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Spatial audio and web VR</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/spatial-audio-web-vr"/>
    
    <updated>2015-03-19T09:00:00-00:00</updated>
    
    <id>https://smus.com/spatial-audio-web-vr</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>Last summer I visited Austria, the capital of classical music. I had the
pleasure of hearing the <a href="http://en.wikipedia.org/wiki/Vespro_della_Beata_Vergine">Vespers of 1610</a> in the great
<a href="http://goo.gl/e3WBB2">Salzburger Dom (photosphere)</a>. The most memorable part of
the piece was that the soloists moved between movements, so their voices
and instruments emanated from surprising parts of the great hall.
Inspired, I returned to the west coast and eventually came around to
building a spatial audio prototypes like this one:</p>

<p><a href="http://borismus.github.io/moving-music"><img src="/spatial-audio-web-vr/collage_small.jpg" alt="Screenshot of a demo" /></a></p>

<p>Spatial audio is an important part of any good VR experience, since the
more senses we simulate, the more compelling it feels to our sense
fusing mind. WebVR, WebGL, and WebAudio all act as complementary specs
to enable this necessary experience. As you would expect, because it
uses the <a href="https://github.com/borismus/webvr-boilerplate">WebVR boilerplate</a>, this demo can be viewed on
mobile, desktop, in Cardboard or an Oculus Rift. In all cases, you will
need headphones :)</p>

<!--more-->

<h2>Early spatial music</h2>

<p>One of the things that made my acoustic experience in the Salzburg Dom
so memorable was the beauty of the space in which it was performed. The
potential for awesome sound was staggering, with one massive organ at
the back, and four smaller organs surrounding the nave. During the
performance of the vespers, the thing that struck me the most was that
as the piece transitioned from movement to movement, choreographed
soloists also moved around the cathedral, resulting in haunting acoustic
effects. Sometimes, a voice would appear quietly from the far end of the
cloister, sounding distant and muffled. Other times, it would come from
the balcony behind the audience, full of unexpected reverb. It was a
truly unique acoustic experience that I will never forget, and it made
me wonder about the role of space in music.</p>

<p>As it turns out, there is a rich history on the <a href="http://en.wikipedia.org/wiki/Spatial_music">topic of spatialization
in music</a> going back to the 16th century. For the
purposes of this blog, I am more interested in the present day. In
particular, given the <a href="http://www.w3.org/TR/webaudio/">excellent state of audio APIs</a> on the
web, what follows is a foray into spatial audio with WebVR.</p>

<h2>Experiments in spatial audio</h2>

<p>How does music sound if in addition to pitch, rhythm and timbre, we
could tweak position and velocity as additional expressive dimension?
My demo places you into a virtual listening space, that you look
around into (using whatever means you have available: mouse and
keyboard, mobile phone gyroscope, or cardboard-like device) -- thanks to
<a href="https://github.com/borismus/webvr-boilerplate">WebVR boilerplate</a>. Each track is visualized as a blob of
particles. These animate according to the instantaneous amplitude of the
track, serving as a per-track visualizer and indicating where the track
is in space.</p>

<p>There is a surprising amount of multi-track music out there, such as
<a href="http://www.cambridge-mt.com/ms-mtk.htm">Cambridge Music Technology's raw recordings archive</a> for aspiring
audio engineers and <a href="https://soundcloud.com/search/sets?q=stems">soundcloud stems</a> which are typically
recorded separately in a studio and then released publicly for <a href="http://www.remixcomps.com/">remix
contests</a>. In the end, I went with a few different sets just to
get a feeling for spatializing a variety of tracks:</p>

<ul>
<li>Multiple people <a href="http://borismus.github.io/moving-music/?set=speech">speaking (demo)</a> simultaneously (the <a href="http://en.wikipedia.org/wiki/Cocktail_party_effect">cocktail party effect</a>).</li>
<li>Multi-track <a href="http://borismus.github.io/moving-music/?set=jazz">live (demo)</a> recording with many mic'ed instruments.</li>
<li>Multi-track <a href="http://borismus.github.io/moving-music/?set=phoenix">studio recording (demo)</a> with cleanly recorded tracks (A <a href="http://en.wikipedia.org/wiki/Phoenix_%28band%29">Phoenix</a> track).</li>
</ul>

<p>In addition to selecting the sounds to spatialize, the demo supports
laying out the tracks in various formations. To cycle between these
modes, hit space on desktop, or tap the screen on mobile:</p>

<ul>
<li>Sources are <a href="http://borismus.github.io/moving-music/?mode=0">clumped together (demo)</a> in one place.</li>
<li>Sources are <a href="http://borismus.github.io/moving-music/?mode=1">positioned around the observer (demo)</a>.</li>
<li>Sources are <a href="http://borismus.github.io/moving-music/?mode=2">moving around the observer (demo)</a>.</li>
</ul>

<p>Given that the code is <a href="https://github.com/borismus/moving-music">open sourced on github</a>, it's pretty
easy to try your own tracks, implement new trajectories or change the
visualizer. Please fork away!</p>

<h2>Implementation details</h2>

<p>In an attempt to eat my own dogfood, this project partly serves as a way
to test the <a href="https://github.com/borismus/webvr-boilerplate">WebVR boilerplate project</a> to make sure that
it is usable, and provides the functionality that it purports to. I've
made a bunch of changes to the boilerplate in parallel, fixing browser
compatibility issues and resolving bugs. Notable improvements since
inception include the ability to mouselook via <a href="http://www.html5rocks.com/en/tutorials/pointerlock/intro/">pointer
lock</a> in regular desktop mode and improved support for iOS
and Firefox nightly. Thanks to <a href="http://www.antoniocosta.eu/">Antonio</a>, my awesome designer
colleague, the WebVR boilerplate has a new icon!</p>

<p>This project relies heavily on audio, but requires the page to be
running in the foreground for you to enjoy the immersive nature of the
experience. Browsers, especially on mobile devices, can have some weird
behaviors when it comes to backgrounded tabs. It's a safe bet to just
prevent this from happening altogether, so I've been using the <a href="http://www.smashingmagazine.com/2015/01/20/creating-sites-with-the-page-visibility-api/">page
visibility API</a> to mute the music when the tab goes out of
focus, and then resume it when it's back in focus. This works super well
across browsers I've tested in and prevents the page-hunt where you're
trying to find which annoying tab/activity/app is playing!</p>

<p>I toyed a little bit with the doppler effect, but found it to be
terrible for music. Because in the moving case, each track moves with
its own velocity relative to the viewer, frequency shifts are
non-uniform, leading to a cacophany of out-of-tune instruments. For
spoken word, it worked quite well, though. The caveat to all this is that the
current <a href="http://crbug.com/439644">doppler API is deprecated</a>, so I
didn't delve too deeply into doppler until we have a new implementation.</p>

<h2>Pitfalls and workarounds</h2>

<p><strong>Set your listener's up vector properly.</strong> Something you should beware
of is to always set the up vector correctly in the
<code>listener.setOrientation(...)</code> call. Initially, I was only setting the
direction vector, keeping up fixed at <code>(0, 1, 0)</code>, but this yielded
unpredictable results and took a long time to track down.</p>

<p><strong>Streaming is broken in mobile implementations.</strong> A couple of issues
related to loading audio bit me as I was developing, proving to be
nearly show stoppers (please star if you feel strongly):</p>

<ul>
<li>Streaming audio doesn't work on Android (or iOS). This means that
every track we play needs to be first loaded, and then decoded:
<a href="http://crbug.com/419446">http://crbug.com/419446</a></li>
<li>Decoding mp3 on Android takes a very very long time (same in Firefox):
<a href="http://crbug.com/232973">http://crbug.com/232973</a></li>
<li>Though it doesn't directly affect my spatial sound experiments, the
inability to bring in remote WebRTC audio streams into the audio graph
is blocking other ideas: <a href="https://crbug.com/121673">https://crbug.com/121673</a></li>
</ul>

<p>I tried to work around the streaming issue by doing my own chunking
locally and then writing a <code>ChunkedAudioPlayer</code>, but this is harder than
it seems, especially when you want to synchronize multiple chunked
tracks.</p>

<p><strong>Beware of implementation differences.</strong> It's also worth noting that
different browsers have slightly different behaviors when it comes to
PannerNodes. In particular, Firefox spatialization can appear to sound
better, but this is simply because it's louder (the same effect can be
replicated in Chrome by just increasing gain). Also, on iOS, it seems
that the spatialization effect is weaker -- potentially because they are
using a different HRTF, or maybe they are just panning.</p>

<p><strong>Spatialization effect can be subtle.</strong> I found that there wasn't
enough oomph to the effect provided by WebAudio's HRTF. Perhaps it is
acoustically correct, but it just wasn't obvious or compelling enough as
is. I had to fudge the situation slightly, and implement a sound cone
for the observer, so that sources that are within the field of view got
a slight gain boost.</p>

<h2>Parting words and links</h2>

<p><a href="http://www.ee.usyd.edu.au/people/philip.leong/UserFiles/File/papers/errors_hr97.pdf">The nature and distribution of errors in sound localization</a>
is a seminal paper from 1997, giving a thorough psychoacoustic analysis
on our hearing limits. In this web audio context, however, it is unclear
how much of this perceptual accuracy is lost due to variations in
headphone style and quality, and software implementation details.  To
truly bring my Austrian cathedral experience to the web, we would
probably need a personalized HRTF, and also a more sophisticated room
model that could simulate reflections from the walls of the building.
This is concievable on the web in the near future, especially with the
prospect of the <a href="https://plus.sandbox.google.com/+ChrisWilson/posts/QapzKucPp6Y">highly anticipated</a> <a href="http://webaudio.github.io/web-audio-api/#audio-worker-examples">AudioWorker</a>.</p>

<p>Let me conclude by linking you to a couple more spatial audio demos:</p>

<ul>
<li>Ilmari wrote an <a href="http://www.html5rocks.com/en/tutorials/webaudio/positional_audio/">html5rocks post</a> a while back about three.js
and the Web Audio API.</li>
<li>Mozilla built the <a href="https://hacks.mozilla.org/2013/10/songs-of-diridum-pushing-the-web-audio-api-to-its-limits/">Songs of Diridum demo</a>, showing a cute
spatialized jazz band.</li>
<li>Arturo recently emailed me, showing me a <a href="http://inspirit.unboring.net/">pretty compelling WebVR +
Audio project</a>, in the spirit of WebVR.</li>
<li>Not web-based, but a painstakingly recorded and tweaked <a href="https://www.youtube.com/watch?v=IUDTlvagjJA">binaural
haircut simulation</a> just to illustrate the potential.</li>
</ul>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Responsive WebVR, headset optional</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/responsive-vr"/>
    
    <updated>2015-02-02T09:00:00-00:00</updated>
    
    <id>https://smus.com/responsive-vr</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>VR on the web threatens to cleave the web platform in twain, like mobile
did before it. The solution then and the solution now is <a href="http://en.wikipedia.org/wiki/Responsive_web_design">Responsive Web
Design</a>, which websites to scale well for all form factors.
Similarly, for VR to succeed on the web, we need to figure out how to
make VR experiences that work both in any VR headset, and also without a
VR headset at all.</p>

<p><img src="/responsive-vr/hmds.png" alt="Various head mounted displays." /></p>

<p><a href="https://github.com/borismus/webvr-boilerplate">WebVR boilerplate</a> is a new starting point for building
responsive web VR experiences that work on popular VR headsets and
degrace gracefully on other platforms. Check out a couple of demos, <a href="http://borismus.github.io/webvr-boilerplate/">a
simple one</a> and one <a href="http://borismus.github.io/sechelt">ported from MozVR</a>.</p>

<!--more-->

<h2>Preview the VR experience for everyone</h2>

<p>Say you visit a webpage, and it opens up in split-screen mode barrel
distortion, chromatic aberration correction, personalized interpupillary
distance, and provides 6 DOF tracking. <a href="https://www.youtube.com/watch?v=PkFyGNjaQ8k">Whoa</a>. You reach for your
VR headset only to find that you forgot it at work! How disappointing!
The vast majority of normal people with no head mounted display lying
around will surely be even more disappointed.</p>

<p>Responsive web design promises content which automatically adapts to
your viewing environment by using fluid layouts, flexible images,
proportional grids; a cocktail of modern web technologies. Similarly,
WebVR experiences need to work even without VR hardware. This has two
obvious advantages:</p>

<ol>
<li>The vast majority of people that don't have VR hardware can still get
a feeling for the experience.</li>
<li>Even if you have VR gear, donning it is a pain. This preview lets you
quickly evaluate whether or not wearing is worth the hassle.</li>
</ol>

<p>What are some reasonable fallbacks to the in-helmet VR experience? The
main question boils down to emulating head tracking without wearing
anything on your head. On mobile phones, the obvious answer is to use
the gyroscope, for a <a href="http://googlespotlightstories.com/">Spotlight Stories</a>-type experience. On
desktop, we use the mouse to free-look, and also support turning using
the arrow keys. This covers enough of the 3DOF orientation that all HMDs
provide. Clearly missing are the three translational degrees of freedom,
but these are provided only by some VR headsets, and we can imagine some
<a href="http://topheman.github.io/parallax/">interesting fallbacks</a> for those too.</p>

<h2>Write once, run in any VR headset</h2>

<p>Remember the old "write once, run anywhere" promise? The web is the
closest thing we have to fulfilling it, but what it actually delivers is
often far from this ideal. The latest VR wave has barely begun and
already the web VR world is fragmented. Case in point,
<a href="http://vr.chromeexperiments.com/">vr.chromeexperiments.com</a> don't work on Oculus, and
<a href="http://mozvr.com/">mozvr.com</a> demos don't work in Cardboard.  The promise of WebVR
is that once it lands, all will be well in the world. However, this
means that we need to wait for WebVR to become fully baked. In other
words, we are blocked on the valiant efforts of our dear <a href="https://twitter.com/vvuk">WebVR</a>
<a href="https://twitter.com/Tojiro">implementers</a> (as well as the heavyweight browser development
process consisting of spec authors, security reviews, binary size, etc).</p>

<p>To speed up the process, we need a polyfill for WebVR which uses web
APIs to provide functionality to the WebVR specification (currently, in
<a href="https://github.com/vvuk/gecko-dev/blob/oculus/dom/webidl/VRDevice.webidl">IDL form</a>). In the absence of a WebVR implementation, the polyfill
kicks in and supports mobile VR headsets like Cardboard and Durovis
Dive, which are passive contraptions that just piggyback on the
smartness found in your smartphone.</p>

<h2>Introducing: WebVR Boilerplate</h2>

<p>The <a href="https://github.com/borismus/webvr-boilerplate">WebVR boilerplate project</a> is on github, and consists of
two parts. Firstly, the <a href="https://github.com/borismus/webvr-polyfill">WebVR polyfill</a> provides WebVR
support for Cardboard-compatible devices, and orientation tracking
fallbacks where no headset is available. The WebVR polyfill can also be
installed from npm (available via <code>npm install webvr-polyfill</code>).</p>

<ol>
<li>A <code>CardboardHMDDevice</code>, provides a reasonable default for
interpupillary distance and field of view for cardboard-like devices.</li>
<li>On mobile devices, a <code>GyroPositionSensorVRDevice</code>, which provides
orientation through the <code>DeviceOrientationEvent</code>.</li>
<li>On PCs, a <code>MouseKeyboardPositionSensorVRDevice</code>, which provides
orientation through keyboard and mouse events.</li>
</ol>

<p>It is designed to be used with <a href="http://threejs.org/">THREE.js</a> plugins that are built
for the WebVR API: <a href="https://github.com/mrdoob/three.js/blob/master/examples/js/controls/VRControls.js">VRControls.js</a> and
<a href="https://github.com/mrdoob/three.js/blob/master/examples/js/effects/VREffect.js">VREffect.js</a>, but of course any valid usage of the WebVR API
should work (modulo bugs).</p>

<p>Secondly, the <a href="https://github.com/borismus/webvr-boilerplate/blob/master/js/webvr-manager.js">WebVR manager</a> surfaces VR compatibility
using consistent iconography and simplifies transitioning in and out of
full VR mode. It also contains some of the best practices for making VR
work on the web, for example, using orientation lock to keep the phone
in landscape orientation, and a means of keeping the phone screen on. If
you're ready to dive in, the <a href="https://github.com/borismus/webvr-boilerplate">github has all of the technical
information</a> available.</p>

<p>WebVR boilerplate is meant to make it easy to develop immersive
experiences that run on all VR hardware, including Oculus and Cardboard,
and also provide reasonable fallbacks when no specialized viewer is
available.</p>

<h2>WebVR boilerplate in action</h2>

<p><img src="/responsive-vr/sechelt.png" alt="Screenshot of the mozvr.com Sechelt demo." /></p>

<p>I really liked Mozilla's <a href="http://mozvr.com/projects/sechelt/">Sechelt demo</a>, inspired by the
eponymous town on British Columbia's beautiful Sunshine Coast. I've
<a href="http://borismus.github.io/sechelt">ported it</a> to WebVR Boilerplate. The result is the same
demo, which works in Cardboard, as well as continuing to work on desktop
and mobile devices, and on the Oculus Rift via <a href="https://nightly.mozilla.org/">Firefox Nightly</a> and
<a href="https://drive.google.com/folderview?id=0BzudLt22BqGRbW9WTHMtOWMzNjQ&amp;usp=sharing#list">Brandon's WebVR Chrome builds</a>. It also yielded less confusing
boilerplate code and a <a href="https://github.com/borismus/sechelt/commit/671cff9fc283b58d2ebce0ae6f0dfa3580050aab">simplified code base</a> since
there is no longer need for an unweildy conditional to determine whether
to use <code>DeviceOrientationControls</code>, <code>OrbitControls</code>, or <code>VRControls</code>,
and decide between <code>VREffect</code> and <code>StereoEffect</code>.</p>

<p>As always, <a href="https://twitter.com/borismus">let me know what you think</a> and feel free to point
out (or fix!) my mistakes on <a href="https://github.com/borismus/webvr-boilerplate">the github</a>.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Web Sensor API: raw and uncut</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/web-sensor-api"/>
    
    <updated>2014-11-13T09:00:00-00:00</updated>
    
    <id>https://smus.com/web-sensor-api</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>Sensors found in smartphones define the mobile experience. GPS and the
magnetometer enable the fluid experience of maps; motion sensing enables
activity recognition and games, and of course the camera and microphone
allow whole categories of rich media applications. Beyond these now
obvious examples, sensors can also enable clever inventions, such as
<a href="http://www.cycloramic.com/">Cycloramic</a>, which used the vibrator motor in iPhones (4 and 5) to
rotate the phone and take a panorama, <a href="https://play.google.com/store/apps/details?id=com.runtastic.android.pushup.pro">pushup counters</a> which
use the proximity sensor to count repetitions, and <a href="https://play.google.com/store/apps/details?id=com.carrotpop.www.smth">Send Me To
Heaven</a>, which uses the accelerometer to determine flight time of
a phone thrown vertically as high as possible. I've had some experience
using and abusing sensors too, most recently for the <a href="http://smus.com/talk/2014/io14/">Cardboard magnet
button</a>.</p>

<iframe width="640" height="360" src="//www.youtube.com/embed/l5775qwORk4" frameborder="0" allowfullscreen></iframe>

<p>However, over the last couple of years, I've had to step away from the
web as a development platform, in part because of the poor state of
sensor APIs.  In this post, I will describe some of the problems, take a
look at sensor APIs on iOS and Android, and suggest a solution in the
spirit of the <a href="https://extensiblewebmanifesto.org/">extensible web manifesto</a>.</p>

<!--more-->

<h2>Existing sensor APIs are underspecified</h2>

<p>One of the most popular sensor APIs on the web is the <a href="http://w3c.github.io/deviceorientation/spec-source-orientation.html#devicemotion">DeviceMotion event
API</a>, which is basically always just an opaque abstraction around the
accelerometer. The web, as always, tries to solve the problem in the
most general way possible:</p>

<blockquote>
  <p>This specification provides several new DOM events for obtaining
  information about the physical orientation and movement of the hosting
  device. The information provided by the events is not raw sensor data,
  but rather high-level data which is agnostic to the underlying source
  of information. Common sources of information include gyroscopes,
  compasses and accelerometers.</p>
</blockquote>

<p>This could be fine in theory, except the specs end up being so vague in
their attempt to please everybody, that they under-specify the behavior
of events such as <code>DeviceOrientation</code>. Throw in some rogue implementers,
and you end up with huge discrepancies in browsers, as <a href="http://www.html5rocks.com/en/tutorials/device/orientation/">Pete found back
in 2011</a>:</p>

<blockquote>
  <p>For most browsers, alpha returns the compass heading, so when the
  device is pointed north, alpha is zero. With Mobile Safari, alpha is
  based on the direction the device was pointing when device orientation
  was first requested. The compass heading is available in the
  webkitCompassHeading parameter.</p>
</blockquote>

<p>A useful sensor abstraction would be to build a compass on top of the
magnetometer (and maybe gyro) sensors, and then expose that as a high
level Compass API. Unfortunately many web sensor APIs give us a
mid-level of abstraction. They don't map reliably to particular hardware
sensors, nor do they provide much use. Sensors allow many applications
that were not originally envisioned by the spec writers. By choosing
poorly specified ivory-tower abstractions, the web limits what can be
done on the platform.</p>

<h2>Low level sensor APIs don't exist</h2>

<p>While you can work around the insanity of <code>Device*</code> style events on the
web with platform-specific shims, you cannot work around missing sensor
APIs. Magnetometers, pressure sensors, proximity, light, temperature,
battery, etc. These are mostly missing, and the ones that are specified
are specified in a very narrow way that does not generalize across to
other types of sensors (eg. <a href="http://www.w3.org/TR/2013/CR-ambient-light-20131001/">DeviceLightEvent</a>).</p>

<p>Unfortunately it seems that previous attempts to push for a general low
level sensor API <a href="http://lists.w3.org/Archives/Public/public-geolocation/2011Oct/0000.html">haven't really gotten much traction</a>. In
fact, it's a bit unclear whether or not the <a href="http://www.w3.org/2009/dap/">Device API working
group</a>, is even the right place for sensor APIs, since their
mandate is supposedly more about services than sensors:</p>

<blockquote>
  <p>[To] enable the development of Web Applications and Web Widgets that
  interact with devices services such as Calendar, Contacts, Camera,
  etc.</p>
</blockquote>

<p>Except <a href="https://dvcs.w3.org/hg/dap/raw-file/default/sensor-api/Overview.html">here's a sensor API</a> from the same group, which
seems to be abandoned... I don't even</p>

<p>There are more recent voices (circa 2014) that seem to be pushing in a
generic sensor API direction, from folks like <a href="https://github.com/rwaldron/sensors">Rick Waldron</a>
and <a href="http://lists.w3.org/Archives/Public/public-device-apis/2014Sep/0024.html">Tim Volodine</a>. Many of these ideas are still working within
the confines of a sensor API for each type of sensor. This does not
scale well for the web, which tends to take a long time for any new web
standard, but this renewed interest is very exciting and promising!</p>

<h2>Sensors on other platforms</h2>

<p>The web is woefully behind native platforms in almost every regard (with
possibly the exception of audio). Sensors on iOS and Android have a rich
history, and ended up in a pretty similar place as the two platforms
have scrambled to converge. Let's take a look.</p>

<p>iOS started off with a <a href="https://developer.apple.com/LIBRARY/ios/documentation/UIKit/Reference/UIAccelerometer_Class/index.html">UIAccelerometer API</a>, which was
replaced by <a href="https://developer.apple.com/LIBRARY/ios/documentation/CoreMotion/Reference/CoreMotion_Reference/index.html">CoreMotion</a> in iOS 5. Rather than providing a
series of specific APIs for each type of sensor API as it had before,
CoreMotion provides a unified framework for sensor events. Each data
type inherits from a common base class <code>CMLogItem</code>, and most of the API is
encapsulated in <code>CMMotionManager</code>, which explicitly lists accelerometer,
gyroscope and magnetometer-related APIs. iOS went from specific to
generic, which makes it super easy to add new types of sensor data. That
said, the API is generic only for motion sensors, which excludes a bunch
of sensors not directly related to motion like temperature, humidity,
etc.</p>

<p>Android started off right, and hasn't had to change much, providing a
<a href="http://developer.android.com/reference/android/hardware/SensorEvent.html">generic API for sensors</a> since API level 3. Android's API
is accessed through a SensorManager, which provides a somewhat overly
abstract API, because of its support for multiple sensors of one type
(eg. two accelerometers) in the same device. Still, the idea is good,
and all of the low level sensor data are well specified (per sensor
type)so the hardware/firmware vendor knows what data format their sensor
should stream. Of course there are still rogue implementations that
don't follow the spec, but that is a perennial problem for any open-ish
ecosystem.</p>

<p>Android also has a <a href="http://developer.android.com/guide/topics/sensors/sensors_overview.html">distinction</a> between software-based sensors and
hardware-based ones. The idea is that the same framework can provide
both the low level data coming directly from the hardware, as well as
useful higher level data obtained through <a href="http://en.wikipedia.org/wiki/Sensor_fusion">sensor fusion</a>. As of
API level 19, Android also provides <a href="http://developer.android.com/reference/android/hardware/SensorManager.html#flush(android.hardware.SensorEventListener)">batch mode</a> for sensor data, which
is very useful for conserving battery and CPU for applications where
some delay is acceptable.</p>

<p>One nice advantage of an iOS style API is that each sensor type has its
own structure (rather than just an amorphous array of floats, as in
Android), which is quite a bit easier to parse. The downside is that
adding new sensor types introduces more overhead, since each one
requires a new structure to be defined and agreed upon. Since we are
talking about web standards, which evolve at a glacial pace, we should
err on a simple API that works well without spec modifications.</p>

<h2>Great artists steal</h2>

<p>There is no need for the web to reinvent the wheel. The wheel has
already been invented by iOS and Android. All we need to do is take the
good parts from these successful sensor platforms, and integrate them
into the web in a way that makes sense. The web is not the place for
innovation, but for standardization.</p>

<p>Conceptually, a sensor provides a stream of data. The developer should
be able to configure the rate at which new data comes in, as well as
batching the data in windows of sensor data (as is customarily done with
audio data, for example). In Android, because of a plurality of devices,
it's important to be able to check if a particular sensor is available.
The same concept maps well to the web.</p>

<h2>Toward A Web Sensor API</h2>

<p>In general, here are the requirements for a Web Sensor API that works:</p>

<ul>
<li>A specification defining the format of the data, similar to
<a href="http://developer.android.com/reference/android/hardware/SensorEvent.html">Android</a>.</li>
<li>A way to feature detect for the existence of a particular sensor.</li>
<li>A way to request (and revoke) a stream of sensor data.</li>
<li>A way to specify how often to poll the sensor.</li>
<li>Bonus: A way to request sensor data in batch form.</li>
</ul>

<p>While bringing an API like this to the web is a huge undertaking, there
is a silver lining. The sensors we're talking about are all considered
(at least for now) low-security, in the sense that on native platforms,
there is no extra permission required to access them. This makes it
possible to simply propose an API, convince everybody of it's worth, and
then have it implemented across the web!</p>

<p>I don't have a strong opinion about how the API itself looks like as
long as it fulfils the above requirements. Here's a simple strawman
which should satisfy them:</p>

<pre><code>// Check for magnetometer support.
if (sensors.Magnetometer === undefined) {
  console.error('No magnetometer found');
}

// Start listening for changes to the sensor.
var magnetometer = sensors.Magnetometer;
magnetometer.addEventListener('changed', onMagnetometer, {
  sample_rate: sensors.POLL_FAST, // In hertz, eg. POLL_FAST == 100
  batch: 1 // Number of data points to provide in a single poll.
});

// Handle sensor events.
function onMagnetometer(event) {
  var data = event.data[0];
  // Get the timestamp (in millis).
  var t = data.timestamp;
  // Get the data (in this case µT, as per spec).
  var x = data.values[0];
  var y = data.values[1];
  var z = data.values[2];
  // Process the data.
  superAdvancedSensorFusionThing.addData(t, x, y, z);
}

// Stop listening.
magnetometer.removeEventListener('changed', onMagnetometer);
</code></pre>

<h2>Conclusion</h2>

<p>If you aren't yet convinced that we need access to low level sensors on
the web, recall web developers scoffing at device pixel ratio (DPR),
really questioning the need for to ever go above 2x. Now that <a href="https://developers.google.com/cardboard/">some
screens</a> are ending up 5cm from your face, the current
generation of 4x displays isn't enough. The same exact thing applies to
sensors. The need is there, but it is not seen as enough of a priority
by the web community.</p>

<p>By enabling low level sensor access, we can allow new experiences never
before possible on the web. <a href="https://play.google.com/store/apps/details?id=com.runtastic.android.pushup.pro">Pushup rep counters</a>, the <a href="http://smus.com/talk/2014/io14/">magnet
button</a> in Cardboard, and myriads more applications
yet to be concieved could all be built on the web platform, eliminating
a big reason why the web is increasingly losing its relevance on mobile
devices. Providing low level sensor access is critical and aligns
perfectly with the <a href="https://extensiblewebmanifesto.org/">extensible web vision</a>.</p>

<p><em>Update (Nov 14, 2014): There was a <a href="http://lists.w3.org/Archives/Public/public-device-apis/2014Nov/0018.html">W3C call</a> about this very
topic yesterday! Kicking off efforts in <a href="https://github.com/w3c/sensors">this github repo</a>.
Join us!</em></p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>UIST 2014 highlights</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/uist-2014"/>
    
    <updated>2014-10-14T09:00:00-00:00</updated>
    
    <id>https://smus.com/uist-2014</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>This year's <a href="http://www.acm.org/uist/uist2014/">UIST</a> was held in Waikiki, Honolulu, the undisputed
tourist capital of Hawaii. I've stuck to my now three year old habit of
taking notes and <a href="http://smus.com/uist-2013">posting my favorite work</a>. Since last year,
the conference has grown an extra track. The split was generally OK for
me, with my track mostly dedicated to user interface innovation
(sensors, etc) and another more concerned with crowdsourcing,
visualization, and more traditional UIs.</p>

<p>My overall feeling was that the research was mostly interesting from a
tech perspective, but focused on solving the wrong problem. For example,
at least 5 papers/posters/demos were focused on typing on smartwatches.
The keynotes were very thought provoking, especially when juxtaposed
with one another.</p>

<!--more-->

<h2>Focused Ultrasonic Arrays</h2>

<p>I got to play with a holographic display with touch-feedback. Sounds
crazy, and it is. HaptoMime uses an array of ultrasonic transducers to
beam-form focused ultrasound to a specific target. The touch feedback
feels like a little electric shock, but it's incredible that it works.
The field of view of the holographic screen is a bit limited:</p>

<iframe width="600" height="339" src="//www.youtube.com/embed/uARGRlpCWg8" frameborder="0" allowfullscreen></iframe>

<p>As a kid, I loved drawing patterns on my grandma's rug with my finger.
This research team was clearly inspired by the same activity, and they
created several ways of automating the process: using a roller device, a
pen, and an focused ultrasonic array:</p>

<iframe width="600" height="339" src="//www.youtube.com/embed/L0hrETGddLQ" frameborder="0" allowfullscreen></iframe>

<h2>Multi-device interactions</h2>

<p>GaussStones built on a bunch of other "Gauss"-prefixed previous work
from the same lab, showing an array of hall sensors used to sense a
variety of shielded magnetic tokens, which can encode an ID using field
strength. You could play physical chess, or even combine magnetic tokens
to create more complex interactions, like a slider or button unit:</p>

<iframe width="600" height="339" src="//www.youtube.com/embed/qlr-15Oto6s" frameborder="0" allowfullscreen></iframe>

<p>Another nice example of multi-device interaction came from MIT, where
a group used this extremely clever way of tracking the phone's position
relative to a laptop, using a 2D gradient, where the color of each pixel
maps to a position in space:</p>

<iframe width="600" height="339" src="//www.youtube.com/embed/hFH6hJLDoLE" frameborder="0" allowfullscreen></iframe>

<p>The awkwardly named Vibkinesis shows a smartphone case which is
equipped with two vibrator motors which give a phone the ability to
translate and rotate on a flat surface. In one example,
notifications caused the phone to rotate by 90 degrees, which had the added benefit of
notifying the user of a notification even if the battery runs out of
juice. This is apparently funny from a Japanese culture perspective,
where characters often die under strange circumstances, leaving no clue
but a "dying message" on or around their person. Another example
involved a fish-eye lens on the front-facing camera to detect the
position of the user's hand (based on skin color), and then physically
nudging the user to get their attention:</p>

<iframe width="600" height="339" src="//www.youtube.com/embed/UlFwVUHotrU" frameborder="0" allowfullscreen></iframe>

<h2>Awesome fabrication techniques</h2>

<p>I'm a huge fan of subtractive techniques (eg. laser cutting) rather than
additive ones (eg. 3D printing). FlatFitFab is a CAD tool for easily
creating balsa dinosaur-style models, and evaluating their stability and
feasibility. Super cool work:</p>

<iframe width="600" height="339" src="//www.youtube.com/embed/HeFQw0chSJY" frameborder="0" allowfullscreen></iframe>

<p>Rather than creating PCBs in something like Eagle, why not just sketch
them with a conductive pen instead? ShrinkyCircuits does just this,
following the principles of Shrinky Dinks, which shrinks when heated.
Because the whole board shrinks, it improves conductivity of the
conductive ink, and the contact points with electronics components.</p>

<iframe width="600" height="339" src="//www.youtube.com/embed/4p-l374rb8M" frameborder="0" allowfullscreen></iframe>

<h2>Spatial AR</h2>

<p>Research from Microsoft showed rooms instrumented with multiple
Projector+Depth Camera rigs, which allowed for some interesting
multi-user interactions:</p>

<iframe width="600" height="339" src="//www.youtube.com/embed/ILb5ExBzHqw" frameborder="0" allowfullscreen></iframe>

<p>Of course, the setup above doesn't allow perspective-corrected scenes.
To remedy this, they had a companion project which split the room in
two, creating head-tracked scenes for two participants. Pretty cool,
though it does not generalize to more than two people, nor does it
support stereo:</p>

<iframe width="600" height="339" src="//www.youtube.com/embed/Df7fZAYVAIE" frameborder="0" allowfullscreen></iframe>

<p>Still, entering either of these VR rooms feels a lot less dorky than
having to don a VR headset.</p>

<h2>Keynotes</h2>

<p>UIST was punctuated by three keynotes, from Ken Perlin, Mark
Bolas and Bret Victor, all of which were thought provoking and sometimes
frightening, but unfortunately not recorded. When I used to be in
Developer Relations, we would be hardpressed to show up at a conference
if the talk was not recorded because so much of the engagement happened
after the fact online. <strong>Academia needs the same culture</strong>.</p>

<p>Ken Perlin kicked off the co-located conference, SUI (Symposium for Spatial
Interaction), off with a nice talk about making computer use more like
performing a music, and less like writing a musical composition. As he
gave the talk, he very effectively used a tool he created called Chalk
Talk, which lets you sketch objects with behaviors in short hand - very
meta. Unfortunately I've been unable to find anything published about
the tool, as it would be interesting to play with. Ken envisioned a
world where you could do something conceptually similar to Chalk Talk
without a computer in the way. If this "virtual chalk" capability was
available to all humans, it would transform the way we communicate. I
wasn't completely convinced. When I'm discussing something with
colleagues, we only use a whiteboard for only very specialized things
like drawing a diagram of multiple objects. So there are two things that
need to happen:</p>

<ol>
<li>This virtual chalk needs to be <strong>easier to access</strong> than a whiteboard
while in a meeting room.</li>
<li><strong>Expand the set of concepts</strong> that can be expressed with virtual chalk.
Text and speech is <a href="http://graydon.livejournal.com/196162.html">pretty powerful</a>.</li>
</ol>

<p>Mark Bolas started UIST with a pretty terrifying keynote on virtual
reality. His premise was that "we are headed into a virtual future,
whether we want it or not". Terrifyingly, Mark seemed to be okay with
this inevitability, even going as far as discounting augmented reality,
since by the time we've built VR, we'll just want to stay in our
helmets. The real world isn't that great anyway. One thing I liked was
his call for creating more <strong>surreal experiences</strong> in VR rather than
trying to emulate the real world. These types of simulations are
conspicuously missing from <a href="https://share.oculusvr.com/category/all">existing VR demos</a>.</p>

<p>Bret Victor ended the conference with a much needed humanist
counterpoint to Mark' vision. I cannot do the talk justice, and eagerly
await a recording of it to try to understand all of the nuances. The big
idea of the talk was that "knowledge work" which started with the
printing press is tyrannical, reducing all of our senses and abilities
to manipulating symbols on a sheet of paper. So many other things that
evolution has designed for us, like hearing, smell, sense of space,
touch, etc, are all thrown out of the window. This problem only gets
compounded as we move to virtualize everything with touch screens. Bret
thinks we're poised to design the next great "dynamic" medium after the
printing press, something that is always interactive and multimodal, and
takes advantage of a wide array of human capabilities.</p>

<p>According to Mark Bolas, the real world is flawed, and we should build a
better virtual one. Bret Victor's vision is that humans are perfect,
having evolved over thousands of years. Rather than changing what it
means to be human, we should build a new medium that adapts to our
inherent strengths and weaknesses. Ken Perlin's "virtual chalk" is a
great example application for this dynamic medium.</p>

<h2>Tracks I missed</h2>

<p>Because UIST has become a multi-track conference, I inevitably missed
interesting parts. In particular, the collaboration track had some
<a href="https://www.youtube.com/watch?v=QtyO-oFlzGg">awesome</a> <a href="https://www.youtube.com/watch?v=jMH_qQF0vKg">work</a>, and there was one <a href="https://www.youtube.com/watch?v=YMfzAstvij0">music-related paper</a>
paper. It was great to have had a good excuse to go this year, showing
Cardboard to the academic community. Looking forward to next year,
although it is to be held in a somewhat <a href="http://uist.acm.org/about">less glamorous location</a>.</p>

<p><img src="/uist-2014/sunset.jpg" alt="Hawaii sunset." /></p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Spectrogram and oscillator</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/spectrogram-and-oscillator"/>
    
    <updated>2014-06-09T09:00:00-00:00</updated>
    
    <id>https://smus.com/spectrogram-and-oscillator</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>A live-input spectrogram written using <a href="http://polymer-project.org">Polymer</a> using the <a href="http://webaudioapi.com">Web
Audio API</a>.</p>

<p><img src="/spectrogram-and-oscillator/screenshot.png" alt="Screenshot of spectrogram" /></p>

<p>If you're running Chrome or Firefox, <a href="http://borismus.github.io/spectrogram">see it in action</a>. Once the
spectrogram is running, see if you can make a pattern with your speech
or by whistling. You can also click anywhere on the page to turn on the
oscillator. For a mind-blowing effect, <a href="https://www.youtube.com/watch?v=M9xMuPWAZW8&amp;t=5m30s">load this</a> in a parallel
tab.</p>

<!--more-->

<h2>Why?</h2>

<p>Having a spectrogram is incredibly handy for a lot of the work I've been
doing recently. So a while ago, I built one that satisfies my needs. It
runs in a full-screen, using the microphone input as the source.</p>

<p>It also includes an oscillator, which plays a sine wave at the frequency
of your pointer. It also shows you the frequency that it plays back, and
plots a short buffer of pointer positions. This is handy for measuring
internal latency:</p>

<p><img src="/spectrogram-and-oscillator/latency.png" alt="Latency estimation" /></p>

<p>Having the oscillator built-in is also pretty fun. You can <a href="/spectrogram-and-oscillator/sounds/morse.wav">send morse
code</a> (short short short, long long, short short long, short
short short), <a href="/spectrogram-and-oscillator/sounds/radio.wav">scan for radio stations</a>, make 8-bit character
<a href="/spectrogram-and-oscillator/sounds/sfx.wav">dying sound effects</a>, simulate <a href="/spectrogram-and-oscillator/sounds/ghosts.wav">aliens, ghosts and
theremins</a>, and annoy <a href="/spectrogram-and-oscillator/sounds/dogs.wav">small, annoying dogs</a>.</p>

<p>I use the tool mostly in Chrome, but it also works in Firefox.
Unfortunately no other browser currently has both <code>getUserMedia</code> and Web
Audio API support.</p>

<h2>Configuration parameters</h2>

<p>The following are HTML attributes of the <code>g-spectrogram</code> component. Many
of them are also configurable via the spectrogram controls component,
which shows up if the <code>controls</code> attribute is set to true.</p>

<ul>
<li><code>controls</code> (boolean): shows a config UI component.</li>
<li><code>log</code> (boolean): enables y-log scale (linear by default).</li>
<li><code>speed</code> (number): how many pixels to move past for every frame.</li>
<li><code>labels</code> (boolean): enables y-axis labels.</li>
<li><code>ticks</code> (number): how many y labels to show.</li>
<li><code>color</code> (boolean): turns on color mode (grayscale by default).</li>
<li><code>oscillator</code> (boolean): enables an oscillator overlay component. When
you click anywhere in the spectrogram, a sine wave plays corresponding
to the frequency you click on.</li>
</ul>

<h2>Using the Polymer component</h2>

<p>If you are inclined to embed this component somewhere, you can,
since it's implemented in Polymer, which, by the way, is an
awesome framework. Once you've <a href="http://www.polymer-project.org/docs/start/getting-the-code.html">gotten started</a>, here's
the simplest possible version:</p>

<pre><code>&lt;g-spectrogram/&gt;
</code></pre>

<p>Enable controls:</p>

<pre><code>&lt;g-spectrogram controls&gt;&lt;/g-spectrogram&gt;
</code></pre>

<p>Pass parameters to the component:</p>

<pre><code>&lt;g-spectrogram log labels ticks="10"&gt;&lt;/g-spectrogram&gt;
</code></pre>

<h2>Future work ideas</h2>

<p>It would be great to add a few things to this tool. If you're interested
in helping, submit your changes as a pull request <a href="https://github.com/borismus/spectrogram">on github</a>.
Some ideas for things that can be done:</p>

<ul>
<li>Improved axis labeling.</li>
<li>Make it work in mobile browsers.</li>
<li>Loading/saving of traces.</li>
<li>Loading audio data from a file.</li>
<li>Zoom support.</li>
<li>Higher precision FFT results (would require writing a custom FFT
rather than using the one built into Web Audio API.)</li>
</ul>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Addressable apps</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/addressable-apps"/>
    
    <updated>2014-05-21T09:00:00-00:00</updated>
    
    <id>https://smus.com/addressable-apps</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>It is human nature to create taxonomies for everything: people, places,
and things.  Without such a system of reference, we become lost and
disoriented.  Imagine your city with street names and addresses blanked
out. Finding your favorite cafe, meeting up with your friend on the
weekend, even locating your own parked car would become incredibly
difficult. Travel outside your city would become far more
challenging.</p>

<p>The web's defining property is addressability. URLs on the web are like
street names and addresses in the physical world. This makes sharing
and cross-linking easy. Non-web platforms are a little bit like our
city with blanked out street names and addresses. There's no good
way of talking about where you currently are, or how to get somewhere
else. These platforms typically give users a crutch to help with the
issue, such as a share button or dialog. But these create an
inherently inferior experience, since addressability is no longer
built-in. Addressability becomes a burden on the app developer, and
as a result, the platform is no longer navigable.</p>

<p>In light of the success of Android and iOS, and given a potential
explosion in new types of lower power computing (wearables, IoT, etc),
it's unclear if <a href="http://smus.com/ebb-of-the-web/">browsers will be as ubiquitous</a> as they are
today (at least in the near term). I'm very interested in seeing if and
how non-web platforms can embrace URLs.  How closely coupled are URLs to
HTML, and do they make sense without a presentation layer?</p>

<!--more-->

<h2>Not all URLs are created equal</h2>

<p>The modern URL can host several very different kinds of entities:</p>

<ol>
<li><strong>Data</strong>: text files, images, audio, movies, JSON, etc.</li>
<li><strong>Hypertext</strong> (content-program hybrid): HTML that can reference
content.</li>
<li><strong>Program</strong>: webapps designed to deliver a bundle of JavaScript that
then constructs the HTML dynamically from other URLs.</li>
</ol>

<p><img src="/addressable-apps/data-vs-program.png" alt="Program vs. data: evolution of web" /></p>

<p>Without an HTML renderer, hypertext and program URLs cannot be
interpreted. Only one of these types of entities makes sense: data. Data
URLs are seen everywhere on the web: whenever you include an <code>&lt;img&gt;</code> tag
on your page, or embed a <code>&lt;video&gt;</code>, reference some CSS, or make an XHR
to fetch some JSON, you are using a data URL.</p>

<p>Apps on other platforms use data URLs too, though not as much. Images
are typically included as part of the app itself, but all API access is
done in exactly the same fashion as on the web: using HTTP requests to
text or binary data.</p>

<p>The similarity isn't entirely superficial. Any sort of web-connected app
can be seen as just a view on top of a series of data URLs (APIs).
However, data URLs are typically hidden from the user. The only types of
URLs that users see are hypertext and program URLs. These are the ones
that are being shared around. But both of these types of URLs ultimately
map to HTML, sometimes via JavaScript. The underlying data URLs  are
concealed inside the page, and aren't exposed to the user.</p>

<h2>The "URL in, URL out" principle</h2>

<p>A user need not understand schemes, domain names, DNS, HTTP or GET
requests. They don't need to think about conceptual distinctions
between URL types to know that a URL is an address that gets you to the
same thing you're looking at right now. Whether it's Android/Java,
Polymer/JS or <em>InsertPlatform/InsertLanguage</em> underneath, the only thing
they want to be able to do is to continue reading their book on whatever
device they happen to be transitioning to. They want to share it with
their friend too, and have them enjoy a good read.</p>

<p>To make a platform URL-friendly, it should satisfy two simple
requirements:</p>

<ol>
<li>The platform should provide a way for apps to reveal the underlying
URL for the view.</li>
<li>Given a URL, the platform should open it in a way that yields the
best available user experience.</li>
</ol>

<p><img src="/addressable-apps/url-in-url-out.png" alt="Platforms handle content URLs and provide them on demand." /></p>

<p>However, to bring URL friendliness to a platform retroactively takes a
lot of effort. Taking a quick look at today's trending web-alternatives,
it's plain to see that Android has some form of URL in (via intent
filters), but no URL out. iOS has neither in, nor out (you're stuck). To
address this lack of URL out in Android, you can imagine all Android
activities having to implement a <code>URLReporter</code> interface like this:</p>

<pre><code>class TwitterProfileViewer extends Activity implements URLReporter {
  @Override
  String reportURL() {
    return String.format("http://twitter.com/%s", username);
  }
}
</code></pre>

<p>Of course, there is the not-unimportant UX question of how to then
reveal the URL and transfer it to other devices and people. But this
question will for now be left unanswered. With this API, a very
tasty carrot, and a very painful stick to force developers to implement
it (and a bit more UX thinking), we can make Android URL-friendly. </p>

<h2>But URLs aren't just identifiers</h2>

<p>You can look at URLs in one of two ways:</p>

<ol>
<li>As a <strong>universal identifier</strong>. The same URL is also the universal and
canonical way of getting to content that you are reading now.</li>
<li>As a <strong>web address</strong>. A URL like <code>http://smus.com/addressable-apps</code>
can be viewed as instructions for getting to a certiain place:
resolve the <code>smus.com</code> to <code>205.251.243.108</code>, connect to port 80 over
TCP, perform a <code>GET /addressable-apps</code> request.</li>
</ol>

<p>The real power of URLs is in both aspects combined. When only one facet
is used, the system is broken. I can't quite put my finger on it, but
something feels wrong in the cases, where the addressability aspect (2)
of URLs are not taken into account:</p>

<ul>
<li>AppLinks: these aren't universal identifiers, but fragile shortcuts to
the platform-specific apps. (violates 1)</li>
<li>Android intent filters: when you register an URL indent filter for
your activity, you aren't actually hosting anything at that URL.
(violates 2)</li>
<li>History API: a hack allowing developers to set the path of the URL to
anything they want. (violates 2)</li>
</ul>

<p>The History API emerged from a trend on the web: highly imperative
applications. These apps have grown so far from being a collection of
hyperlinked markup that they no longer have a natural URL-to-HTML-page
mapping. Because they are so script heavy, they need to be able to
pretend to respond to URLs. The History API is the webapp's hack for
URL-out.</p>

<h2>Mixed feelings</h2>

<p>This is my umpteenth attempt at finishing a post on the complicated
subject of URLs in non-web apps. And you have had the pleasure of
reading it not because the ideas in my head have crystallized into a
something coherent, but because I feel that the topic is difficult and
fundamentally unresolvable. In light of that, this post contains more
questions than answers. Sorry to disappoint :)</p>

<p>I'm still torn between maintaining the ideological and benefits of the
<a href="http://www.polymer-project.org/">mostly-declarative web</a> and the practicality of <a href="http://smus.com/installable-webapps/">jumping out
of the web's sandbox</a>. While I would be happy to see more
native platforms embrace URL in URL out, I don't think that the solution
is a clean one, nor do I think that first-class URLs are likely to
emerge in any platform after-the-fact. Unfortunately I don't think that
there is a clean solution.</p>

<p>However, as an optimist, I must believe in the long-term victory of the
web, though not in the sense that the web will rule over all other
user-facing platforms unopposed. Instead, platforms will continue to
rise and fall; the web's influence will ebb and flow as well. But the
web must be the one that wins out the most, keeping the idea of
addressability alive.</p>

<p>That URLs become a universal address that works across all platforms,
and not just the web, is a proposition worth considering.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>The ebb of the web</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/ebb-of-the-web"/>
    
    <updated>2014-04-15T09:00:00-00:00</updated>
    
    <id>https://smus.com/ebb-of-the-web</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>Tech pundits like to lament that the web has <a href="http://cdixon.org/2014/04/07/the-decline-of-the-mobile-web/">no viable
future</a>, while web idealists hold that in fact the web is
totally fine, with a "too big to fail" <a href="http://schepers.cc/the-recline-of-the-mobile-web">sort of attitude</a>.</p>

<p>At the root of this disagreement are poorly defined terms. The web can
mean many different things to different people. Though it started from a
pretty abstract notion of a series of interlinked documents, it has now
evolved to refer to a very specific technology stack of hyperlinked HTML
documents styled with CSS, enhanced with JavaScript, all served on top
of HTTP. In light of an increasing movement away from desktop-style
computing, we've seen a big shift away from the web in mobile platforms. </p>

<p>Let's take apart this gob of web technology in light of the increasingly
complex landscape of computing and try to make sense of what the web is
and where it's going.</p>

<p><img src="/ebb-of-the-web/webiness.png" alt="A framework for webiness" /></p>

<!--more-->

<h2>Webiness: how far down the rabbit hole?</h2>

<p>I want to introduce the concept of webiness, a framework for evaluating
how deeply an application embraces "the web":</p>

<p>Games are typical examples of apps that are <strong>pure native</strong> and not
webby at all (barring high score servers, etc). Because many are
playable offline, with no added benefit of having an internet
connection, there is obviously no room for web. They are built entirely
on native APIs, benefiting from being as close to the hardware as
possible for performance reasons.</p>

<p>Another class of native apps are Twitter, Facebook and the like, which
essentially act as <strong>specialized browsers</strong>. They largely use HTTP to
access RESTful endpoints that serve up JSON, which is rendered by the
native clients. Because a specialized browser is designed for a specific
use case in mind (eg. interacting with Twitter), it can be streamlined
for that purpose and doesn't need to deal with processing the web's
presentation layer (HTML, CSS, JavaScript). Therefore it presents a
more focused experience and benefits from being faster at start-up and
during interaction, offline support, and a generally better experience.</p>

<p><strong>Embedded browsers</strong> (also called hybrid apps) embed a webview into the
user interface of the page to use the web's presentation layer for part
of the user interface. How much of the user interface is created using
web technologies varies widely. This approach is beneficial because it
allows web developers to be featured in app stores, and also gives them
access to native APIs that are not available on the open web.</p>

<p>Lastly, <strong>browser-based</strong> pages and apps are ones where even the code
itself is fetched over HTTP. The app is written in a cross-browser way
to ensure that regardless of which browser loads the page, the user is
presented with a reasonable experience. The idea of this is to be able
to write once, deploy everywhere. Like the other types of apps on our
spectrum, these also use HTTP to interact with the server. All of the
content is rendered using HTML/CSS/JavaScript. Additionally, the HTML is
often hyperlinked.</p>

<h2>The web as greatest common divisor</h2>

<p>In mathematics, the greatest common divisor (GCD) between multiple
digits is the largest positive integer that divides the numbers without
a remainder. Observe that the GCD of a set of numbers (S) is by
definition less than or equal to each number. Furthermore, if the
numbers in S are not multiples of one another, the inequality
becomes strict. So <code>foreach n in S, gcd(S) &lt; n</code>.</p>

<p>In the webbiest case above, where we write once for all browsers, the
web user interface becomes the GCD for all existing interfaces. It is
guaranteed to be slower, less featureful, etc, than each individual
native platform, but when the web was conceived, the benefits outweighed
the costs. As the web evolved in the 90s, native platforms evolved
alongside it. Computing at the time was very desktop-centric, requiring
a physical keyboard, mouse, and relatively large display at, let's say
1024x768 pixels. At best, variance was between operating systems. The
computer geeks were on the Linux fringe. The art and music geeks used
Macs. But the hardware was pretty much the same.</p>

<p>Because of this uniformity, it was easy to standardize on a set of input
and outputs that would work reasonably well across a bunch of existing
computer configurations and operating systems. Computer hardware was all
very similar, just off by some factor. And the GCD of this orderly set
was pretty large: <code>gcd(200, 300, 400) = 100</code>.</p>

<h2>We're not in Kansas anymore</h2>

<p>Contrast this uniformity to today, when our mots du jour are "mobile
first", or even "mobile only". But even these notions are becoming
passé, as our day-to-day tech encounters start including wearable
sensors for health, chips embedded in your appliances, shoes, and
computers on your face. Even with the most conservative notion of what
mobile means - small screens and touch input - we've really thrown a
wrench into the big-screen, mouse-and-keyboard web. Scott Jenson is
absolutely right in remarking in an <a href="https://www.youtube.com/watch?v=6u03xYkwMVI">Edge conf panel</a>, that the
web hasn't even recovered from the fact that screens have gotten
smaller. With today's extended notion of computing, the GCD of all
of the devices and use cases the web is trying to support becomes very
small: <code>gcd(100, 200, 300, 50, 99, 198, 33) = 1</code>.</p>

<p>At the same Edge conf panel on the future of the web, somebody
asked the question of how the web would work on hardware without a
display. Answers from the panel were incoherent, but it's unclear how
this would be built into today's frankenweb, which is already a snowball
of many, often redundant technologies. And this is largely because of the
notion of <strong>THE WEB</strong> as a single platform. This is both its greatest
strength, and ultimately its tragic flaw, as the legacy of the early 90s
causes the singular web to cave in on itself, as we are experiencing
today.</p>

<p>We can no longer have a one-web-for-all approach. We need to focus on
having many different webs, each specializing on a particular subset of
our universe of devices. Taking our set above, we can split it in two
subsets, <code>S1 = {100, 200, 300, 50}</code>, and <code>S2 = {99, 198, 33}</code>.  Imagine
S1 are the desktop-like devices, and S2 are the phone-like devices. Now,
we have pretty okay GCDs: <code>gcd(S1) = 50</code>, and <code>gcd(S2) = 33</code>. Our worst
case GCD is now 33, which is a lot better than 1!</p>

<h2>The web is dead, long live the web!</h2>

<p>I'm pretty sure that HTTP is here to stay. Our desire for content is
universal, and that content needs to live somewhere. Regardless of how
that content is presented to us, it is likely to be served to us through
the cloud, over HTTP in the forseeable future.</p>

<p>What is indisputably being downplayed in the vibrant and variant near
future of computing, is the web's presentation layer - HTML, JavaScript
and CSS. These comprise the lingua franca of the web, and have no real
competition. However, this browser-served presentation layer of the web
will become less relevant as fewer things are done through the browser,
especially on mobile platforms.</p>

<p>To me, the critical thing is that content be addressable by URL, and
cross-linkable in some reasonable way. This is conventionally achieved
with HTML's <code>&lt;a&gt;</code> elements, but can also be done with JavaScript (eg. a
button that runs <code>javascript:window.location.href = myUrl;</code>, or a
<code>&lt;form&gt;</code> that creates a POST request to some other resource. This can
even be done without HTML at all. As long as we continue using HTTP, we
are guaranteed to have content that is available at a given URL. And as
long as we can guarantee that there's some handler for that content, the
spirit of the web lives on.</p>

<h2>Specialized browsers are a good solution</h2>

<p>RSS readers are good examples of specialized browsers focused on
presenting a good reading experience to the user. They are a single
entry point for all of the interesting things on the internet for the
user to read.</p>

<p>The Twitter app on your mobile device is an example of a specialized
browser designed for reading and sending tweets. This specialized
browser relies on the web's ability to link to various kinds of content
addressable by URLs. Tweets typically include an article or an image
which are served up using HTTP, and sometimes require HTML to render.
Twitter is all about hyperlinked content, without ever using an <code>&lt;a&gt;</code>
tag.</p>

<p>Apple also has several projects that are specialized browsers in spirit,
though they rarely link out to the wild west of the world wide
web. Generally, these specialized browsers focus on giving a great
experience for the user aiming to do something specific. Similar in
function to an RSS reader, Newsstand is an entry point to the magazines
and newspapers you read. Passbook is a specialized browser for tracking
event tickets, boarding passes and coupons. From a developer
perspective, both of these browsers require you to setup a server and
write some iOS code. (Unfortunately you're then stuck in Apple's
ecosystem forever.)</p>

<p>By identifying common patterns of functionality, Apple is able to
successfully introduce a specialized browser notion that spans across
multiple services, unifying it with a consistent user experience. This
begins to address the concern that there are <a href="http://designmind.frogdesign.com/blog/mobile-apps-must-die.html">too many apps for
everything</a>. Of course you don't want separate apps for
NYTimes, WSJ, USA Today, LA Times, etc. And of course you don't want
separate apps for each event booking service, airline and coupon
company.</p>

<p>The problem is endemic to the web community. The standards process
behemoth is slow, heavy and change-averse. Its focus is on a monolothic
web, <strong>THE WEB</strong>. Imagine if we focused on use cases in the same way
that Apple does, and created specialized sub-webs for various related
things. The next big thing is wearable health. Imagine a sub-web for
that, where we get to define the way all of these devices can talk to
one another. There would be a browser for that sub-web, providing a good
experience to see historical data, analyze trends, and plot data over
time. This is not the stuff of a web browser, but a completely different
beast.</p>

<p>Android's intent filters are a step in the direction of a specialized
browser. Intent filters let apps register to handle specific URL
patterns. If the URL pattern is opened by the user, she is presented
with a dialog of all of the possible handlers for that resource, which
may include Chrome, other web browsers, and specialized browsers that
subscribe to that URL. Once you're in an Android app, however, you're
stuck. Unless the app provides the ability to share content, there's no
way to send your state to someone else. In contrast, with a web browser,
you just take the URL and send it to your friend and if they have access
to that content, they get to see it.</p>

<h2>The web, the good parts</h2>

<p>When tech pundits hail the ebb of the web, they mean that mobile native
apps are eating away at the presentation layer on the mobile web and
beyond. After some contemplation, I have made peace with this
possible future. HTML is not the best possible way of creating content,
nor is CSS a reasonable way of laying out content. JavaScript may be
commonly used, but that does not make it a very good language. The
presentation layer is optimized for content consumption using pointers
and keyboard input, or if you're really adventurous, a smaller touch
screen.</p>

<p>Apps are another story. Frameworks like iOS and Android provide a much
more modern, cogent way of developing apps for mobile platforms. But
unfortunately they aren't linkable or portable across platforms. As I
mentioned earlier, there's no need for <code>&lt;a&gt;</code> elements, or anything from
the presentation layer to preserve benefits of linkability. But what
does the web look like once we've gutted the presentation layer?  Here
are some characteristics that we should preserve:</p>

<ul>
<li>The ability to take any content that is currently being shown and
serialize it into a URL.</li>
<li>The ability to open a URL with the right specialized browser of the
user's choosing.</li>
<li>If no specialized browser is installed, some way of presenting the
user with a list of good browsers.</li>
</ul>

<p>By dropping the notion of <strong>THE WEB</strong> (singular), and ushering an era of
specialized browsers, we can split our universe of devices into subsets
and increase the baseline greatest common denominator. Trying to extend
the web to work for every possible case will lead to even more feature
creep in a web platform that is already keeling over.</p>

<p>The web community should take a look at verticals and spec and build
specialized browsers for them. How would a web for boarding passes,
concert tickets and coupons look like? How about a web for personal
health tracking data? A web for content that should be consumed on an
audio-only device?</p>

<p>This has been my slightly edited brain dump on the future of the web.
Thanks for reading, I eagerly await your thoughts :)</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Remote controls for web media</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/remote-controls-web-media"/>
    
    <updated>2014-01-27T09:00:00-00:00</updated>
    
    <id>https://smus.com/remote-controls-web-media</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>When the world wide web was first conceived, it was as a collection of
interlinked textual documents. Today's web is full of rich media.
YouTube and other video sites alone consume an enormous 53% of all
internet traffic. Web denizens often have an open audio player in one of
their tabs. Web-based photo sharing services such as Flickr are the most
common way of enjoying photos on our computers. The remote control,
foundations of which are attributed to everyone's favorite inventor
Nikola Tesla in <a href="https://www.google.com/patents/US613809">patent US613809</a>, has been the preferred way of
controlling media for over half a century.</p>

<p>Yet the only way we can control all of this web media is via the
on-screen user interfaces that the websites provide. The web has no
remote control, and this is a big usability problem. Many use the
desktop versions of streaming services like Spotify and Rdio rather than
their web player, exclusively because of mac media key support. For
scenarios where you're far from the screen, like showing friends a
slideshow of photos on a TV, the lack of remote controllability is a
non-starter.</p>

<p>This post is a concrete proposal for what a remote controls for the web
should be like. To get a sense for how it might feel, try a <a href="http://borismus.github.io/media-control-prototype/">rough
prototype</a>.</p>

<!--more-->

<p><img src="/remote-controls-web-media/inputs.png" alt="Ways of controlling media: dedicated keyboard buttons,headphone
remotes, hardware remote controls, second-screen remote controls,
camera-based gestures, voice commands" /></p>

<h2>Related attempts to solve this problem</h2>

<p>Many platforms, especially Android, Mac and iOS, do a pretty good job of
supporting some of the inputs from the above image. The web, one of the
most common platforms for consuming media, supports none of them. The
only exception, of course, is the mouse and keyboard, but only when the
player tab is in the foreground.</p>

<p>On the web, there have been a number of proposals and half-solutions to
address this problem. Back in 2011, I shared <a href="http://smus.com/chrome-media-keys-revisited/">KeySocket</a>, a
Menu Bar app for OS X that handles media keys on the mac keyboard and
sends them to a companion Chrome extension that injects content scripts
into web-based media players. A similar project, <a href="http://sway.fm/">Sway.fm</a> built
support for media keys as an NPAPI plugin (a now <a href="http://blog.chromium.org/2013/09/saying-goodbye-to-our-old-friend-npapi.html">deprecated</a>
technology). The <a href="https://flutterapp.com/">Flutter app</a> takes a similar approach (native
app and companion extension), but enables webcam-based gestures for
controlling media.</p>

<p>Recently, I contributed the Mac implementation to the new <a href="https://codereview.chromium.org/60353008/">global
keyboard shortcuts API</a> for Chrome Apps and
Extensions. This API lets developers bind to any global shortcut,
including media keys. This is a good start since it brings the media key
handling infrastructure into Chrome, but does not address the problem
for the web in general.</p>

<h2>Starting with a good user experience</h2>

<p>Since we have a blank slate when it comes to controlling media on the
web, how should media controls behave? Let's start with some
sub-optimal behaviors. Here's one: all media events to get routed to all
open tabs capable of handling them. Imagine the case with many YouTube
tabs open, and the ensuing cacophony! Another bad approach is to route
commands only to the foreground tab, since a very common case for
needing media controls occurs when music is playing in the background.</p>

<p><a href="http://borismus.github.io/media-control-prototype/">This rough prototype</a> illustrates what I think is a pretty
good experience. It follows a focus-based model inspired by mobile
operating systems like iOS and Android. However, the web is messier than
the app model and edge cases like multiple sources of media playing
simultaneously (eg. music player and YouTube video) are likely to
happen, so we need to be careful.</p>

<p>Here is what happens when a user issues a play/pause command. I'll
define the bold terms in a second.</p>

<ol>
<li>If any media is <strong>playing</strong> in a <strong>background tab</strong>, it should pause.</li>
<li>Otherwise, if the <strong>foreground tab</strong> supports <strong>media events</strong>, it
should receive the media control and be pushed to the <strong>media focus
stack</strong>.</li>
<li>Otherwise, if the <strong>media focus stack</strong> is non-empty, the event
should be routed to the tab at the top of the stack.</li>
<li>Otherwise (if the stack is empty), find the first open tab
supporting <strong>media events</strong>, relay the event to that page and push it
on the <strong>media focus stack</strong>.</li>
<li>If there are no open tabs supporting <strong>media events</strong>, do nothing.
Optionally alert the user with a non-modal notification (eg. audible
chime).</li>
</ol>

<p>When a next or previous control is issued, the command should be routed
to the tab with <strong>media focus</strong>. If there are no tabs with <strong>media
focus</strong> and none capable of media control, we drop the event.</p>

<p>If a tab closes, remove it from the <strong>media focus stack</strong> and ensure
that <strong>media focus</strong> is granted to the tab at the top of the stack.</p>

<p>To clarify the description above, here are a few terms:</p>

<ul>
<li><strong>Foreground</strong>: the active tab of the foreground browser window.</li>
<li><strong>Background</strong>: every tab that is not in the foreground.</li>
<li><strong>Media events</strong>: a new event type that a page can listen to,
indicating how to interpret media controls (see the next section).</li>
<li><strong>Playing tab</strong>: a tab that is currently playing audio or video
content.</li>
<li><strong>Media focused tab</strong>: the tab which is the default receiver of media
control events.</li>
<li><strong>Media focus stack</strong>: a stack of tabs where the top-most tab
is the one that currently has media focus. If that tab is
popped off the stack, the next one gets media focus.</li>
</ul>

<p>The dry description above and <a href="http://borismus.github.io/media-control-prototype/">the prototype</a> should give a
sense of what tab should handle basic media controls, regardless of
their origin: keyboard, remote control hardware, gesture, etc.</p>

<p>Now, when a command comes in, how does the page know how to interpret
it? That's up to the web developer, and is done through <code>media</code> events,
described in the next section.</p>

<h2>Enabling media controls using media events</h2>

<p>A fundamental missing piece so far is a way for a web page to indicate
that it can receive media controls, and a way for it to specify how it
wants to handle them. The solution to this is to create a new type of
event, the <code>media</code> event, which are defined on a page-level, bound to
the window object. This suggestion is not new, and first (as far as I
can tell) came up in this <a href="http://paulrouget.com/e/mediaevents/">blog post by Paul Rouget</a>. Here's
how media events work:</p>

<pre><code>// Subscribing to media events.
window.addEventListener('media', function(e) {
  if (e.data == e.MEDIA_PLAY) {
    myPlayer.play();
  } else if (e.data == e.MEDIA_PAUSE) {
    myPlayer.pause();
  } else if (e.data == e.MEDIA_NEXT_TRACK) {
    myPlayer.next();
  } else if (e.data == e.MEDIA_PREVIOUS_TRACK) {
    myPlayer.previous();
  }
});
</code></pre>

<p>This code tells the browser that this page can accept media controls,
and what this page should do when a particular media control is
received.</p>

<h2>Determining user-initiated media playback change</h2>

<p>Another missing piece in the narrative so far is how to populate the
focus stack. So far, we know that a closed tab should be popped from the
stack, and that play/pause sometimes causes a tab to be pushed onto the
stack. But this is not enough, since the user can still interact with
media using the UI of the player. For example, if I start listening to
music through a Spotify tab, and then switch tabs, media commands should
obviously go to the Spotify tab, despite me never having issued any
media controls.</p>

<p>One option is to, as the user navigates between tabs, push any tab with
supporting media events onto the stack. This approach fails for the case
where you are listening to music in the background, and then change
tabs, passing an open YouTube video on the way. In this case, that
YouTube video would become focused and there would be no way to control
the music (until you close the YouTube tab). What we actually need is to
be able to <strong>track when the user interacts with the page using the media
player UI</strong>, in order to then push page to the media focus stack.</p>

<p>A browser can distinguish user-initiated events (like clicks and
keyboard presses) from programatic ones (like a timer firing, or a page
loading). <a href="https://developer.apple.com/library/safari/documentation/AudioVideo/Conceptual/Using_HTML5_Audio_Video/Device-SpecificConsiderations/Device-SpecificConsiderations.html">iOS does this</a> to prevent annoying pages from
autoplaying music (remember the 90s?). Using the same idea, browsers can
also track when a media player's playback state changes due to user
input.</p>

<p>Even so, there may be special cases that aren't perfect. For example,
imagine a music app with media controls and a video ad on the side. If
the user then clicks the video ad, it doesn't mean that the page should
now have media focus. There are other tricky cases such as a page full
of videos. Here, if a user starts playing a particular video, and then
wants to stop it using media controls, the expectation is that the same
video pauses. If the web developer does not handle this case gracefully,
another video may start playing concurrently.</p>

<h2>Response at FOMS was positive</h2>

<p>Pitching this idea at <a href="http://www.foms-workshop.org/foms2013/pmwiki.php/Main/MediaFocus">FOMS 2013</a> a few months ago, folks seemed
receptive. There was an understanding that a lack of media controls on
the web is a genuine problem. Additionally, I got good feedback on the
solution, which helped to iterate and get to this stage. This is all
very encouraging, and I've written this post to keep the discussion
alive and keep the momentum going. To make remote controls for the web a
reality, we need is a critical mass of interested browser implementers.</p>

<p>As always, thanks for reading, and let me know if you have thoughts or
suggestions on this topic, especially if you make browsers for a living
and want to help standardize!</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>UIST 2013 highlights</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/uist-2013"/>
    
    <updated>2013-10-25T09:00:00-00:00</updated>
    
    <id>https://smus.com/uist-2013</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>I just got back from Scotland, where I had the pleasure of attending
UIST 2013 in St. Andrews. This was my second time attending, and again
it was incredibly engaging and interesting content. I was impressed
enough to take notes just like <a href="http://smus.com/uist-2011/">my last UIST in 2011</a>. What
follows are my favorite talks with demo videos. I grouped them into
topics of interest: gestural interfaces, tangibles and GUIs.</p>

<!--more-->

<h3>Quadrotor Tricks</h3>

<p>UIST kicked off with a very compelling demos from Rafaello D'Andrea,
professor at ETH, co-founder of Kiva. He currently works on the <a href="http://www.flyingmachinearena.org/">flying
machine arena</a>, a lab at ETH working on quadrotor control systems.</p>

<p>I really liked the flight assembled architecture idea: a building
assembled by quadrotors.</p>

<iframe width="560" height="315" src="//www.youtube.com/embed/JnkMyfQ5YfY" frameborder="0" allowfullscreen></iframe>

<p>Rafaello also showed off a kinect controlled quadrotor. A pointing
interface to control quadrotors. Other highlights included the ability
to place the quadrotor with your hand, and simulating environments like
controlled gravity, virtual walls, springs, and damped oscillations.</p>

<h3>Mime: Compact, Low Power 3D Gesture Sensing</h3>

<p>An MIT Media Lab group presented a pretty neat approach for gesture
tracking combining time-of-flight and RGB cameras. The approach is
compact enough to be embedded on a HUD device like Google Glass.</p>

<p>The specs are impressive: 100 FPS, sub-centimeter resolution, low-power
(45 mW). Showed glasses hardware with 3 cameras (baseline = face) and an
IR LED. Here's roughly how it works:</p>

<ol>
<li>Illuminate scene with IR. Backscatter light captured by cameras.</li>
<li>Time-of-flight approach. Source <code>s(t)</code> and response <code>r_n(t)</code>. Look
for time-shifted waveforms.</li>
<li>...Lots of crazy math reducing to convex optimization...</li>
</ol>

<p>Applications presented were a bit limited, mostly focused on in-air
writing and drawing. They also presented some cringe-worthy menu
navigation. The last and most obvious application was games.</p>

<h3>Gaze Locking: Passive Eye Contact Detection for Human–Object Interaction</h3>

<p>Surprisingly insightful project from Columbia based on a simple idea:
gaze tracking is hard. Knowing WHERE the user is looking is very
difficult, but knowing IF the user is looking is much easier. I loved
the approach of <a href="http://blog.kenperlin.com/?p=13296">solving the simpler problem</a>.</p>

<p>Detector approach:</p>

<ol>
<li>Eye corner detection</li>
<li>Geometric rectification</li>
<li>Mask eye area</li>
<li>Extract features from 96x26px rectangle.</li>
<li>PCA + MDA compression</li>
<li>Binary classifier (gaze locked or not).</li>
</ol>

<p>They also generated a Gaze Data set (6K images). The detector actually
does better than human vision. Works well from 18m away, though the
presenter claimed there was no degradation as a function of distance,
which was very suspicious.</p>

<p>They also presented a series of compelling applications:</p>

<ul>
<li>Human-object interaction (very cool video of iPads powering on based
on gaze).</li>
<li>Ad analytics (wow, incredible potential for Google/Signs team).</li>
<li>Sort/filter images by eye contact (as a measure of photo quality).</li>
<li>Gaze-triggered photography (when everyone is looking at the camera).</li>
</ul>

<p>More info on <a href="http://www.cs.columbia.edu/CAVE/projects/gaze_locking/index.php">the lab's site</a>.</p>

<h3>BodyAvatar: Creating 3-D Avatars with Your Body and Imagination</h3>

<p>Setting your avatar in video games is annoying. You basically go through
a wizard based on a GUI. This delightful implementation from Microsoft
Research uses your body to build your character's avatar. Creation
begins from the first person, as you create a general skeleton for the
avatar. Then the perspective changes to third person as you add
customizations using gestures. The final stage lets you paint your
avatar from the third person perspective.</p>

<iframe width="560" height="315" src="//www.youtube.com/embed/yU2Ai18tft4" frameborder="0" allowfullscreen></iframe>

<p>They also showed some impressive demos of stepping into limbs for
particularily complex models (eg. butterfly with 6 limbs). Super cool!</p>

<h3>Sauron: Embedded Single-Camera Sensing of Printed Physical User Interfaces</h3>

<p>Excellent work from Berkeley showing how a single camera can drive a
whole printed physical UI. The idea is that you 3D print an object,
insert a camera and have a fully functional input device.</p>

<p>Sauron simulates full motions of all components, ensures that everything
is visible via ray casting. One problem is that you can't always see the
whole interior. So Sauron modifies the design by extruding inputs,
adding mirrors.</p>

<p>A good question was asked about doing the same for output. Using a
transparent material you might also be able to light up specific areas
of the prototype, but apparently 3d printers can't print
transparent/translucent plastics. Cool future work might be to design
mobile tangibles that snap to a phone and use the phone's camera.</p>

<iframe width="560" height="315" src="//www.youtube.com/embed/GNdCnmm-cw8" frameborder="0" allowfullscreen></iframe>

<p>Ok, that's all for vision and gestures. Now on to tangibles:</p>

<h3>PneUI: pneumatically activated soft materials</h3>

<p>Ishii's group presented nature-inspired interfaces that are
transformative and responsive. Using mostly air pockets, they set out to
create tangible UIs inspired by soft marine organisms. Some examples of
the applications:</p>

<ol>
<li><p>Curvature: folding wristband/phone. Wraps up when placed on wrist.
Unwraps when used as a tablet. Pulsates shape changes to indicate
incoming calls.</p></li>
<li><p>Volume-change based interfaces with underlying origami substructure.
Application: origami accordion with variable height and input.</p></li>
<li><p>Micro + macro elastomers to create transformable textures.
Application: "feel" GPS on the steering wheel rather than see/hear.</p></li>
</ol>

<iframe src="//player.vimeo.com/video/63591283" width="500" height="281" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>

<p>Mind = blown.</p>

<h3>Paper Generators: Harvesting Energy from Touching, Rubbing and Sliding</h3>

<p>Disney research presented a way of harvesting energy from interaction,
primarily for popup book-type applications. Based mostly on static
electricity, they used teflon, which has low electron affinity. Rubbing
it on paper causes a discharge. Rubbing generates 500 µA, 1200 V.
Tapping generates 60 mW.</p>

<p>The approach is easy to build, printable with conductive ink cartridges.
In addition to rubbing, showed a bunch of different widgets that can
generate electricity - buttons, cranks, </p>

<p>Approach 1: direct energy usage. (eg. animations on e-ink displays.)
Approach 2: store and release if more energy is needed. (eg. actuate servos.)</p>

<iframe width="560" height="315" src="//www.youtube.com/embed/4WaUcXSfPTg" frameborder="0" allowfullscreen></iframe>

<h3>Touch &amp; Activate: Adding Interactivity via Active Acoustic Sensing</h3>

<p>Tsukuba University presented a very cool paper on adding acoustic
sensing to hard objects using contact mics and speakers. The basic idea
is that touching an object changes its bounding conditions, depending on
how it is touched. </p>

<p>The way it works is they vibrate objects at a wide frequency range and
capture the response.</p>

<ol>
<li>Attach contact speaker and microphone.</li>
<li>Make the object vibrate, doing a sweep signal from 20-40 KHz (inaudible).</li>
<li>Vibration response determined by object properties.</li>
<li>Extract features via FFT</li>
<li>Classify via SVM</li>
</ol>

<p>Applications:</p>

<ul>
<li>Simple music player based on duplo blocks.</li>
<li>Interactive animal body</li>
<li>Grasp recognition system for phone using a case.</li>
</ul>

<iframe width="560" height="315" src="//www.youtube.com/embed/XgxXi6w8IQc" frameborder="0" allowfullscreen></iframe>

<p>How cool is that? Anyway, now for something a bit more traditional:</p>

<h3>Transmogrifiers: Casual Manipulations of Visualizations</h3>

<p>University of Calgary presented their awesome visualization toolkit.
Their goal is to enable exploration and manipulation of data that is
stored in images with no underlying data.</p>

<p>The idea is to pick a "lens" shape which acts as a template and is
placed on an image. Also provide an output shape to serve as the target.</p>

<p>Applications:</p>

<ul>
<li><a href="http://upload.wikimedia.org/wikipedia/commons/5/5a/1862_Johnson_and_Ward_Map_or_Chart_of_the_World%27s_Mountains_and_Rivers_-_Geographicus_-_MtsRvrs-j-1861.jpg">Tracing rivers to 1D</a> to compare their lengths.</li>
<li>Mutate data chart types (eg. ring chart ==&gt; bar chart)</li>
</ul>

<iframe width="560" height="315" src="//www.youtube.com/embed/S1Roi2NOmx8" frameborder="0" allowfullscreen></iframe>

<h3>Content-based Tools for Editing Audio Stories</h3>

<p>A Berkeley PhD student showed his project, which aims to edit audio
stories (radio shows, podcasts, audio books) at a semantic level much
higher than the current industry standard (waveforms). Not that
technically challenging, just a really cool idea. Might be a very
compelling product.</p>

<p>Cool interactions:</p>

<ul>
<li>Edit speech (eg. copy, paste) in a text editor.</li>
<li>Lets you pick sentences from a list of takes.</li>
<li>Insert breaths and pauses where needed.</li>
<li>Retarget music by segmenting song by beats and automatically finding music change points.</li>
<li>Specify speech emphasis points manually, and use them as alignment points to music change points.</li>
</ul>

<iframe width="560" height="315" src="//www.youtube.com/embed/RHtI4G5L31w" frameborder="0" allowfullscreen></iframe>

<p>Here's to <a href="https://twitter.com/ACMUIST/status/390958095939407872">next UIST</a>. Hang loose!</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Responsive image workflow</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/responsive-image-workflow"/>
    
    <updated>2013-09-09T09:00:00-00:00</updated>
    
    <id>https://smus.com/responsive-image-workflow</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>About a year ago, I wrote an overview of many of the different <a href="http://www.html5rocks.com/en/mobile/high-dpi/">responsive
image approaches</a> in an HTML5Rocks article, all of which try to solve the
fundamental problem:</p>

<p><strong>Serve the optimal image to the device.</strong></p>

<p>Sounds simple, but the devil's in the details. For the purposes of this here
discussion, I will focus on optimal image size and fidelity, and much to your
chagrin, will completely ignore the art direction component of the problem.</p>

<p>Even for tackling screen density, a lot of the solutions out there involve a
lot of extra work for web developers. I'll go into two solutions (client and
server side) on the horizon that serve the right density images. In both cases,
all you need to do is:</p>

<pre><code>&lt;img src="img.jpg"/&gt;
</code></pre>

<!--more-->

<h2>Nobody cares about responsive images (that much)</h2>

<p>Let me start with an underlying problem: for one reason or another, most developers
don't really care that much about responsive images. Even if left unsolved,
the images still get to their destination, they're just a little crummier than
they should be. If fidelity doesn't matter much to you and your app, then no
big deal.</p>

<p>Others may not even know about the problem. If you're not a high density screen
user, you may have not been disappointed by the gulf in quality between crisp
images in native apps and blurry images in web apps. Some applications may prioritize
performance over fidelity, and want to deliberately send low resolution images.</p>

<p><strong>A vast majority of devs know about the problem, but are just waiting for a
solution that works well</strong>. We're all inherently lazy and in my opinion, a
reasonable solution is one that requires little to no extra work.</p>

<h2>Good solutions require almost no extra work</h2>

<p>How can we serve the optimal image to the device with as little work as
possible?  One approach is to always serve a highly compressed but high density
image, as I outlined in <a href="http://www.html5rocks.com/en/mobile/easy-high-dpi-images/">Easy High DPI Images</a> on HTML5Rocks. This
approach is better than nothing, but isn't really optimal since you end up
sending high density images to low density screens.</p>

<p>Two promising standards are on the horizon to wider adoption: the <code>srcset</code>
attribute for <code>img</code> elements, and the <code>CH</code> client hint header.</p>

<h3>Solution 1: Client-side build step with srcset &amp; friends</h3>

<p>The <code>srcset</code> attribute <a href="https://www.webkit.org/blog/2910/improved-support-for-high-resolution-displays-with-the-srcset-image-attribute/">recently landed in WebKit</a>, and it looks like
others will follow. Though it's more terse than <code>&lt;picture&gt;</code> and friends, <code>srcset</code>
still requires quite a bit of extra work to implement:</p>

<pre><code>&lt;img src="img.jpg" srcset="img-1.5x.jpg 1.5x, img-2x.jpg 2x, img-3x.jpg 3x"&gt;
</code></pre>

<p><a href="http://www.w3.org/TR/css4-images/#image-set-notation"><code>image-set</code></a> is the CSS equivalent, and looks quite similar.
Unfortunately it requires even more work:</p>

<pre><code>selector {
  background-image: url(img.jpg);
  background-image: -webkit-image-set(
      url(img-1.5x.jpg) 1.5x, url(img-2x.jpg) 2x, url(img-3x.jpg) 3x);
  background-image: -moz-image-set(
      url(img-1.5x.jpg) 1.5x, url(img-2x.jpg) 2x, url(img-3x.jpg) 3x);
  background-image: -ms-image-set(
      url(img-1.5x.jpg) 1.5x, url(img-2x.jpg) 2x, url(img-3x.jpg) 3x);
  background-image: -o-image-set(
      url(img-1.5x.jpg) 1.5x, url(img-2x.jpg) 2x, url(img-3x.jpg) 3x);
  /* Hehe, moar prefixes! */
}
</code></pre>

<p>Phew! After you have exploded your markup, you need to generate multiple images
of different sizes and decide on appropriate compression levels for each.</p>

<p>You'll notice that this extra work is very formulaic. It almost looks like it
could be automated! Let's skip the busywork and write our web pages like we do
today, specifying a very high quality asset (eg. 3x), and running a build
script. In your markup, all you need to do is:</p>

<pre><code>&lt;img src="img.jpg" /&gt;
</code></pre>

<p>or</p>

<pre><code>selector {
  background-image: url(img.jpg);
}
</code></pre>

<p>This magic time-saving script would need to do two things. First, it
generates images:</p>

<ol>
<li>Find all image files on the site.</li>
<li>Downsize all image files to the right size depending on desired density breakpoints (eg. <code>1x, 1.5x, 2x, 3x</code>).</li>
<li>Name the images according to some convention (eg. <code>${image}-${density}.${format}</code>).</li>
</ol>

<p>Image resizing already has a <a href="http://addyosmani.com/blog/generate-multi-resolution-images-for-srcset-with-grunt/">grunt-based solution</a>, and many
others will surely follow. The second part is rewriting the HTML and
CSS. Here's how it works:</p>

<ol>
<li>Parse all image references from HTML (eg. <code>img</code>) and CSS (eg. <code>background</code>,
<code>background-image</code>).</li>
<li>Augment all HTML <code>img</code> elements with the right srcset. Augment all CSS
<code>background</code> and <code>background-image</code> properties with the right (and prefixed)
image-set value.</li>
</ol>

<p>Now we're talking! And all you need to do is provide one set of high quality
image assets and add this script to your build step (you have a build step,
right?). Keep writing those <code>&lt;img src&gt;</code>s!</p>

<h3>Solution 2: Server-side build step with Client-Hints</h3>

<p>The <a href="http://tools.ietf.org/html/draft-grigorik-http-client-hints-00">Client-Hints proposal</a> (CH) is another promising (read: minimal
developer effort required) future direction that would help solve the
responsive image problem on the server. Ilya Grigorik goes into much
more detail in <a href="http://www.igvita.com/2013/08/29/automating-dpr-switching-with-client-hints/">his post</a>.</p>

<p>Currently, the main thing a server has to identify a client is its
User-Agent (UA) header. The UA header is insufficient to infer basic
things like display density, even in conjunction with a <a href="http://en.wikipedia.org/wiki/WURFL">UA
database</a>. CH is a new header used to pass information to the
server about the user agent.  With it, you can specify the
<code>devicePixelRatio</code> (DPR) of your device explicitly:</p>

<pre><code>CH: dpr=2
</code></pre>

<p>Once browsers send this CH header, you can imagine some really simple
server-side logic to serve the best asset for the DPR specified. You will need
either a smart image generator (and cache) on the server, or a build script for
generating images at different densities. Luckily this build script is the same
as the first half of solution 2, so less work for us! Once the images are
generated, it's just a matter of producing the right redirects based on the CH
header, which Ilya provides his <a href="http://www.igvita.com/2013/08/29/automating-dpr-switching-with-client-hints/">article</a>.</p>

<p>One benefit of solving this problem server side is that it's universal
and completely transparent to the client. A drawback to the first
(client-side) solution is that it will not work when setting <code>&lt;img src&gt;</code>
with JavaScript, although this can be remedied easily with a loader that
you use to specify the image asset. In practice, instead of specifying
the image asset directly, you would need to go through a small image URL
rewriter. Imagine something like this:</p>

<pre><code>var imagePath = images.get('img.jpg');
// imagePath is now img-2x.jpg if on a 2x display.
imageEl.src = imagePath;
</code></pre>

<p>Another benefit of the server-side approach is that there's no need for
parsing HTML and CSS (the second part of the build step) which can be
tricky and error prone.</p>

<h2>Both solutions are good</h2>

<p>In summary, both solutions have merit, and since <code>srcset</code> has <a href="https://www.webkit.org/blog/2910/improved-support-for-high-resolution-displays-with-the-srcset-image-attribute/">momentum
already</a>, it should be standardized and broadly supported as soon as
possible. Many designers may not have access to server side configuration, so
for them the client-side build script would make sense. Conversely, many
developers that have access to server-side image generators and advanced
caching techniques should take advantage of Client-Hints once it's
available, which <a href="https://groups.google.com/a/chromium.org/forum/#!topic/blink-dev/c38s7y6dH-Q">may be soon</a>!</p>

<p>Now, to write that build script... Any volunteers?</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Ultrasonic networking on the web</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/ultrasonic-networking"/>
    
    <updated>2013-08-08T09:00:00-00:00</updated>
    
    <id>https://smus.com/ultrasonic-networking</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>The phone in your pocket is an amazing, fluid, multi-functional tool.
When it comes to talking to other devices, such as your TV or laptop,
the user experience drops off sharply. Bill Buxton <a href="http://www.youtube.com/watch?v=ZQJIwjlaPCQ&amp;feature=youtu.be&amp;t=21m00">speaks
eloquently</a> on the subject, describing three stages of
high tech evolution:</p>

<ol>
<li>Device works: feature completeness and stability</li>
<li>Device flows: good user experience</li>
<li>Many devices work together</li>
</ol>

<p>But connecting devices is a pain and we have been squarely at stage 2
since the release of the iPhone. There are many competing approaches to
do this: Bluetooth, Bluetooth LE, WiFi direct, discovery over the same
local WiFi network, and many many others. This post is dedicated to
attacking this problem from an unexpected angle: using ultrasound to
broadcast and receive data between nearby devices. Best of all, the
approach uses the Web Audio API, making it viable for pure web
applications:</p>

<iframe width="640" height="360" src="//www.youtube.com/embed/w6lRq5spQmc" frameborder="0" allowfullscreen="true"></iframe>

<!--more-->

<h2>A device tower of babel</h2>

<p><a href="http://www.apple.com/airplay/">Airplay</a> and <a href="http://www.google.com/chromecast">Chromecast</a> are great approaches to a subset of the
problem for devices within the same ecosystem (eg. Apple, or Google),
but the general problem remains hard to solve.</p>

<p>Because there are so many possible technical approaches, chances are
that the pair of devices that you happen to be using don't have a common
language to speak. Even if both devices have Bluetooth, one of them may
require a profile the other doesn't support, or support a different
version of the standard. This is especially common with Bluetooth today,
where many devices have the hardware to support Bluetooth 4.0 (aka
BTLE), but many devices don't currently support the new protocol for
various reasons.</p>

<p>On the web, the problem is even worse, since low level device connection
APIs aren't exposed for <a href="http://smus.com/installable-webapps/">security sandbox reasons</a>. Because of
how slowly the web evolves, it's hard to imagine this changing any time
soon.</p>

<h2>Transmitting data in interesting ways</h2>

<p><a href="http://www.youtube.com/watch?v=sVWlQNzU4Ak">Blinkup from Electric Imp</a> is an interesting approach to
cross-device communication. It uses a series of blinks to transfer
configuration data between a smart phone and an Imp, a small SD-card
shaped device with a light sensor.</p>

<p>Dial-up modems did a similar thing. They encoded and decoded digital
data onto an analog phone line. Remember those annoying connection
noises? Dial-up modems would turn on their speaker to give the user an
idea of how the handshake is progressing. If you don't remember, here's
a <a href="http://www.windytan.com/2012/11/the-sound-of-dialup-pictured.html">refresher</a>. Even today on analog phones, the sounds you hear
when pressing numbers on a dialer correspond to the frequencies the
phone system uses for analog-to-digital conversion. The conversion
happens using <a href="http://en.wikipedia.org/wiki/Dual-tone_multi-frequency_signaling">Dual-tone multi-frequency signaling (DTMF)</a>.</p>

<p>Your phone and a lot of other devices around you has a speaker and a
microphone. These two pieces of hardware can be used for sending and
receiving data using sounds, similar to how modems did it over phone
lines. Better yet, if the OS supports high enough frequency sending and
receiving, we can create an inaudible data channel.</p>

<h2>Transmitting data using sound</h2>

<p>I should note that encoding data in sound is not new. The idea of <a href="http://en.wikipedia.org/wiki/Audio_watermark">audio
watermarking</a> is to encode a signature into music that is not
discernable by the listener (due to the way humans hear), but can be
picked up by a machine. This is used as a clever piracy detection
scheme. </p>

<p>Most commodity speakers are capable of producing sound with a 44.1KHz
sample rate (resulting in a maximum frequency of about 22KHz by the
<a href="http://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem">Nyquist-Shannon sampling theorem</a>). This lets us encode data
not just as sound, but as sound that adults can't hear. Children and
non-human animals are still susceptible, though :)</p>

<p>One technical caveat is that microphones are sometimes not as capable as
speakers, especially in phones, since they are often optimized for human
speech, which sounds fine with a lower sample rate. In other cases,
even though the hardware is capable, the firmware runs at a lower sample
rate for energy efficiency. If this is the case, one of the devices will
not be able to receive the wave and the sound-based connection will be
one-way only.</p>

<h2>Sonicnet.js, a web audio implementation</h2>

<p>To illustrate these concepts, I built a <a href="https://github.com/borismus/sonicnet.js">JavaScript library</a>
that can send and receive data as sounds. My approach is and not nearly
as sophisticated as the audio watermarking technique, and even simpler
than the DTMF approach. Basically, you can specify a range of
frequencies to use, and an alphabet of characters that can be
transmitted. The frequency spectrum is split into ranges corresponding
to the specified alphabet and start/end codes, with each character/code
corresponding to a part of the full frequency range.</p>

<p>The sending side converts each character of the word to be sent into the
center of the corresponding frequency range, and transmits that
frequency for a certain duration. The receiving side does a continuous
fourier transform of the signal and looks for peaks in the specified
frequency range. Upon finding a peak for a significant duration, it does
the conversion back from frequency to character. This is essentially
a <a href="http://en.wikipedia.org/wiki/Selective_calling#Tone_burst_or_single_tone">single-tone multi-frequency signaling (STMF)</a> scheme.</p>

<p>There is a timing issue: on the sending side, how long should each
character be transmitted for, and on the receiving side, how long should
the listened for? An easy workaround for this is to disallow adjacently
repeated characters.</p>

<p>I built a socket-like API for sonic networking. Client code
looks like this:</p>

<pre><code>ssocket = new SonicSocket({alphabet: '0123456789'});
function onButton() {
  ssocket.send('31415');
}
</code></pre>

<p>And the server can look like this:</p>

<pre><code>sserver = new SonicServer({alphabet: '0123456789'});
sserver.on('message', function(message) {
  // Expect message to be '31415'.
  console.log(message);
});
sserver.start();
</code></pre>

<p>The library is available for use on <a href="https://github.com/borismus/sonicnet.js/tree/master/lib">github</a>.</p>

<p>Of course, using it requires a Web Audio implementation (mostly
<code>OscillatorNode</code> on the sending side, and <code>AnalyserNode</code> on the
receiving side) and good enough hardware. I have experimented with
Chrome-to-Chrome transmission on Mac Books, as well as between Chrome
for Android (beta) and Chrome for Mac.</p>

<p>I wrote a couple of demos to illustrate the idea. These appear in the
<a href="http://www.youtube.com/watch?v=w6lRq5spQmc">video I embedded at the top of the post</a>. The first demo lets
you <a href="http://borismus.github.io/sonicnet.js/emoticons">send emoticons</a> from one device to the other. It uses a
small alphabet of just 6 characters - one for each emoticon. You pick
one of 6 emoticons, and the corresponding character is sent over the
sonic network, received and shown prominently on the other end.</p>

<p>A more realistic use for sonicnet.js is this <a href="http://borismus.github.io/sonicnet.js/chat-pair">chat
application</a>, which generates a non-repeating 5-digit token
and uses it to create connections between two devices. This is done with
the help of a pairing server, which helps establish a proxied connection
between the two devices, over a websocket. Once the connection is
established, the chats themselves are sent through the websocket. The
<a href="https://github.com/borismus/sonicnet.js/tree/master/server">server code</a> is hosted on <a href="https://www.nodejitsu.com/">nodejitsu</a>.</p>

<h2>Conclusions and a request</h2>

<p>It's great to see that the Web Audio API has come far enough that
applications like these are possible. I'm fascinated by the implications
of sonicnet.js for the Web of Things. It is a pure web technology that
can be used to pair devices together. Because of the ubiquity of web
browsers and audio hardware, the combination can be a huge win, even
among commodity hardware, without having to wait for Bluetooth and other
close-range connectivity technology to become available to the web
platform.</p>

<p>If this post has piqued your interest and you are interested in helping,
try writing an app using sonicnet.js. As I mentioned earlier, receiving
high frequency sounds does not work on all devices because of
firmware/hardware limitations so I'd love to know which devices it does
and does not work on. My expectation is that most phones should be able
to send only, and that most laptops should be able to both send and
receive. Please fill out <a href="https://docs.google.com/forms/d/1dAgNdVdhss-QR-Owm556RZch-MV_ntnAMP8_ZJi5XLA/viewform">this form</a> once you try the <a href="http://borismus.github.io/sonicnet.js/emoticons">emoticons
demo</a> on your own hardware. At the time of writing, <a href="http://crbug.com/242894">live
input is not supported</a> in Chrome for Android Beta, so sending
data from mobile device to laptop is the only possible configuration.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Easier link blogging</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/easier-link-blogging"/>
    
    <updated>2013-07-29T09:00:00-00:00</updated>
    
    <id>https://smus.com/easier-link-blogging</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>As usual, I want two conflicting things. Firstly, I want to own the
content I write, and control how it is authored. My weapon of choice is
MacVim and <a href="https://github.com/borismus/lightning">Lightning</a>, a static blog engine I wrote to
address my <a href="http://smus.com/site/">very specific requirements</a>: </p>

<p><img src="/easier-link-blogging/composing-long-post.png" alt="Composing a blog post with Lightning" /></p>

<p>Secondly, I want people to read the things I write and follow the
stories that I link to, since it feels good, and sometimes generates
interesting discussions. I wrote a Mac GUI that automates link blogging
and <a href="http://indiewebcamp.com/POSSE">POSSE style</a> cross-posting to social networks.</p>

<!--more-->

<p>Before I carry on, let me take you back to a simpler time. A time before
Justin Bieber, Black Eyed Peas and social networks. Here's how blogging
used to work: writers wrote to their blogs, and readers subscribed to
them. They would read blog posts from RSS feed readers. Some people ran
link blogs that were kind of like twitter -- short updates discussing a
link. It's was a very clean, federated model. Unfortunately that's not
how things work today. I have already lamented this fact in a <a href="http://smus.com/really-simple-social-syndication/">previous
blog post</a>.</p>

<p>For this site, RSS feed readership is a tiny fraction of the inbound
traffic for new posts. Most of non-search traffic comes from social
networks. As it turns out, the value of these networks is (wait for it)
in their network effect! It only takes a few prominent reshares to have
a post become relatively widely read. By not seeding your content to
social networks, you lose that benefit.</p>

<h2>Cross-posting to social networks is hard</h2>

<p>To seed content to social networks, we look to POSSE. From the
<a href="http://indiewebcamp.com/POSSE">indiewebcamp.com</a> definition:</p>

<blockquote>
  <p>POSSE is an acronym/abbreviation for Publish (on your) Own Site,
  Syndicate Elsewhere. It's a Syndication Model where the flow involves
  posting your content on your own domain first, then syndicating out
  copies to 3rd party services with perma(short)links back to the
  original version.</p>
</blockquote>

<p>The problem is that posting to social networks is a pain in the rear.
Each network has its own limitations (eg. 140 characters on Twitter,
network-specific markup on G+). There are services which cross-post to
these networks, but they tend to sweep the subtle differences between
the networks under the carpet. For example, services like
<a href="http://manageflitter.com/">ManageFlitter</a> and <a href="http://friendsplus.me/">Friends+Me</a> can cross-post from G+ to
Twitter, but if the post is too long to fit in 140 chars, they include a
link back to the original G+ post. I find cross-linking between social
networks to be questionable, so I have stopped using such tools.</p>

<h2>Making link-style updates easier</h2>

<p>A while ago, I realized that my use of social networks is remarkably
close to a link blog. While I'll sometimes @reply/comment and +1/star
things, I hardly ever post broadcast-style updates without a URL.</p>

<p>About a year ago, I added a special "link" type of content on this blog,
specially for this purpose. This type of content is just like a post,
except one link is prominently shown as the title of the post, and the
post itself is focused on commentary about the link. My plan is to use
this type of content more, whenever I want to broadcast a URL and have
something to say about it.</p>

<p>So to scratch my itch, I made a little GUI that automates creating a
link entry on the local static blog (with commentary). After the link is
posted and deployed publicly, it also broadcasts the content to
supported social networks, sometimes linking back to the link page on
this site.</p>

<p><img src="/easier-link-blogging/lightning-link.png" alt="Lightning linker Mac GUI" /></p>

<p>As you can see, there are three fields: the URL, a title (pre-populated
from the <code>&lt;body&gt;&lt;title&gt;</code> of the URL), and the body of the post (in
Markdown), all of which are clearly visible in the link page.
Technically, posting to social networks is easy enough. If they provide
a write API, it's just a matter of doing the OAuth dance and hanging on
to an access token to authorize requests. A more interesting question is
how to re-arrange the above three fields to form social network updates.</p>

<h2>Posting to other networks</h2>

<p>Once the link is posted on the site, Lightning Link has a set of
heuristics to decide what to post to each supported social network. This
can be quite challenging if you are faced with a strict character limit.</p>

<p>Here are some options I considered for Twitter:</p>

<ol>
<li>Truncated plaintext body, followed by the URL</li>
<li>Title colon space, the truncated plaintext body, and then the URL</li>
<li>Title and URL</li>
</ol>

<p>With Option 1, the truncated plaintext body doesn't necessarily reflect
the main idea of the commentary on the link, since the body can be much
longer than 140 chars, and I might just be warming up :) Option 2 leaves
very little room for the body at all, except for about half of a
sentence. I went with Option 3, which lends itself well to short,
Twitter-style updates.</p>

<p>Posting to G+ is relatively easy: take the title and slap on the
plaintextified (from markdown) body, while attaching the URL. </p>

<p>The other question is about URLs. There are two URLs of relevance in
each link blog post:</p>

<ol>
<li>The one on the link blog (eg. <a href="http://smus.com/link/2013/not-terrible-javascript-modules/">http://smus.com/link/2013/not-terrible-javascript-modules/</a>), and</li>
<li>The linked material (eg. <a href="http://github.com/substack/node-browserify">http://github.com/substack/node-browserify</a>).</li>
</ol>

<p>My approach is to use the linked material directly (2) if the comments
can fit entirely into the space alotted by the social network, falling
back to the link blog URL (1).</p>

<p>The Lightning Link app isn't general enough for me to recommend unless
you either like pain, use lightning already, or have a static blog very
setup similar to mine, with a "link" type of post. If you'd still like
to try it, the <a href="https://github.com/borismus/lightning/tree/master/link">code is on github</a>. If you have a similar
link-blogging approach with automatic syndication to social networks,
tell me about it!</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Installable webapps: extend the sandbox</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/installable-webapps"/>
    
    <updated>2013-06-25T09:00:00-00:00</updated>
    
    <id>https://smus.com/installable-webapps</id>
    <content type="html">
      <![CDATA[
      <div>
        <p><style>
article img.border {
  margin: 0 auto;
  max-width: 100%;
  box-shadow: inset 0 0 10px #999
}
</style>
<a name="problem"></a></p>

<p>Have you seen the <a href="http://extensiblewebmanifesto.org/">extensible web manifesto</a>? It's the
formalization of a recent trend in web standards: a tendency towards
lower level APIs. Lower levels of abstraction enable developers to build
more on top of a solid foundation. By going down a level of abstraction
in the web platform, web developers can contribute to the platform
itself in a more fundamental way, working along with browser vendors and
spec writers. This is <a href="/how-the-web-should-work/">how the web should work</a>.</p>

<p>But there is a big missing piece in the extensible web vision. Our
beloved platform is stuck in a constrictive security sandbox. The "drive
by" web's security philosophy is that users of the web should be able to
feel safe on any webpage they visit. While very important for the well
being of web denizens, it prevents developers from using increasingly
important features enjoyed by native platforms such as access to
contacts, TCP/UDP sockets, interfaces to external USB/Bluetooth devices.
Breaching this sandbox is a huge barrier for the web as a compelling
application platform.</p>

<p>Some recent features, such as <code>getUserMedia</code>, which gives web developers
access to the audio and video streams of your device's camera and
microphone, have started to break out of the sandbox. There are two
approaches to this problem today: (a) infobars and (b) packaged apps. In
the rest of this post I'll describe why these are bad solutions,
deconstruct them down into small pieces and then glue the pieces back
together. The ultimate goal is a modest proposal for installable web
apps. Read on for my take on the background of the problem, or skip
ahead to read my <a href="/installable-webapps/#solution">illustrated proposal</a> for fixing it.</p>

<!--more-->

<h2>The extensible web is a good idea</h2>

<p>There are many recent examples of the extensible web philosophy across
many areas of the web. Because of the low level nature of WebGL and Web
Audio, these technologies open up a wide variety of applications to be
built. Under the hood, these APIs are relatively thin wrappers around
underlying native technology, not compromising performance (much) while
adding developer usability. General purpose low level computing
technologies like <a href="http://asmjs.org/">asm.js</a> and <a href="https://developers.google.com/native-client/">NaCl</a> enable computationally
intensive algorithms to run far more efficiently. </p>

<p>Finally, frameworks like <a href="http://www.polymer-project.org/">Polymer</a> use other kinds of low level
APIs like <a href="http://www.youtube.com/watch?v=fqULJBBEVQE">web components</a> and <a href="https://github.com/Polymer/mdv">model-driven views</a> to let
developers invent new types of HTML elements with custom functionality. </p>

<h2>Sandbox vs. low level APIs</h2>

<p>Restrictive web security makes a lot of sense. You should never have to
worry about malicious or careless developers erasing files from your
local filesystem, even if you frequent the most notorious <code>.ru</code> domains!
That said, if the web is to be a viable application platform that stands
a chance against native platform, it needs to have access to certain
data that is sensitive.</p>

<p>Many of the APIs that align with the extensible web philosophy have
already tested the bounds of the web's sandbox. Some have resulted in
security vulnerabilities, such as 2011's <a href="http://blog.chromium.org/2011/07/using-cross-domain-images-in-webgl-and.html">cross-domain WebGL texture
attack</a>. Others have required extending the web platform
with additional levels of security. The earliest of these is probably
the geolocation API. More recent additions include <code>getUserMedia</code>, which
gives developers a stream of the microphone and camera. These APIs could
obviously lead to very serious privacy breaches if turned on by default
on all web pages. I don't want the Russians knowing where I live, or
eavesdropping on my conversations (NSA already knows).</p>

<p>The "drive-by" web solves this problem through infobars. I will explain
later why this is a terrible idea. The other solution is packaged web
apps. Packaging circumvent the web completely by copying the
distribution model of native apps, bundling your whole application into
a locally downloaded zip. This model also features an installation step
which sometimes also grants additional permissions up front. Both of
these so-called solutions reduce the likelihood of new low level APIs
from coming to the web platform.</p>

<h2>Infobars are a bad user experience</h2>

<p>First, look at this:</p>

<p><img src="/installable-webapps/infobar-apocalypse.png"/></p>

<p>This gem, courtesy of <a href="http://persistent.info">Mihai Parparita</a>, is my favorite
explanation for why infobars suck. You can immediately see several
problems stemming from the obvious fact that the infobar model does not
scale well. In the infobar world, each feature requires its own
permission, leading to far too many stacked dialogs that are just ugly.
From a usability perspective, your users have to click through each one
of the Allow/OK dialogs before they can do anything with your
application. If you then reload the page, many of these infobars will
again return to haunt you, forcing you to click OK five more times
before being able to use the webapp.</p>

<p>In some cases, your browser might remember that you accepted an infobar,
and choose not to show it to you again. For example, this happens if you
grant <code>getUserMedia</code> access on an HTTPS site, selecting the "save this
preference" option. This is remembered on a per-domain basis, and in
Chrome, is available via <code>Preferences -&gt; Content Settings</code>. In general,
conditions for when exactly the browser remembers how you responded to
an infobar are unclear and underdefined.</p>

<p>There are also some less obvious issues with the infobar model. Because
infobars are non-modal, users often don't realize that they have to
accept them before they can use the webapp's functionality. For example,
if you have a <a href="http://webcamtoy.com">photo booth application</a>, it will be
completely useless until you accept the "access to video stream"
infobar, yet many of your users may not notice the infobar at the top of
your browser window. If you attempt to draw your users' attention to the
infobar via some illustration, you may end up pointing to the Deny
button by accident because of variations in placement across various
browsers and browser versions.</p>

<p>To summarize, infobars are broken in the following ways:</p>

<ol>
<li>Does not scale with number of permissions.</li>
<li>Visually jarring at scale. Sometimes not visually obvious enough.</li>
<li>Permission granting should often be modal.</li>
<li>Inconsistent persistence, poor management.</li>
</ol>

<p>Part of the problem might be addressable via a <a href="https://code.google.com/p/chromium/issues/detail?id=250797">visual
refresh</a> of infobars (as Paul Neave suggests), but I
suspect that a broader rethink of the problem is in order. Until this is
resolved, many new low level APIs will increases Mihai's stack of
apocalyptic infobars, reducing their chance of coming to the platform in
the first place.</p>

<h2>Packaged apps...</h2>

<p>Packaged apps are an odd marriage between native app distribution and
web technologies. The packaged web app model consists of a few moving
parts:</p>

<ul>
<li>A directory for discovering and installing apps (eg. Firefox
Marketplace, Chrome Web Store).</li>
<li>A set of platform-specific APIs built on top of the web platform for
use in these apps.</li>
<li>A manifest describing each app, which can specify permissions to
enable either the above platform specific APIs or restricted open web APIs.</li>
</ul>

<p>There are certainly benefits to this approach, such as a sane offline
story, since all of the assets of the application can be packaged
together into a bundle that is downloaded at install time, circumventing
painful technology like <a href="/installable-webapps/appcache.png">AppCache</a>. There is a clear install
step, during which you can grant an application permissions beyond the
scope of the open web platform. Also, it's very easy to add features to
packaged apps, since there are no annoying standards to worry about,
amirite?</p>

<h2>...are bad for the web</h2>

<p>Unfortunately, packaged web apps provide the worst of both worlds,
combining relatively poor web developer ergonomics with the longer
development and distribution cycle of native apps. Also, many of the
drawbacks of packaging are at odds with the philosophy of the open web
platform.</p>

<p>The first and most obvious problem is the lack of URLs for packaged
apps. URLs are critically important as unique identifiers for content
found on the web. They are great for sharing content, indexing, and
bookmarking. Secondly, there is no standard packaged app format across
platforms, which means that the packaging formats and APIs available are
completely different between Chrome, Firefox, and other packaged app
providers. This cross-platform aspect is the main economic reason to
develop for the web. Another drawback is that each of these packaged app
vendors has its own app store, sometimes complete with approval
processes similar to the much reviled App Store approval flow. </p>

<p>Lastly, once a browser vendor has a packaged app model, it's very
tempting for them to just implement new low level features there and not
on the open web. This effectively lifts the pressure for browser vendors
to go through the pain of standardization. The standard response can now
be "just go build a packaged app". A summary of these issues with packaged
apps:</p>

<ol>
<li>No URLs</li>
<li>Not cross platform</li>
<li>Dependent on centralized directories</li>
<li>Vendors have an excuse to punt on adding new features to the web
platform.</li>
</ol>

<p>Packaged apps are at odds with the web. To the unintiated, it feels as
if their inventors slapped web technology on top of the Apple app store
model. I know that there are some legitimate, security-motivated reasons
for their decisions, but believe that these are surmountable.</p>

<h2>Installable web apps</h2>

<p>If you have an iOS device at your disposal, take a look at
<a href="http://forecast.io/">forecast.io</a>. Forecast.io is an example of an <a href="http://blog.forecast.io/its-not-a-web-app-its-an-app-you-install-from-the-web/">app you install
from the web</a>. This approach is interesting because it combines
the best of both worlds. On one hand, you retain the benefits of the
web: URLs, cross-linking, lack of centralized control. On the other, you
get the benefit of elevated permissions.</p>

<p>A benefit of this approach is that there is a clear install step during
which you can request additional permissions, which is a natural place
for breaking out of the web's sandbox in a user-friendly manner. The
result of installation is a homescreen shortcut, which is both a launch
convenience, and a way of managing permissions. Removing that shortcut
can also mean revoking special permissions for that application.</p>

<p>Another benefit is that there is no centralized appstore - you can
discover apps in the same way that you discover the web today - through
search engines, links in your email inbox, feed readers and through any
other URL-based sharing scheme. There is no reason to conflate
installation with the presence of a centralized directory. Google search
is already revealing apps in search results. If you search for "Angry
Birds", you will find both the iOS and Android versions on the first
page.</p>

<p><a name="solution"></a></p>

<h1>Proposal: apps you can install from the web</h1>

<p>So far I've described the <a href="/installable-webapps//installable-webapps/#problem">problem</a>: a major barrier to the
vision of the extensible web: there is no good way of getting outside of
the sandbox. I have been complaining a lot without providing any
constructive answers.</p>

<p>In order to keep things constructive, the second half of the post
proposes a solution to get us out of the sandbox. There's a whole world
out there! Here is my birds-eye-view of the install-from-the-web world:</p>

<p><img src="/installable-webapps/flow.png" alt="flow" /></p>

<p>This diagram is intended to be general enough to work across
operating systems and device types, but the mocks themselves will be
sketched out with a phone form factor in mind. We'll be installing
<code>app.io</code>, a mobile app that lets you leave audio notes.</p>

<p><img src="/installable-webapps/screen1.png" class="border" /></p>

<p><em>Screen 1: App.io example.</em></p>

<h2>An API for installing webapps</h2>

<p>This can be done with an iOS-style approach (and corresponding Chrome
for Android <a href="https://code.google.com/p/chromium/issues/detail?id=153066">feature request</a>), which presents a generic UI
for adding apps to the homescreen (see Screen 2).</p>

<p><img src="/installable-webapps/screen2.png" class="border" /></p>

<p><em>Screen 2: Add via browser button.</em></p>

<p>There are trade-offs between having a button or an opt-in developer
<strong>API for installing web apps</strong>. With a button, any URL can be added to
the home screen, which may not make sense. But with an API, the
developer has to provide an explicit call to action for you to install
their app. The button UX will always be consistent, since it's part of
the browser. An API-based install path may be ugly or spammy. However,
an API can also provide a consistent experience across browsers without
the need for guessing where each browser places the button. Many
forecast.io-style apps on iOS have callouts on the page pointing to the
button in the browser chrome which would be broken if another browser
had a different method of adding to homescreen.</p>

<p>My opinion is that button- and API-based approaches both have a place.
For webapps that make more sense installed, the API can be a nice touch.
Other pages might be useful as webapps without their developer realizing
it, so the button-based approach is useful there.</p>

<p>How would the installation API look like? A JavaScript-based API only
callable on user action, similar to how audio playback in mobile
browsers prevents the annoying situation where visiting a page
automatically prompts you to install it. Installing a webapp should
come with a default set of permissions above and beyond what the web
platform provides.</p>

<pre><code>var button = document.querySelector('button#install');
button.addEventListener('click', window.app.requestInstall);
</code></pre>

<p>You should also be able to request additional permissions at
install-time. For example, to request installation and audio capture,
the following code should work:</p>

<pre><code>window.app.requestInstall({permissions: ['audioCapture']});
</code></pre>

<p>This action should also result in a standard browser-specific dialog to
accept installation, showing which permissions have been requested
(Screen 3).</p>

<p><img src="/installable-webapps/screen3.png" class="border" /></p>

<p><em>Screen 3: Confirm installation.</em></p>

<p>Once accepted, a launcher shortcut should be created (Screen 4). Two
pieces of metadata are necessary for this launcher:</p>

<ol>
<li><p>Icon, which should first look for a large enough version of the
<a href="http://en.wikipedia.org/wiki/Favicon#HTML5_recommendation_for_icons_in_multiple_sizes">multiresolution favicon</a> as determined by the UA. If none
exists, it should look for the <a href="http://goo.gl/6Qdi3">apple-touch-icon</a> in the
<code>&lt;head&gt;</code>. If none is specified, a screenshot of the page can be used
as in iOS.</p></li>
<li><p>Title, which can be extracted from the <code>&lt;title&gt;</code> element in the head.
If none is specified, the user can be prompted to input their own
title.</p></li>
</ol>

<p><img src="/installable-webapps/screen4.png" class="border" /></p>

<p><em>Screen 4: New launcher added to the home screen.</em></p>

<h2>Launching in standalone mode</h2>

<p>iOS already has an <strong>API to know if a webapp was launched in standalone
mode</strong> (ie. from the launcher) or if it was opened from a browser. This
functionality is available via <code>window.navigator.standalone</code>. It also
opens the app in full-screen mode.</p>

<p>Other vendors should standardize and implement similar functionality.
For example, something like <code>window.app.standalone</code>, if only for naming
consistency could be implemented, and a polyfill provided for the Apple
spec. It would also make sense to launch homescreen apps in full screen,
providing the same UX as the full-screen API (Screen 5):</p>

<p><img src="/installable-webapps/screen5.png" class="border" /></p>

<p><em>Screen 5: App launched in standalone mode.</em></p>

<h2>Requesting additional permissions</h2>

<p>Apps might need additional permissions that go beyond the default
baseline of permissions granted to the app at install time. Access to
your camera would fall into this bucket. An <strong>API call to request extra
permissions</strong> might look like the following:</p>

<pre><code>window.app.requestExtraPermissions(['videoCapture']);
</code></pre>

<p>Running this command would also require user-initiation and prompt a
modal optional permissions dialog (Screen 6) similar to the one seen at
installation. After granting it, the associated API call (in this case,
<code>getUserMedia</code>) can be invoked without incurring any infobars.</p>

<p><img src="/installable-webapps/screen6.png" class="border" /></p>

<p><em>Screen 6: Additional permissions request.</em></p>

<h2>Removing installed web apps and extra permissions</h2>

<p>If the installed webapp has a native launcher, removing the launcher can
do this implicitly. There should also be a browser- or system- UI
similar to existing app management interfaces that lets you remove
installed apps, or revoke granted permissions.</p>

<h2>That's it folks</h2>

<p>So there you have it: my strawman fixing the security model of the web,
which, as I outlined at the <a href="/installable-webapps//installable-webapps/#problem">beginning of this post</a>, is
critically important to address for the continued success of the web.
To recap, the solution consists of an API surface in the <code>window.app</code>
namespace, and a number of new screens that are part of the installation
process.</p>

<p>If tackled, this could solve one of the most important issues on the web
today. Otherwise, we may find ourselves in a place where the web
platform is irrelevant to application developers, who will just build
for packaged platforms.</p>

<p>I'm not silly enough to think that this proposal is the ultimately
correct and best solution for elevated priveledges on the open web.
There is a huge amount of work required to refine the flow, think of all
of the edge cases, implement it across browsers, etc. The above is just
a draft to re-ignite the web permissions discussion that died several
years ago. Please blog something in response or in the worst case, tweet
or email your opinion. Looking forward to hearing from you.</p>

<h1>Update: important links</h1>

<p><strong>July 18, 2013</strong>: Several people have pointed out that I've missed some
important links.  My public apologies!</p>

<p><a href="https://developers.google.com/chrome/apps/docs/developers_guide">Chrome hosted apps</a> are a somewhat similar concept,
but suffered from security issues that still need to be resolved to make
this proposal a reality. There was even an effort to make hosted web
apps installable from the web, called <a href="http://blog.persistent.info/2011/07/theres-web-app-for-that-site.html">CRX-less web
apps</a> (preserved on Mihai's blog), which today is little
more than a <a href="http://code.google.com/intl/en-US/chrome/apps/docs/no_crx.html">broken link</a>.</p>

<p>To my knowledge, the most active project along the lines of this
proposal is the <a href="https://developer.mozilla.org/en-US/docs/Web/Apps">Open Web Apps</a> work from Firefox OS. </p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Gestural music direction</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/gestural-music-direction"/>
    
    <updated>2013-05-03T09:00:00-00:00</updated>
    
    <id>https://smus.com/gestural-music-direction</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>Imagine this: you start conducting as if you were in front of a great
orchestra, and music fades in out of thin air, matching your tempo and time
signature. Your nuanced gestures can indicate changes in intensity, and of
course affect the speed of the piece. You'd first need to learn some
basic conducting patterns, like these:</p>

<p><img src="/gestural-music-direction/conduct-time-signatures.png" alt="Conducting patterns for various time signatures" /></p>

<!--more-->

<p>You would also have to wait for me to finish this project, which uses a LEAP
motion device (or your trackpad), the Web Audio API, and some signal processing
to achieve a scaled back version of the idea described above.</p>

<h2>Prototype</h2>

<p>The current prototype lets you control the tempo of a song called "Phantom"
from the excellent <a href="http://www.parovstelar.com/">Parov Stelar</a>. You can do this by making simple
conducting patterns, similar to the 2/4 pattern above. In practice, you can use
any pattern in which your hand oscillates between two points in space to play
with this prototype. You can even use your mouse instead of a LEAP motion
device. Just click in to enable pointer lock. This will ensure that your mouse
will always be focused inside your browser.</p>

<p>I built a visualizer which is an 8-bit inspired frequency graph which
also shows directional changes as pulsating red dots, and clusters which
flash to the beat.</p>

<p><a href="http://borismus.github.io/gestural-music-direction/"><img src="/gestural-music-direction/screenshot.png" alt="Screenshot of leap conductor" /></a></p>

<p>If you'd like to try it live, the <a href="http://borismus.github.io/gestural-music-direction/">demo lives here</a>.</p>

<h2>Handling input</h2>

<p>With a LEAP device plugged in, the prototype maps the palm's 3D center
to 2D. It works just as well with just a trackpad or mouse attached to
your computer, which directly outputs 2D coordinates. The input handling
algorithm then uses the resulting (x, y) pairs to do roughly the
following:</p>

<ul>
<li><p>First, track positions and first (velocity) and second (acceleration)
order history, including times. Store in a ring buffer, which is
implemented in <a href="https://github.com/borismus/gestural-music-direction/blob/master/js/ring-buffer.js">ring-buffer.js</a>.</p></li>
<li><p>Extract sudden changes of direction based on heuristics related to
velocity and acceleration history.</p></li>
<li><p>Cluster directional changes using K-means or similar clustering
algorithm which is implemented in <a href="https://github.com/borismus/gestural-music-direction/blob/master/js/clusterizer.js">clusterizer.js</a>. I run this
K-means implementation with 3 values of k in [2, 3, 4] and pick the
one with the lowest error. I've also build a standalone
<a href="http://borismus.github.io/gestural-music-direction/cluster.html">clustering test page</a> with the following output:</p></li>
</ul>

<p><img src="/gestural-music-direction/cluster.png" alt="Clustering algorithm visualization" /></p>

<ul>
<li><p>To calculate tempo, pick a cluster and calculate mode of the deltas
between adjacent points.</p></li>
<li><p>The time signature is just the number of clusters over 4 (for the
simple 2/4, 3/4 and 4/4 patterns).</p></li>
</ul>

<h2>Changing tempo in real-time</h2>

<p>Once we have an idea of what pattern the user is creating with their hands, we
need to match up the song to the pattern, and continuously adapt the song's
playback rate to the user's motions.</p>

<p>The Web Audio API makes it dead simple to change the playback rate of an audio
buffer for a source node'ss entire duration. However, things get a bit
trickier if this rate changes continuously over time. Chris Wilson
describes a scheduling technique which addresses this exact problem in
his <a href="http://www.html5rocks.com/en/tutorials/audio/scheduling/">"Tale of Two Clocks" HTML5Rocks article</a>. You can also see
a simple version of it inaction in his <a href="http://www.html5rocks.com/en/tutorials/audio/scheduling/goodmetronome.html">metronome demo</a>.</p>

<p>I used this idea to do a bit of granular synthesis on an audio buffer. I
schedule a bit of the buffer into the future, at the current tempo. As the
tempo changes, new bits of the buffer are scheduled at a different
playbackRate. I keep track of how far into the buffer we've gone and use that
as the grainOffset. Here's some code that illustrates this (but see the
<a href="https://github.com/borismus/gestural-music-direction/blob/master/js/music-player.js">variable rate music player</a> for the full code):</p>

<pre><code>MusicPlayer.prototype.loop_ = function() {
  // Schedule the next bar if it's not yet scheduled.
  while (this.nextNoteTime &lt; audioContext.currentTime + this.scheduleAheadTime) {
    this.scheduleSegment_(this.grainOffset, this.nextNoteTime);
    this.nextNote_();
  }
}

MusicPlayer.prototype.scheduleSegment_ = function(grainOffset, time) {
  // Get the part of the buffer that we're going to play.
  var source = audioContext.createBufferSource();
  source.buffer = this.buffer;
  source.connect(audioContext.destination);

  var rate = this.getPlaybackRate_();
  source.playbackRate.value = rate;

  var secondsPerBeat = 60.0 / this.tempo;
  source.noteGrainOn(time, grainOffset, secondsPerBeat * rate);
}

MusicPlayer.prototype.nextNote_ = function() {
  // Advance current note and time by a 16th note...
  var secondsPerBeat = 60.0 / this.tempo;
  // Notice this picks up the CURRENT tempo value to calculate beat length.
  this.nextNoteTime += secondsPerBeat;
  // Get the next grain.
  var rate = this.getPlaybackRate_();
  this.grainOffset += secondsPerBeat * rate;
}
</code></pre>

<p>In practice, I'm unfortunately hitting some rounding errors, so the
grains aren't stitched together as seamlessly as I wanted. You can
sometimes hear artifacts if you slow the tempo way down.</p>

<h2>A work in progress</h2>

<p>My initial idea was to use <a href="http://developer.echonest.com/">The Echo Nest</a> to pick the right song
(based on time signature and tempo), and then stream that song from some
streaming music service. Unfortunately it's quite hard to get at PCM versions
of tracks from Rdio and Spotify. That said, it can be <a href="https://github.com/oampo/AmbientCloud">done with
Soundcloud</a>. Long story short, the prototype currently only
supports one song.</p>

<p>A time signature recognizer is mainly useful for classical music, since so much
of popular music is in common time (with <a href="http://twentytwowords.com/2011/05/18/6-pop-songs-in-unusual-time-signatures/">rare exceptions of popular music with
complex time signatures</a>). But applying simple transformations
like changing the tempo just feels wrong for complex music without a very
obvious rhythmic structure.</p>

<p>Lastly, LEAP's palm tracking is still quite noisy (even after drastic
improvements to palm tracking as of <a href="https://developer.leapmotion.com/blog/sdk-0-7-7-released-new-palm-tracking-and-gesture-settings">SDK 0.7.7</a>). Also, the
bay windows in my living room lets in tons of infrared light which often puts
the device into a low fidelity tracking mode.</p>

<p>As always, let me know what you think, and of course, feel free to fork
and evolve on <a href="https://github.com/borismus/gestural-music-direction">github</a>.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Web Audio book</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/webaudio-book"/>
    
    <updated>2013-03-18T09:00:00-00:00</updated>
    
    <id>https://smus.com/webaudio-book</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>I wrote a short book about the Web Audio API. The book is meant as an
introduction to the web audio API, as well as some audio basics for web
developers with little audio experience. It is <a href="http://chimera.labs.oreilly.com/books/1234000001552/">available for free on
Chimera</a>, a web-based book viewer, which presents a nicely laid
out page and lets you leave per-paragraph comments. The online version
also includes inline samples from <a href="http://webaudioapi.com">webaudioapi.com</a>. If you don't
like reading on the web, you can also <a href="http://shop.oreilly.com/product/0636920025948.do">buy a physical copy or an
ebook</a> from O'Reilly.</p>

<!--more-->

<p>So I got this cool bat on the cover. I am told that technically
speaking, it is a brown long-eared bat (<a href="http://en.wikipedia.org/wiki/Brown_long-eared_bat">Plecotus auritus</a>). Though
it's no orca (an ideal O'Reilly cover, don't you think?), I'm very happy
that it's a <a href="http://en.wikipedia.org/wiki/Animal_echolocation">sound related animal</a>.</p>

<p><img src="/webaudio-book/cover.jpg" alt="Web Audio Book Cover" /></p>

<p>The book was written on my laptop in a Google Doc. I hand-drew some
illustrations in a notebook and brought them into the doc. Once I was
ready for feedback, I sent the doc around to my technical reviewers, who
left feedback in comments. After incorporating their feedback, I got
some editorial feedback, still in the doc. Once the draft was more or
less ready to go, I created an <a href="https://www.odesk.com/">oDesk</a> task to convert the Google
Doc into docbook.xml format. The contractor did a great job and charged
me about $100. This was my first time paying anyone to do work for me.</p>

<p>Thanks to all of the reviewers, editors, illustrators and organizers for
helping. Also, thanks to <a href="http://www.kevincennis.com/">Kevin Ennis</a> who kindly donated the
<a href="http://webaudioapi.com">webaudioapi.com</a> domain which I'm currently using to host the
samples.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Really simple social syndication</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/really-simple-social-syndication"/>
    
    <updated>2013-03-14T09:00:00-00:00</updated>
    
    <id>https://smus.com/really-simple-social-syndication</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>I've been thinking about this for a while, but the recent <a href="http://googlereader.blogspot.ca/2013/03/powering-down-google-reader.html">sunset
announcement of Google Reader</a> made me revisit this topic.
Google Reader isn't the only thing that's dead. RSS (aka. Really Simple
Syndication) has long been proclaimed dead as well. In fact most people
never even knew what RSS was. That said, it was a very useful tool for
me and many others that like to stay up-to-date in their areas of
interest. Increasingly, I've been getting my dose of news through social
networks. However, social networks contain a lot of noise that I care
little for. I want to rebuild the RSS spirit using modern social
networks. This post describes one possible approach, which I refer to as
<strong>Really Simple Social Syndication (RSSS)</strong>.</p>

<!--more-->

<h2>Really simple syndication: the good old days</h2>

<p>Here's what the content flow used to be with blogs and RSS:</p>

<p><img src="/really-simple-social-syndication/rss-flow.png" alt="RSS-based content syndication" /></p>

<p>It was simple. Really simple, actually!</p>

<h2>Social syndication: today</h2>

<p>I don't care much for social networks. I mostly see them as a two-way
utility for ultimately connecting content creators to content consumers.</p>

<p>One way, social networks like Facebook, Twitter, G+, etc are just
vehicles for finding out what to read based on your interests. I spend
too much time checking them individually for my news, and this is
unfortunate. </p>

<p>The other way, social networks make it easier to have your content read
by a bunch of the right people. In my case, the vast majority of
non-organic search traffic comes from social sources (mostly twitter). I
waste a bit of time tweeting, and G+ing new posts on my blog as they
come out, but sometimes they are picked up by others and I don't
actually need to do that.</p>

<p>As long as the content itself stays outside of the walled gardens of the
social networks, I think we can come to a syndication solution that
might rival the old RSS-based one, but enjoy the benefits of having some
extra signals from all of the social network junk. Here's the model I'm
thinking of:</p>

<p><img src="/really-simple-social-syndication/social-flow.png" alt="Social-based content syndication" /></p>

<p>Now for a little bit more about Pub and Sub.</p>

<h2>Sub: Requirements for consumption</h2>

<ul>
<li><p><strong>Reuse existing feeds</strong>. I don't want to re-create my sources. Use
existing people you follow on Twitter, from G+ circles, Facebook
friends.</p></li>
<li><p><strong>De-duplicated content</strong>. Some people post the same link on multiple
networks. Some popular posts are reshared by everyone. I only want to
see a post once.</p></li>
<li><p><strong>Content centric</strong>. Show me the content in some standard, readable
way. Hide the social stuff unless I explicitly ask. Most of the time I
don't care where it came from, don't care how many people liked it, or
what they wrote in the comments. Sometimes I'm curious and want a way
to trace it back.</p></li>
<li><p><strong>Social signals as a metric</strong>. If many of my sources share something,
I probably should at least take a skim.</p></li>
<li><p><strong>Web based service</strong> so that I can use it anywhere.</p></li>
<li><p>Set a <strong>volume-based daily quota</strong>. If I'm busy today I'd like to only
see the top N articles, sorted by some transparent metric of my
choosing.</p></li>
</ul>

<p>Flipboard, Pulse, Feedly all sort of fit into this class of readers.
Ideally this would be an API that just lets me connect a few social
accounts, and get back a filtered feed of content. This could be the API
for the product - a Really Simple Social Syndication (RSSS) feed.</p>

<p>Anyone could then build a UI on top of it for their favorite platform
(Google Reader-like).</p>

<h2>Pub: Requirements for content production</h2>

<ul>
<li><p>Automatically post content to a bunch of social services. </p></li>
<li><p>Intelligent shortening of links and content (eg. for twitter, to fit
in 140 chars).</p></li>
</ul>

<p>Wordpress plugins, Tumblr and others provide ways to automatically tweet
and otherwise post new updates to your content. There needs to be some
other way that works in general for any type of content. Such a service
could be similar to feedburner, in that it would take an existing RSS
feed and socialize it.</p>

<h2>Social networks, let's be friends!</h2>

<p>I'm not religious about Google Reader or RSS. It was a good solution for
content syndication at the time, but I'm ready to accept that perhaps
it's time to move on. Hopefully with tools like the above, we can have
something that comes close to the utility of RSS feeds.</p>

<p>Social networks could do some evil stuff which would preclude RSSS from
happening. Economically, they are incentivized own content, create
walled gardens, insert advertisements, and prevent access to their
feeds. I'm hoping that some human element will prevent that from
happening.</p>

<p>Here is a short list of what we need from the social networks:</p>

<ul>
<li><p>The content itself must be free from walled gardens (eg. paywalls,
login walls, etc)</p></li>
<li><p>Social network feeds are available to read in full, as is, without
magical suggestions, collaborative filtering, etc.</p></li>
<li><p>Social networks provide some programmatic way to post content.</p></li>
</ul>

<p>Once we have a good flow for the production and consumption of content,
using social networks as a delivery mechanism, I will be very happy to
minimize the amount of time spent on social networks directly, and focus
on consuming and producing interesting things. Also, happy &pi; day!</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Interactive touch laptop experiments</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/touch-laptop-experiments"/>
    
    <updated>2013-02-21T09:00:00-00:00</updated>
    
    <id>https://smus.com/touch-laptop-experiments</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>Largely because of the plummeting price and thickness of touch screens,
these devices are increasingly ubiquitous. One of the latest trends is
touch screen laptops, spearheaded by <a href="http://www.microsoft.com/Surface/en-US">Surface</a> devices and the
recently announced <a href="http://chrome.blogspot.com/2013/02/the-chromebook-pixel-for-whats-next.html">Chromebook Pixel</a>. In this post I'll dive into
some experiements around this new form factor. The main goal is to try
to convince myself that this form factor makes sense for reasons other
than economic ones.</p>

<p>In exploring the interaction design angle of these new devices, I came
across a couple of what I think are a couple of interesting ideas that
I'd like to share with you: <strong>responsive input</strong> and <strong>simultaneous
interactions</strong> using both mouse/trackpad and touchscreen. I wrote some
demos that illustrate these ideas. <em>A touchscreen laptop is required for
these demos to work properly</em>.</p>

<ul>
<li><a href="http://borismus.github.com/touch-laptop-experiments/responsive">Auto scaling in response to input type</a>.</li>
<li><a href="http://borismus.github.com/touch-laptop-experiments/map">Mouse-to-map and touch-to-mark</a>.</li>
<li><a href="http://borismus.github.com/touch-laptop-experiments/transform">Multimodal transform demo</a>.</li>
</ul>

<!--more-->

<p>Because you probably don't have a touch screen laptop, I recorded a
rough video showing some of these interactions:</p>

<iframe width="640" height="360" src="//www.youtube.com/embed/rcE2z9tudGw" frameborder="0" allowfullscreen></iframe>

<p>Hopefully this gives you a better sense of what I mean by responsive
input and simultaneous touch and mouse interactions.</p>

<h2>Responsive input</h2>

<p>The touch laptop class of device has a two main interaction modes:</p>

<ol>
<li>As a regular laptop with trackpad (or external mouse) and keyboard.</li>
<li>As a touch tablet with a keyboard.</li>
</ol>

<p>These two interaction modes differ fundamentally in many ways. The
following are some examples of these differences:</p>

<ul>
<li>Touch has no hover state.</li>
<li>Touch is less precise than mouse and requires bigger targets.</li>
<li>Touch requires that you are closer to the screen.</li>
</ul>

<p><img src="/touch-laptop-experiments/touch-laptop.png" alt="Chrome Pixel" /></p>

<p>Ideally, you want to provide optimal experiences for both cases. For the
mouse case, this means taking advantage of hover states and a finer
pointer. For the touch case, this means ensuring that touch targets are
big enough to be tapped, not relying on hover at all.</p>

<p>So I explored a user interface concept that adapts touch laptop
interfaces to the user's current input mode. The tricky bit is detecting
the user's current input mode. Several adaptation options are possible:</p>

<ol>
<li>Immediately transform to the mouse-style UI as soon as the input mode
changes (simplest, but can cause transitions to fire too rapidly
between the two modes, which may be jarring).</li>
<li>Transform only after some period of not using the other input mode
(eg. go to touch mode only if the user is actively using touch, and
not touching the mouse at all).</li>
<li>Transform based on some external criteria, like whether or not the
screen is docked to a mouse, or based on input from sensors other
than mouse/touchscreen.</li>
</ol>

<p>The first approach is problematic in that your first touch transforms
the page. If this transformation causes your target to move away from
it's initial position, you will miss it entirely. This can be mitigated
by having intelligent resizing which does not affect anything directly
under the touch point, but may result in a lopsidedly zoomed interface.</p>

<p>The second approach is problematic since the mode switching will happen
automatically after some period of inactivity, which may be jarring. The
last approach is either obvious (eg. mouse removed), or an area of
research (eg. predicting when the user will touch based on camera).</p>

<p>I wrote a <a href="http://borismus.github.com/touch-laptop-experiments/responsive">demo of auto scaling in response to input type</a>.
If you use the mouse, click targets will decrease in size. If you use
your finger, touch targets increase in size. (Of course, this will only
work on a touchscreen laptop).</p>

<h2>Simultaneous touchscreen + mouse/trackpad interactions</h2>

<p>In the above section, I described an automatic way to switch between
touch and mouse mode However, there is a middle ground between the two:
multimodal interactions that involve both touchscreen and
mouse/trackpad. Simultaneous bimodal interaction is already common. For
example, using mouse and keyboard simultaneously makes a very efficient
interface for FPS games, with the movement via the WASD keys, and
mouse-look.</p>

<p>One experiment involves using the mouse or trackpad as a navigation
device and using the touch screen as a way to input positional data.
This is illustrated through Google maps. You pan and zoom the map using
mouse events, and place markers on the map using the touch screen. Try
out this demonstration of <a href="http://borismus.github.com/touch-laptop-experiments/map">mouse-to-map and touch-to-mark</a> (again,
this requires a touchscreen laptop).</p>

<p>Another experiment involves manipulating geometric objects on the
screen. The idea here was to use the touch screen to select objects, and
use the trackpad/mouse as way of manipulating the selected object. In
this demo, you can manipulate the object in a number of ways:</p>

<ol>
<li>Move it by simply dragging it around on the screen with touch.</li>
<li>Rotate by selecting the object on the touchscreen, and then
performing a mousemove (either by moving a mouse or dragging one
finger on a trackpad). The rotation happens around the point where
you touched the object, which acts as a fulcrum. </li>
<li>Scale it in the same fashion as rotation (selecting object and
transformation origin with the touchscreen), except with a two-finger
drag on the trackpad, or using the mousewheel if a mouse is attached.</li>
</ol>

<p>With no selection, the canvas itself can be zoomed and panned with the
mouse/trackpad directly. Try out this <a href="http://borismus.github.com/touch-laptop-experiments/transform">multimodal transform
demo</a> (requires touchscreen laptop).</p>

<h2>Missing pieces</h2>

<p>Like any brave new world, the one of multimodal input has its own set of
challenges.</p>

<p>It's currently impossible to distinguish a touch laptop from any other
touch screen. Notably, this means that you should never assume that
touch support implies no mouse support. In practice, make sure that you
always bind to mouse events. If you also have touch event handlers, just
use <code>event.preventDefault()</code> there to ensure that you aren't handling
one event in multiple handlers. If you're interested in this, follow the
discussion at <a href="http://crbug.com/174553">http://crbug.com/174553</a>.</p>

<p>As a generalization of the above, there is currently no way to determine
which kinds of input are available in the browser. A fully fledged Input
Availability API might seem like overkill, but there are already some
cases beyond touch laptops that are relevant. For example, detecting the
presence of a physical keyboard would be very useful. Further, detecting
hardware features like an attached camera and microphone could fall into
the same bucket rather than relying on exception handling from APIs like
<code>getUserMedia</code>. Lastly, having such an API would allow websites to react
dynamically to changes in input (eg. a tablet gets docked to a physical
keyboard, or a mouse is attached).</p>

<p>The final missing piece is that dealing with two different event models
(mouse and touch) is definitely clunky. I have already <a href="http://smus.com/mouse-touch-pointer/">written
extensively about pointer events</a> and a <a href="https://github.com/borismus/pointer.js">pointer event
polyfill</a>. In this particular case, pointer events would be
great, because although they provide a consolidated model for input,
it's very easy and natural to distinguish between the two modalities.</p>

<p>These experiments are all available <a href="https://github.com/borismus/touch-laptop-experiments">on github</a>. </p>

<h2>Your turn!</h2>

<p>Do you have thoughts or demos around new types of interactions using
touch laptops? Please share them below.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>From VPS to static hosting</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/moved-to-s3"/>
    
    <updated>2013-01-16T09:00:00-00:00</updated>
    
    <id>https://smus.com/moved-to-s3</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>All good things must come to an end. VPS hosting paid for by my former
university is no exception! Ever since the University of
Madeira-provided credit card paying for the account expired, I began
wondering whether it's worth paying for a VPS that I hardly use.
Combined with two consecutive 10-minute stretches of downtime last week,
I had my answer.</p>

<p>I run this blog, my mother's site and a handful of mini-sites, all of
which are inherentily static content. Today, I moved them all away from
my VPS completely. I migrated the relatively complex sites to the
<a href="https://github.com/borismus/lightning">lightning</a> engine, and updated the engine with a couple of nice
features: fixed content links in list pages and feeds, and support for
publishing to S3.</p>

<!--more-->

<h2>System administration</h2>

<p>In my early Linux days, I ran an AMD Athlon server off my parents'
internet connection. I took pride in configuration, maintenance,
administration, endlessly recompiling updates and dealing with broken
dependencies. I enjoyed the challenge and got very good at it. By
sinking enough time into any problem, I was confident that I would
ultimately solve it. Sometimes I contributed an ebuild or two to
portage. I learned a lot, and eventually my web server outgrew my
parents' internet connection.</p>

<p>So I turned to managed hosting. Several years later, sick of the crappy
management UI, and yearning to flex some sysadmin muscle, I jumped on
VPS opportunity for performance reasons. While clearly overkill for
static sites, it was appealing from a "what if?" perspective: what if
suddenly I wanted to run a complex webapp? No problem, VPS was ready!</p>

<h2>Except system administration sucks</h2>

<p>My VPS slice was running Ubuntu 8. Since Ubuntu 12 was released, I was
greeted with a "48 packages are out of date" message upon logging into
the machine. </p>

<p>Long ago, this message would have sent me down a rabbit hole of emerging
all of the outdated packages, resolving dependencies and rewriting
config files. It was gratifying to be on the bleeding edge, to have a
clean system with all of the daemons dancing to your tune in perfect
harmony.</p>

<p>These days, I could care less about being up-to-date. In fact, I
actively dislike upgrading. An upgrade is a risk, likely to lead to
something breaking, likely without me noticing at first. So rather than
the "ooh, new shiny" feeling I used to have when Apache needed an
update, I actively dread needing to update anything. I don't want to
need to tweak configurations, especially because I've forgotten a lot of
the domain-specific config languages. </p>

<h2>S3 for static hosting, PaaS for everything else</h2>

<p>Happily, all of my sites are currently static. Blogs and mini-sites all
lend themselves very well to being hosted on S3.</p>

<p>For the hypothetical case that I require a dynamic web server on
the internet, I'll turn to a Platform-as-a-service solution like
<a href="http://nodejitsu.com/">Nodejitsu</a> or <a href="https://developers.google.com/appengine/">AppEngine</a> to avoid doing rote configuration
tasks.</p>

<p>Being a sysadmin is not a part time job.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
</feed>